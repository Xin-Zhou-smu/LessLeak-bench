[
        {
                "data_len": 21
        },
        [
                {
                        "id": "test_evocodebench_data_195",
                        "content": "#!/usr/bin/env python3\n# Copyright 2004-present Facebook. All Rights Reserved.\nfrom ..structures import Instances\nfrom detectron2.utils.registry import Registry\nfrom ..config.config import CfgNode as CfgNode_\nfrom detectron2.config import configurable\nTRACKER_HEADS_REGISTRY = Registry(\"TRACKER_HEADS\")\nTRACKER_HEADS_REGISTRY.__doc__ = \"\"\"\nRegistry for tracking classes.\n\"\"\"\nclass BaseTracker(object):\n\"\"\"\nA parent class for all trackers\n\"\"\"\n@configurable\ndef __init__(self, **kwargs):\nself._prev_instances = None  # (D2)instances for previous frame\nself._matched_idx = set()  # indices in prev_instances found matching\nself._matched_ID = set()  # idendities in prev_instances found matching\nself._untracked_prev_idx = set()  # indices in prev_instances not found matching\nself._id_count = 0  # used to assign new id\n@classmethod\ndef from_config(cls, cfg: CfgNode_):\nraise NotImplementedError(\"Calling BaseTracker::from_config\")\ndef update(self, predictions: Instances) -> Instances:\n\"\"\"\nArgs:\npredictions: D2 Instances for predictions of the current frame\nReturn:\nD2 Instances for predictions of the current frame with ID assigned\n_prev_instances and instances will have the following fields:\n.pred_boxes               (shape=[N, 4])\n.scores                   (shape=[N,])\n.pred_classes             (shape=[N,])\n.pred_keypoints           (shape=[N, M, 3], Optional)\n.pred_masks               (shape=List[2D_MASK], Optional)   2D_MASK: shape=[H, W]\n.ID                       (shape=[N,])\nN: # of detected bboxes\nH and W: height and width of 2D mask\n\"\"\"\nraise NotImplementedError(\"Calling BaseTracker::update\")\ndef build_tracker_head(cfg: CfgNode_) -> BaseTracker:\n\"\"\"\nBuild a tracker head from `cfg.TRACKER_HEADS.TRACKER_NAME`.\nArgs:\ncfg: D2 CfgNode, config file with tracker information\nReturn:\ntracker object\n\"\"\"\nname = cfg.TRACKER_HEADS.TRACKER_NAME\ntracker_class = TRACKER_HEADS_REGISTRY.get(name)\nreturn tracker_class(cfg)",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 194
                },
                {
                        "id": "pretrain_python_data_1619689",
                        "content": "<reponame>zengxianyu/detectron2\n#!/usr/bin/env python3\n# Copyright 2004-present Facebook. All Rights Reserved.\nfrom detectron2.config import CfgNode as CfgNode_\nfrom detectron2.config import configurable\nfrom detectron2.structures import Instances\nfrom detectron2.utils.registry import Registry\nTRACKER_HEADS_REGISTRY = Registry(\"TRACKER_HEADS\")\nTRACKER_HEADS_REGISTRY.__doc__ = \"\"\"\nRegistry for tracking classes.\n\"\"\"\nclass BaseTracker(object):\n\"\"\"\nA parent class for all trackers\n\"\"\"\n@configurable\ndef __init__(self, **kwargs):\nself._prev_instances = None  # (D2)instances for previous frame\nself._matched_idx = set()  # indices in prev_instances found matching\nself._matched_ID = set()  # idendities in prev_instances found matching\nself._untracked_prev_idx = set()  # indices in prev_instances not found matching\nself._id_count = 0  # used to assign new id\n@classmethod\ndef from_config(cls, cfg: CfgNode_):\nraise NotImplementedError(\"Calling BaseTracker::from_config\")\ndef update(self, predictions: Instances) -> Instances:\n\"\"\"\nArgs:\npredictions: D2 Instances for predictions of the current frame\nReturn:\nD2 Instances for predictions of the current frame with ID assigned\n_prev_instances and instances will have the following fields:\n.pred_boxes               (shape=[N, 4])\n.scores                   (shape=[N,])\n.pred_classes             (shape=[N,])\n.pred_keypoints           (shape=[N, M, 3], Optional)\n.pred_masks               (shape=List[2D_MASK], Optional)   2D_MASK: shape=[H, W]\n.ID                       (shape=[N,])\nN: # of detected bboxes\nH and W: height and width of 2D mask\n\"\"\"\nraise NotImplementedError(\"Calling BaseTracker::update\")\ndef build_tracker_head(cfg: CfgNode_) -> BaseTracker:\n\"\"\"\nBuild a semantic segmentation head from `cfg.MODEL.SEM_SEG_HEAD.NAME`.\n\"\"\"\nname = cfg.TRACKING.TRACKER_NAME\nreturn TRACKER_HEADS_REGISTRY.get(name)(cfg)",
                        "max_stars_repo_path": "detectron2/tracking/base_tracker.py",
                        "max_stars_repo_name": "zengxianyu/detectron2",
                        "max_stars_count": 0,
                        "__cluster__": 194
                },
                {
                        "real_dup": "2"
                }
        ],


        [
                {
                        "id": "test_evocodebench_data_202",
                        "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\nfrom mmcv.cnn import MODELS as MMCV_MODELS\nfrom mmcv.utils import Registry\nfrom mmdet.models.builder import BACKBONES as MMDET_BACKBONES\nfrom mmdet.models.builder import DETECTORS as MMDET_DETECTORS\nfrom mmdet.models.builder import HEADS as MMDET_HEADS\nfrom mmdet.models.builder import LOSSES as MMDET_LOSSES\nfrom mmdet.models.builder import NECKS as MMDET_NECKS\nfrom mmdet.models.builder import ROI_EXTRACTORS as MMDET_ROI_EXTRACTORS\nfrom mmdet.models.builder import SHARED_HEADS as MMDET_SHARED_HEADS\nfrom mmseg.models.builder import LOSSES as MMSEG_LOSSES\nMODELS = Registry('models', parent=MMCV_MODELS)\nBACKBONES = MODELS\nNECKS = MODELS\nROI_EXTRACTORS = MODELS\nSHARED_HEADS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDETECTORS = MODELS\nVOXEL_ENCODERS = MODELS\nMIDDLE_ENCODERS = MODELS\nFUSION_LAYERS = MODELS\nSEGMENTORS = MODELS\ndef build_backbone(cfg):\n\"\"\"Build backbone.\"\"\"\nif cfg['type'] in BACKBONES._module_dict.keys():\nreturn BACKBONES.build(cfg)\nelse:\nreturn MMDET_BACKBONES.build(cfg)\ndef build_neck(cfg):\n\"\"\"Build neck.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn NECKS.build(cfg)\nelse:\nreturn MMDET_NECKS.build(cfg)\ndef build_roi_extractor(cfg):\n\"\"\"Build RoI feature extractor.\"\"\"\nif cfg['type'] in ROI_EXTRACTORS._module_dict.keys():\nreturn ROI_EXTRACTORS.build(cfg)\nelse:\nreturn MMDET_ROI_EXTRACTORS.build(cfg)\ndef build_shared_head(cfg):\n\"\"\"Build shared head of detector.\"\"\"\nif cfg['type'] in SHARED_HEADS._module_dict.keys():\nreturn SHARED_HEADS.build(cfg)\nelse:\nreturn MMDET_SHARED_HEADS.build(cfg)\ndef build_head(cfg):\n\"\"\"Build head.\"\"\"\nif cfg['type'] in HEADS._module_dict.keys():\nreturn HEADS.build(cfg)\nelse:\nreturn MMDET_HEADS.build(cfg)\ndef build_loss(cfg):\n\"\"\"Build loss function.\"\"\"\nif cfg['type'] in LOSSES._module_dict.keys():\nreturn LOSSES.build(cfg)\nelif cfg['type'] in MMDET_LOSSES._module_dict.keys():\nreturn MMDET_LOSSES.build(cfg)\nelse:\nreturn MMSEG_LOSSES.build(cfg)\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build detector.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nif cfg['type'] in DETECTORS._module_dict.keys():\nreturn DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\nelse:\nreturn MMDET_DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_segmentor(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build segmentor.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nreturn SEGMENTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_model(cfg, train_cfg=None, test_cfg=None):\n\"\"\"A function warpper for building 3D detector or segmentor according to\ncfg.\nShould be deprecated in the future.\n\"\"\"\nif cfg.type in ['EncoderDecoder3D']:\nreturn build_segmentor(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\nelse:\nreturn build_detector(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\ndef build_voxel_encoder(cfg):\n\"\"\"Build voxel encoder.\"\"\"\nreturn VOXEL_ENCODERS.build(cfg)\ndef build_middle_encoder(cfg):\n\"\"\"Build middle level encoder.\"\"\"\nreturn MIDDLE_ENCODERS.build(cfg)\ndef build_fusion_layer(cfg):\n\"\"\"Build fusion layer.\"\"\"\nreturn FUSION_LAYERS.build(cfg)",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 201
                },
                {
                        "id": "pretrain_python_data_1903542",
                        "content": "<reponame>ammaryasirnaich/mmdetection3d<filename>mmdet3d/models/builder.py\n# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\nfrom mmcv.cnn import MODELS as MMCV_MODELS\nfrom mmcv.utils import Registry\nfrom mmdet.models.builder import BACKBONES as MMDET_BACKBONES\nfrom mmdet.models.builder import DETECTORS as MMDET_DETECTORS\nfrom mmdet.models.builder import HEADS as MMDET_HEADS\nfrom mmdet.models.builder import LOSSES as MMDET_LOSSES\nfrom mmdet.models.builder import NECKS as MMDET_NECKS\nfrom mmdet.models.builder import ROI_EXTRACTORS as MMDET_ROI_EXTRACTORS\nfrom mmdet.models.builder import SHARED_HEADS as MMDET_SHARED_HEADS\nfrom mmseg.models.builder import LOSSES as MMSEG_LOSSES\nMODELS = Registry('models', parent=MMCV_MODELS)\nBACKBONES = MODELS\nNECKS = MODELS\nROI_EXTRACTORS = MODELS\nSHARED_HEADS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDETECTORS = MODELS\nVOXEL_ENCODERS = MODELS\nMIDDLE_ENCODERS = MODELS\nFUSION_LAYERS = MODELS\nSEGMENTORS = MODELS\ndef build_backbone(cfg):\n\"\"\"Build backbone.\"\"\"\nif cfg['type'] in BACKBONES._module_dict.keys():\nreturn BACKBONES.build(cfg)\nelse:\nreturn MMDET_BACKBONES.build(cfg)\ndef build_neck(cfg):\n\"\"\"Build neck.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn NECKS.build(cfg)\nelse:\nreturn MMDET_NECKS.build(cfg)\ndef build_roi_extractor(cfg):\n\"\"\"Build RoI feature extractor.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn ROI_EXTRACTORS.build(cfg)\nelse:\nreturn MMDET_ROI_EXTRACTORS.build(cfg)\ndef build_shared_head(cfg):\n\"\"\"Build shared head of detector.\"\"\"\nif cfg['type'] in SHARED_HEADS._module_dict.keys():\nreturn SHARED_HEADS.build(cfg)\nelse:\nreturn MMDET_SHARED_HEADS.build(cfg)\ndef build_head(cfg):\n\"\"\"Build head.\"\"\"\nif cfg['type'] in HEADS._module_dict.keys():\nreturn HEADS.build(cfg)\nelse:\nreturn MMDET_HEADS.build(cfg)\ndef build_loss(cfg):\n\"\"\"Build loss function.\"\"\"\nif cfg['type'] in LOSSES._module_dict.keys():\nreturn LOSSES.build(cfg)\nelif cfg['type'] in MMDET_LOSSES._module_dict.keys():\nreturn MMDET_LOSSES.build(cfg)\nelse:\nreturn MMSEG_LOSSES.build(cfg)\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build detector.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nif cfg['type'] in DETECTORS._module_dict.keys():\nreturn DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\nelse:\nreturn MMDET_DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_segmentor(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build segmentor.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nreturn SEGMENTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_model(cfg, train_cfg=None, test_cfg=None):\n\"\"\"A function warpper for building 3D detector or segmentor according to\ncfg.\nShould be deprecated in the future.\n\"\"\"\nif cfg.type in ['EncoderDecoder3D']:\nreturn build_segmentor(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\nelse:\nreturn build_detector(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\ndef build_voxel_encoder(cfg):\n\"\"\"Build voxel encoder.\"\"\"\nreturn VOXEL_ENCODERS.build(cfg)\ndef build_middle_encoder(cfg):\n\"\"\"Build middle level encoder.\"\"\"\nreturn MIDDLE_ENCODERS.build(cfg)\ndef build_fusion_layer(cfg):\n\"\"\"Build fusion layer.\"\"\"\nreturn FUSION_LAYERS.build(cfg)",
                        "max_stars_repo_path": "mmdet3d/models/builder.py",
                        "max_stars_repo_name": "ammaryasirnaich/mmdetection3d",
                        "max_stars_count": 0,
                        "__cluster__": 201
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "test_evocodebench_data_203",
                        "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\nfrom mmcv.cnn import MODELS as MMCV_MODELS\nfrom mmcv.utils import Registry\nfrom mmdet.models.builder import BACKBONES as MMDET_BACKBONES\nfrom mmdet.models.builder import DETECTORS as MMDET_DETECTORS\nfrom mmdet.models.builder import HEADS as MMDET_HEADS\nfrom mmdet.models.builder import LOSSES as MMDET_LOSSES\nfrom mmdet.models.builder import NECKS as MMDET_NECKS\nfrom mmdet.models.builder import ROI_EXTRACTORS as MMDET_ROI_EXTRACTORS\nfrom mmdet.models.builder import SHARED_HEADS as MMDET_SHARED_HEADS\nfrom mmseg.models.builder import LOSSES as MMSEG_LOSSES\nMODELS = Registry('models', parent=MMCV_MODELS)\nBACKBONES = MODELS\nNECKS = MODELS\nROI_EXTRACTORS = MODELS\nSHARED_HEADS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDETECTORS = MODELS\nVOXEL_ENCODERS = MODELS\nMIDDLE_ENCODERS = MODELS\nFUSION_LAYERS = MODELS\nSEGMENTORS = MODELS\ndef build_backbone(cfg):\n\"\"\"Build backbone.\"\"\"\nif cfg['type'] in BACKBONES._module_dict.keys():\nreturn BACKBONES.build(cfg)\nelse:\nreturn MMDET_BACKBONES.build(cfg)\ndef build_neck(cfg):\n\"\"\"Build neck.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn NECKS.build(cfg)\nelse:\nreturn MMDET_NECKS.build(cfg)\ndef build_roi_extractor(cfg):\n\"\"\"Build RoI feature extractor.\"\"\"\nif cfg['type'] in ROI_EXTRACTORS._module_dict.keys():\nreturn ROI_EXTRACTORS.build(cfg)\nelse:\nreturn MMDET_ROI_EXTRACTORS.build(cfg)\ndef build_shared_head(cfg):\n\"\"\"Build shared head of detector.\"\"\"\nif cfg['type'] in SHARED_HEADS._module_dict.keys():\nreturn SHARED_HEADS.build(cfg)\nelse:\nreturn MMDET_SHARED_HEADS.build(cfg)\ndef build_head(cfg):\n\"\"\"Build head.\"\"\"\nif cfg['type'] in HEADS._module_dict.keys():\nreturn HEADS.build(cfg)\nelse:\nreturn MMDET_HEADS.build(cfg)\ndef build_loss(cfg):\n\"\"\"Build loss function.\"\"\"\nif cfg['type'] in LOSSES._module_dict.keys():\nreturn LOSSES.build(cfg)\nelif cfg['type'] in MMDET_LOSSES._module_dict.keys():\nreturn MMDET_LOSSES.build(cfg)\nelse:\nreturn MMSEG_LOSSES.build(cfg)\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build detector.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nif cfg['type'] in DETECTORS._module_dict.keys():\nreturn DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\nelse:\nreturn MMDET_DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_segmentor(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build segmentor.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nreturn SEGMENTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_model(cfg, train_cfg=None, test_cfg=None):\n\"\"\"A function warpper for building 3D detector or segmentor according to\ncfg.\nShould be deprecated in the future.\n\"\"\"\nif cfg.type in ['EncoderDecoder3D']:\nreturn build_segmentor(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\nelse:\nreturn build_detector(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\ndef build_voxel_encoder(cfg):\n\"\"\"Build voxel encoder.\"\"\"\nreturn VOXEL_ENCODERS.build(cfg)\ndef build_middle_encoder(cfg):\n\"\"\"Build middle level encoder.\"\"\"\nreturn MIDDLE_ENCODERS.build(cfg)\ndef build_fusion_layer(cfg):\n\"\"\"Build fusion layer.\"\"\"\nreturn FUSION_LAYERS.build(cfg)",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 201
                },
                {
                        "id": "pretrain_python_data_1903542",
                        "content": "<reponame>ammaryasirnaich/mmdetection3d<filename>mmdet3d/models/builder.py\n# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\nfrom mmcv.cnn import MODELS as MMCV_MODELS\nfrom mmcv.utils import Registry\nfrom mmdet.models.builder import BACKBONES as MMDET_BACKBONES\nfrom mmdet.models.builder import DETECTORS as MMDET_DETECTORS\nfrom mmdet.models.builder import HEADS as MMDET_HEADS\nfrom mmdet.models.builder import LOSSES as MMDET_LOSSES\nfrom mmdet.models.builder import NECKS as MMDET_NECKS\nfrom mmdet.models.builder import ROI_EXTRACTORS as MMDET_ROI_EXTRACTORS\nfrom mmdet.models.builder import SHARED_HEADS as MMDET_SHARED_HEADS\nfrom mmseg.models.builder import LOSSES as MMSEG_LOSSES\nMODELS = Registry('models', parent=MMCV_MODELS)\nBACKBONES = MODELS\nNECKS = MODELS\nROI_EXTRACTORS = MODELS\nSHARED_HEADS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDETECTORS = MODELS\nVOXEL_ENCODERS = MODELS\nMIDDLE_ENCODERS = MODELS\nFUSION_LAYERS = MODELS\nSEGMENTORS = MODELS\ndef build_backbone(cfg):\n\"\"\"Build backbone.\"\"\"\nif cfg['type'] in BACKBONES._module_dict.keys():\nreturn BACKBONES.build(cfg)\nelse:\nreturn MMDET_BACKBONES.build(cfg)\ndef build_neck(cfg):\n\"\"\"Build neck.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn NECKS.build(cfg)\nelse:\nreturn MMDET_NECKS.build(cfg)\ndef build_roi_extractor(cfg):\n\"\"\"Build RoI feature extractor.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn ROI_EXTRACTORS.build(cfg)\nelse:\nreturn MMDET_ROI_EXTRACTORS.build(cfg)\ndef build_shared_head(cfg):\n\"\"\"Build shared head of detector.\"\"\"\nif cfg['type'] in SHARED_HEADS._module_dict.keys():\nreturn SHARED_HEADS.build(cfg)\nelse:\nreturn MMDET_SHARED_HEADS.build(cfg)\ndef build_head(cfg):\n\"\"\"Build head.\"\"\"\nif cfg['type'] in HEADS._module_dict.keys():\nreturn HEADS.build(cfg)\nelse:\nreturn MMDET_HEADS.build(cfg)\ndef build_loss(cfg):\n\"\"\"Build loss function.\"\"\"\nif cfg['type'] in LOSSES._module_dict.keys():\nreturn LOSSES.build(cfg)\nelif cfg['type'] in MMDET_LOSSES._module_dict.keys():\nreturn MMDET_LOSSES.build(cfg)\nelse:\nreturn MMSEG_LOSSES.build(cfg)\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build detector.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nif cfg['type'] in DETECTORS._module_dict.keys():\nreturn DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\nelse:\nreturn MMDET_DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_segmentor(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build segmentor.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nreturn SEGMENTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_model(cfg, train_cfg=None, test_cfg=None):\n\"\"\"A function warpper for building 3D detector or segmentor according to\ncfg.\nShould be deprecated in the future.\n\"\"\"\nif cfg.type in ['EncoderDecoder3D']:\nreturn build_segmentor(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\nelse:\nreturn build_detector(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\ndef build_voxel_encoder(cfg):\n\"\"\"Build voxel encoder.\"\"\"\nreturn VOXEL_ENCODERS.build(cfg)\ndef build_middle_encoder(cfg):\n\"\"\"Build middle level encoder.\"\"\"\nreturn MIDDLE_ENCODERS.build(cfg)\ndef build_fusion_layer(cfg):\n\"\"\"Build fusion layer.\"\"\"\nreturn FUSION_LAYERS.build(cfg)",
                        "max_stars_repo_path": "mmdet3d/models/builder.py",
                        "max_stars_repo_name": "ammaryasirnaich/mmdetection3d",
                        "max_stars_count": 0,
                        "__cluster__": 201
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "test_evocodebench_data_204",
                        "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\nfrom mmcv.cnn import MODELS as MMCV_MODELS\nfrom mmcv.utils import Registry\nfrom mmdet.models.builder import BACKBONES as MMDET_BACKBONES\nfrom mmdet.models.builder import DETECTORS as MMDET_DETECTORS\nfrom mmdet.models.builder import HEADS as MMDET_HEADS\nfrom mmdet.models.builder import LOSSES as MMDET_LOSSES\nfrom mmdet.models.builder import NECKS as MMDET_NECKS\nfrom mmdet.models.builder import ROI_EXTRACTORS as MMDET_ROI_EXTRACTORS\nfrom mmdet.models.builder import SHARED_HEADS as MMDET_SHARED_HEADS\nfrom mmseg.models.builder import LOSSES as MMSEG_LOSSES\nMODELS = Registry('models', parent=MMCV_MODELS)\nBACKBONES = MODELS\nNECKS = MODELS\nROI_EXTRACTORS = MODELS\nSHARED_HEADS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDETECTORS = MODELS\nVOXEL_ENCODERS = MODELS\nMIDDLE_ENCODERS = MODELS\nFUSION_LAYERS = MODELS\nSEGMENTORS = MODELS\ndef build_backbone(cfg):\n\"\"\"Build backbone.\"\"\"\nif cfg['type'] in BACKBONES._module_dict.keys():\nreturn BACKBONES.build(cfg)\nelse:\nreturn MMDET_BACKBONES.build(cfg)\ndef build_neck(cfg):\n\"\"\"Build neck.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn NECKS.build(cfg)\nelse:\nreturn MMDET_NECKS.build(cfg)\ndef build_roi_extractor(cfg):\n\"\"\"Build RoI feature extractor.\"\"\"\nif cfg['type'] in ROI_EXTRACTORS._module_dict.keys():\nreturn ROI_EXTRACTORS.build(cfg)\nelse:\nreturn MMDET_ROI_EXTRACTORS.build(cfg)\ndef build_shared_head(cfg):\n\"\"\"Build shared head of detector.\"\"\"\nif cfg['type'] in SHARED_HEADS._module_dict.keys():\nreturn SHARED_HEADS.build(cfg)\nelse:\nreturn MMDET_SHARED_HEADS.build(cfg)\ndef build_head(cfg):\n\"\"\"Build head.\"\"\"\nif cfg['type'] in HEADS._module_dict.keys():\nreturn HEADS.build(cfg)\nelse:\nreturn MMDET_HEADS.build(cfg)\ndef build_loss(cfg):\n\"\"\"Build loss function.\"\"\"\nif cfg['type'] in LOSSES._module_dict.keys():\nreturn LOSSES.build(cfg)\nelif cfg['type'] in MMDET_LOSSES._module_dict.keys():\nreturn MMDET_LOSSES.build(cfg)\nelse:\nreturn MMSEG_LOSSES.build(cfg)\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build detector.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nif cfg['type'] in DETECTORS._module_dict.keys():\nreturn DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\nelse:\nreturn MMDET_DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_segmentor(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build segmentor.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nreturn SEGMENTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_model(cfg, train_cfg=None, test_cfg=None):\n\"\"\"A function warpper for building 3D detector or segmentor according to\ncfg.\nShould be deprecated in the future.\n\"\"\"\nif cfg.type in ['EncoderDecoder3D']:\nreturn build_segmentor(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\nelse:\nreturn build_detector(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\ndef build_voxel_encoder(cfg):\n\"\"\"Build voxel encoder.\"\"\"\nreturn VOXEL_ENCODERS.build(cfg)\ndef build_middle_encoder(cfg):\n\"\"\"Build middle level encoder.\"\"\"\nreturn MIDDLE_ENCODERS.build(cfg)\ndef build_fusion_layer(cfg):\n\"\"\"Build fusion layer.\"\"\"\nreturn FUSION_LAYERS.build(cfg)",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 201
                },
                {
                        "id": "pretrain_python_data_1903542",
                        "content": "<reponame>ammaryasirnaich/mmdetection3d<filename>mmdet3d/models/builder.py\n# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\nfrom mmcv.cnn import MODELS as MMCV_MODELS\nfrom mmcv.utils import Registry\nfrom mmdet.models.builder import BACKBONES as MMDET_BACKBONES\nfrom mmdet.models.builder import DETECTORS as MMDET_DETECTORS\nfrom mmdet.models.builder import HEADS as MMDET_HEADS\nfrom mmdet.models.builder import LOSSES as MMDET_LOSSES\nfrom mmdet.models.builder import NECKS as MMDET_NECKS\nfrom mmdet.models.builder import ROI_EXTRACTORS as MMDET_ROI_EXTRACTORS\nfrom mmdet.models.builder import SHARED_HEADS as MMDET_SHARED_HEADS\nfrom mmseg.models.builder import LOSSES as MMSEG_LOSSES\nMODELS = Registry('models', parent=MMCV_MODELS)\nBACKBONES = MODELS\nNECKS = MODELS\nROI_EXTRACTORS = MODELS\nSHARED_HEADS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDETECTORS = MODELS\nVOXEL_ENCODERS = MODELS\nMIDDLE_ENCODERS = MODELS\nFUSION_LAYERS = MODELS\nSEGMENTORS = MODELS\ndef build_backbone(cfg):\n\"\"\"Build backbone.\"\"\"\nif cfg['type'] in BACKBONES._module_dict.keys():\nreturn BACKBONES.build(cfg)\nelse:\nreturn MMDET_BACKBONES.build(cfg)\ndef build_neck(cfg):\n\"\"\"Build neck.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn NECKS.build(cfg)\nelse:\nreturn MMDET_NECKS.build(cfg)\ndef build_roi_extractor(cfg):\n\"\"\"Build RoI feature extractor.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn ROI_EXTRACTORS.build(cfg)\nelse:\nreturn MMDET_ROI_EXTRACTORS.build(cfg)\ndef build_shared_head(cfg):\n\"\"\"Build shared head of detector.\"\"\"\nif cfg['type'] in SHARED_HEADS._module_dict.keys():\nreturn SHARED_HEADS.build(cfg)\nelse:\nreturn MMDET_SHARED_HEADS.build(cfg)\ndef build_head(cfg):\n\"\"\"Build head.\"\"\"\nif cfg['type'] in HEADS._module_dict.keys():\nreturn HEADS.build(cfg)\nelse:\nreturn MMDET_HEADS.build(cfg)\ndef build_loss(cfg):\n\"\"\"Build loss function.\"\"\"\nif cfg['type'] in LOSSES._module_dict.keys():\nreturn LOSSES.build(cfg)\nelif cfg['type'] in MMDET_LOSSES._module_dict.keys():\nreturn MMDET_LOSSES.build(cfg)\nelse:\nreturn MMSEG_LOSSES.build(cfg)\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build detector.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nif cfg['type'] in DETECTORS._module_dict.keys():\nreturn DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\nelse:\nreturn MMDET_DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_segmentor(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build segmentor.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nreturn SEGMENTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_model(cfg, train_cfg=None, test_cfg=None):\n\"\"\"A function warpper for building 3D detector or segmentor according to\ncfg.\nShould be deprecated in the future.\n\"\"\"\nif cfg.type in ['EncoderDecoder3D']:\nreturn build_segmentor(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\nelse:\nreturn build_detector(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\ndef build_voxel_encoder(cfg):\n\"\"\"Build voxel encoder.\"\"\"\nreturn VOXEL_ENCODERS.build(cfg)\ndef build_middle_encoder(cfg):\n\"\"\"Build middle level encoder.\"\"\"\nreturn MIDDLE_ENCODERS.build(cfg)\ndef build_fusion_layer(cfg):\n\"\"\"Build fusion layer.\"\"\"\nreturn FUSION_LAYERS.build(cfg)",
                        "max_stars_repo_path": "mmdet3d/models/builder.py",
                        "max_stars_repo_name": "ammaryasirnaich/mmdetection3d",
                        "max_stars_count": 0,
                        "__cluster__": 201
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "test_evocodebench_data_205",
                        "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\nfrom mmcv.cnn import MODELS as MMCV_MODELS\nfrom mmcv.utils import Registry\nfrom mmdet.models.builder import BACKBONES as MMDET_BACKBONES\nfrom mmdet.models.builder import DETECTORS as MMDET_DETECTORS\nfrom mmdet.models.builder import HEADS as MMDET_HEADS\nfrom mmdet.models.builder import LOSSES as MMDET_LOSSES\nfrom mmdet.models.builder import NECKS as MMDET_NECKS\nfrom mmdet.models.builder import ROI_EXTRACTORS as MMDET_ROI_EXTRACTORS\nfrom mmdet.models.builder import SHARED_HEADS as MMDET_SHARED_HEADS\nfrom mmseg.models.builder import LOSSES as MMSEG_LOSSES\nMODELS = Registry('models', parent=MMCV_MODELS)\nBACKBONES = MODELS\nNECKS = MODELS\nROI_EXTRACTORS = MODELS\nSHARED_HEADS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDETECTORS = MODELS\nVOXEL_ENCODERS = MODELS\nMIDDLE_ENCODERS = MODELS\nFUSION_LAYERS = MODELS\nSEGMENTORS = MODELS\ndef build_backbone(cfg):\n\"\"\"Build backbone.\"\"\"\nif cfg['type'] in BACKBONES._module_dict.keys():\nreturn BACKBONES.build(cfg)\nelse:\nreturn MMDET_BACKBONES.build(cfg)\ndef build_neck(cfg):\n\"\"\"Build neck.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn NECKS.build(cfg)\nelse:\nreturn MMDET_NECKS.build(cfg)\ndef build_roi_extractor(cfg):\n\"\"\"Build RoI feature extractor.\"\"\"\nif cfg['type'] in ROI_EXTRACTORS._module_dict.keys():\nreturn ROI_EXTRACTORS.build(cfg)\nelse:\nreturn MMDET_ROI_EXTRACTORS.build(cfg)\ndef build_shared_head(cfg):\n\"\"\"Build shared head of detector.\"\"\"\nif cfg['type'] in SHARED_HEADS._module_dict.keys():\nreturn SHARED_HEADS.build(cfg)\nelse:\nreturn MMDET_SHARED_HEADS.build(cfg)\ndef build_head(cfg):\n\"\"\"Build head.\"\"\"\nif cfg['type'] in HEADS._module_dict.keys():\nreturn HEADS.build(cfg)\nelse:\nreturn MMDET_HEADS.build(cfg)\ndef build_loss(cfg):\n\"\"\"Build loss function.\"\"\"\nif cfg['type'] in LOSSES._module_dict.keys():\nreturn LOSSES.build(cfg)\nelif cfg['type'] in MMDET_LOSSES._module_dict.keys():\nreturn MMDET_LOSSES.build(cfg)\nelse:\nreturn MMSEG_LOSSES.build(cfg)\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build detector.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nif cfg['type'] in DETECTORS._module_dict.keys():\nreturn DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\nelse:\nreturn MMDET_DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_segmentor(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build segmentor.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nreturn SEGMENTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_model(cfg, train_cfg=None, test_cfg=None):\n\"\"\"A function warpper for building 3D detector or segmentor according to\ncfg.\nShould be deprecated in the future.\n\"\"\"\nif cfg.type in ['EncoderDecoder3D']:\nreturn build_segmentor(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\nelse:\nreturn build_detector(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\ndef build_voxel_encoder(cfg):\n\"\"\"Build voxel encoder.\"\"\"\nreturn VOXEL_ENCODERS.build(cfg)\ndef build_middle_encoder(cfg):\n\"\"\"Build middle level encoder.\"\"\"\nreturn MIDDLE_ENCODERS.build(cfg)\ndef build_fusion_layer(cfg):\n\"\"\"Build fusion layer.\"\"\"\nreturn FUSION_LAYERS.build(cfg)",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 201
                },
                {
                        "id": "pretrain_python_data_1903542",
                        "content": "<reponame>ammaryasirnaich/mmdetection3d<filename>mmdet3d/models/builder.py\n# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\nfrom mmcv.cnn import MODELS as MMCV_MODELS\nfrom mmcv.utils import Registry\nfrom mmdet.models.builder import BACKBONES as MMDET_BACKBONES\nfrom mmdet.models.builder import DETECTORS as MMDET_DETECTORS\nfrom mmdet.models.builder import HEADS as MMDET_HEADS\nfrom mmdet.models.builder import LOSSES as MMDET_LOSSES\nfrom mmdet.models.builder import NECKS as MMDET_NECKS\nfrom mmdet.models.builder import ROI_EXTRACTORS as MMDET_ROI_EXTRACTORS\nfrom mmdet.models.builder import SHARED_HEADS as MMDET_SHARED_HEADS\nfrom mmseg.models.builder import LOSSES as MMSEG_LOSSES\nMODELS = Registry('models', parent=MMCV_MODELS)\nBACKBONES = MODELS\nNECKS = MODELS\nROI_EXTRACTORS = MODELS\nSHARED_HEADS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDETECTORS = MODELS\nVOXEL_ENCODERS = MODELS\nMIDDLE_ENCODERS = MODELS\nFUSION_LAYERS = MODELS\nSEGMENTORS = MODELS\ndef build_backbone(cfg):\n\"\"\"Build backbone.\"\"\"\nif cfg['type'] in BACKBONES._module_dict.keys():\nreturn BACKBONES.build(cfg)\nelse:\nreturn MMDET_BACKBONES.build(cfg)\ndef build_neck(cfg):\n\"\"\"Build neck.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn NECKS.build(cfg)\nelse:\nreturn MMDET_NECKS.build(cfg)\ndef build_roi_extractor(cfg):\n\"\"\"Build RoI feature extractor.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn ROI_EXTRACTORS.build(cfg)\nelse:\nreturn MMDET_ROI_EXTRACTORS.build(cfg)\ndef build_shared_head(cfg):\n\"\"\"Build shared head of detector.\"\"\"\nif cfg['type'] in SHARED_HEADS._module_dict.keys():\nreturn SHARED_HEADS.build(cfg)\nelse:\nreturn MMDET_SHARED_HEADS.build(cfg)\ndef build_head(cfg):\n\"\"\"Build head.\"\"\"\nif cfg['type'] in HEADS._module_dict.keys():\nreturn HEADS.build(cfg)\nelse:\nreturn MMDET_HEADS.build(cfg)\ndef build_loss(cfg):\n\"\"\"Build loss function.\"\"\"\nif cfg['type'] in LOSSES._module_dict.keys():\nreturn LOSSES.build(cfg)\nelif cfg['type'] in MMDET_LOSSES._module_dict.keys():\nreturn MMDET_LOSSES.build(cfg)\nelse:\nreturn MMSEG_LOSSES.build(cfg)\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build detector.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nif cfg['type'] in DETECTORS._module_dict.keys():\nreturn DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\nelse:\nreturn MMDET_DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_segmentor(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build segmentor.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nreturn SEGMENTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_model(cfg, train_cfg=None, test_cfg=None):\n\"\"\"A function warpper for building 3D detector or segmentor according to\ncfg.\nShould be deprecated in the future.\n\"\"\"\nif cfg.type in ['EncoderDecoder3D']:\nreturn build_segmentor(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\nelse:\nreturn build_detector(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\ndef build_voxel_encoder(cfg):\n\"\"\"Build voxel encoder.\"\"\"\nreturn VOXEL_ENCODERS.build(cfg)\ndef build_middle_encoder(cfg):\n\"\"\"Build middle level encoder.\"\"\"\nreturn MIDDLE_ENCODERS.build(cfg)\ndef build_fusion_layer(cfg):\n\"\"\"Build fusion layer.\"\"\"\nreturn FUSION_LAYERS.build(cfg)",
                        "max_stars_repo_path": "mmdet3d/models/builder.py",
                        "max_stars_repo_name": "ammaryasirnaich/mmdetection3d",
                        "max_stars_count": 0,
                        "__cluster__": 201
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "test_evocodebench_data_206",
                        "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\nfrom mmcv.cnn import MODELS as MMCV_MODELS\nfrom mmcv.utils import Registry\nfrom mmdet.models.builder import BACKBONES as MMDET_BACKBONES\nfrom mmdet.models.builder import DETECTORS as MMDET_DETECTORS\nfrom mmdet.models.builder import HEADS as MMDET_HEADS\nfrom mmdet.models.builder import LOSSES as MMDET_LOSSES\nfrom mmdet.models.builder import NECKS as MMDET_NECKS\nfrom mmdet.models.builder import ROI_EXTRACTORS as MMDET_ROI_EXTRACTORS\nfrom mmdet.models.builder import SHARED_HEADS as MMDET_SHARED_HEADS\nfrom mmseg.models.builder import LOSSES as MMSEG_LOSSES\nMODELS = Registry('models', parent=MMCV_MODELS)\nBACKBONES = MODELS\nNECKS = MODELS\nROI_EXTRACTORS = MODELS\nSHARED_HEADS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDETECTORS = MODELS\nVOXEL_ENCODERS = MODELS\nMIDDLE_ENCODERS = MODELS\nFUSION_LAYERS = MODELS\nSEGMENTORS = MODELS\ndef build_backbone(cfg):\n\"\"\"Build backbone.\"\"\"\nif cfg['type'] in BACKBONES._module_dict.keys():\nreturn BACKBONES.build(cfg)\nelse:\nreturn MMDET_BACKBONES.build(cfg)\ndef build_neck(cfg):\n\"\"\"Build neck.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn NECKS.build(cfg)\nelse:\nreturn MMDET_NECKS.build(cfg)\ndef build_roi_extractor(cfg):\n\"\"\"Build RoI feature extractor.\"\"\"\nif cfg['type'] in ROI_EXTRACTORS._module_dict.keys():\nreturn ROI_EXTRACTORS.build(cfg)\nelse:\nreturn MMDET_ROI_EXTRACTORS.build(cfg)\ndef build_shared_head(cfg):\n\"\"\"Build shared head of detector.\"\"\"\nif cfg['type'] in SHARED_HEADS._module_dict.keys():\nreturn SHARED_HEADS.build(cfg)\nelse:\nreturn MMDET_SHARED_HEADS.build(cfg)\ndef build_head(cfg):\n\"\"\"Build head.\"\"\"\nif cfg['type'] in HEADS._module_dict.keys():\nreturn HEADS.build(cfg)\nelse:\nreturn MMDET_HEADS.build(cfg)\ndef build_loss(cfg):\n\"\"\"Build loss function.\"\"\"\nif cfg['type'] in LOSSES._module_dict.keys():\nreturn LOSSES.build(cfg)\nelif cfg['type'] in MMDET_LOSSES._module_dict.keys():\nreturn MMDET_LOSSES.build(cfg)\nelse:\nreturn MMSEG_LOSSES.build(cfg)\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build detector.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nif cfg['type'] in DETECTORS._module_dict.keys():\nreturn DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\nelse:\nreturn MMDET_DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_segmentor(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build segmentor.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nreturn SEGMENTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_model(cfg, train_cfg=None, test_cfg=None):\n\"\"\"A function warpper for building 3D detector or segmentor according to\ncfg.\nShould be deprecated in the future.\n\"\"\"\nif cfg.type in ['EncoderDecoder3D']:\nreturn build_segmentor(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\nelse:\nreturn build_detector(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\ndef build_voxel_encoder(cfg):\n\"\"\"Build voxel encoder.\"\"\"\nreturn VOXEL_ENCODERS.build(cfg)\ndef build_middle_encoder(cfg):\n\"\"\"Build middle level encoder.\"\"\"\nreturn MIDDLE_ENCODERS.build(cfg)\ndef build_fusion_layer(cfg):\n\"\"\"Build fusion layer.\"\"\"\nreturn FUSION_LAYERS.build(cfg)",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 201
                },
                {
                        "id": "pretrain_python_data_1903542",
                        "content": "<reponame>ammaryasirnaich/mmdetection3d<filename>mmdet3d/models/builder.py\n# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\nfrom mmcv.cnn import MODELS as MMCV_MODELS\nfrom mmcv.utils import Registry\nfrom mmdet.models.builder import BACKBONES as MMDET_BACKBONES\nfrom mmdet.models.builder import DETECTORS as MMDET_DETECTORS\nfrom mmdet.models.builder import HEADS as MMDET_HEADS\nfrom mmdet.models.builder import LOSSES as MMDET_LOSSES\nfrom mmdet.models.builder import NECKS as MMDET_NECKS\nfrom mmdet.models.builder import ROI_EXTRACTORS as MMDET_ROI_EXTRACTORS\nfrom mmdet.models.builder import SHARED_HEADS as MMDET_SHARED_HEADS\nfrom mmseg.models.builder import LOSSES as MMSEG_LOSSES\nMODELS = Registry('models', parent=MMCV_MODELS)\nBACKBONES = MODELS\nNECKS = MODELS\nROI_EXTRACTORS = MODELS\nSHARED_HEADS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDETECTORS = MODELS\nVOXEL_ENCODERS = MODELS\nMIDDLE_ENCODERS = MODELS\nFUSION_LAYERS = MODELS\nSEGMENTORS = MODELS\ndef build_backbone(cfg):\n\"\"\"Build backbone.\"\"\"\nif cfg['type'] in BACKBONES._module_dict.keys():\nreturn BACKBONES.build(cfg)\nelse:\nreturn MMDET_BACKBONES.build(cfg)\ndef build_neck(cfg):\n\"\"\"Build neck.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn NECKS.build(cfg)\nelse:\nreturn MMDET_NECKS.build(cfg)\ndef build_roi_extractor(cfg):\n\"\"\"Build RoI feature extractor.\"\"\"\nif cfg['type'] in NECKS._module_dict.keys():\nreturn ROI_EXTRACTORS.build(cfg)\nelse:\nreturn MMDET_ROI_EXTRACTORS.build(cfg)\ndef build_shared_head(cfg):\n\"\"\"Build shared head of detector.\"\"\"\nif cfg['type'] in SHARED_HEADS._module_dict.keys():\nreturn SHARED_HEADS.build(cfg)\nelse:\nreturn MMDET_SHARED_HEADS.build(cfg)\ndef build_head(cfg):\n\"\"\"Build head.\"\"\"\nif cfg['type'] in HEADS._module_dict.keys():\nreturn HEADS.build(cfg)\nelse:\nreturn MMDET_HEADS.build(cfg)\ndef build_loss(cfg):\n\"\"\"Build loss function.\"\"\"\nif cfg['type'] in LOSSES._module_dict.keys():\nreturn LOSSES.build(cfg)\nelif cfg['type'] in MMDET_LOSSES._module_dict.keys():\nreturn MMDET_LOSSES.build(cfg)\nelse:\nreturn MMSEG_LOSSES.build(cfg)\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build detector.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nif cfg['type'] in DETECTORS._module_dict.keys():\nreturn DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\nelse:\nreturn MMDET_DETECTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_segmentor(cfg, train_cfg=None, test_cfg=None):\n\"\"\"Build segmentor.\"\"\"\nif train_cfg is not None or test_cfg is not None:\nwarnings.warn(\n'train_cfg and test_cfg is deprecated, '\n'please specify them in model', UserWarning)\nassert cfg.get('train_cfg') is None or train_cfg is None, \\\n'train_cfg specified in both outer field and model field '\nassert cfg.get('test_cfg') is None or test_cfg is None, \\\n'test_cfg specified in both outer field and model field '\nreturn SEGMENTORS.build(\ncfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\ndef build_model(cfg, train_cfg=None, test_cfg=None):\n\"\"\"A function warpper for building 3D detector or segmentor according to\ncfg.\nShould be deprecated in the future.\n\"\"\"\nif cfg.type in ['EncoderDecoder3D']:\nreturn build_segmentor(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\nelse:\nreturn build_detector(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\ndef build_voxel_encoder(cfg):\n\"\"\"Build voxel encoder.\"\"\"\nreturn VOXEL_ENCODERS.build(cfg)\ndef build_middle_encoder(cfg):\n\"\"\"Build middle level encoder.\"\"\"\nreturn MIDDLE_ENCODERS.build(cfg)\ndef build_fusion_layer(cfg):\n\"\"\"Build fusion layer.\"\"\"\nreturn FUSION_LAYERS.build(cfg)",
                        "max_stars_repo_path": "mmdet3d/models/builder.py",
                        "max_stars_repo_name": "ammaryasirnaich/mmdetection3d",
                        "max_stars_count": 0,
                        "__cluster__": 201
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "test_evocodebench_data_185",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n# -*- coding: utf-8 -*-\nimport typing\nfrom typing import Any, List\nimport fvcore\nfrom fvcore.nn import activation_count, flop_count, parameter_count, parameter_count_table\nfrom torch import nn\nfrom detectron2.export import TracingAdapter\n__all__ = [\n\"activation_count_operators\",\n\"flop_count_operators\",\n\"parameter_count_table\",\n\"parameter_count\",\n\"FlopCountAnalysis\",\n]\nFLOPS_MODE = \"flops\"\nACTIVATIONS_MODE = \"activations\"\n# Some extra ops to ignore from counting, including elementwise and reduction ops\n_IGNORED_OPS = {\n\"aten::add\",\n\"aten::add_\",\n\"aten::argmax\",\n\"aten::argsort\",\n\"aten::batch_norm\",\n\"aten::constant_pad_nd\",\n\"aten::div\",\n\"aten::div_\",\n\"aten::exp\",\n\"aten::log2\",\n\"aten::max_pool2d\",\n\"aten::meshgrid\",\n\"aten::mul\",\n\"aten::mul_\",\n\"aten::neg\",\n\"aten::nonzero_numpy\",\n\"aten::reciprocal\",\n\"aten::repeat_interleave\",\n\"aten::rsub\",\n\"aten::sigmoid\",\n\"aten::sigmoid_\",\n\"aten::softmax\",\n\"aten::sort\",\n\"aten::sqrt\",\n\"aten::sub\",\n\"torchvision::nms\",  # TODO estimate flop for nms\n}\nclass FlopCountAnalysis(fvcore.nn.FlopCountAnalysis):\n\"\"\"\nSame as :class:`fvcore.nn.FlopCountAnalysis`, but supports detectron2 models.\n\"\"\"\ndef __init__(self, model, inputs):\n\"\"\"\nArgs:\nmodel (nn.Module):\ninputs (Any): inputs of the given model. Does not have to be tuple of tensors.\n\"\"\"\nwrapper = TracingAdapter(model, inputs, allow_non_tensor=True)\nsuper().__init__(wrapper, wrapper.flattened_inputs)\nself.set_op_handle(**{k: None for k in _IGNORED_OPS})\ndef flop_count_operators(model: nn.Module, inputs: list) -> typing.DefaultDict[str, float]:\n\"\"\"\nImplement operator-level flops counting using jit.\nThis is a wrapper of :func:`fvcore.nn.flop_count` and adds supports for standard\ndetection models in detectron2.\nPlease use :class:`FlopCountAnalysis` for more advanced functionalities.\nNote:\nThe function runs the input through the model to compute flops.\nThe flops of a detection model is often input-dependent, for example,\nthe flops of box & mask head depends on the number of proposals &\nthe number of detected objects.\nTherefore, the flops counting using a single input may not accurately\nreflect the computation cost of a model. It's recommended to average\nacross a number of inputs.\nArgs:\nmodel: a detectron2 model that takes `list[dict]` as input.\ninputs (list[dict]): inputs to model, in detectron2's standard format.\nOnly \"image\" key will be used.\nsupported_ops (dict[str, Handle]): see documentation of :func:`fvcore.nn.flop_count`\nReturns:\nCounter: Gflop count per operator\n\"\"\"\nold_train = model.training\nmodel.eval()\nret = FlopCountAnalysis(model, inputs).by_operator()\nmodel.train(old_train)\nreturn {k: v / 1e9 for k, v in ret.items()}\ndef activation_count_operators(\nmodel: nn.Module, inputs: list, **kwargs\n) -> typing.DefaultDict[str, float]:\n\"\"\"\nImplement operator-level activations counting using jit.\nThis is a wrapper of fvcore.nn.activation_count, that supports standard detection models\nin detectron2.\nNote:\nThe function runs the input through the model to compute activations.\nThe activations of a detection model is often input-dependent, for example,\nthe activations of box & mask head depends on the number of proposals &\nthe number of detected objects.\nArgs:\nmodel: a detectron2 model that takes `list[dict]` as input.\ninputs (list[dict]): inputs to model, in detectron2's standard format.\nOnly \"image\" key will be used.\nReturns:\nCounter: activation count per operator\n\"\"\"\nreturn _wrapper_count_operators(model=model, inputs=inputs, mode=ACTIVATIONS_MODE, **kwargs)\ndef _wrapper_count_operators(\nmodel: nn.Module, inputs: list, mode: str, **kwargs\n) -> typing.DefaultDict[str, float]:\n# ignore some ops\nsupported_ops = {k: lambda *args, **kwargs: {} for k in _IGNORED_OPS}\nsupported_ops.update(kwargs.pop(\"supported_ops\", {}))\nkwargs[\"supported_ops\"] = supported_ops\nassert len(inputs) == 1, \"Please use batch size=1\"\ntensor_input = inputs[0][\"image\"]\ninputs = [{\"image\": tensor_input}]  # remove other keys, in case there are any\nold_train = model.training\nif isinstance(model, (nn.parallel.distributed.DistributedDataParallel, nn.DataParallel)):\nmodel = model.module\nwrapper = TracingAdapter(model, inputs)\nwrapper.eval()\nif mode == FLOPS_MODE:\nret = flop_count(wrapper, (tensor_input,), **kwargs)\nelif mode == ACTIVATIONS_MODE:\nret = activation_count(wrapper, (tensor_input,), **kwargs)\nelse:\nraise NotImplementedError(\"Count for mode {} is not supported yet.\".format(mode))\n# compatible with change in fvcore\nif isinstance(ret, tuple):\nret = ret[0]\nmodel.train(old_train)\nreturn ret\ndef find_unused_parameters(model: nn.Module, inputs: Any) -> List[str]:\n\"\"\"\nGiven a model, find parameters that do not contribute\nto the loss.\nArgs:\nmodel: a model in training mode that returns losses\ninputs: argument or a tuple of arguments. Inputs of the model\nReturns:\nlist[str]: the name of unused parameters\n\"\"\"\nassert model.training\nfor _, prm in model.named_parameters():\nprm.grad = None\nif isinstance(inputs, tuple):\nlosses = model(*inputs)\nelse:\nlosses = model(inputs)\nif isinstance(losses, dict):\nlosses = sum(losses.values())\nlosses.backward()\nunused: List[str] = []\nfor name, prm in model.named_parameters():\nif prm.grad is None:\nunused.append(name)\nprm.grad = None\nreturn unused",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 184
                },
                {
                        "id": "pretrain_python_data_5156284",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n# -*- coding: utf-8 -*-\nimport typing\nimport fvcore\nfrom fvcore.nn import activation_count, flop_count, parameter_count, parameter_count_table\nfrom torch import nn\nfrom ..export import TracingAdapter\n__all__ = [\n\"activation_count_operators\",\n\"flop_count_operators\",\n\"parameter_count_table\",\n\"parameter_count\",\n]\nFLOPS_MODE = \"flops\"\nACTIVATIONS_MODE = \"activations\"\n# Some extra ops to ignore from counting, including elementwise and reduction ops\n_IGNORED_OPS = {\n\"aten::add\",\n\"aten::add_\",\n\"aten::argmax\",\n\"aten::argsort\",\n\"aten::batch_norm\",\n\"aten::constant_pad_nd\",\n\"aten::div\",\n\"aten::div_\",\n\"aten::exp\",\n\"aten::log2\",\n\"aten::max_pool2d\",\n\"aten::meshgrid\",\n\"aten::mul\",\n\"aten::mul_\",\n\"aten::neg\",\n\"aten::nonzero_numpy\",\n\"aten::reciprocal\",\n\"aten::rsub\",\n\"aten::sigmoid\",\n\"aten::sigmoid_\",\n\"aten::softmax\",\n\"aten::sort\",\n\"aten::sqrt\",\n\"aten::sub\",\n\"torchvision::nms\",  # TODO estimate flop for nms\n}\nclass FlopCountAnalysis(fvcore.nn.FlopCountAnalysis):\n\"\"\"\nSame as :class:`fvcore.nn.FlopCountAnalysis`, but supports detectron2 models.\n\"\"\"\ndef __init__(self, model, inputs):\n\"\"\"\nArgs:\nmodel (nn.Module):\ninputs (Any): inputs of the given model. Does not have to be tuple of tensors.\n\"\"\"\nwrapper = TracingAdapter(model, inputs, allow_non_tensor=True)\nsuper().__init__(wrapper, wrapper.flattened_inputs)\nself.set_op_handle(**{k: None for k in _IGNORED_OPS})\ndef flop_count_operators(model: nn.Module, inputs: list) -> typing.DefaultDict[str, float]:\n\"\"\"\nImplement operator-level flops counting using jit.\nThis is a wrapper of :func:`fvcore.nn.flop_count` and adds supports for standard\ndetection models in detectron2.\nPlease use :class:`FlopCountAnalysis` for more advanced functionalities.\nNote:\nThe function runs the input through the model to compute flops.\nThe flops of a detection model is often input-dependent, for example,\nthe flops of box & mask head depends on the number of proposals &\nthe number of detected objects.\nTherefore, the flops counting using a single input may not accurately\nreflect the computation cost of a model. It's recommended to average\nacross a number of inputs.\nArgs:\nmodel: a detectron2 model that takes `list[dict]` as input.\ninputs (list[dict]): inputs to model, in detectron2's standard format.\nOnly \"image\" key will be used.\nsupported_ops (dict[str, Handle]): see documentation of :func:`fvcore.nn.flop_count`\nReturns:\nCounter: Gflop count per operator\n\"\"\"\nold_train = model.training\nmodel.eval()\nret = FlopCountAnalysis(model, inputs).by_operator()\nmodel.train(old_train)\nreturn {k: v / 1e9 for k, v in ret.items()}\ndef activation_count_operators(\nmodel: nn.Module, inputs: list, **kwargs\n) -> typing.DefaultDict[str, float]:\n\"\"\"\nImplement operator-level activations counting using jit.\nThis is a wrapper of fvcore.nn.activation_count, that supports standard detection models\nin detectron2.\nNote:\nThe function runs the input through the model to compute activations.\nThe activations of a detection model is often input-dependent, for example,\nthe activations of box & mask head depends on the number of proposals &\nthe number of detected objects.\nArgs:\nmodel: a detectron2 model that takes `list[dict]` as input.\ninputs (list[dict]): inputs to model, in detectron2's standard format.\nOnly \"image\" key will be used.\nReturns:\nCounter: activation count per operator\n\"\"\"\nreturn _wrapper_count_operators(model=model, inputs=inputs, mode=ACTIVATIONS_MODE, **kwargs)\ndef _wrapper_count_operators(\nmodel: nn.Module, inputs: list, mode: str, **kwargs\n) -> typing.DefaultDict[str, float]:\n# ignore some ops\nsupported_ops = {k: lambda *args, **kwargs: {} for k in _IGNORED_OPS}\nsupported_ops.update(kwargs.pop(\"supported_ops\", {}))\nkwargs[\"supported_ops\"] = supported_ops\nassert len(inputs) == 1, \"Please use batch size=1\"\ntensor_input = inputs[0][\"image\"]\ninputs = [{\"image\": tensor_input}]  # remove other keys, in case there are any\nold_train = model.training\nif isinstance(model, (nn.parallel.distributed.DistributedDataParallel, nn.DataParallel)):\nmodel = model.module\nwrapper = TracingAdapter(model, inputs)\nwrapper.eval()\nif mode == FLOPS_MODE:\nret = flop_count(wrapper, (tensor_input,), **kwargs)\nelif mode == ACTIVATIONS_MODE:\nret = activation_count(wrapper, (tensor_input,), **kwargs)\nelse:\nraise NotImplementedError(\"Count for mode {} is not supported yet.\".format(mode))\n# compatible with change in fvcore\nif isinstance(ret, tuple):\nret = ret[0]\nmodel.train(old_train)\nreturn ret",
                        "max_stars_repo_path": "detectron2/utils/analysis.py",
                        "max_stars_repo_name": "akashAD98/detectron2",
                        "max_stars_count": 0,
                        "__cluster__": 184
                },
                {
                        "real_dup": "2"
                }
        ],


        [
                {
                        "id": "test_evocodebench_data_193",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nfrom detectron2.utils.registry import Registry\nPROPOSAL_GENERATOR_REGISTRY = Registry(\"PROPOSAL_GENERATOR\")\nPROPOSAL_GENERATOR_REGISTRY.__doc__ = \"\"\"\nRegistry for proposal generator, which produces object proposals from feature maps.\nThe registered object will be called with `obj(cfg, input_shape)`.\nThe call should return a `nn.Module` object.\n\"\"\"\nfrom . import rpn, rrpn  # noqa F401 isort:skip\ndef build_proposal_generator(cfg, input_shape):\n\"\"\"\nBuild a proposal generator from `cfg.MODEL.PROPOSAL_GENERATOR.NAME`.\nThe name can be \"PrecomputedProposals\" to use no proposal generator.\n\"\"\"\nname = cfg.MODEL.PROPOSAL_GENERATOR.NAME\nif name == \"PrecomputedProposals\":\nreturn None\nreturn PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 192
                },
                {
                        "id": "pretrain_python_data_3794362",
                        "content": "<gh_stars>0\n# Copyright (c) Facebook, Inc. and its affiliates.\nfrom detectron.utils.registry import Registry\nPROPOSAL_GENERATOR_REGISTRY = Registry(\"PROPOSAL_GENERATOR\")\nPROPOSAL_GENERATOR_REGISTRY.__doc__ = \"\"\"\nRegistry for proposal generator, which produces object proposals from feature maps.\nThe registered object will be called with `obj(cfg, input_shape)`.\nThe call should return a `nn.Module` object.\n\"\"\"\nfrom . import rpn, rrpn  # noqa F401 isort:skip\ndef build_proposal_generator(cfg, input_shape):\n\"\"\"\nBuild a proposal generator from `cfg.MODEL.PROPOSAL_GENERATOR.NAME`.\nThe name can be \"PrecomputedProposals\" to use no proposal generator.\n\"\"\"\nname = cfg.MODEL.PROPOSAL_GENERATOR.NAME\nif name == \"PrecomputedProposals\":\nreturn None\nreturn PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)",
                        "max_stars_repo_path": "detectron_/modeling/proposal_generator/build.py",
                        "max_stars_repo_name": "TalhaUsuf/RetinaNet_W9_form",
                        "max_stars_count": 0,
                        "__cluster__": 192
                },
                {
                        "real_dup": "2"
                }
        ],


        [
                {
                        "id": "test_evocodebench_data_200",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nimport math\nfrom typing import List, Tuple\nimport torch\nfrom detectron2.layers.rotated_boxes import pairwise_iou_rotated\nfrom .boxes import Boxes\nclass RotatedBoxes(Boxes):\n\"\"\"\nThis structure stores a list of rotated boxes as a Nx5 torch.Tensor.\nIt supports some common methods about boxes\n(`area`, `clip`, `nonempty`, etc),\nand also behaves like a Tensor\n(support indexing, `to(device)`, `.device`, and iteration over all boxes)\n\"\"\"\ndef __init__(self, tensor: torch.Tensor):\n\"\"\"\nArgs:\ntensor (Tensor[float]): a Nx5 matrix.  Each row is\n(x_center, y_center, width, height, angle),\nin which angle is represented in degrees.\nWhile there's no strict range restriction for it,\nthe recommended principal range is between [-180, 180) degrees.\nAssume we have a horizontal box B = (x_center, y_center, width, height),\nwhere width is along the x-axis and height is along the y-axis.\nThe rotated box B_rot (x_center, y_center, width, height, angle)\ncan be seen as:\n1. When angle == 0:\nB_rot == B\n2. When angle > 0:\nB_rot is obtained by rotating B w.r.t its center by :math:`|angle|` degrees CCW;\n3. When angle < 0:\nB_rot is obtained by rotating B w.r.t its center by :math:`|angle|` degrees CW.\nMathematically, since the right-handed coordinate system for image space\nis (y, x), where y is top->down and x is left->right, the 4 vertices of the\nrotated rectangle :math:`(yr_i, xr_i)` (i = 1, 2, 3, 4) can be obtained from\nthe vertices of the horizontal rectangle :math:`(y_i, x_i)` (i = 1, 2, 3, 4)\nin the following way (:math:`\\\\theta = angle*\\\\pi/180` is the angle in radians,\n:math:`(y_c, x_c)` is the center of the rectangle):\n.. math::\nyr_i = \\\\cos(\\\\theta) (y_i - y_c) - \\\\sin(\\\\theta) (x_i - x_c) + y_c,\nxr_i = \\\\sin(\\\\theta) (y_i - y_c) + \\\\cos(\\\\theta) (x_i - x_c) + x_c,\nwhich is the standard rigid-body rotation transformation.\nIntuitively, the angle is\n(1) the rotation angle from y-axis in image space\nto the height vector (top->down in the box's local coordinate system)\nof the box in CCW, and\n(2) the rotation angle from x-axis in image space\nto the width vector (left->right in the box's local coordinate system)\nof the box in CCW.\nMore intuitively, consider the following horizontal box ABCD represented\nin (x1, y1, x2, y2): (3, 2, 7, 4),\ncovering the [3, 7] x [2, 4] region of the continuous coordinate system\nwhich looks like this:\n.. code:: none\nO--------> x\n|\n|  A---B\n|  |   |\n|  D---C\n|\nv y\nNote that each capital letter represents one 0-dimensional geometric point\ninstead of a 'square pixel' here.\nIn the example above, using (x, y) to represent a point we have:\n.. math::\nO = (0, 0), A = (3, 2), B = (7, 2), C = (7, 4), D = (3, 4)\nWe name vector AB = vector DC as the width vector in box's local coordinate system, and\nvector AD = vector BC as the height vector in box's local coordinate system. Initially,\nwhen angle = 0 degree, they're aligned with the positive directions of x-axis and y-axis\nin the image space, respectively.\nFor better illustration, we denote the center of the box as E,\n.. code:: none\nO--------> x\n|\n|  A---B\n|  | E |\n|  D---C\n|\nv y\nwhere the center E = ((3+7)/2, (2+4)/2) = (5, 3).\nAlso,\n.. math::\nwidth = |AB| = |CD| = 7 - 3 = 4,\nheight = |AD| = |BC| = 4 - 2 = 2.\nTherefore, the corresponding representation for the same shape in rotated box in\n(x_center, y_center, width, height, angle) format is:\n(5, 3, 4, 2, 0),\nNow, let's consider (5, 3, 4, 2, 90), which is rotated by 90 degrees\nCCW (counter-clockwise) by definition. It looks like this:\n.. code:: none\nO--------> x\n|   B-C\n|   | |\n|   |E|\n|   | |\n|   A-D\nv y\nThe center E is still located at the same point (5, 3), while the vertices\nABCD are rotated by 90 degrees CCW with regard to E:\nA = (4, 5), B = (4, 1), C = (6, 1), D = (6, 5)\nHere, 90 degrees can be seen as the CCW angle to rotate from y-axis to\nvector AD or vector BC (the top->down height vector in box's local coordinate system),\nor the CCW angle to rotate from x-axis to vector AB or vector DC (the left->right\nwidth vector in box's local coordinate system).\n.. math::\nwidth = |AB| = |CD| = 5 - 1 = 4,\nheight = |AD| = |BC| = 6 - 4 = 2.\nNext, how about (5, 3, 4, 2, -90), which is rotated by 90 degrees CW (clockwise)\nby definition? It looks like this:\n.. code:: none\nO--------> x\n|   D-A\n|   | |\n|   |E|\n|   | |\n|   C-B\nv y\nThe center E is still located at the same point (5, 3), while the vertices\nABCD are rotated by 90 degrees CW with regard to E:\nA = (6, 1), B = (6, 5), C = (4, 5), D = (4, 1)\n.. math::\nwidth = |AB| = |CD| = 5 - 1 = 4,\nheight = |AD| = |BC| = 6 - 4 = 2.\nThis covers exactly the same region as (5, 3, 4, 2, 90) does, and their IoU\nwill be 1. However, these two will generate different RoI Pooling results and\nshould not be treated as an identical box.\nOn the other hand, it's easy to see that (X, Y, W, H, A) is identical to\n(X, Y, W, H, A+360N), for any integer N. For example (5, 3, 4, 2, 270) would be\nidentical to (5, 3, 4, 2, -90), because rotating the shape 270 degrees CCW is\nequivalent to rotating the same shape 90 degrees CW.\nWe could rotate further to get (5, 3, 4, 2, 180), or (5, 3, 4, 2, -180):\n.. code:: none\nO--------> x\n|\n|  C---D\n|  | E |\n|  B---A\n|\nv y\n.. math::\nA = (7, 4), B = (3, 4), C = (3, 2), D = (7, 2),\nwidth = |AB| = |CD| = 7 - 3 = 4,\nheight = |AD| = |BC| = 4 - 2 = 2.\nFinally, this is a very inaccurate (heavily quantized) illustration of\nhow (5, 3, 4, 2, 60) looks like in case anyone wonders:\n.. code:: none\nO--------> x\n|     B\\\n|    /  C\n|   /E /\n|  A  /\n|   `D\nv y\nIt's still a rectangle with center of (5, 3), width of 4 and height of 2,\nbut its angle (and thus orientation) is somewhere between\n(5, 3, 4, 2, 0) and (5, 3, 4, 2, 90).\n\"\"\"\ndevice = tensor.device if isinstance(tensor, torch.Tensor) else torch.device(\"cpu\")\ntensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\nif tensor.numel() == 0:\n# Use reshape, so we don't end up creating a new tensor that does not depend on\n# the inputs (and consequently confuses jit)\ntensor = tensor.reshape((0, 5)).to(dtype=torch.float32, device=device)\nassert tensor.dim() == 2 and tensor.size(-1) == 5, tensor.size()\nself.tensor = tensor\ndef clone(self) -> \"RotatedBoxes\":\n\"\"\"\nClone the RotatedBoxes.\nReturns:\nRotatedBoxes\n\"\"\"\nreturn RotatedBoxes(self.tensor.clone())\ndef to(self, device: torch.device):\n# Boxes are assumed float32 and does not support to(dtype)\nreturn RotatedBoxes(self.tensor.to(device=device))\ndef area(self) -> torch.Tensor:\n\"\"\"\nComputes the area of all the boxes.\nReturns:\ntorch.Tensor: a vector with areas of each box.\n\"\"\"\nbox = self.tensor\narea = box[:, 2] * box[:, 3]\nreturn area\ndef normalize_angles(self) -> None:\n\"\"\"\nRestrict angles to the range of [-180, 180) degrees\n\"\"\"\nself.tensor[:, 4] = (self.tensor[:, 4] + 180.0) % 360.0 - 180.0\ndef clip(self, box_size: Tuple[int, int], clip_angle_threshold: float = 1.0) -> None:\n\"\"\"\nClip (in place) the boxes by limiting x coordinates to the range [0, width]\nand y coordinates to the range [0, height].\nFor RRPN:\nOnly clip boxes that are almost horizontal with a tolerance of\nclip_angle_threshold to maintain backward compatibility.\nRotated boxes beyond this threshold are not clipped for two reasons:\n1. There are potentially multiple ways to clip a rotated box to make it\nfit within the image.\n2. It's tricky to make the entire rectangular box fit within the image\nand still be able to not leave out pixels of interest.\nTherefore we rely on ops like RoIAlignRotated to safely handle this.\nArgs:\nbox_size (height, width): The clipping box's size.\nclip_angle_threshold:\nIff. abs(normalized(angle)) <= clip_angle_threshold (in degrees),\nwe do the clipping as horizontal boxes.\n\"\"\"\nh, w = box_size\n# normalize angles to be within (-180, 180] degrees\nself.normalize_angles()\nidx = torch.where(torch.abs(self.tensor[:, 4]) <= clip_angle_threshold)[0]\n# convert to (x1, y1, x2, y2)\nx1 = self.tensor[idx, 0] - self.tensor[idx, 2] / 2.0\ny1 = self.tensor[idx, 1] - self.tensor[idx, 3] / 2.0\nx2 = self.tensor[idx, 0] + self.tensor[idx, 2] / 2.0\ny2 = self.tensor[idx, 1] + self.tensor[idx, 3] / 2.0\n# clip\nx1.clamp_(min=0, max=w)\ny1.clamp_(min=0, max=h)\nx2.clamp_(min=0, max=w)\ny2.clamp_(min=0, max=h)\n# convert back to (xc, yc, w, h)\nself.tensor[idx, 0] = (x1 + x2) / 2.0\nself.tensor[idx, 1] = (y1 + y2) / 2.0\n# make sure widths and heights do not increase due to numerical errors\nself.tensor[idx, 2] = torch.min(self.tensor[idx, 2], x2 - x1)\nself.tensor[idx, 3] = torch.min(self.tensor[idx, 3], y2 - y1)\ndef nonempty(self, threshold: float = 0.0) -> torch.Tensor:\n\"\"\"\nFind boxes that are non-empty.\nA box is considered empty, if either of its side is no larger than threshold.\nReturns:\nTensor: a binary vector which represents\nwhether each box is empty (False) or non-empty (True).\n\"\"\"\nbox = self.tensor\nwidths = box[:, 2]\nheights = box[:, 3]\nkeep = (widths > threshold) & (heights > threshold)\nreturn keep\ndef __getitem__(self, item) -> \"RotatedBoxes\":\n\"\"\"\nReturns:\nRotatedBoxes: Create a new :class:`RotatedBoxes` by indexing.\nThe following usage are allowed:\n1. `new_boxes = boxes[3]`: return a `RotatedBoxes` which contains only one box.\n2. `new_boxes = boxes[2:10]`: return a slice of boxes.\n3. `new_boxes = boxes[vector]`, where vector is a torch.ByteTensor\nwith `length = len(boxes)`. Nonzero elements in the vector will be selected.\nNote that the returned RotatedBoxes might share storage with this RotatedBoxes,\nsubject to Pytorch's indexing semantics.\n\"\"\"\nif isinstance(item, int):\nreturn RotatedBoxes(self.tensor[item].view(1, -1))\nb = self.tensor[item]\nassert b.dim() == 2, \"Indexing on RotatedBoxes with {} failed to return a matrix!\".format(\nitem\n)\nreturn RotatedBoxes(b)\ndef __len__(self) -> int:\nreturn self.tensor.shape[0]\ndef __repr__(self) -> str:\nreturn \"RotatedBoxes(\" + str(self.tensor) + \")\"\ndef inside_box(self, box_size: Tuple[int, int], boundary_threshold: int = 0) -> torch.Tensor:\n\"\"\"\nArgs:\nbox_size (height, width): Size of the reference box covering\n[0, width] x [0, height]\nboundary_threshold (int): Boxes that extend beyond the reference box\nboundary by more than boundary_threshold are considered \"outside\".\nFor RRPN, it might not be necessary to call this function since it's common\nfor rotated box to extend to outside of the image boundaries\n(the clip function only clips the near-horizontal boxes)\nReturns:\na binary vector, indicating whether each box is inside the reference box.\n\"\"\"\nheight, width = box_size\ncnt_x = self.tensor[..., 0]\ncnt_y = self.tensor[..., 1]\nhalf_w = self.tensor[..., 2] / 2.0\nhalf_h = self.tensor[..., 3] / 2.0\na = self.tensor[..., 4]\nc = torch.abs(torch.cos(a * math.pi / 180.0))\ns = torch.abs(torch.sin(a * math.pi / 180.0))\n# This basically computes the horizontal bounding rectangle of the rotated box\nmax_rect_dx = c * half_w + s * half_h\nmax_rect_dy = c * half_h + s * half_w\ninds_inside = (\n(cnt_x - max_rect_dx >= -boundary_threshold)\n& (cnt_y - max_rect_dy >= -boundary_threshold)\n& (cnt_x + max_rect_dx < width + boundary_threshold)\n& (cnt_y + max_rect_dy < height + boundary_threshold)\n)\nreturn inds_inside\ndef get_centers(self) -> torch.Tensor:\n\"\"\"\nReturns:\nThe box centers in a Nx2 array of (x, y).\n\"\"\"\nreturn self.tensor[:, :2]\ndef scale(self, scale_x: float, scale_y: float) -> None:\n\"\"\"\nScale the rotated box with horizontal and vertical scaling factors\nNote: when scale_factor_x != scale_factor_y,\nthe rotated box does not preserve the rectangular shape when the angle\nis not a multiple of 90 degrees under resize transformation.\nInstead, the shape is a parallelogram (that has skew)\nHere we make an approximation by fitting a rotated rectangle to the parallelogram.\n\"\"\"\nself.tensor[:, 0] *= scale_x\nself.tensor[:, 1] *= scale_y\ntheta = self.tensor[:, 4] * math.pi / 180.0\nc = torch.cos(theta)\ns = torch.sin(theta)\n# In image space, y is top->down and x is left->right\n# Consider the local coordintate system for the rotated box,\n# where the box center is located at (0, 0), and the four vertices ABCD are\n# A(-w / 2, -h / 2), B(w / 2, -h / 2), C(w / 2, h / 2), D(-w / 2, h / 2)\n# the midpoint of the left edge AD of the rotated box E is:\n# E = (A+D)/2 = (-w / 2, 0)\n# the midpoint of the top edge AB of the rotated box F is:\n# F(0, -h / 2)\n# To get the old coordinates in the global system, apply the rotation transformation\n# (Note: the right-handed coordinate system for image space is yOx):\n# (old_x, old_y) = (s * y + c * x, c * y - s * x)\n# E(old) = (s * 0 + c * (-w/2), c * 0 - s * (-w/2)) = (-c * w / 2, s * w / 2)\n# F(old) = (s * (-h / 2) + c * 0, c * (-h / 2) - s * 0) = (-s * h / 2, -c * h / 2)\n# After applying the scaling factor (sfx, sfy):\n# E(new) = (-sfx * c * w / 2, sfy * s * w / 2)\n# F(new) = (-sfx * s * h / 2, -sfy * c * h / 2)\n# The new width after scaling tranformation becomes:\n# w(new) = |E(new) - O| * 2\n#        = sqrt[(sfx * c * w / 2)^2 + (sfy * s * w / 2)^2] * 2\n#        = sqrt[(sfx * c)^2 + (sfy * s)^2] * w\n# i.e., scale_factor_w = sqrt[(sfx * c)^2 + (sfy * s)^2]\n#\n# For example,\n# when angle = 0 or 180, |c| = 1, s = 0, scale_factor_w == scale_factor_x;\n# when |angle| = 90, c = 0, |s| = 1, scale_factor_w == scale_factor_y\nself.tensor[:, 2] *= torch.sqrt((scale_x * c) ** 2 + (scale_y * s) ** 2)\n# h(new) = |F(new) - O| * 2\n#        = sqrt[(sfx * s * h / 2)^2 + (sfy * c * h / 2)^2] * 2\n#        = sqrt[(sfx * s)^2 + (sfy * c)^2] * h\n# i.e., scale_factor_h = sqrt[(sfx * s)^2 + (sfy * c)^2]\n#\n# For example,\n# when angle = 0 or 180, |c| = 1, s = 0, scale_factor_h == scale_factor_y;\n# when |angle| = 90, c = 0, |s| = 1, scale_factor_h == scale_factor_x\nself.tensor[:, 3] *= torch.sqrt((scale_x * s) ** 2 + (scale_y * c) ** 2)\n# The angle is the rotation angle from y-axis in image space to the height\n# vector (top->down in the box's local coordinate system) of the box in CCW.\n#\n# angle(new) = angle_yOx(O - F(new))\n#            = angle_yOx( (sfx * s * h / 2, sfy * c * h / 2) )\n#            = atan2(sfx * s * h / 2, sfy * c * h / 2)\n#            = atan2(sfx * s, sfy * c)\n#\n# For example,\n# when sfx == sfy, angle(new) == atan2(s, c) == angle(old)\nself.tensor[:, 4] = torch.atan2(scale_x * s, scale_y * c) * 180 / math.pi\n@classmethod\ndef cat(cls, boxes_list: List[\"RotatedBoxes\"]) -> \"RotatedBoxes\":\n\"\"\"\nConcatenates a list of RotatedBoxes into a single RotatedBoxes\nArguments:\nboxes_list (list[RotatedBoxes])\nReturns:\nRotatedBoxes: the concatenated RotatedBoxes\n\"\"\"\nassert isinstance(boxes_list, (list, tuple))\nif len(boxes_list) == 0:\nreturn cls(torch.empty(0))\nassert all([isinstance(box, RotatedBoxes) for box in boxes_list])\n# use torch.cat (v.s. layers.cat) so the returned boxes never share storage with input\ncat_boxes = cls(torch.cat([b.tensor for b in boxes_list], dim=0))\nreturn cat_boxes\n@property\ndef device(self) -> torch.device:\nreturn self.tensor.device\n@torch.jit.unused\ndef __iter__(self):\n\"\"\"\nYield a box as a Tensor of shape (5,) at a time.\n\"\"\"\nyield from self.tensor\ndef pairwise_iou(boxes1: RotatedBoxes, boxes2: RotatedBoxes) -> None:\n\"\"\"\nGiven two lists of rotated boxes of size N and M,\ncompute the IoU (intersection over union)\nbetween **all** N x M pairs of boxes.\nThe box order must be (x_center, y_center, width, height, angle).\nArgs:\nboxes1, boxes2 (RotatedBoxes):\ntwo `RotatedBoxes`. Contains N & M rotated boxes, respectively.\nReturns:\nTensor: IoU, sized [N,M].\n\"\"\"\nreturn pairwise_iou_rotated(boxes1.tensor, boxes2.tensor)",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 191
                },
                {
                        "id": "pretrain_python_data_6818280",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\nimport math\nfrom typing import Any, Iterator, Tuple, Union\nimport torch\nfrom detectron2.layers.rotated_boxes import pairwise_iou_rotated\nfrom .boxes import Boxes\nclass RotatedBoxes(Boxes):\n\"\"\"\nThis structure stores a list of rotated boxes as a Nx5 torch.Tensor.\nIt supports some common methods about boxes\n(`area`, `clip`, `nonempty`, etc),\nand also behaves like a Tensor\n(support indexing, `to(device)`, `.device`, and iteration over all boxes)\n\"\"\"\ndef __init__(self, tensor: torch.Tensor):\n\"\"\"\nArgs:\ntensor (Tensor[float]): a Nx5 matrix.  Each row is\n(x_center, y_center, width, height, angle),\nin which angle is represented in degrees.\nWhile there's no strict range restriction for it,\nthe recommended principal range is between [-180, 180) degrees.\nAssume we have a horizontal box B = (x_center, y_center, width, height),\nwhere width is along the x-axis and height is along the y-axis.\nThe rotated box B_rot (x_center, y_center, width, height, angle)\ncan be seen as:\n1. When angle == 0:\nB_rot == B\n2. When angle > 0:\nB_rot is obtained by rotating B w.r.t its center by :math:`|angle|` degrees CCW;\n3. When angle < 0:\nB_rot is obtained by rotating B w.r.t its center by :math:`|angle|` degrees CW.\nMathematically, since the right-handed coordinate system for image space\nis (y, x), where y is top->down and x is left->right, the 4 vertices of the\nrotated rectangle :math:`(yr_i, xr_i)` (i = 1, 2, 3, 4) can be obtained from\nthe vertices of the horizontal rectangle (y_i, x_i) (i = 1, 2, 3, 4)\nin the following way (:math:`\\\\theta = angle*\\\\pi/180` is the angle in radians,\n(y_c, x_c) is the center of the rectangle):\n.. math::\nyr_i = \\\\cos(\\\\theta) (y_i - y_c) - \\\\sin(\\\\theta) (x_i - x_c) + y_c,\nxr_i = \\\\sin(\\\\theta) (y_i - y_c) + \\\\cos(\\\\theta) (x_i - x_c) + x_c,\nwhich is the standard rigid-body rotation transformation.\nIntuitively, the angle is\n(1) the rotation angle from y-axis in image space\nto the height vector (top->down in the box's local coordinate system)\nof the box in CCW, and\n(2) the rotation angle from x-axis in image space\nto the width vector (left->right in the box's local coordinate system)\nof the box in CCW.\nMore intuitively, consider the following horizontal box ABCD represented\nin (x1, y1, x2, y2): (3, 2, 7, 4),\ncovering the [3, 7] x [2, 4] region of the continuous coordinate system\nwhich looks like this:\n.. code:: none\nO--------> x\n|\n|  A---B\n|  |   |\n|  D---C\n|\nv y\nNote that each capital letter represents one 0-dimensional geometric point\ninstead of a 'square pixel' here.\nIn the example above, using (x, y) to represent a point we have:\n.. math::\nO = (0, 0), A = (3, 2), B = (7, 2), C = (7, 4), D = (3, 4)\nWe name vector AB = vector DC as the width vector in box's local coordinate system, and\nvector AD = vector BC as the height vector in box's local coordinate system. Initially,\nwhen angle = 0 degree, they're aligned with the positive directions of x-axis and y-axis\nin the image space, respectively.\nFor better illustration, we denote the center of the box as E,\n.. code:: none\nO--------> x\n|\n|  A---B\n|  | E |\n|  D---C\n|\nv y\nwhere the center E = ((3+7)/2, (2+4)/2) = (5, 3).\nAlso,\n.. math::\nwidth = |AB| = |CD| = 7 - 3 = 4,\nheight = |AD| = |BC| = 4 - 2 = 2.\nTherefore, the corresponding representation for the same shape in rotated box in\n(x_center, y_center, width, height, angle) format is:\n(5, 3, 4, 2, 0),\nNow, let's consider (5, 3, 4, 2, 90), which is rotated by 90 degrees\nCCW (counter-clockwise) by definition. It looks like this:\n.. code:: none\nO--------> x\n|   B-C\n|   | |\n|   |E|\n|   | |\n|   A-D\nv y\nThe center E is still located at the same point (5, 3), while the vertices\nABCD are rotated by 90 degrees CCW with regard to E:\nA = (4, 5), B = (4, 1), C = (6, 1), D = (6, 5)\nHere, 90 degrees can be seen as the CCW angle to rotate from y-axis to\nvector AD or vector BC (the top->down height vector in box's local coordinate system),\nor the CCW angle to rotate from x-axis to vector AB or vector DC (the left->right\nwidth vector in box's local coordinate system).\n.. math::\nwidth = |AB| = |CD| = 5 - 1 = 4,\nheight = |AD| = |BC| = 6 - 4 = 2.\nNext, how about (5, 3, 4, 2, -90), which is rotated by 90 degrees CW (clockwise)\nby definition? It looks like this:\n.. code:: none\nO--------> x\n|   D-A\n|   | |\n|   |E|\n|   | |\n|   C-B\nv y\nThe center E is still located at the same point (5, 3), while the vertices\nABCD are rotated by 90 degrees CW with regard to E:\nA = (6, 1), B = (6, 5), C = (4, 5), D = (4, 1)\n.. math::\nwidth = |AB| = |CD| = 5 - 1 = 4,\nheight = |AD| = |BC| = 6 - 4 = 2.\nThis covers exactly the same region as (5, 3, 4, 2, 90) does, and their IoU\nwill be 1. However, these two will generate different RoI Pooling results and\nshould not be treated as an identical box.\nOn the other hand, it's easy to see that (X, Y, W, H, A) is identical to\n(X, Y, W, H, A+360N), for any integer N. For example (5, 3, 4, 2, 270) would be\nidentical to (5, 3, 4, 2, -90), because rotating the shape 270 degrees CCW is\nequivalent to rotating the same shape 90 degrees CW.\nWe could rotate further to get (5, 3, 4, 2, 180), or (5, 3, 4, 2, -180):\n.. code:: none\nO--------> x\n|\n|  C---D\n|  | E |\n|  B---A\n|\nv y\n.. math::\nA = (7, 4), B = (3, 4), C = (3, 2), D = (7, 2),\nwidth = |AB| = |CD| = 7 - 3 = 4,\nheight = |AD| = |BC| = 4 - 2 = 2.\nFinally, this is a very inaccurate (heavily quantized) illustration of\nhow (5, 3, 4, 2, 60) looks like in case anyone wonders:\n.. code:: none\nO--------> x\n|     B\\\n|    /  C\n|   /E /\n|  A  /\n|   `D\nv y\nIt's still a rectangle with center of (5, 3), width of 4 and height of 2,\nbut its angle (and thus orientation) is somewhere between\n(5, 3, 4, 2, 0) and (5, 3, 4, 2, 90).\n\"\"\"\ndevice = tensor.device if isinstance(tensor, torch.Tensor) else torch.device(\"cpu\")\ntensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\nif tensor.numel() == 0:\n# Use reshape, so we don't end up creating a new tensor that does not depend on\n# the inputs (and consequently confuses jit)\ntensor = tensor.reshape((0, 5)).to(dtype=torch.float32, device=device)\nassert tensor.dim() == 2 and tensor.size(-1) == 5, tensor.size()\nself.tensor = tensor\ndef clone(self) -> \"RotatedBoxes\":\n\"\"\"\nClone the RotatedBoxes.\nReturns:\nRotatedBoxes\n\"\"\"\nreturn RotatedBoxes(self.tensor.clone())\ndef to(self, *args: Any, **kwargs: Any) -> \"RotatedBoxes\":\nreturn RotatedBoxes(self.tensor.to(*args, **kwargs))\ndef area(self) -> torch.Tensor:\n\"\"\"\nComputes the area of all the boxes.\nReturns:\ntorch.Tensor: a vector with areas of each box.\n\"\"\"\nbox = self.tensor\narea = box[:, 2] * box[:, 3]\nreturn area\ndef normalize_angles(self) -> None:\n\"\"\"\nRestrict angles to the range of [-180, 180) degrees\n\"\"\"\nself.tensor[:, 4] = (self.tensor[:, 4] + 180.0) % 360.0 - 180.0\ndef clip(self, box_size: Tuple[int, int], clip_angle_threshold: float = 1.0) -> None:\n\"\"\"\nClip (in place) the boxes by limiting x coordinates to the range [0, width]\nand y coordinates to the range [0, height].\nFor RRPN:\nOnly clip boxes that are almost horizontal with a tolerance of\nclip_angle_threshold to maintain backward compatibility.\nRotated boxes beyond this threshold are not clipped for two reasons:\n1. There are potentially multiple ways to clip a rotated box to make it\nfit within the image.\n2. It's tricky to make the entire rectangular box fit within the image\nand still be able to not leave out pixels of interest.\nTherefore we rely on ops like RoIAlignRotated to safely handle this.\nArgs:\nbox_size (height, width): The clipping box's size.\nclip_angle_threshold:\nIff. abs(normalized(angle)) <= clip_angle_threshold (in degrees),\nwe do the clipping as horizontal boxes.\n\"\"\"\nh, w = box_size\n# normalize angles to be within (-180, 180] degrees\nself.normalize_angles()\nidx = torch.where(torch.abs(self.tensor[:, 4]) <= clip_angle_threshold)[0]\n# convert to (x1, y1, x2, y2)\nx1 = self.tensor[idx, 0] - self.tensor[idx, 2] / 2.0\ny1 = self.tensor[idx, 1] - self.tensor[idx, 3] / 2.0\nx2 = self.tensor[idx, 0] + self.tensor[idx, 2] / 2.0\ny2 = self.tensor[idx, 1] + self.tensor[idx, 3] / 2.0\n# clip\nx1.clamp_(min=0, max=w)\ny1.clamp_(min=0, max=h)\nx2.clamp_(min=0, max=w)\ny2.clamp_(min=0, max=h)\n# convert back to (xc, yc, w, h)\nself.tensor[idx, 0] = (x1 + x2) / 2.0\nself.tensor[idx, 1] = (y1 + y2) / 2.0\n# make sure widths and heights do not increase due to numerical errors\nself.tensor[idx, 2] = torch.min(self.tensor[idx, 2], x2 - x1)\nself.tensor[idx, 3] = torch.min(self.tensor[idx, 3], y2 - y1)\ndef nonempty(self, threshold: float = 0.0) -> torch.Tensor:\n\"\"\"\nFind boxes that are non-empty.\nA box is considered empty, if either of its side is no larger than threshold.\nReturns:\nTensor: a binary vector which represents\nwhether each box is empty (False) or non-empty (True).\n\"\"\"\nbox = self.tensor\nwidths = box[:, 2]\nheights = box[:, 3]\nkeep = (widths > threshold) & (heights > threshold)\nreturn keep\ndef __getitem__(self, item: Union[int, slice, torch.BoolTensor]) -> \"RotatedBoxes\":\n\"\"\"\nReturns:\nRotatedBoxes: Create a new :class:`RotatedBoxes` by indexing.\nThe following usage are allowed:\n1. `new_boxes = boxes[3]`: return a `RotatedBoxes` which contains only one box.\n2. `new_boxes = boxes[2:10]`: return a slice of boxes.\n3. `new_boxes = boxes[vector]`, where vector is a torch.ByteTensor\nwith `length = len(boxes)`. Nonzero elements in the vector will be selected.\nNote that the returned RotatedBoxes might share storage with this RotatedBoxes,\nsubject to Pytorch's indexing semantics.\n\"\"\"\nif isinstance(item, int):\nreturn RotatedBoxes(self.tensor[item].view(1, -1))\nb = self.tensor[item]\nassert b.dim() == 2, \"Indexing on RotatedBoxes with {} failed to return a matrix!\".format(\nitem\n)\nreturn RotatedBoxes(b)\ndef __len__(self) -> int:\nreturn self.tensor.shape[0]\ndef __repr__(self) -> str:\nreturn \"RotatedBoxes(\" + str(self.tensor) + \")\"\ndef inside_box(self, box_size: Tuple[int, int], boundary_threshold: int = 0) -> torch.Tensor:\n\"\"\"\nArgs:\nbox_size (height, width): Size of the reference box covering\n[0, width] x [0, height]\nboundary_threshold (int): Boxes that extend beyond the reference box\nboundary by more than boundary_threshold are considered \"outside\".\nFor RRPN, it might not be necessary to call this function since it's common\nfor rotated box to extend to outside of the image boundaries\n(the clip function only clips the near-horizontal boxes)\nReturns:\na binary vector, indicating whether each box is inside the reference box.\n\"\"\"\nheight, width = box_size\ncnt_x = self.tensor[..., 0]\ncnt_y = self.tensor[..., 1]\nhalf_w = self.tensor[..., 2] / 2.0\nhalf_h = self.tensor[..., 3] / 2.0\na = self.tensor[..., 4]\nc = torch.abs(torch.cos(a * math.pi / 180.0))\ns = torch.abs(torch.sin(a * math.pi / 180.0))\n# This basically computes the horizontal bounding rectangle of the rotated box\nmax_rect_dx = c * half_w + s * half_h\nmax_rect_dy = c * half_h + s * half_w\ninds_inside = (\n(cnt_x - max_rect_dx >= -boundary_threshold)\n& (cnt_y - max_rect_dy >= -boundary_threshold)\n& (cnt_x + max_rect_dx < width + boundary_threshold)\n& (cnt_y + max_rect_dy < height + boundary_threshold)\n)\nreturn inds_inside\ndef get_centers(self) -> torch.Tensor:\n\"\"\"\nReturns:\nThe box centers in a Nx2 array of (x, y).\n\"\"\"\nreturn self.tensor[:, :2]\ndef scale(self, scale_x: float, scale_y: float) -> None:\n\"\"\"\nScale the rotated box with horizontal and vertical scaling factors\nNote: when scale_factor_x != scale_factor_y,\nthe rotated box does not preserve the rectangular shape when the angle\nis not a multiple of 90 degrees under resize transformation.\nInstead, the shape is a parallelogram (that has skew)\nHere we make an approximation by fitting a rotated rectangle to the parallelogram.\n\"\"\"\nself.tensor[:, 0] *= scale_x\nself.tensor[:, 1] *= scale_y\ntheta = self.tensor[:, 4] * math.pi / 180.0\nc = torch.cos(theta)\ns = torch.sin(theta)\n# In image space, y is top->down and x is left->right\n# Consider the local coordintate system for the rotated box,\n# where the box center is located at (0, 0), and the four vertices ABCD are\n# A(-w / 2, -h / 2), B(w / 2, -h / 2), C(w / 2, h / 2), D(-w / 2, h / 2)\n# the midpoint of the left edge AD of the rotated box E is:\n# E = (A+D)/2 = (-w / 2, 0)\n# the midpoint of the top edge AB of the rotated box F is:\n# F(0, -h / 2)\n# To get the old coordinates in the global system, apply the rotation transformation\n# (Note: the right-handed coordinate system for image space is yOx):\n# (old_x, old_y) = (s * y + c * x, c * y - s * x)\n# E(old) = (s * 0 + c * (-w/2), c * 0 - s * (-w/2)) = (-c * w / 2, s * w / 2)\n# F(old) = (s * (-h / 2) + c * 0, c * (-h / 2) - s * 0) = (-s * h / 2, -c * h / 2)\n# After applying the scaling factor (sfx, sfy):\n# E(new) = (-sfx * c * w / 2, sfy * s * w / 2)\n# F(new) = (-sfx * s * h / 2, -sfy * c * h / 2)\n# The new width after scaling tranformation becomes:\n# w(new) = |E(new) - O| * 2\n#        = sqrt[(sfx * c * w / 2)^2 + (sfy * s * w / 2)^2] * 2\n#        = sqrt[(sfx * c)^2 + (sfy * s)^2] * w\n# i.e., scale_factor_w = sqrt[(sfx * c)^2 + (sfy * s)^2]\n#\n# For example,\n# when angle = 0 or 180, |c| = 1, s = 0, scale_factor_w == scale_factor_x;\n# when |angle| = 90, c = 0, |s| = 1, scale_factor_w == scale_factor_y\nself.tensor[:, 2] *= torch.sqrt((scale_x * c) ** 2 + (scale_y * s) ** 2)\n# h(new) = |F(new) - O| * 2\n#        = sqrt[(sfx * s * h / 2)^2 + (sfy * c * h / 2)^2] * 2\n#        = sqrt[(sfx * s)^2 + (sfy * c)^2] * h\n# i.e., scale_factor_h = sqrt[(sfx * s)^2 + (sfy * c)^2]\n#\n# For example,\n# when angle = 0 or 180, |c| = 1, s = 0, scale_factor_h == scale_factor_y;\n# when |angle| = 90, c = 0, |s| = 1, scale_factor_h == scale_factor_x\nself.tensor[:, 3] *= torch.sqrt((scale_x * s) ** 2 + (scale_y * c) ** 2)\n# The angle is the rotation angle from y-axis in image space to the height\n# vector (top->down in the box's local coordinate system) of the box in CCW.\n#\n# angle(new) = angle_yOx(O - F(new))\n#            = angle_yOx( (sfx * s * h / 2, sfy * c * h / 2) )\n#            = atan2(sfx * s * h / 2, sfy * c * h / 2)\n#            = atan2(sfx * s, sfy * c)\n#\n# For example,\n# when sfx == sfy, angle(new) == atan2(s, c) == angle(old)\nself.tensor[:, 4] = torch.atan2(scale_x * s, scale_y * c) * 180 / math.pi\n@property\ndef device(self) -> str:\nreturn self.tensor.device\ndef __iter__(self) -> Iterator[torch.Tensor]:\n\"\"\"\nYield a box as a Tensor of shape (5,) at a time.\n\"\"\"\nyield from self.tensor\ndef pairwise_iou(boxes1: RotatedBoxes, boxes2: RotatedBoxes) -> None:\n\"\"\"\nGiven two lists of rotated boxes of size N and M,\ncompute the IoU (intersection over union)\nbetween __all__ N x M pairs of boxes.\nThe box order must be (x_center, y_center, width, height, angle).\nArgs:\nboxes1, boxes2 (RotatedBoxes):\ntwo `RotatedBoxes`. Contains N & M rotated boxes, respectively.\nReturns:\nTensor: IoU, sized [N,M].\n\"\"\"\nreturn pairwise_iou_rotated(boxes1.tensor, boxes2.tensor)",
                        "max_stars_repo_path": "detectron2/structures/rotated_boxes.py",
                        "max_stars_repo_name": "PedroUria/detectron2",
                        "max_stars_count": 780,
                        "__cluster__": 191
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "test_evocodebench_data_192",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nimport math\nfrom typing import List, Tuple\nimport torch\nfrom detectron2.layers.rotated_boxes import pairwise_iou_rotated\nfrom .boxes import Boxes\nclass RotatedBoxes(Boxes):\n\"\"\"\nThis structure stores a list of rotated boxes as a Nx5 torch.Tensor.\nIt supports some common methods about boxes\n(`area`, `clip`, `nonempty`, etc),\nand also behaves like a Tensor\n(support indexing, `to(device)`, `.device`, and iteration over all boxes)\n\"\"\"\ndef __init__(self, tensor: torch.Tensor):\n\"\"\"\nArgs:\ntensor (Tensor[float]): a Nx5 matrix.  Each row is\n(x_center, y_center, width, height, angle),\nin which angle is represented in degrees.\nWhile there's no strict range restriction for it,\nthe recommended principal range is between [-180, 180) degrees.\nAssume we have a horizontal box B = (x_center, y_center, width, height),\nwhere width is along the x-axis and height is along the y-axis.\nThe rotated box B_rot (x_center, y_center, width, height, angle)\ncan be seen as:\n1. When angle == 0:\nB_rot == B\n2. When angle > 0:\nB_rot is obtained by rotating B w.r.t its center by :math:`|angle|` degrees CCW;\n3. When angle < 0:\nB_rot is obtained by rotating B w.r.t its center by :math:`|angle|` degrees CW.\nMathematically, since the right-handed coordinate system for image space\nis (y, x), where y is top->down and x is left->right, the 4 vertices of the\nrotated rectangle :math:`(yr_i, xr_i)` (i = 1, 2, 3, 4) can be obtained from\nthe vertices of the horizontal rectangle :math:`(y_i, x_i)` (i = 1, 2, 3, 4)\nin the following way (:math:`\\\\theta = angle*\\\\pi/180` is the angle in radians,\n:math:`(y_c, x_c)` is the center of the rectangle):\n.. math::\nyr_i = \\\\cos(\\\\theta) (y_i - y_c) - \\\\sin(\\\\theta) (x_i - x_c) + y_c,\nxr_i = \\\\sin(\\\\theta) (y_i - y_c) + \\\\cos(\\\\theta) (x_i - x_c) + x_c,\nwhich is the standard rigid-body rotation transformation.\nIntuitively, the angle is\n(1) the rotation angle from y-axis in image space\nto the height vector (top->down in the box's local coordinate system)\nof the box in CCW, and\n(2) the rotation angle from x-axis in image space\nto the width vector (left->right in the box's local coordinate system)\nof the box in CCW.\nMore intuitively, consider the following horizontal box ABCD represented\nin (x1, y1, x2, y2): (3, 2, 7, 4),\ncovering the [3, 7] x [2, 4] region of the continuous coordinate system\nwhich looks like this:\n.. code:: none\nO--------> x\n|\n|  A---B\n|  |   |\n|  D---C\n|\nv y\nNote that each capital letter represents one 0-dimensional geometric point\ninstead of a 'square pixel' here.\nIn the example above, using (x, y) to represent a point we have:\n.. math::\nO = (0, 0), A = (3, 2), B = (7, 2), C = (7, 4), D = (3, 4)\nWe name vector AB = vector DC as the width vector in box's local coordinate system, and\nvector AD = vector BC as the height vector in box's local coordinate system. Initially,\nwhen angle = 0 degree, they're aligned with the positive directions of x-axis and y-axis\nin the image space, respectively.\nFor better illustration, we denote the center of the box as E,\n.. code:: none\nO--------> x\n|\n|  A---B\n|  | E |\n|  D---C\n|\nv y\nwhere the center E = ((3+7)/2, (2+4)/2) = (5, 3).\nAlso,\n.. math::\nwidth = |AB| = |CD| = 7 - 3 = 4,\nheight = |AD| = |BC| = 4 - 2 = 2.\nTherefore, the corresponding representation for the same shape in rotated box in\n(x_center, y_center, width, height, angle) format is:\n(5, 3, 4, 2, 0),\nNow, let's consider (5, 3, 4, 2, 90), which is rotated by 90 degrees\nCCW (counter-clockwise) by definition. It looks like this:\n.. code:: none\nO--------> x\n|   B-C\n|   | |\n|   |E|\n|   | |\n|   A-D\nv y\nThe center E is still located at the same point (5, 3), while the vertices\nABCD are rotated by 90 degrees CCW with regard to E:\nA = (4, 5), B = (4, 1), C = (6, 1), D = (6, 5)\nHere, 90 degrees can be seen as the CCW angle to rotate from y-axis to\nvector AD or vector BC (the top->down height vector in box's local coordinate system),\nor the CCW angle to rotate from x-axis to vector AB or vector DC (the left->right\nwidth vector in box's local coordinate system).\n.. math::\nwidth = |AB| = |CD| = 5 - 1 = 4,\nheight = |AD| = |BC| = 6 - 4 = 2.\nNext, how about (5, 3, 4, 2, -90), which is rotated by 90 degrees CW (clockwise)\nby definition? It looks like this:\n.. code:: none\nO--------> x\n|   D-A\n|   | |\n|   |E|\n|   | |\n|   C-B\nv y\nThe center E is still located at the same point (5, 3), while the vertices\nABCD are rotated by 90 degrees CW with regard to E:\nA = (6, 1), B = (6, 5), C = (4, 5), D = (4, 1)\n.. math::\nwidth = |AB| = |CD| = 5 - 1 = 4,\nheight = |AD| = |BC| = 6 - 4 = 2.\nThis covers exactly the same region as (5, 3, 4, 2, 90) does, and their IoU\nwill be 1. However, these two will generate different RoI Pooling results and\nshould not be treated as an identical box.\nOn the other hand, it's easy to see that (X, Y, W, H, A) is identical to\n(X, Y, W, H, A+360N), for any integer N. For example (5, 3, 4, 2, 270) would be\nidentical to (5, 3, 4, 2, -90), because rotating the shape 270 degrees CCW is\nequivalent to rotating the same shape 90 degrees CW.\nWe could rotate further to get (5, 3, 4, 2, 180), or (5, 3, 4, 2, -180):\n.. code:: none\nO--------> x\n|\n|  C---D\n|  | E |\n|  B---A\n|\nv y\n.. math::\nA = (7, 4), B = (3, 4), C = (3, 2), D = (7, 2),\nwidth = |AB| = |CD| = 7 - 3 = 4,\nheight = |AD| = |BC| = 4 - 2 = 2.\nFinally, this is a very inaccurate (heavily quantized) illustration of\nhow (5, 3, 4, 2, 60) looks like in case anyone wonders:\n.. code:: none\nO--------> x\n|     B\\\n|    /  C\n|   /E /\n|  A  /\n|   `D\nv y\nIt's still a rectangle with center of (5, 3), width of 4 and height of 2,\nbut its angle (and thus orientation) is somewhere between\n(5, 3, 4, 2, 0) and (5, 3, 4, 2, 90).\n\"\"\"\ndevice = tensor.device if isinstance(tensor, torch.Tensor) else torch.device(\"cpu\")\ntensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\nif tensor.numel() == 0:\n# Use reshape, so we don't end up creating a new tensor that does not depend on\n# the inputs (and consequently confuses jit)\ntensor = tensor.reshape((0, 5)).to(dtype=torch.float32, device=device)\nassert tensor.dim() == 2 and tensor.size(-1) == 5, tensor.size()\nself.tensor = tensor\ndef clone(self) -> \"RotatedBoxes\":\n\"\"\"\nClone the RotatedBoxes.\nReturns:\nRotatedBoxes\n\"\"\"\nreturn RotatedBoxes(self.tensor.clone())\ndef to(self, device: torch.device):\n# Boxes are assumed float32 and does not support to(dtype)\nreturn RotatedBoxes(self.tensor.to(device=device))\ndef area(self) -> torch.Tensor:\n\"\"\"\nComputes the area of all the boxes.\nReturns:\ntorch.Tensor: a vector with areas of each box.\n\"\"\"\nbox = self.tensor\narea = box[:, 2] * box[:, 3]\nreturn area\ndef normalize_angles(self) -> None:\n\"\"\"\nRestrict angles to the range of [-180, 180) degrees\n\"\"\"\nself.tensor[:, 4] = (self.tensor[:, 4] + 180.0) % 360.0 - 180.0\ndef clip(self, box_size: Tuple[int, int], clip_angle_threshold: float = 1.0) -> None:\n\"\"\"\nClip (in place) the boxes by limiting x coordinates to the range [0, width]\nand y coordinates to the range [0, height].\nFor RRPN:\nOnly clip boxes that are almost horizontal with a tolerance of\nclip_angle_threshold to maintain backward compatibility.\nRotated boxes beyond this threshold are not clipped for two reasons:\n1. There are potentially multiple ways to clip a rotated box to make it\nfit within the image.\n2. It's tricky to make the entire rectangular box fit within the image\nand still be able to not leave out pixels of interest.\nTherefore we rely on ops like RoIAlignRotated to safely handle this.\nArgs:\nbox_size (height, width): The clipping box's size.\nclip_angle_threshold:\nIff. abs(normalized(angle)) <= clip_angle_threshold (in degrees),\nwe do the clipping as horizontal boxes.\n\"\"\"\nh, w = box_size\n# normalize angles to be within (-180, 180] degrees\nself.normalize_angles()\nidx = torch.where(torch.abs(self.tensor[:, 4]) <= clip_angle_threshold)[0]\n# convert to (x1, y1, x2, y2)\nx1 = self.tensor[idx, 0] - self.tensor[idx, 2] / 2.0\ny1 = self.tensor[idx, 1] - self.tensor[idx, 3] / 2.0\nx2 = self.tensor[idx, 0] + self.tensor[idx, 2] / 2.0\ny2 = self.tensor[idx, 1] + self.tensor[idx, 3] / 2.0\n# clip\nx1.clamp_(min=0, max=w)\ny1.clamp_(min=0, max=h)\nx2.clamp_(min=0, max=w)\ny2.clamp_(min=0, max=h)\n# convert back to (xc, yc, w, h)\nself.tensor[idx, 0] = (x1 + x2) / 2.0\nself.tensor[idx, 1] = (y1 + y2) / 2.0\n# make sure widths and heights do not increase due to numerical errors\nself.tensor[idx, 2] = torch.min(self.tensor[idx, 2], x2 - x1)\nself.tensor[idx, 3] = torch.min(self.tensor[idx, 3], y2 - y1)\ndef nonempty(self, threshold: float = 0.0) -> torch.Tensor:\n\"\"\"\nFind boxes that are non-empty.\nA box is considered empty, if either of its side is no larger than threshold.\nReturns:\nTensor: a binary vector which represents\nwhether each box is empty (False) or non-empty (True).\n\"\"\"\nbox = self.tensor\nwidths = box[:, 2]\nheights = box[:, 3]\nkeep = (widths > threshold) & (heights > threshold)\nreturn keep\ndef __getitem__(self, item) -> \"RotatedBoxes\":\n\"\"\"\nReturns:\nRotatedBoxes: Create a new :class:`RotatedBoxes` by indexing.\nThe following usage are allowed:\n1. `new_boxes = boxes[3]`: return a `RotatedBoxes` which contains only one box.\n2. `new_boxes = boxes[2:10]`: return a slice of boxes.\n3. `new_boxes = boxes[vector]`, where vector is a torch.ByteTensor\nwith `length = len(boxes)`. Nonzero elements in the vector will be selected.\nNote that the returned RotatedBoxes might share storage with this RotatedBoxes,\nsubject to Pytorch's indexing semantics.\n\"\"\"\nif isinstance(item, int):\nreturn RotatedBoxes(self.tensor[item].view(1, -1))\nb = self.tensor[item]\nassert b.dim() == 2, \"Indexing on RotatedBoxes with {} failed to return a matrix!\".format(\nitem\n)\nreturn RotatedBoxes(b)\ndef __len__(self) -> int:\nreturn self.tensor.shape[0]\ndef __repr__(self) -> str:\nreturn \"RotatedBoxes(\" + str(self.tensor) + \")\"\ndef inside_box(self, box_size: Tuple[int, int], boundary_threshold: int = 0) -> torch.Tensor:\n\"\"\"\nArgs:\nbox_size (height, width): Size of the reference box covering\n[0, width] x [0, height]\nboundary_threshold (int): Boxes that extend beyond the reference box\nboundary by more than boundary_threshold are considered \"outside\".\nFor RRPN, it might not be necessary to call this function since it's common\nfor rotated box to extend to outside of the image boundaries\n(the clip function only clips the near-horizontal boxes)\nReturns:\na binary vector, indicating whether each box is inside the reference box.\n\"\"\"\nheight, width = box_size\ncnt_x = self.tensor[..., 0]\ncnt_y = self.tensor[..., 1]\nhalf_w = self.tensor[..., 2] / 2.0\nhalf_h = self.tensor[..., 3] / 2.0\na = self.tensor[..., 4]\nc = torch.abs(torch.cos(a * math.pi / 180.0))\ns = torch.abs(torch.sin(a * math.pi / 180.0))\n# This basically computes the horizontal bounding rectangle of the rotated box\nmax_rect_dx = c * half_w + s * half_h\nmax_rect_dy = c * half_h + s * half_w\ninds_inside = (\n(cnt_x - max_rect_dx >= -boundary_threshold)\n& (cnt_y - max_rect_dy >= -boundary_threshold)\n& (cnt_x + max_rect_dx < width + boundary_threshold)\n& (cnt_y + max_rect_dy < height + boundary_threshold)\n)\nreturn inds_inside\ndef get_centers(self) -> torch.Tensor:\n\"\"\"\nReturns:\nThe box centers in a Nx2 array of (x, y).\n\"\"\"\nreturn self.tensor[:, :2]\ndef scale(self, scale_x: float, scale_y: float) -> None:\n\"\"\"\nScale the rotated box with horizontal and vertical scaling factors\nNote: when scale_factor_x != scale_factor_y,\nthe rotated box does not preserve the rectangular shape when the angle\nis not a multiple of 90 degrees under resize transformation.\nInstead, the shape is a parallelogram (that has skew)\nHere we make an approximation by fitting a rotated rectangle to the parallelogram.\n\"\"\"\nself.tensor[:, 0] *= scale_x\nself.tensor[:, 1] *= scale_y\ntheta = self.tensor[:, 4] * math.pi / 180.0\nc = torch.cos(theta)\ns = torch.sin(theta)\n# In image space, y is top->down and x is left->right\n# Consider the local coordintate system for the rotated box,\n# where the box center is located at (0, 0), and the four vertices ABCD are\n# A(-w / 2, -h / 2), B(w / 2, -h / 2), C(w / 2, h / 2), D(-w / 2, h / 2)\n# the midpoint of the left edge AD of the rotated box E is:\n# E = (A+D)/2 = (-w / 2, 0)\n# the midpoint of the top edge AB of the rotated box F is:\n# F(0, -h / 2)\n# To get the old coordinates in the global system, apply the rotation transformation\n# (Note: the right-handed coordinate system for image space is yOx):\n# (old_x, old_y) = (s * y + c * x, c * y - s * x)\n# E(old) = (s * 0 + c * (-w/2), c * 0 - s * (-w/2)) = (-c * w / 2, s * w / 2)\n# F(old) = (s * (-h / 2) + c * 0, c * (-h / 2) - s * 0) = (-s * h / 2, -c * h / 2)\n# After applying the scaling factor (sfx, sfy):\n# E(new) = (-sfx * c * w / 2, sfy * s * w / 2)\n# F(new) = (-sfx * s * h / 2, -sfy * c * h / 2)\n# The new width after scaling tranformation becomes:\n# w(new) = |E(new) - O| * 2\n#        = sqrt[(sfx * c * w / 2)^2 + (sfy * s * w / 2)^2] * 2\n#        = sqrt[(sfx * c)^2 + (sfy * s)^2] * w\n# i.e., scale_factor_w = sqrt[(sfx * c)^2 + (sfy * s)^2]\n#\n# For example,\n# when angle = 0 or 180, |c| = 1, s = 0, scale_factor_w == scale_factor_x;\n# when |angle| = 90, c = 0, |s| = 1, scale_factor_w == scale_factor_y\nself.tensor[:, 2] *= torch.sqrt((scale_x * c) ** 2 + (scale_y * s) ** 2)\n# h(new) = |F(new) - O| * 2\n#        = sqrt[(sfx * s * h / 2)^2 + (sfy * c * h / 2)^2] * 2\n#        = sqrt[(sfx * s)^2 + (sfy * c)^2] * h\n# i.e., scale_factor_h = sqrt[(sfx * s)^2 + (sfy * c)^2]\n#\n# For example,\n# when angle = 0 or 180, |c| = 1, s = 0, scale_factor_h == scale_factor_y;\n# when |angle| = 90, c = 0, |s| = 1, scale_factor_h == scale_factor_x\nself.tensor[:, 3] *= torch.sqrt((scale_x * s) ** 2 + (scale_y * c) ** 2)\n# The angle is the rotation angle from y-axis in image space to the height\n# vector (top->down in the box's local coordinate system) of the box in CCW.\n#\n# angle(new) = angle_yOx(O - F(new))\n#            = angle_yOx( (sfx * s * h / 2, sfy * c * h / 2) )\n#            = atan2(sfx * s * h / 2, sfy * c * h / 2)\n#            = atan2(sfx * s, sfy * c)\n#\n# For example,\n# when sfx == sfy, angle(new) == atan2(s, c) == angle(old)\nself.tensor[:, 4] = torch.atan2(scale_x * s, scale_y * c) * 180 / math.pi\n@classmethod\ndef cat(cls, boxes_list: List[\"RotatedBoxes\"]) -> \"RotatedBoxes\":\n\"\"\"\nConcatenates a list of RotatedBoxes into a single RotatedBoxes\nArguments:\nboxes_list (list[RotatedBoxes])\nReturns:\nRotatedBoxes: the concatenated RotatedBoxes\n\"\"\"\nassert isinstance(boxes_list, (list, tuple))\nif len(boxes_list) == 0:\nreturn cls(torch.empty(0))\nassert all([isinstance(box, RotatedBoxes) for box in boxes_list])\n# use torch.cat (v.s. layers.cat) so the returned boxes never share storage with input\ncat_boxes = cls(torch.cat([b.tensor for b in boxes_list], dim=0))\nreturn cat_boxes\n@property\ndef device(self) -> torch.device:\nreturn self.tensor.device\n@torch.jit.unused\ndef __iter__(self):\n\"\"\"\nYield a box as a Tensor of shape (5,) at a time.\n\"\"\"\nyield from self.tensor\ndef pairwise_iou(boxes1: RotatedBoxes, boxes2: RotatedBoxes) -> None:\n\"\"\"\nGiven two lists of rotated boxes of size N and M,\ncompute the IoU (intersection over union)\nbetween **all** N x M pairs of boxes.\nThe box order must be (x_center, y_center, width, height, angle).\nArgs:\nboxes1, boxes2 (RotatedBoxes):\ntwo `RotatedBoxes`. Contains N & M rotated boxes, respectively.\nReturns:\nTensor: IoU, sized [N,M].\n\"\"\"\nreturn pairwise_iou_rotated(boxes1.tensor, boxes2.tensor)",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 191
                },
                {
                        "id": "pretrain_python_data_6818280",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\nimport math\nfrom typing import Any, Iterator, Tuple, Union\nimport torch\nfrom detectron2.layers.rotated_boxes import pairwise_iou_rotated\nfrom .boxes import Boxes\nclass RotatedBoxes(Boxes):\n\"\"\"\nThis structure stores a list of rotated boxes as a Nx5 torch.Tensor.\nIt supports some common methods about boxes\n(`area`, `clip`, `nonempty`, etc),\nand also behaves like a Tensor\n(support indexing, `to(device)`, `.device`, and iteration over all boxes)\n\"\"\"\ndef __init__(self, tensor: torch.Tensor):\n\"\"\"\nArgs:\ntensor (Tensor[float]): a Nx5 matrix.  Each row is\n(x_center, y_center, width, height, angle),\nin which angle is represented in degrees.\nWhile there's no strict range restriction for it,\nthe recommended principal range is between [-180, 180) degrees.\nAssume we have a horizontal box B = (x_center, y_center, width, height),\nwhere width is along the x-axis and height is along the y-axis.\nThe rotated box B_rot (x_center, y_center, width, height, angle)\ncan be seen as:\n1. When angle == 0:\nB_rot == B\n2. When angle > 0:\nB_rot is obtained by rotating B w.r.t its center by :math:`|angle|` degrees CCW;\n3. When angle < 0:\nB_rot is obtained by rotating B w.r.t its center by :math:`|angle|` degrees CW.\nMathematically, since the right-handed coordinate system for image space\nis (y, x), where y is top->down and x is left->right, the 4 vertices of the\nrotated rectangle :math:`(yr_i, xr_i)` (i = 1, 2, 3, 4) can be obtained from\nthe vertices of the horizontal rectangle (y_i, x_i) (i = 1, 2, 3, 4)\nin the following way (:math:`\\\\theta = angle*\\\\pi/180` is the angle in radians,\n(y_c, x_c) is the center of the rectangle):\n.. math::\nyr_i = \\\\cos(\\\\theta) (y_i - y_c) - \\\\sin(\\\\theta) (x_i - x_c) + y_c,\nxr_i = \\\\sin(\\\\theta) (y_i - y_c) + \\\\cos(\\\\theta) (x_i - x_c) + x_c,\nwhich is the standard rigid-body rotation transformation.\nIntuitively, the angle is\n(1) the rotation angle from y-axis in image space\nto the height vector (top->down in the box's local coordinate system)\nof the box in CCW, and\n(2) the rotation angle from x-axis in image space\nto the width vector (left->right in the box's local coordinate system)\nof the box in CCW.\nMore intuitively, consider the following horizontal box ABCD represented\nin (x1, y1, x2, y2): (3, 2, 7, 4),\ncovering the [3, 7] x [2, 4] region of the continuous coordinate system\nwhich looks like this:\n.. code:: none\nO--------> x\n|\n|  A---B\n|  |   |\n|  D---C\n|\nv y\nNote that each capital letter represents one 0-dimensional geometric point\ninstead of a 'square pixel' here.\nIn the example above, using (x, y) to represent a point we have:\n.. math::\nO = (0, 0), A = (3, 2), B = (7, 2), C = (7, 4), D = (3, 4)\nWe name vector AB = vector DC as the width vector in box's local coordinate system, and\nvector AD = vector BC as the height vector in box's local coordinate system. Initially,\nwhen angle = 0 degree, they're aligned with the positive directions of x-axis and y-axis\nin the image space, respectively.\nFor better illustration, we denote the center of the box as E,\n.. code:: none\nO--------> x\n|\n|  A---B\n|  | E |\n|  D---C\n|\nv y\nwhere the center E = ((3+7)/2, (2+4)/2) = (5, 3).\nAlso,\n.. math::\nwidth = |AB| = |CD| = 7 - 3 = 4,\nheight = |AD| = |BC| = 4 - 2 = 2.\nTherefore, the corresponding representation for the same shape in rotated box in\n(x_center, y_center, width, height, angle) format is:\n(5, 3, 4, 2, 0),\nNow, let's consider (5, 3, 4, 2, 90), which is rotated by 90 degrees\nCCW (counter-clockwise) by definition. It looks like this:\n.. code:: none\nO--------> x\n|   B-C\n|   | |\n|   |E|\n|   | |\n|   A-D\nv y\nThe center E is still located at the same point (5, 3), while the vertices\nABCD are rotated by 90 degrees CCW with regard to E:\nA = (4, 5), B = (4, 1), C = (6, 1), D = (6, 5)\nHere, 90 degrees can be seen as the CCW angle to rotate from y-axis to\nvector AD or vector BC (the top->down height vector in box's local coordinate system),\nor the CCW angle to rotate from x-axis to vector AB or vector DC (the left->right\nwidth vector in box's local coordinate system).\n.. math::\nwidth = |AB| = |CD| = 5 - 1 = 4,\nheight = |AD| = |BC| = 6 - 4 = 2.\nNext, how about (5, 3, 4, 2, -90), which is rotated by 90 degrees CW (clockwise)\nby definition? It looks like this:\n.. code:: none\nO--------> x\n|   D-A\n|   | |\n|   |E|\n|   | |\n|   C-B\nv y\nThe center E is still located at the same point (5, 3), while the vertices\nABCD are rotated by 90 degrees CW with regard to E:\nA = (6, 1), B = (6, 5), C = (4, 5), D = (4, 1)\n.. math::\nwidth = |AB| = |CD| = 5 - 1 = 4,\nheight = |AD| = |BC| = 6 - 4 = 2.\nThis covers exactly the same region as (5, 3, 4, 2, 90) does, and their IoU\nwill be 1. However, these two will generate different RoI Pooling results and\nshould not be treated as an identical box.\nOn the other hand, it's easy to see that (X, Y, W, H, A) is identical to\n(X, Y, W, H, A+360N), for any integer N. For example (5, 3, 4, 2, 270) would be\nidentical to (5, 3, 4, 2, -90), because rotating the shape 270 degrees CCW is\nequivalent to rotating the same shape 90 degrees CW.\nWe could rotate further to get (5, 3, 4, 2, 180), or (5, 3, 4, 2, -180):\n.. code:: none\nO--------> x\n|\n|  C---D\n|  | E |\n|  B---A\n|\nv y\n.. math::\nA = (7, 4), B = (3, 4), C = (3, 2), D = (7, 2),\nwidth = |AB| = |CD| = 7 - 3 = 4,\nheight = |AD| = |BC| = 4 - 2 = 2.\nFinally, this is a very inaccurate (heavily quantized) illustration of\nhow (5, 3, 4, 2, 60) looks like in case anyone wonders:\n.. code:: none\nO--------> x\n|     B\\\n|    /  C\n|   /E /\n|  A  /\n|   `D\nv y\nIt's still a rectangle with center of (5, 3), width of 4 and height of 2,\nbut its angle (and thus orientation) is somewhere between\n(5, 3, 4, 2, 0) and (5, 3, 4, 2, 90).\n\"\"\"\ndevice = tensor.device if isinstance(tensor, torch.Tensor) else torch.device(\"cpu\")\ntensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\nif tensor.numel() == 0:\n# Use reshape, so we don't end up creating a new tensor that does not depend on\n# the inputs (and consequently confuses jit)\ntensor = tensor.reshape((0, 5)).to(dtype=torch.float32, device=device)\nassert tensor.dim() == 2 and tensor.size(-1) == 5, tensor.size()\nself.tensor = tensor\ndef clone(self) -> \"RotatedBoxes\":\n\"\"\"\nClone the RotatedBoxes.\nReturns:\nRotatedBoxes\n\"\"\"\nreturn RotatedBoxes(self.tensor.clone())\ndef to(self, *args: Any, **kwargs: Any) -> \"RotatedBoxes\":\nreturn RotatedBoxes(self.tensor.to(*args, **kwargs))\ndef area(self) -> torch.Tensor:\n\"\"\"\nComputes the area of all the boxes.\nReturns:\ntorch.Tensor: a vector with areas of each box.\n\"\"\"\nbox = self.tensor\narea = box[:, 2] * box[:, 3]\nreturn area\ndef normalize_angles(self) -> None:\n\"\"\"\nRestrict angles to the range of [-180, 180) degrees\n\"\"\"\nself.tensor[:, 4] = (self.tensor[:, 4] + 180.0) % 360.0 - 180.0\ndef clip(self, box_size: Tuple[int, int], clip_angle_threshold: float = 1.0) -> None:\n\"\"\"\nClip (in place) the boxes by limiting x coordinates to the range [0, width]\nand y coordinates to the range [0, height].\nFor RRPN:\nOnly clip boxes that are almost horizontal with a tolerance of\nclip_angle_threshold to maintain backward compatibility.\nRotated boxes beyond this threshold are not clipped for two reasons:\n1. There are potentially multiple ways to clip a rotated box to make it\nfit within the image.\n2. It's tricky to make the entire rectangular box fit within the image\nand still be able to not leave out pixels of interest.\nTherefore we rely on ops like RoIAlignRotated to safely handle this.\nArgs:\nbox_size (height, width): The clipping box's size.\nclip_angle_threshold:\nIff. abs(normalized(angle)) <= clip_angle_threshold (in degrees),\nwe do the clipping as horizontal boxes.\n\"\"\"\nh, w = box_size\n# normalize angles to be within (-180, 180] degrees\nself.normalize_angles()\nidx = torch.where(torch.abs(self.tensor[:, 4]) <= clip_angle_threshold)[0]\n# convert to (x1, y1, x2, y2)\nx1 = self.tensor[idx, 0] - self.tensor[idx, 2] / 2.0\ny1 = self.tensor[idx, 1] - self.tensor[idx, 3] / 2.0\nx2 = self.tensor[idx, 0] + self.tensor[idx, 2] / 2.0\ny2 = self.tensor[idx, 1] + self.tensor[idx, 3] / 2.0\n# clip\nx1.clamp_(min=0, max=w)\ny1.clamp_(min=0, max=h)\nx2.clamp_(min=0, max=w)\ny2.clamp_(min=0, max=h)\n# convert back to (xc, yc, w, h)\nself.tensor[idx, 0] = (x1 + x2) / 2.0\nself.tensor[idx, 1] = (y1 + y2) / 2.0\n# make sure widths and heights do not increase due to numerical errors\nself.tensor[idx, 2] = torch.min(self.tensor[idx, 2], x2 - x1)\nself.tensor[idx, 3] = torch.min(self.tensor[idx, 3], y2 - y1)\ndef nonempty(self, threshold: float = 0.0) -> torch.Tensor:\n\"\"\"\nFind boxes that are non-empty.\nA box is considered empty, if either of its side is no larger than threshold.\nReturns:\nTensor: a binary vector which represents\nwhether each box is empty (False) or non-empty (True).\n\"\"\"\nbox = self.tensor\nwidths = box[:, 2]\nheights = box[:, 3]\nkeep = (widths > threshold) & (heights > threshold)\nreturn keep\ndef __getitem__(self, item: Union[int, slice, torch.BoolTensor]) -> \"RotatedBoxes\":\n\"\"\"\nReturns:\nRotatedBoxes: Create a new :class:`RotatedBoxes` by indexing.\nThe following usage are allowed:\n1. `new_boxes = boxes[3]`: return a `RotatedBoxes` which contains only one box.\n2. `new_boxes = boxes[2:10]`: return a slice of boxes.\n3. `new_boxes = boxes[vector]`, where vector is a torch.ByteTensor\nwith `length = len(boxes)`. Nonzero elements in the vector will be selected.\nNote that the returned RotatedBoxes might share storage with this RotatedBoxes,\nsubject to Pytorch's indexing semantics.\n\"\"\"\nif isinstance(item, int):\nreturn RotatedBoxes(self.tensor[item].view(1, -1))\nb = self.tensor[item]\nassert b.dim() == 2, \"Indexing on RotatedBoxes with {} failed to return a matrix!\".format(\nitem\n)\nreturn RotatedBoxes(b)\ndef __len__(self) -> int:\nreturn self.tensor.shape[0]\ndef __repr__(self) -> str:\nreturn \"RotatedBoxes(\" + str(self.tensor) + \")\"\ndef inside_box(self, box_size: Tuple[int, int], boundary_threshold: int = 0) -> torch.Tensor:\n\"\"\"\nArgs:\nbox_size (height, width): Size of the reference box covering\n[0, width] x [0, height]\nboundary_threshold (int): Boxes that extend beyond the reference box\nboundary by more than boundary_threshold are considered \"outside\".\nFor RRPN, it might not be necessary to call this function since it's common\nfor rotated box to extend to outside of the image boundaries\n(the clip function only clips the near-horizontal boxes)\nReturns:\na binary vector, indicating whether each box is inside the reference box.\n\"\"\"\nheight, width = box_size\ncnt_x = self.tensor[..., 0]\ncnt_y = self.tensor[..., 1]\nhalf_w = self.tensor[..., 2] / 2.0\nhalf_h = self.tensor[..., 3] / 2.0\na = self.tensor[..., 4]\nc = torch.abs(torch.cos(a * math.pi / 180.0))\ns = torch.abs(torch.sin(a * math.pi / 180.0))\n# This basically computes the horizontal bounding rectangle of the rotated box\nmax_rect_dx = c * half_w + s * half_h\nmax_rect_dy = c * half_h + s * half_w\ninds_inside = (\n(cnt_x - max_rect_dx >= -boundary_threshold)\n& (cnt_y - max_rect_dy >= -boundary_threshold)\n& (cnt_x + max_rect_dx < width + boundary_threshold)\n& (cnt_y + max_rect_dy < height + boundary_threshold)\n)\nreturn inds_inside\ndef get_centers(self) -> torch.Tensor:\n\"\"\"\nReturns:\nThe box centers in a Nx2 array of (x, y).\n\"\"\"\nreturn self.tensor[:, :2]\ndef scale(self, scale_x: float, scale_y: float) -> None:\n\"\"\"\nScale the rotated box with horizontal and vertical scaling factors\nNote: when scale_factor_x != scale_factor_y,\nthe rotated box does not preserve the rectangular shape when the angle\nis not a multiple of 90 degrees under resize transformation.\nInstead, the shape is a parallelogram (that has skew)\nHere we make an approximation by fitting a rotated rectangle to the parallelogram.\n\"\"\"\nself.tensor[:, 0] *= scale_x\nself.tensor[:, 1] *= scale_y\ntheta = self.tensor[:, 4] * math.pi / 180.0\nc = torch.cos(theta)\ns = torch.sin(theta)\n# In image space, y is top->down and x is left->right\n# Consider the local coordintate system for the rotated box,\n# where the box center is located at (0, 0), and the four vertices ABCD are\n# A(-w / 2, -h / 2), B(w / 2, -h / 2), C(w / 2, h / 2), D(-w / 2, h / 2)\n# the midpoint of the left edge AD of the rotated box E is:\n# E = (A+D)/2 = (-w / 2, 0)\n# the midpoint of the top edge AB of the rotated box F is:\n# F(0, -h / 2)\n# To get the old coordinates in the global system, apply the rotation transformation\n# (Note: the right-handed coordinate system for image space is yOx):\n# (old_x, old_y) = (s * y + c * x, c * y - s * x)\n# E(old) = (s * 0 + c * (-w/2), c * 0 - s * (-w/2)) = (-c * w / 2, s * w / 2)\n# F(old) = (s * (-h / 2) + c * 0, c * (-h / 2) - s * 0) = (-s * h / 2, -c * h / 2)\n# After applying the scaling factor (sfx, sfy):\n# E(new) = (-sfx * c * w / 2, sfy * s * w / 2)\n# F(new) = (-sfx * s * h / 2, -sfy * c * h / 2)\n# The new width after scaling tranformation becomes:\n# w(new) = |E(new) - O| * 2\n#        = sqrt[(sfx * c * w / 2)^2 + (sfy * s * w / 2)^2] * 2\n#        = sqrt[(sfx * c)^2 + (sfy * s)^2] * w\n# i.e., scale_factor_w = sqrt[(sfx * c)^2 + (sfy * s)^2]\n#\n# For example,\n# when angle = 0 or 180, |c| = 1, s = 0, scale_factor_w == scale_factor_x;\n# when |angle| = 90, c = 0, |s| = 1, scale_factor_w == scale_factor_y\nself.tensor[:, 2] *= torch.sqrt((scale_x * c) ** 2 + (scale_y * s) ** 2)\n# h(new) = |F(new) - O| * 2\n#        = sqrt[(sfx * s * h / 2)^2 + (sfy * c * h / 2)^2] * 2\n#        = sqrt[(sfx * s)^2 + (sfy * c)^2] * h\n# i.e., scale_factor_h = sqrt[(sfx * s)^2 + (sfy * c)^2]\n#\n# For example,\n# when angle = 0 or 180, |c| = 1, s = 0, scale_factor_h == scale_factor_y;\n# when |angle| = 90, c = 0, |s| = 1, scale_factor_h == scale_factor_x\nself.tensor[:, 3] *= torch.sqrt((scale_x * s) ** 2 + (scale_y * c) ** 2)\n# The angle is the rotation angle from y-axis in image space to the height\n# vector (top->down in the box's local coordinate system) of the box in CCW.\n#\n# angle(new) = angle_yOx(O - F(new))\n#            = angle_yOx( (sfx * s * h / 2, sfy * c * h / 2) )\n#            = atan2(sfx * s * h / 2, sfy * c * h / 2)\n#            = atan2(sfx * s, sfy * c)\n#\n# For example,\n# when sfx == sfy, angle(new) == atan2(s, c) == angle(old)\nself.tensor[:, 4] = torch.atan2(scale_x * s, scale_y * c) * 180 / math.pi\n@property\ndef device(self) -> str:\nreturn self.tensor.device\ndef __iter__(self) -> Iterator[torch.Tensor]:\n\"\"\"\nYield a box as a Tensor of shape (5,) at a time.\n\"\"\"\nyield from self.tensor\ndef pairwise_iou(boxes1: RotatedBoxes, boxes2: RotatedBoxes) -> None:\n\"\"\"\nGiven two lists of rotated boxes of size N and M,\ncompute the IoU (intersection over union)\nbetween __all__ N x M pairs of boxes.\nThe box order must be (x_center, y_center, width, height, angle).\nArgs:\nboxes1, boxes2 (RotatedBoxes):\ntwo `RotatedBoxes`. Contains N & M rotated boxes, respectively.\nReturns:\nTensor: IoU, sized [N,M].\n\"\"\"\nreturn pairwise_iou_rotated(boxes1.tensor, boxes2.tensor)",
                        "max_stars_repo_path": "detectron2/structures/rotated_boxes.py",
                        "max_stars_repo_name": "PedroUria/detectron2",
                        "max_stars_count": 780,
                        "__cluster__": 191
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "pretrain_python_data_7355715",
                        "content": "<gh_stars>0\n# Copyright (c) Facebook, Inc. and its affiliates.\nfrom typing import Any\nimport pydoc\nfrom fvcore.common.registry import Registry  # for backward compatibility.\n\"\"\"\n``Registry`` and `locate` provide ways to map a string (typically found\nin config files) to callable objects.\n\"\"\"\n__all__ = [\"Registry\", \"locate\"]\ndef _convert_target_to_string(t: Any) -> str:\n\"\"\"\nInverse of ``locate()``.\nArgs:\nt: any object with ``__module__`` and ``__qualname__``\n\"\"\"\nmodule, qualname = t.__module__, t.__qualname__\n# Compress the path to this object, e.g. ``module.submodule._impl.class``\n# may become ``module.submodule.class``, if the later also resolves to the same\n# object. This simplifies the string, and also is less affected by moving the\n# class implementation.\nmodule_parts = module.split(\".\")\nfor k in range(1, len(module_parts)):\nprefix = \".\".join(module_parts[:k])\ncandidate = f\"{prefix}.{qualname}\"\ntry:\nif locate(candidate) is t:\nreturn candidate\nexcept ImportError:\npass\nreturn f\"{module}.{qualname}\"\ndef locate(name: str) -> Any:\n\"\"\"\nLocate and return an object ``features`` using an input string ``{features.__module__}.{features.__qualname__}``,\nsuch as \"module.submodule.class_name\".\nRaise Exception if it cannot be found.\n\"\"\"\nobj = pydoc.locate(name)\n# Some cases (e.g. torch.optim.sgd.SGD) not handled correctly\n# by pydoc.locate. Try a private function from hydra.\nif obj is None:\ntry:\n# from hydra.utils import get_method - will print many errors\nfrom hydra.utils import _locate\nexcept ImportError as e:\nraise ImportError(f\"Cannot dynamically locate object {name}!\") from e\nelse:\nobj = _locate(name)  # it raises if fails\nreturn obj",
                        "max_stars_repo_path": "AdelaiDet/detectron2/detectron2/utils/registry.py",
                        "max_stars_repo_name": "km1562/AdelaiDet2",
                        "max_stars_count": 0,
                        "__cluster__": 47
                },
                {
                        "id": "test_evocodebench_data_48",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nfrom typing import Any\nimport pydoc\nfrom fvcore.common.registry import Registry  # for backward compatibility.\n\"\"\"\n``Registry`` and `locate` provide ways to map a string (typically found\nin config files) to callable objects.\n\"\"\"\n__all__ = [\"Registry\", \"locate\"]\ndef _convert_target_to_string(t: Any) -> str:\n\"\"\"\nInverse of ``locate()``.\nArgs:\nt: any object with ``__module__`` and ``__qualname__``\n\"\"\"\nmodule, qualname = t.__module__, t.__qualname__\n# Compress the path to this object, e.g. ``module.submodule._impl.class``\n# may become ``module.submodule.class``, if the later also resolves to the same\n# object. This simplifies the string, and also is less affected by moving the\n# class implementation.\nmodule_parts = module.split(\".\")\nfor k in range(1, len(module_parts)):\nprefix = \".\".join(module_parts[:k])\ncandidate = f\"{prefix}.{qualname}\"\ntry:\nif locate(candidate) is t:\nreturn candidate\nexcept ImportError:\npass\nreturn f\"{module}.{qualname}\"\ndef locate(name: str) -> Any:\n\"\"\"\nLocate and return an object ``x`` using an input string ``{x.__module__}.{x.__qualname__}``,\nsuch as \"module.submodule.class_name\".\nRaise Exception if it cannot be found.\n\"\"\"\nobj = pydoc.locate(name)\n# Some cases (e.g. torch.optim.sgd.SGD) not handled correctly\n# by pydoc.locate. Try a private function from hydra.\nif obj is None:\ntry:\n# from hydra.utils import get_method - will print many errors\nfrom hydra.utils import _locate\nexcept ImportError as e:\nraise ImportError(f\"Cannot dynamically locate object {name}!\") from e\nelse:\nobj = _locate(name)  # it raises if fails\nreturn obj",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 47
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "pretrain_python_data_7524367",
                        "content": "<gh_stars>0\n# Copyright (c) Facebook, Inc. and its affiliates.\nimport colorsys\nimport logging\nimport math\nimport numpy as np\nfrom enum import Enum, unique\nimport cv2\nimport matplotlib as mpl\nimport matplotlib.colors as mplc\nimport matplotlib.figure as mplfigure\nimport pycocotools.mask as mask_util\nimport torch\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\nfrom PIL import Image\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.structures import BitMasks, Boxes, BoxMode, Keypoints, PolygonMasks, RotatedBoxes\nfrom detectron2.utils.file_io import PathManager\nfrom .colormap import random_color\nlogger = logging.getLogger(__name__)\n__all__ = [\"ColorMode\", \"VisImage\", \"Visualizer\"]\n_SMALL_OBJECT_AREA_THRESH = 1000\n_LARGE_MASK_AREA_THRESH = 120000\n_OFF_WHITE = (1.0, 1.0, 240.0 / 255)\n_BLACK = (0, 0, 0)\n_RED = (1.0, 0, 0)\n_KEYPOINT_THRESHOLD = 0.05\n@unique\nclass ColorMode(Enum):\n\"\"\"\nEnum of different color modes to use for instance visualizations.\n\"\"\"\nIMAGE = 0\n\"\"\"\nPicks a random color for every instance and overlay segmentations with low opacity.\n\"\"\"\nSEGMENTATION = 1\n\"\"\"\nLet instances of the same category have similar colors\n(from metadata.thing_colors), and overlay them with\nhigh opacity. This provides more attention on the quality of segmentation.\n\"\"\"\nIMAGE_BW = 2\n\"\"\"\nSame as IMAGE, but convert all areas without masks to gray-scale.\nOnly available for drawing per-instance mask predictions.\n\"\"\"\nclass GenericMask:\n\"\"\"\nAttribute:\npolygons (list[ndarray]): list[ndarray]: polygons for this mask.\nEach ndarray has format [x, y, x, y, ...]\nmask (ndarray): a binary mask\n\"\"\"\ndef __init__(self, mask_or_polygons, height, width):\nself._mask = self._polygons = self._has_holes = None\nself.height = height\nself.width = width\nm = mask_or_polygons\nif isinstance(m, dict):\n# RLEs\nassert \"counts\" in m and \"size\" in m\nif isinstance(m[\"counts\"], list):  # uncompressed RLEs\nh, w = m[\"size\"]\nassert h == height and w == width\nm = mask_util.frPyObjects(m, h, w)\nself._mask = mask_util.decode(m)[:, :]\nreturn\nif isinstance(m, list):  # list[ndarray]\nself._polygons = [np.asarray(x).reshape(-1) for x in m]\nreturn\nif isinstance(m, np.ndarray):  # assumed to be a binary mask\nassert m.shape[1] != 2, m.shape\nassert m.shape == (\nheight,\nwidth,\n), f\"mask shape: {m.shape}, target dims: {height}, {width}\"\nself._mask = m.astype(\"uint8\")\nreturn\nraise ValueError(\"GenericMask cannot handle object {} of type '{}'\".format(m, type(m)))\n@property\ndef mask(self):\nif self._mask is None:\nself._mask = self.polygons_to_mask(self._polygons)\nreturn self._mask\n@property\ndef polygons(self):\nif self._polygons is None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nreturn self._polygons\n@property\ndef has_holes(self):\nif self._has_holes is None:\nif self._mask is not None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nelse:\nself._has_holes = False  # if original format is polygon, does not have holes\nreturn self._has_holes\ndef mask_to_polygons(self, mask):\n# cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level\n# hierarchy. External contours (boundary) of the object are placed in hierarchy-1.\n# Internal contours (holes) are placed in hierarchy-2.\n# cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.\nmask = np.ascontiguousarray(mask)  # some versions of cv2 does not support incontiguous arr\nres = cv2.findContours(mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\nhierarchy = res[-1]\nif hierarchy is None:  # empty mask\nreturn [], False\nhas_holes = (hierarchy.reshape(-1, 4)[:, 3] >= 0).sum() > 0\nres = res[-2]\nres = [x.flatten() for x in res]\n# These coordinates from OpenCV are integers in range [0, W-1 or H-1].\n# We add 0.5 to turn them into real-value coordinate space. A better solution\n# would be to first +0.5 and then dilate the returned polygon by 0.5.\nres = [x + 0.5 for x in res if len(x) >= 6]\nreturn res, has_holes\ndef polygons_to_mask(self, polygons):\nrle = mask_util.frPyObjects(polygons, self.height, self.width)\nrle = mask_util.merge(rle)\nreturn mask_util.decode(rle)[:, :]\ndef area(self):\nreturn self.mask.sum()\ndef bbox(self):\np = mask_util.frPyObjects(self.polygons, self.height, self.width)\np = mask_util.merge(p)\nbbox = mask_util.toBbox(p)\nbbox[2] += bbox[0]\nbbox[3] += bbox[1]\nreturn bbox\nclass _PanopticPrediction:\n\"\"\"\nUnify different panoptic annotation/prediction formats\n\"\"\"\ndef __init__(self, panoptic_seg, segments_info, metadata=None):\nif segments_info is None:\nassert metadata is not None\n# If \"segments_info\" is None, we assume \"panoptic_img\" is a\n# H*W int32 image storing the panoptic_id in the format of\n# category_id * label_divisor + instance_id. We reserve -1 for\n# VOID label.\nlabel_divisor = metadata.label_divisor\nsegments_info = []\nfor panoptic_label in np.unique(panoptic_seg.numpy()):\nif panoptic_label == -1:\n# VOID region.\ncontinue\npred_class = panoptic_label // label_divisor\nisthing = pred_class in metadata.thing_dataset_id_to_contiguous_id.values()\nsegments_info.append(\n{\n\"id\": int(panoptic_label),\n\"category_id\": int(pred_class),\n\"isthing\": bool(isthing),\n}\n)\ndel metadata\nself._seg = panoptic_seg\nself._sinfo = {s[\"id\"]: s for s in segments_info}  # seg id -> seg info\nsegment_ids, areas = torch.unique(panoptic_seg, sorted=True, return_counts=True)\nareas = areas.numpy()\nsorted_idxs = np.argsort(-areas)\nself._seg_ids, self._seg_areas = segment_ids[sorted_idxs], areas[sorted_idxs]\nself._seg_ids = self._seg_ids.tolist()\nfor sid, area in zip(self._seg_ids, self._seg_areas):\nif sid in self._sinfo:\nself._sinfo[sid][\"area\"] = float(area)\ndef non_empty_mask(self):\n\"\"\"\nReturns:\n(H, W) array, a mask for all pixels that have a prediction\n\"\"\"\nempty_ids = []\nfor id in self._seg_ids:\nif id not in self._sinfo:\nempty_ids.append(id)\nif len(empty_ids) == 0:\nreturn np.zeros(self._seg.shape, dtype=np.uint8)\nassert (\nlen(empty_ids) == 1\n), \">1 ids corresponds to no labels. This is currently not supported\"\nreturn (self._seg != empty_ids[0]).numpy().astype(np.bool)\ndef semantic_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or sinfo[\"isthing\"]:\n# Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions.\ncontinue\nyield (self._seg == sid).numpy().astype(np.bool), sinfo\ndef instance_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or not sinfo[\"isthing\"]:\ncontinue\nmask = (self._seg == sid).numpy().astype(np.bool)\nif mask.sum() > 0:\nyield mask, sinfo\ndef _create_text_labels(classes, scores, class_names, is_crowd=None):\n\"\"\"\nArgs:\nclasses (list[int] or None):\nscores (list[float] or None):\nclass_names (list[str] or None):\nis_crowd (list[bool] or None):\nReturns:\nlist[str] or None\n\"\"\"\nlabels = None\nif classes is not None:\nif class_names is not None and len(class_names) > 0:\nlabels = [class_names[i] for i in classes]\nelse:\nlabels = [str(i) for i in classes]\nif scores is not None:\nif labels is None:\nlabels = [\"{:.0f}%\".format(s * 100) for s in scores]\nelse:\nlabels = [\"{} {:.0f}%\".format(l, s * 100) for l, s in zip(labels, scores)]\nif labels is not None and is_crowd is not None:\nlabels = [l + (\"|crowd\" if crowd else \"\") for l, crowd in zip(labels, is_crowd)]\nreturn labels\nclass VisImage:\ndef __init__(self, img, scale=1.0):\n\"\"\"\nArgs:\nimg (ndarray): an RGB image of shape (H, W, 3) in range [0, 255].\nscale (float): scale the input image\n\"\"\"\nself.img = img\nself.scale = scale\nself.width, self.height = img.shape[1], img.shape[0]\nself._setup_figure(img)\ndef _setup_figure(self, img):\n\"\"\"\nArgs:\nSame as in :meth:`__init__()`.\nReturns:\nfig (matplotlib.pyplot.figure): top level container for all the image plot elements.\nax (matplotlib.pyplot.Axes): contains figure elements and sets the coordinate system.\n\"\"\"\nfig = mplfigure.Figure(frameon=False)\nself.dpi = fig.get_dpi()\n# add a small 1e-2 to avoid precision lost due to matplotlib's truncation\n# (https://github.com/matplotlib/matplotlib/issues/15363)\nfig.set_size_inches(\n(self.width * self.scale + 1e-2) / self.dpi,\n(self.height * self.scale + 1e-2) / self.dpi,\n)\nself.canvas = FigureCanvasAgg(fig)\n# self.canvas = mpl.backends.backend_cairo.FigureCanvasCairo(fig)\nax = fig.add_axes([0.0, 0.0, 1.0, 1.0])\nax.axis(\"off\")\nself.fig = fig\nself.ax = ax\nself.reset_image(img)\ndef reset_image(self, img):\n\"\"\"\nArgs:\nimg: same as in __init__\n\"\"\"\nimg = img.astype(\"uint8\")\nself.ax.imshow(img, extent=(0, self.width, self.height, 0), interpolation=\"nearest\")\ndef save(self, filepath):\n\"\"\"\nArgs:\nfilepath (str): a string that contains the absolute path, including the file name, where\nthe visualized image will be saved.\n\"\"\"\nself.fig.savefig(filepath)\ndef get_image(self):\n\"\"\"\nReturns:\nndarray:\nthe visualized image of shape (H, W, 3) (RGB) in uint8 type.\nThe shape is scaled w.r.t the input image using the given `scale` argument.\n\"\"\"\ncanvas = self.canvas\ns, (width, height) = canvas.print_to_buffer()\n# buf = io.BytesIO()  # works for cairo backend\n# canvas.print_rgba(buf)\n# width, height = self.width, self.height\n# s = buf.getvalue()\nbuffer = np.frombuffer(s, dtype=\"uint8\")\nimg_rgba = buffer.reshape(height, width, 4)\nrgb, alpha = np.split(img_rgba, [3], axis=2)\nreturn rgb.astype(\"uint8\")\nclass Visualizer:\n\"\"\"\nVisualizer that draws data about detection/segmentation on images.\nIt contains methods like `draw_{text,box,circle,line,binary_mask,polygon}`\nthat draw primitive objects to images, as well as high-level wrappers like\n`draw_{instance_predictions,sem_seg,panoptic_seg_predictions,dataset_dict}`\nthat draw composite data in some pre-defined style.\nNote that the exact visualization style for the high-level wrappers are subject to change.\nStyle such as color, opacity, label contents, visibility of labels, or even the visibility\nof objects themselves (e.g. when the object is too small) may change according\nto different heuristics, as long as the results still look visually reasonable.\nTo obtain a consistent style, you can implement custom drawing functions with the\nabovementioned primitive methods instead. If you need more customized visualization\nstyles, you can process the data yourself following their format documented in\ntutorials (:doc:`/tutorials/models`, :doc:`/tutorials/datasets`). This class does not\nintend to satisfy everyone's preference on drawing styles.\nThis visualizer focuses on high rendering quality rather than performance. It is not\ndesigned to be used for real-time applications.\n\"\"\"\n# TODO implement a fast, rasterized version using OpenCV\ndef __init__(self, img_rgb, metadata=None, scale=1.0, instance_mode=ColorMode.IMAGE):\n\"\"\"\nArgs:\nimg_rgb: a numpy array of shape (H, W, C), where H and W correspond to\nthe height and width of the image respectively. C is the number of\ncolor channels. The image is required to be in RGB format since that\nis a requirement of the Matplotlib library. The image is also expected\nto be in the range [0, 255].\nmetadata (Metadata): dataset metadata (e.g. class names and colors)\ninstance_mode (ColorMode): defines one of the pre-defined style for drawing\ninstances on an image.\n\"\"\"\nself.img = np.asarray(img_rgb).clip(0, 255).astype(np.uint8)\nif metadata is None:\nmetadata = MetadataCatalog.get(\"__nonexist__\")\nself.metadata = metadata\nself.output = VisImage(self.img, scale=scale)\nself.cpu_device = torch.device(\"cpu\")\n# too small texts are useless, therefore clamp to 9\nself._default_font_size = max(\nnp.sqrt(self.output.height * self.output.width) // 90, 10 // scale\n)\nself._instance_mode = instance_mode\nself.keypoint_threshold = _KEYPOINT_THRESHOLD\ndef draw_instance_predictions(self, predictions):\n\"\"\"\nDraw instance-level prediction results on an image.\nArgs:\npredictions (Instances): the output of an instance detection/segmentation\nmodel. Following fields will be used to draw:\n\"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nboxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\nscores = predictions.scores if predictions.has(\"scores\") else None\nclasses = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\nlabels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\nkeypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\nif predictions.has(\"pred_masks\"):\nmasks = np.asarray(predictions.pred_masks)\nmasks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\nelse:\nmasks = None\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n]\nalpha = 0.8\nelse:\ncolors = None\nalpha = 0.5\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(\nself._create_grayscale_image(\n(predictions.pred_masks.any(dim=0) > 0).numpy()\nif predictions.has(\"pred_masks\")\nelse None\n)\n)\nalpha = 0.3\nself.overlay_instances(\nmasks=masks,\nboxes=boxes,\nlabels=labels,\nkeypoints=keypoints,\nassigned_colors=colors,\nalpha=alpha,\n)\nreturn self.output\ndef draw_sem_seg(self, sem_seg, area_threshold=None, alpha=0.8):\n\"\"\"\nDraw semantic segmentation predictions/labels.\nArgs:\nsem_seg (Tensor or ndarray): the segmentation of shape (H, W).\nEach value is the integer label of the pixel.\narea_threshold (int): segments with less than `area_threshold` are not drawn.\nalpha (float): the larger it is, the more opaque the segmentations are.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nif isinstance(sem_seg, torch.Tensor):\nsem_seg = sem_seg.numpy()\nlabels, areas = np.unique(sem_seg, return_counts=True)\nsorted_idxs = np.argsort(-areas).tolist()\nlabels = labels[sorted_idxs]\nfor label in filter(lambda l: l < len(self.metadata.stuff_classes), labels):\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[label]]\nexcept (AttributeError, IndexError):\nmask_color = None\nbinary_mask = (sem_seg == label).astype(np.uint8)\ntext = self.metadata.stuff_classes[label]\nself.draw_binary_mask(\nbinary_mask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\nreturn self.output\ndef draw_panoptic_seg(self, panoptic_seg, segments_info, area_threshold=None, alpha=0.7):\n\"\"\"\nDraw panoptic prediction annotations or results.\nArgs:\npanoptic_seg (Tensor): of shape (height, width) where the values are ids for each\nsegment.\nsegments_info (list[dict] or None): Describe each segment in `panoptic_seg`.\nIf it is a ``list[dict]``, each dict contains keys \"id\", \"category_id\".\nIf None, category id of each pixel is computed by\n``pixel // metadata.label_divisor``.\narea_threshold (int): stuff segments with less than `area_threshold` are not drawn.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nsemant_vect = []\npred = _PanopticPrediction(panoptic_seg, segments_info, self.metadata)\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(self._create_grayscale_image(pred.non_empty_mask()))\n# draw mask for all semantic segments first i.e. \"stuff\"\nfor mask, sinfo in pred.semantic_masks():\ncategory_idx = sinfo[\"category_id\"]\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]\nexcept AttributeError:\nmask_color = None\ntext = self.metadata.stuff_classes[category_idx]\n_ , pos_semant=self.draw_binary_mask(\nmask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\nsemant_vect.append([pos_semant, text])\n# draw mask for all instances second\nall_instances = list(pred.instance_masks())\nif len(all_instances) == 0:\nreturn self.output, semant_vect\nmasks, sinfo = list(zip(*all_instances))\ncategory_ids = [x[\"category_id\"] for x in sinfo]\ntry:\nscores = [x[\"score\"] for x in sinfo]\nexcept KeyError:\nscores = None\nlabels = _create_text_labels(\ncategory_ids, scores, self.metadata.thing_classes, [x.get(\"iscrowd\", 0) for x in sinfo]\n)\ntry:\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in category_ids\n]\nexcept AttributeError:\ncolors = None\n_, pos_obj_semant= self.overlay_instances(masks=masks, labels=labels, assigned_colors=colors, alpha=alpha)\nsemant_vect.extend(pos_obj_semant)\nreturn self.output, semant_vect\n#return semant_vect.append(pos_obj_semant)\ndraw_panoptic_seg_predictions = draw_panoptic_seg  # backward compatibility\ndef draw_dataset_dict(self, dic):\n\"\"\"\nDraw annotations/segmentaions in Detectron2 Dataset format.\nArgs:\ndic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nannos = dic.get(\"annotations\", None)\nif annos:\nif \"segmentation\" in annos[0]:\nmasks = [x[\"segmentation\"] for x in annos]\nelse:\nmasks = None\nif \"keypoints\" in annos[0]:\nkeypts = [x[\"keypoints\"] for x in annos]\nkeypts = np.array(keypts).reshape(len(annos), -1, 3)\nelse:\nkeypts = None\nboxes = [\nBoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS)\nif len(x[\"bbox\"]) == 4\nelse x[\"bbox\"]\nfor x in annos\n]\ncolors = None\ncategory_ids = [x[\"category_id\"] for x in annos]\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]])\nfor c in category_ids\n]\nnames = self.metadata.get(\"thing_classes\", None)\nlabels = _create_text_labels(\ncategory_ids,\nscores=None,\nclass_names=names,\nis_crowd=[x.get(\"iscrowd\", 0) for x in annos],\n)\nself.overlay_instances(\nlabels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n)\nsem_seg = dic.get(\"sem_seg\", None)\nif sem_seg is None and \"sem_seg_file_name\" in dic:\nwith PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\nsem_seg = Image.open(f)\nsem_seg = np.asarray(sem_seg, dtype=\"uint8\")\nif sem_seg is not None:\nself.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\npan_seg = dic.get(\"pan_seg\", None)\nif pan_seg is None and \"pan_seg_file_name\" in dic:\nwith PathManager.open(dic[\"pan_seg_file_name\"], \"rb\") as f:\npan_seg = Image.open(f)\npan_seg = np.asarray(pan_seg)\nfrom panopticapi.utils import rgb2id\npan_seg = rgb2id(pan_seg)\nif pan_seg is not None:\nsegments_info = dic[\"segments_info\"]\npan_seg = torch.tensor(pan_seg)\nself.draw_panoptic_seg(pan_seg, segments_info, area_threshold=0, alpha=0.5)\nreturn self.output\ndef overlay_instances(\nself,\n*,\nboxes=None,\nlabels=None,\nmasks=None,\nkeypoints=None,\nassigned_colors=None,\nalpha=0.5,\n):\n\"\"\"\nArgs:\nboxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,\nor an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,\nor a :class:`RotatedBoxes`,\nor an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image,\nlabels (list[str]): the text to be displayed for each instance.\nmasks (masks-like object): Supported types are:\n* :class:`detectron2.structures.PolygonMasks`,\n:class:`detectron2.structures.BitMasks`.\n* list[list[ndarray]]: contains the segmentation masks for all objects in one image.\nThe first level of the list corresponds to individual instances. The second\nlevel to all the polygon that compose the instance, and the third level\nto the polygon coordinates. The third level should have the format of\n[x0, y0, x1, y1, ..., xn, yn] (n >= 3).\n* list[ndarray]: each ndarray is a binary mask of shape (H, W).\n* list[dict]: each dict is a COCO-style RLE.\nkeypoints (Keypoint or array like): an array-like object of shape (N, K, 3),\nwhere the N is the number of instances and K is the number of keypoints.\nThe last dimension corresponds to (x, y, visibility or score).\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = 0\nif boxes is not None:\nboxes = self._convert_boxes(boxes)\nnum_instances = len(boxes)\nif masks is not None:\nmasks = self._convert_masks(masks)\nif num_instances:\nassert len(masks) == num_instances\nelse:\nnum_instances = len(masks)\nif keypoints is not None:\nif num_instances:\nassert len(keypoints) == num_instances\nelse:\nnum_instances = len(keypoints)\nkeypoints = self._convert_keypoints(keypoints)\nif labels is not None:\nassert len(labels) == num_instances\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\nif boxes is not None and boxes.shape[1] == 5:\nreturn self.overlay_rotated_instances(\nboxes=boxes, labels=labels, assigned_colors=assigned_colors\n)\n# Display in largest to smallest order to reduce occlusion.\nareas = None\nif boxes is not None:\nareas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\nelif masks is not None:\nareas = np.asarray([x.area() for x in masks])\nif areas is not None:\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs] if boxes is not None else None\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\nmasks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\nassigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\nkeypoints = keypoints[sorted_idxs] if keypoints is not None else None\n#print(num_instances)\n#print(labels)\nobj_vect=[]\nfor i in range(num_instances):\ncolor = assigned_colors[i]\nif boxes is not None:\nself.draw_box(boxes[i], edge_color=color)\nif masks is not None:\nfor segment in masks[i].polygons:\nself.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\nif labels is not None:\n# first get a box\nif boxes is not None:\nx0, y0, x1, y1 = boxes[i]\ntext_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\nhoriz_align = \"left\"\nelif masks is not None:\n# skip small mask without polygon\nif len(masks[i].polygons) == 0:\ncontinue\nx0, y0, x1, y1 = masks[i].bbox()\n# draw text in the center (defined by median) when box is not drawn\n# median is less sensitive to outliers.\ntext_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\nhoriz_align = \"center\"\nelse:\ncontinue  # drawing the box confidence for keypoints isn't very useful.\n# for small objects, draw text at the side to avoid occlusion\ninstance_area = (y1 - y0) * (x1 - x0)\nif (\ninstance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\nor y1 - y0 < 40 * self.output.scale\n):\nif y1 >= self.output.height - 5:\ntext_pos = (x1, y0)\nelse:\ntext_pos = (x0, y1)\nheight_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n* 0.5\n* self._default_font_size\n)\nself.draw_text(\nlabels[i],\ntext_pos,\ncolor=lighter_color,\nhorizontal_alignment=horiz_align,\nfont_size=font_size,\n)\nobj_vect.append([text_pos,labels[i]])\n# draw keypoints\nif keypoints is not None:\nfor keypoints_per_instance in keypoints:\nself.draw_and_connect_keypoints(keypoints_per_instance)\nreturn self.output, obj_vect\ndef overlay_rotated_instances(self, boxes=None, labels=None, assigned_colors=None):\n\"\"\"\nArgs:\nboxes (ndarray): an Nx5 numpy array of\n(x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image.\nlabels (list[str]): the text to be displayed for each instance.\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = len(boxes)\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\n# Display in largest to smallest order to reduce occlusion.\nif boxes is not None:\nareas = boxes[:, 2] * boxes[:, 3]\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs]\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\ncolors = [assigned_colors[idx] for idx in sorted_idxs]\nfor i in range(num_instances):\nself.draw_rotated_box_with_label(\nboxes[i], edge_color=colors[i], label=labels[i] if labels is not None else None\n)\nreturn self.output\ndef draw_and_connect_keypoints(self, keypoints):\n\"\"\"\nDraws keypoints of an instance and follows the rules for keypoint connections\nto draw lines between appropriate keypoints. This follows color heuristics for\nline color.\nArgs:\nkeypoints (Tensor): a tensor of shape (K, 3), where K is the number of keypoints\nand the last dimension corresponds to (x, y, probability).\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nvisible = {}\nkeypoint_names = self.metadata.get(\"keypoint_names\")\nfor idx, keypoint in enumerate(keypoints):\n# draw keypoint\nx, y, prob = keypoint\nif prob > self.keypoint_threshold:\nself.draw_circle((x, y), color=_RED)\nif keypoint_names:\nkeypoint_name = keypoint_names[idx]\nvisible[keypoint_name] = (x, y)\nif self.metadata.get(\"keypoint_connection_rules\"):\nfor kp0, kp1, color in self.metadata.keypoint_connection_rules:\nif kp0 in visible and kp1 in visible:\nx0, y0 = visible[kp0]\nx1, y1 = visible[kp1]\ncolor = tuple(x / 255.0 for x in color)\nself.draw_line([x0, x1], [y0, y1], color=color)\n# draw lines from nose to mid-shoulder and mid-shoulder to mid-hip\n# Note that this strategy is specific to person keypoints.\n# For other keypoints, it should just do nothing\ntry:\nls_x, ls_y = visible[\"left_shoulder\"]\nrs_x, rs_y = visible[\"right_shoulder\"]\nmid_shoulder_x, mid_shoulder_y = (ls_x + rs_x) / 2, (ls_y + rs_y) / 2\nexcept KeyError:\npass\nelse:\n# draw line from nose to mid-shoulder\nnose_x, nose_y = visible.get(\"nose\", (None, None))\nif nose_x is not None:\nself.draw_line([nose_x, mid_shoulder_x], [nose_y, mid_shoulder_y], color=_RED)\ntry:\n# draw line from mid-shoulder to mid-hip\nlh_x, lh_y = visible[\"left_hip\"]\nrh_x, rh_y = visible[\"right_hip\"]\nexcept KeyError:\npass\nelse:\nmid_hip_x, mid_hip_y = (lh_x + rh_x) / 2, (lh_y + rh_y) / 2\nself.draw_line([mid_hip_x, mid_shoulder_x], [mid_hip_y, mid_shoulder_y], color=_RED)\nreturn self.output\n\"\"\"\nPrimitive drawing functions:\n\"\"\"\ndef draw_text(\nself,\ntext,\nposition,\n*,\nfont_size=None,\ncolor=\"g\",\nhorizontal_alignment=\"center\",\nrotation=0,\n):\n\"\"\"\nArgs:\ntext (str): class label\nposition (tuple): a tuple of the x and y coordinates to place text on image.\nfont_size (int, optional): font of the text. If not provided, a font size\nproportional to the image width is calculated and used.\ncolor: color of the text. Refer to `matplotlib.colors` for full list\nof formats that are accepted.\nhorizontal_alignment (str): see `matplotlib.text.Text`\nrotation: rotation angle in degrees CCW\nReturns:\noutput (VisImage): image object with text drawn.\n\"\"\"\nif not font_size:\nfont_size = self._default_font_size\n# since the text background is dark, we don't want the text to be dark\ncolor = np.maximum(list(mplc.to_rgb(color)), 0.2)\ncolor[np.argmax(color)] = max(0.8, np.max(color))\nx, y = position\nself.output.ax.text(\nx,\ny,\ntext,\nsize=font_size * self.output.scale,\nfamily=\"sans-serif\",\nbbox={\"facecolor\": \"black\", \"alpha\": 0.8, \"pad\": 0.7, \"edgecolor\": \"none\"},\nverticalalignment=\"top\",\nhorizontalalignment=horizontal_alignment,\ncolor=color,\nzorder=10,\nrotation=rotation,\n)\nreturn self.output\ndef draw_box(self, box_coord, alpha=0.5, edge_color=\"g\", line_style=\"-\"):\n\"\"\"\nArgs:\nbox_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0\nare the coordinates of the image's top left corner. x1 and y1 are the\ncoordinates of the image's bottom right corner.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx0, y0, x1, y1 = box_coord\nwidth = x1 - x0\nheight = y1 - y0\nlinewidth = max(self._default_font_size / 4, 1)\nself.output.ax.add_patch(\nmpl.patches.Rectangle(\n(x0, y0),\nwidth,\nheight,\nfill=False,\nedgecolor=edge_color,\nlinewidth=linewidth * self.output.scale,\nalpha=alpha,\nlinestyle=line_style,\n)\n)\nreturn self.output\ndef draw_rotated_box_with_label(\nself, rotated_box, alpha=0.5, edge_color=\"g\", line_style=\"-\", label=None\n):\n\"\"\"\nDraw a rotated box with label on its top-left corner.\nArgs:\nrotated_box (tuple): a tuple containing (cnt_x, cnt_y, w, h, angle),\nwhere cnt_x and cnt_y are the center coordinates of the box.\nw and h are the width and height of the box. angle represents how\nmany degrees the box is rotated CCW with regard to the 0-degree box.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nlabel (string): label for rotated box. It will not be rendered when set to None.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\ncnt_x, cnt_y, w, h, angle = rotated_box\narea = w * h\n# use thinner lines when the box is small\nlinewidth = self._default_font_size / (\n6 if area < _SMALL_OBJECT_AREA_THRESH * self.output.scale else 3\n)\ntheta = angle * math.pi / 180.0\nc = math.cos(theta)\ns = math.sin(theta)\nrect = [(-w / 2, h / 2), (-w / 2, -h / 2), (w / 2, -h / 2), (w / 2, h / 2)]\n# x: left->right ; y: top->down\nrotated_rect = [(s * yy + c * xx + cnt_x, c * yy - s * xx + cnt_y) for (xx, yy) in rect]\nfor k in range(4):\nj = (k + 1) % 4\nself.draw_line(\n[rotated_rect[k][0], rotated_rect[j][0]],\n[rotated_rect[k][1], rotated_rect[j][1]],\ncolor=edge_color,\nlinestyle=\"--\" if k == 1 else line_style,\nlinewidth=linewidth,\n)\nif label is not None:\ntext_pos = rotated_rect[1]  # topleft corner\nheight_ratio = h / np.sqrt(self.output.height * self.output.width)\nlabel_color = self._change_color_brightness(edge_color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2) * 0.5 * self._default_font_size\n)\nself.draw_text(label, text_pos, color=label_color, font_size=font_size, rotation=angle)\nreturn self.output\ndef draw_circle(self, circle_coord, color, radius=3):\n\"\"\"\nArgs:\ncircle_coord (list(int) or tuple(int)): contains the x and y coordinates\nof the center of the circle.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nradius (int): radius of the circle.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx, y = circle_coord\nself.output.ax.add_patch(\nmpl.patches.Circle(circle_coord, radius=radius, fill=True, color=color)\n)\nreturn self.output\ndef draw_line(self, x_data, y_data, color, linestyle=\"-\", linewidth=None):\n\"\"\"\nArgs:\nx_data (list[int]): a list containing x values of all the points being drawn.\nLength of list should match the length of y_data.\ny_data (list[int]): a list containing y values of all the points being drawn.\nLength of list should match the length of x_data.\ncolor: color of the line. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nlinestyle: style of the line. Refer to `matplotlib.lines.Line2D`\nfor a full list of formats that are accepted.\nlinewidth (float or None): width of the line. When it's None,\na default value will be computed and used.\nReturns:\noutput (VisImage): image object with line drawn.\n\"\"\"\nif linewidth is None:\nlinewidth = self._default_font_size / 3\nlinewidth = max(linewidth, 1)\nself.output.ax.add_line(\nmpl.lines.Line2D(\nx_data,\ny_data,\nlinewidth=linewidth * self.output.scale,\ncolor=color,\nlinestyle=linestyle,\n)\n)\nreturn self.output\ndef draw_binary_mask(\nself, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n):\n\"\"\"\nArgs:\nbinary_mask (ndarray): numpy array of shape (H, W), where H is the image height and\nW is the image width. Each value in the array is either a 0 or 1 value of uint8\ntype.\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\narea_threshold (float): a connected component smaller than this area will not be shown.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nhas_valid_segment = False\nbinary_mask = binary_mask.astype(\"uint8\")  # opencv needs uint8\nmask = GenericMask(binary_mask, self.output.height, self.output.width)\nshape2d = (binary_mask.shape[0], binary_mask.shape[1])\nif not mask.has_holes:\n# draw polygons for regular masks\nfor segment in mask.polygons:\narea = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))\nif area < (area_threshold or 0):\ncontinue\nhas_valid_segment = True\nsegment = segment.reshape(-1, 2)\nself.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)\nelse:\n# TODO: Use Path/PathPatch to draw vector graphics:\n# https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = (mask.mask == 1).astype(\"float32\") * alpha\nhas_valid_segment = True\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None and has_valid_segment:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\ncenter = self._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output, center\ndef draw_soft_mask(self, soft_mask, color=None, *, text=None, alpha=0.5):\n\"\"\"\nArgs:\nsoft_mask (ndarray): float array of shape (H, W), each value in [0, 1].\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nshape2d = (soft_mask.shape[0], soft_mask.shape[1])\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = soft_mask * alpha\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nbinary_mask = (soft_mask > 0.5).astype(\"uint8\")\nself._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output\ndef draw_polygon(self, segment, color, edge_color=None, alpha=0.5):\n\"\"\"\nArgs:\nsegment: numpy array of shape Nx2, containing all the points in the polygon.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted. If not provided, a darker shade\nof the polygon color will be used instead.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with polygon drawn.\n\"\"\"\nif edge_color is None:\n# make edge color darker than the polygon color\nif alpha > 0.8:\nedge_color = self._change_color_brightness(color, brightness_factor=-0.7)\nelse:\nedge_color = color\nedge_color = mplc.to_rgb(edge_color) + (1,)\npolygon = mpl.patches.Polygon(\nsegment,\nfill=True,\nfacecolor=mplc.to_rgb(color) + (alpha,),\nedgecolor=edge_color,\nlinewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n)\nself.output.ax.add_patch(polygon)\nreturn self.output\n\"\"\"\nInternal methods:\n\"\"\"\ndef _jitter(self, color):\n\"\"\"\nRandomly modifies given color to produce a slightly different color than the color given.\nArgs:\ncolor (tuple[double]): a tuple of 3 elements, containing the RGB values of the color\npicked. The values in the list are in the [0.0, 1.0] range.\nReturns:\njittered_color (tuple[double]): a tuple of 3 elements, containing the RGB values of the\ncolor after being jittered. The values in the list are in the [0.0, 1.0] range.\n\"\"\"\ncolor = mplc.to_rgb(color)\nvec = np.random.rand(3)\n# better to do it in another color space\nvec = vec / np.linalg.norm(vec) * 0.5\nres = np.clip(vec + color, 0, 1)\nreturn tuple(res)\ndef _create_grayscale_image(self, mask=None):\n\"\"\"\nCreate a grayscale version of the original image.\nThe colors in masked area, if given, will be kept.\n\"\"\"\nimg_bw = self.img.astype(\"f4\").mean(axis=2)\nimg_bw = np.stack([img_bw] * 3, axis=2)\nif mask is not None:\nimg_bw[mask] = self.img[mask]\nreturn img_bw\ndef _change_color_brightness(self, color, brightness_factor):\n\"\"\"\nDepending on the brightness_factor, gives a lighter or darker color i.e. a color with\nless or more saturation than the original color.\nArgs:\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nbrightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of\n0 will correspond to no change, a factor in [-1.0, 0) range will result in\na darker color and a factor in (0, 1.0] range will result in a lighter color.\nReturns:\nmodified_color (tuple[double]): a tuple containing the RGB values of the\nmodified color. Each value in the tuple is in the [0.0, 1.0] range.\n\"\"\"\nassert brightness_factor >= -1.0 and brightness_factor <= 1.0\ncolor = mplc.to_rgb(color)\npolygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))\nmodified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])\nmodified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness\nmodified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness\nmodified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])\nreturn modified_color\ndef _convert_boxes(self, boxes):\n\"\"\"\nConvert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n\"\"\"\nif isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):\nreturn boxes.tensor.detach().numpy()\nelse:\nreturn np.asarray(boxes)\ndef _convert_masks(self, masks_or_polygons):\n\"\"\"\nConvert different format of masks or polygons to a tuple of masks and polygons.\nReturns:\nlist[GenericMask]:\n\"\"\"\nm = masks_or_polygons\nif isinstance(m, PolygonMasks):\nm = m.polygons\nif isinstance(m, BitMasks):\nm = m.tensor.numpy()\nif isinstance(m, torch.Tensor):\nm = m.numpy()\nret = []\nfor x in m:\nif isinstance(x, GenericMask):\nret.append(x)\nelse:\nret.append(GenericMask(x, self.output.height, self.output.width))\nreturn ret\ndef _draw_text_in_mask(self, binary_mask, text, color):\n\"\"\"\nFind proper places to draw text given a binary mask.\n\"\"\"\n# TODO sometimes drawn on wrong objects. the heuristics here can improve.\n_num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, 8)\nif stats[1:, -1].size == 0:\nreturn\nlargest_component_id = np.argmax(stats[1:, -1]) + 1\n# draw text on the largest component, as well as other very large components.\nfor cid in range(1, _num_cc):\nif cid == largest_component_id or stats[cid, -1] > _LARGE_MASK_AREA_THRESH:\n# median is more stable than centroid\n# center = centroids[largest_component_id]\ncenter = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]\nself.draw_text(text, center, color=color)\nreturn center\ndef _convert_keypoints(self, keypoints):\nif isinstance(keypoints, Keypoints):\nkeypoints = keypoints.tensor\nkeypoints = np.asarray(keypoints)\nreturn keypoints\ndef get_output(self):\n\"\"\"\nReturns:\noutput (VisImage): the image output containing the visualizations added\nto the image.\n\"\"\"\nreturn self.output",
                        "max_stars_repo_path": "detectron2/utils/visualizer.py",
                        "max_stars_repo_name": "robolableonardo/detectron2",
                        "max_stars_count": 0,
                        "__cluster__": 186
                },
                {
                        "id": "test_evocodebench_data_187",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nimport colorsys\nimport logging\nimport math\nfrom enum import Enum, unique\nimport cv2\nimport matplotlib as mpl\nimport matplotlib.colors as mplc\nimport matplotlib.figure as mplfigure\nimport numpy as np\nimport pycocotools.mask as mask_util\nimport torch\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.structures import (\nBitMasks,\nBoxes,\nBoxMode,\nKeypoints,\nPolygonMasks,\nRotatedBoxes,\n)\nfrom detectron2.utils.file_io import PathManager\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\nfrom PIL import Image\nfrom .colormap import random_color\nlogger = logging.getLogger(__name__)\n__all__ = [\"ColorMode\", \"VisImage\", \"Visualizer\"]\n_SMALL_OBJECT_AREA_THRESH = 1000\n_LARGE_MASK_AREA_THRESH = 120000\n_OFF_WHITE = (1.0, 1.0, 240.0 / 255)\n_BLACK = (0, 0, 0)\n_RED = (1.0, 0, 0)\n_KEYPOINT_THRESHOLD = 0.05\n@unique\nclass ColorMode(Enum):\n\"\"\"\nEnum of different color modes to use for instance visualizations.\n\"\"\"\nIMAGE = 0\n\"\"\"\nPicks a random color for every instance and overlay segmentations with low opacity.\n\"\"\"\nSEGMENTATION = 1\n\"\"\"\nLet instances of the same category have similar colors\n(from metadata.thing_colors), and overlay them with\nhigh opacity. This provides more attention on the quality of segmentation.\n\"\"\"\nIMAGE_BW = 2\n\"\"\"\nSame as IMAGE, but convert all areas without masks to gray-scale.\nOnly available for drawing per-instance mask predictions.\n\"\"\"\nclass GenericMask:\n\"\"\"\nAttribute:\npolygons (list[ndarray]): list[ndarray]: polygons for this mask.\nEach ndarray has format [x, y, x, y, ...]\nmask (ndarray): a binary mask\n\"\"\"\ndef __init__(self, mask_or_polygons, height, width):\nself._mask = self._polygons = self._has_holes = None\nself.height = height\nself.width = width\nm = mask_or_polygons\nif isinstance(m, dict):\n# RLEs\nassert \"counts\" in m and \"size\" in m\nif isinstance(m[\"counts\"], list):  # uncompressed RLEs\nh, w = m[\"size\"]\nassert h == height and w == width\nm = mask_util.frPyObjects(m, h, w)\nself._mask = mask_util.decode(m)[:, :]\nreturn\nif isinstance(m, list):  # list[ndarray]\nself._polygons = [np.asarray(x).reshape(-1) for x in m]\nreturn\nif isinstance(m, np.ndarray):  # assumed to be a binary mask\nassert m.shape[1] != 2, m.shape\nassert m.shape == (\nheight,\nwidth,\n), f\"mask shape: {m.shape}, target dims: {height}, {width}\"\nself._mask = m.astype(\"uint8\")\nreturn\nraise ValueError(\"GenericMask cannot handle object {} of type '{}'\".format(m, type(m)))\n@property\ndef mask(self):\nif self._mask is None:\nself._mask = self.polygons_to_mask(self._polygons)\nreturn self._mask\n@property\ndef polygons(self):\nif self._polygons is None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nreturn self._polygons\n@property\ndef has_holes(self):\nif self._has_holes is None:\nif self._mask is not None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nelse:\nself._has_holes = False  # if original format is polygon, does not have holes\nreturn self._has_holes\ndef mask_to_polygons(self, mask):\n# cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level\n# hierarchy. External contours (boundary) of the object are placed in hierarchy-1.\n# Internal contours (holes) are placed in hierarchy-2.\n# cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.\nmask = np.ascontiguousarray(mask)  # some versions of cv2 does not support incontiguous arr\nres = cv2.findContours(mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\nhierarchy = res[-1]\nif hierarchy is None:  # empty mask\nreturn [], False\nhas_holes = (hierarchy.reshape(-1, 4)[:, 3] >= 0).sum() > 0\nres = res[-2]\nres = [x.flatten() for x in res]\n# These coordinates from OpenCV are integers in range [0, W-1 or H-1].\n# We add 0.5 to turn them into real-value coordinate space. A better solution\n# would be to first +0.5 and then dilate the returned polygon by 0.5.\nres = [x + 0.5 for x in res if len(x) >= 6]\nreturn res, has_holes\ndef polygons_to_mask(self, polygons):\nrle = mask_util.frPyObjects(polygons, self.height, self.width)\nrle = mask_util.merge(rle)\nreturn mask_util.decode(rle)[:, :]\ndef area(self):\nreturn self.mask.sum()\ndef bbox(self):\np = mask_util.frPyObjects(self.polygons, self.height, self.width)\np = mask_util.merge(p)\nbbox = mask_util.toBbox(p)\nbbox[2] += bbox[0]\nbbox[3] += bbox[1]\nreturn bbox\nclass _PanopticPrediction:\n\"\"\"\nUnify different panoptic annotation/prediction formats\n\"\"\"\ndef __init__(self, panoptic_seg, segments_info, metadata=None):\nif segments_info is None:\nassert metadata is not None\n# If \"segments_info\" is None, we assume \"panoptic_img\" is a\n# H*W int32 image storing the panoptic_id in the format of\n# category_id * label_divisor + instance_id. We reserve -1 for\n# VOID label.\nlabel_divisor = metadata.label_divisor\nsegments_info = []\nfor panoptic_label in np.unique(panoptic_seg.numpy()):\nif panoptic_label == -1:\n# VOID region.\ncontinue\npred_class = panoptic_label // label_divisor\nisthing = pred_class in metadata.thing_dataset_id_to_contiguous_id.values()\nsegments_info.append(\n{\n\"id\": int(panoptic_label),\n\"category_id\": int(pred_class),\n\"isthing\": bool(isthing),\n}\n)\ndel metadata\nself._seg = panoptic_seg\nself._sinfo = {s[\"id\"]: s for s in segments_info}  # seg id -> seg info\nsegment_ids, areas = torch.unique(panoptic_seg, sorted=True, return_counts=True)\nareas = areas.numpy()\nsorted_idxs = np.argsort(-areas)\nself._seg_ids, self._seg_areas = segment_ids[sorted_idxs], areas[sorted_idxs]\nself._seg_ids = self._seg_ids.tolist()\nfor sid, area in zip(self._seg_ids, self._seg_areas):\nif sid in self._sinfo:\nself._sinfo[sid][\"area\"] = float(area)\ndef non_empty_mask(self):\n\"\"\"\nReturns:\n(H, W) array, a mask for all pixels that have a prediction\n\"\"\"\nempty_ids = []\nfor id in self._seg_ids:\nif id not in self._sinfo:\nempty_ids.append(id)\nif len(empty_ids) == 0:\nreturn np.zeros(self._seg.shape, dtype=np.uint8)\nassert (\nlen(empty_ids) == 1\n), \">1 ids corresponds to no labels. This is currently not supported\"\nreturn (self._seg != empty_ids[0]).numpy().astype(np.bool)\ndef semantic_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or sinfo[\"isthing\"]:\n# Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions.\ncontinue\nyield (self._seg == sid).numpy().astype(np.bool), sinfo\ndef instance_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or not sinfo[\"isthing\"]:\ncontinue\nmask = (self._seg == sid).numpy().astype(np.bool)\nif mask.sum() > 0:\nyield mask, sinfo\ndef _create_text_labels(classes, scores, class_names, is_crowd=None):\n\"\"\"\nArgs:\nclasses (list[int] or None):\nscores (list[float] or None):\nclass_names (list[str] or None):\nis_crowd (list[bool] or None):\nReturns:\nlist[str] or None\n\"\"\"\nlabels = None\nif classes is not None:\nif class_names is not None and len(class_names) > 0:\nlabels = [class_names[i] for i in classes]\nelse:\nlabels = [str(i) for i in classes]\nif scores is not None:\nif labels is None:\nlabels = [\"{:.0f}%\".format(s * 100) for s in scores]\nelse:\nlabels = [\"{} {:.0f}%\".format(l, s * 100) for l, s in zip(labels, scores)]\nif labels is not None and is_crowd is not None:\nlabels = [l + (\"|crowd\" if crowd else \"\") for l, crowd in zip(labels, is_crowd)]\nreturn labels\nclass VisImage:\ndef __init__(self, img, scale=1.0):\n\"\"\"\nArgs:\nimg (ndarray): an RGB image of shape (H, W, 3) in range [0, 255].\nscale (float): scale the input image\n\"\"\"\nself.img = img\nself.scale = scale\nself.width, self.height = img.shape[1], img.shape[0]\nself._setup_figure(img)\ndef _setup_figure(self, img):\n\"\"\"\nArgs:\nSame as in :meth:`__init__()`.\nReturns:\nfig (matplotlib.pyplot.figure): top level container for all the image plot elements.\nax (matplotlib.pyplot.Axes): contains figure elements and sets the coordinate system.\n\"\"\"\nfig = mplfigure.Figure(frameon=False)\nself.dpi = fig.get_dpi()\n# add a small 1e-2 to avoid precision lost due to matplotlib's truncation\n# (https://github.com/matplotlib/matplotlib/issues/15363)\nfig.set_size_inches(\n(self.width * self.scale + 1e-2) / self.dpi,\n(self.height * self.scale + 1e-2) / self.dpi,\n)\nself.canvas = FigureCanvasAgg(fig)\n# self.canvas = mpl.backends.backend_cairo.FigureCanvasCairo(fig)\nax = fig.add_axes([0.0, 0.0, 1.0, 1.0])\nax.axis(\"off\")\nself.fig = fig\nself.ax = ax\nself.reset_image(img)\ndef reset_image(self, img):\n\"\"\"\nArgs:\nimg: same as in __init__\n\"\"\"\nimg = img.astype(\"uint8\")\nself.ax.imshow(img, extent=(0, self.width, self.height, 0), interpolation=\"nearest\")\ndef save(self, filepath):\n\"\"\"\nArgs:\nfilepath (str): a string that contains the absolute path, including the file name, where\nthe visualized image will be saved.\n\"\"\"\nself.fig.savefig(filepath)\ndef get_image(self):\n\"\"\"\nReturns:\nndarray:\nthe visualized image of shape (H, W, 3) (RGB) in uint8 type.\nThe shape is scaled w.r.t the input image using the given `scale` argument.\n\"\"\"\ncanvas = self.canvas\ns, (width, height) = canvas.print_to_buffer()\n# buf = io.BytesIO()  # works for cairo backend\n# canvas.print_rgba(buf)\n# width, height = self.width, self.height\n# s = buf.getvalue()\nbuffer = np.frombuffer(s, dtype=\"uint8\")\nimg_rgba = buffer.reshape(height, width, 4)\nrgb, alpha = np.split(img_rgba, [3], axis=2)\nreturn rgb.astype(\"uint8\")\nclass Visualizer:\n\"\"\"\nVisualizer that draws data about detection/segmentation on images.\nIt contains methods like `draw_{text,box,circle,line,binary_mask,polygon}`\nthat draw primitive objects to images, as well as high-level wrappers like\n`draw_{instance_predictions,sem_seg,panoptic_seg_predictions,dataset_dict}`\nthat draw composite data in some pre-defined style.\nNote that the exact visualization style for the high-level wrappers are subject to change.\nStyle such as color, opacity, label contents, visibility of labels, or even the visibility\nof objects themselves (e.g. when the object is too small) may change according\nto different heuristics, as long as the results still look visually reasonable.\nTo obtain a consistent style, you can implement custom drawing functions with the\nabovementioned primitive methods instead. If you need more customized visualization\nstyles, you can process the data yourself following their format documented in\ntutorials (:doc:`/tutorials/models`, :doc:`/tutorials/datasets`). This class does not\nintend to satisfy everyone's preference on drawing styles.\nThis visualizer focuses on high rendering quality rather than performance. It is not\ndesigned to be used for real-time applications.\n\"\"\"\n# TODO implement a fast, rasterized version using OpenCV\ndef __init__(self, img_rgb, metadata=None, scale=1.0, instance_mode=ColorMode.IMAGE):\n\"\"\"\nArgs:\nimg_rgb: a numpy array of shape (H, W, C), where H and W correspond to\nthe height and width of the image respectively. C is the number of\ncolor channels. The image is required to be in RGB format since that\nis a requirement of the Matplotlib library. The image is also expected\nto be in the range [0, 255].\nmetadata (Metadata): dataset metadata (e.g. class names and colors)\ninstance_mode (ColorMode): defines one of the pre-defined style for drawing\ninstances on an image.\n\"\"\"\nself.img = np.asarray(img_rgb).clip(0, 255).astype(np.uint8)\nif metadata is None:\nmetadata = MetadataCatalog.get(\"__nonexist__\")\nself.metadata = metadata\nself.output = VisImage(self.img, scale=scale)\nself.cpu_device = torch.device(\"cpu\")\n# too small texts are useless, therefore clamp to 9\nself._default_font_size = max(\nnp.sqrt(self.output.height * self.output.width) // 90, 10 // scale\n)\nself._instance_mode = instance_mode\nself.keypoint_threshold = _KEYPOINT_THRESHOLD\ndef draw_instance_predictions(self, predictions):\n\"\"\"\nDraw instance-level prediction results on an image.\nArgs:\npredictions (Instances): the output of an instance detection/segmentation\nmodel. Following fields will be used to draw:\n\"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nboxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\nscores = predictions.scores if predictions.has(\"scores\") else None\nclasses = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\nlabels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\nkeypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\nif predictions.has(\"pred_masks\"):\nmasks = np.asarray(predictions.pred_masks)\nmasks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\nelse:\nmasks = None\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n]\nalpha = 0.8\nelse:\ncolors = None\nalpha = 0.5\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(\nself._create_grayscale_image(\n(predictions.pred_masks.any(dim=0) > 0).numpy()\nif predictions.has(\"pred_masks\")\nelse None\n)\n)\nalpha = 0.3\nself.overlay_instances(\nmasks=masks,\nboxes=boxes,\nlabels=labels,\nkeypoints=keypoints,\nassigned_colors=colors,\nalpha=alpha,\n)\nreturn self.output\ndef draw_sem_seg(self, sem_seg, area_threshold=None, alpha=0.8):\n\"\"\"\nDraw semantic segmentation predictions/labels.\nArgs:\nsem_seg (Tensor or ndarray): the segmentation of shape (H, W).\nEach value is the integer label of the pixel.\narea_threshold (int): segments with less than `area_threshold` are not drawn.\nalpha (float): the larger it is, the more opaque the segmentations are.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nif isinstance(sem_seg, torch.Tensor):\nsem_seg = sem_seg.numpy()\nlabels, areas = np.unique(sem_seg, return_counts=True)\nsorted_idxs = np.argsort(-areas).tolist()\nlabels = labels[sorted_idxs]\nfor label in filter(lambda l: l < len(self.metadata.stuff_classes), labels):\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[label]]\nexcept (AttributeError, IndexError):\nmask_color = None\nbinary_mask = (sem_seg == label).astype(np.uint8)\ntext = self.metadata.stuff_classes[label]\nself.draw_binary_mask(\nbinary_mask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\nreturn self.output\ndef draw_panoptic_seg(self, panoptic_seg, segments_info, area_threshold=None, alpha=0.7):\n\"\"\"\nDraw panoptic prediction annotations or results.\nArgs:\npanoptic_seg (Tensor): of shape (height, width) where the values are ids for each\nsegment.\nsegments_info (list[dict] or None): Describe each segment in `panoptic_seg`.\nIf it is a ``list[dict]``, each dict contains keys \"id\", \"category_id\".\nIf None, category id of each pixel is computed by\n``pixel // metadata.label_divisor``.\narea_threshold (int): stuff segments with less than `area_threshold` are not drawn.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\npred = _PanopticPrediction(panoptic_seg, segments_info, self.metadata)\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(self._create_grayscale_image(pred.non_empty_mask()))\n# draw mask for all semantic segments first i.e. \"stuff\"\nfor mask, sinfo in pred.semantic_masks():\ncategory_idx = sinfo[\"category_id\"]\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]\nexcept AttributeError:\nmask_color = None\ntext = self.metadata.stuff_classes[category_idx]\nself.draw_binary_mask(\nmask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\n# draw mask for all instances second\nall_instances = list(pred.instance_masks())\nif len(all_instances) == 0:\nreturn self.output\nmasks, sinfo = list(zip(*all_instances))\ncategory_ids = [x[\"category_id\"] for x in sinfo]\ntry:\nscores = [x[\"score\"] for x in sinfo]\nexcept KeyError:\nscores = None\nlabels = _create_text_labels(\ncategory_ids, scores, self.metadata.thing_classes, [x.get(\"iscrowd\", 0) for x in sinfo]\n)\ntry:\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in category_ids\n]\nexcept AttributeError:\ncolors = None\nself.overlay_instances(masks=masks, labels=labels, assigned_colors=colors, alpha=alpha)\nreturn self.output\ndraw_panoptic_seg_predictions = draw_panoptic_seg  # backward compatibility\ndef draw_dataset_dict(self, dic):\n\"\"\"\nDraw annotations/segmentaions in Detectron2 Dataset format.\nArgs:\ndic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nannos = dic.get(\"annotations\", None)\nif annos:\nif \"segmentation\" in annos[0]:\nmasks = [x[\"segmentation\"] for x in annos]\nelse:\nmasks = None\nif \"keypoints\" in annos[0]:\nkeypts = [x[\"keypoints\"] for x in annos]\nkeypts = np.array(keypts).reshape(len(annos), -1, 3)\nelse:\nkeypts = None\nboxes = [\nBoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS)\nif len(x[\"bbox\"]) == 4\nelse x[\"bbox\"]\nfor x in annos\n]\ncolors = None\ncategory_ids = [x[\"category_id\"] for x in annos]\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]])\nfor c in category_ids\n]\nnames = self.metadata.get(\"thing_classes\", None)\nlabels = _create_text_labels(\ncategory_ids,\nscores=None,\nclass_names=names,\nis_crowd=[x.get(\"iscrowd\", 0) for x in annos],\n)\nself.overlay_instances(\nlabels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n)\nsem_seg = dic.get(\"sem_seg\", None)\nif sem_seg is None and \"sem_seg_file_name\" in dic:\nwith PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\nsem_seg = Image.open(f)\nsem_seg = np.asarray(sem_seg, dtype=\"uint8\")\nif sem_seg is not None:\nself.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\npan_seg = dic.get(\"pan_seg\", None)\nif pan_seg is None and \"pan_seg_file_name\" in dic:\nwith PathManager.open(dic[\"pan_seg_file_name\"], \"rb\") as f:\npan_seg = Image.open(f)\npan_seg = np.asarray(pan_seg)\nfrom panopticapi.utils import rgb2id\npan_seg = rgb2id(pan_seg)\nif pan_seg is not None:\nsegments_info = dic[\"segments_info\"]\npan_seg = torch.tensor(pan_seg)\nself.draw_panoptic_seg(pan_seg, segments_info, area_threshold=0, alpha=0.5)\nreturn self.output\ndef overlay_instances(\nself,\n*,\nboxes=None,\nlabels=None,\nmasks=None,\nkeypoints=None,\nassigned_colors=None,\nalpha=0.5,\n):\n\"\"\"\nArgs:\nboxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,\nor an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,\nor a :class:`RotatedBoxes`,\nor an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image,\nlabels (list[str]): the text to be displayed for each instance.\nmasks (masks-like object): Supported types are:\n* :class:`detectron2.structures.PolygonMasks`,\n:class:`detectron2.structures.BitMasks`.\n* list[list[ndarray]]: contains the segmentation masks for all objects in one image.\nThe first level of the list corresponds to individual instances. The second\nlevel to all the polygon that compose the instance, and the third level\nto the polygon coordinates. The third level should have the format of\n[x0, y0, x1, y1, ..., xn, yn] (n >= 3).\n* list[ndarray]: each ndarray is a binary mask of shape (H, W).\n* list[dict]: each dict is a COCO-style RLE.\nkeypoints (Keypoint or array like): an array-like object of shape (N, K, 3),\nwhere the N is the number of instances and K is the number of keypoints.\nThe last dimension corresponds to (x, y, visibility or score).\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = 0\nif boxes is not None:\nboxes = self._convert_boxes(boxes)\nnum_instances = len(boxes)\nif masks is not None:\nmasks = self._convert_masks(masks)\nif num_instances:\nassert len(masks) == num_instances\nelse:\nnum_instances = len(masks)\nif keypoints is not None:\nif num_instances:\nassert len(keypoints) == num_instances\nelse:\nnum_instances = len(keypoints)\nkeypoints = self._convert_keypoints(keypoints)\nif labels is not None:\nassert len(labels) == num_instances\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\nif boxes is not None and boxes.shape[1] == 5:\nreturn self.overlay_rotated_instances(\nboxes=boxes, labels=labels, assigned_colors=assigned_colors\n)\n# Display in largest to smallest order to reduce occlusion.\nareas = None\nif boxes is not None:\nareas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\nelif masks is not None:\nareas = np.asarray([x.area() for x in masks])\nif areas is not None:\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs] if boxes is not None else None\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\nmasks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\nassigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\nkeypoints = keypoints[sorted_idxs] if keypoints is not None else None\nfor i in range(num_instances):\ncolor = assigned_colors[i]\nif boxes is not None:\nself.draw_box(boxes[i], edge_color=color)\nif masks is not None:\nfor segment in masks[i].polygons:\nself.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\nif labels is not None:\n# first get a box\nif boxes is not None:\nx0, y0, x1, y1 = boxes[i]\ntext_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\nhoriz_align = \"left\"\nelif masks is not None:\n# skip small mask without polygon\nif len(masks[i].polygons) == 0:\ncontinue\nx0, y0, x1, y1 = masks[i].bbox()\n# draw text in the center (defined by median) when box is not drawn\n# median is less sensitive to outliers.\ntext_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\nhoriz_align = \"center\"\nelse:\ncontinue  # drawing the box confidence for keypoints isn't very useful.\n# for small objects, draw text at the side to avoid occlusion\ninstance_area = (y1 - y0) * (x1 - x0)\nif (\ninstance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\nor y1 - y0 < 40 * self.output.scale\n):\nif y1 >= self.output.height - 5:\ntext_pos = (x1, y0)\nelse:\ntext_pos = (x0, y1)\nheight_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n* 0.5\n* self._default_font_size\n)\nself.draw_text(\nlabels[i],\ntext_pos,\ncolor=lighter_color,\nhorizontal_alignment=horiz_align,\nfont_size=font_size,\n)\n# draw keypoints\nif keypoints is not None:\nfor keypoints_per_instance in keypoints:\nself.draw_and_connect_keypoints(keypoints_per_instance)\nreturn self.output\ndef overlay_rotated_instances(self, boxes=None, labels=None, assigned_colors=None):\n\"\"\"\nArgs:\nboxes (ndarray): an Nx5 numpy array of\n(x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image.\nlabels (list[str]): the text to be displayed for each instance.\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = len(boxes)\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\n# Display in largest to smallest order to reduce occlusion.\nif boxes is not None:\nareas = boxes[:, 2] * boxes[:, 3]\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs]\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\ncolors = [assigned_colors[idx] for idx in sorted_idxs]\nfor i in range(num_instances):\nself.draw_rotated_box_with_label(\nboxes[i], edge_color=colors[i], label=labels[i] if labels is not None else None\n)\nreturn self.output\ndef draw_and_connect_keypoints(self, keypoints):\n\"\"\"\nDraws keypoints of an instance and follows the rules for keypoint connections\nto draw lines between appropriate keypoints. This follows color heuristics for\nline color.\nArgs:\nkeypoints (Tensor): a tensor of shape (K, 3), where K is the number of keypoints\nand the last dimension corresponds to (x, y, probability).\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nvisible = {}\nkeypoint_names = self.metadata.get(\"keypoint_names\")\nfor idx, keypoint in enumerate(keypoints):\n# draw keypoint\nx, y, prob = keypoint\nif prob > self.keypoint_threshold:\nself.draw_circle((x, y), color=_RED)\nif keypoint_names:\nkeypoint_name = keypoint_names[idx]\nvisible[keypoint_name] = (x, y)\nif self.metadata.get(\"keypoint_connection_rules\"):\nfor kp0, kp1, color in self.metadata.keypoint_connection_rules:\nif kp0 in visible and kp1 in visible:\nx0, y0 = visible[kp0]\nx1, y1 = visible[kp1]\ncolor = tuple(x / 255.0 for x in color)\nself.draw_line([x0, x1], [y0, y1], color=color)\n# draw lines from nose to mid-shoulder and mid-shoulder to mid-hip\n# Note that this strategy is specific to person keypoints.\n# For other keypoints, it should just do nothing\ntry:\nls_x, ls_y = visible[\"left_shoulder\"]\nrs_x, rs_y = visible[\"right_shoulder\"]\nmid_shoulder_x, mid_shoulder_y = (ls_x + rs_x) / 2, (ls_y + rs_y) / 2\nexcept KeyError:\npass\nelse:\n# draw line from nose to mid-shoulder\nnose_x, nose_y = visible.get(\"nose\", (None, None))\nif nose_x is not None:\nself.draw_line([nose_x, mid_shoulder_x], [nose_y, mid_shoulder_y], color=_RED)\ntry:\n# draw line from mid-shoulder to mid-hip\nlh_x, lh_y = visible[\"left_hip\"]\nrh_x, rh_y = visible[\"right_hip\"]\nexcept KeyError:\npass\nelse:\nmid_hip_x, mid_hip_y = (lh_x + rh_x) / 2, (lh_y + rh_y) / 2\nself.draw_line([mid_hip_x, mid_shoulder_x], [mid_hip_y, mid_shoulder_y], color=_RED)\nreturn self.output\n\"\"\"\nPrimitive drawing functions:\n\"\"\"\ndef draw_text(\nself,\ntext,\nposition,\n*,\nfont_size=None,\ncolor=\"g\",\nhorizontal_alignment=\"center\",\nrotation=0,\n):\n\"\"\"\nArgs:\ntext (str): class label\nposition (tuple): a tuple of the x and y coordinates to place text on image.\nfont_size (int, optional): font of the text. If not provided, a font size\nproportional to the image width is calculated and used.\ncolor: color of the text. Refer to `matplotlib.colors` for full list\nof formats that are accepted.\nhorizontal_alignment (str): see `matplotlib.text.Text`\nrotation: rotation angle in degrees CCW\nReturns:\noutput (VisImage): image object with text drawn.\n\"\"\"\nif not font_size:\nfont_size = self._default_font_size\n# since the text background is dark, we don't want the text to be dark\ncolor = np.maximum(list(mplc.to_rgb(color)), 0.2)\ncolor[np.argmax(color)] = max(0.8, np.max(color))\nx, y = position\nself.output.ax.text(\nx,\ny,\ntext,\nsize=font_size * self.output.scale,\nfamily=\"sans-serif\",\nbbox={\"facecolor\": \"black\", \"alpha\": 0.8, \"pad\": 0.7, \"edgecolor\": \"none\"},\nverticalalignment=\"top\",\nhorizontalalignment=horizontal_alignment,\ncolor=color,\nzorder=10,\nrotation=rotation,\n)\nreturn self.output\ndef draw_box(self, box_coord, alpha=0.5, edge_color=\"g\", line_style=\"-\"):\n\"\"\"\nArgs:\nbox_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0\nare the coordinates of the image's top left corner. x1 and y1 are the\ncoordinates of the image's bottom right corner.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx0, y0, x1, y1 = box_coord\nwidth = x1 - x0\nheight = y1 - y0\nlinewidth = max(self._default_font_size / 4, 1)\nself.output.ax.add_patch(\nmpl.patches.Rectangle(\n(x0, y0),\nwidth,\nheight,\nfill=False,\nedgecolor=edge_color,\nlinewidth=linewidth * self.output.scale,\nalpha=alpha,\nlinestyle=line_style,\n)\n)\nreturn self.output\ndef draw_rotated_box_with_label(\nself, rotated_box, alpha=0.5, edge_color=\"g\", line_style=\"-\", label=None\n):\n\"\"\"\nDraw a rotated box with label on its top-left corner.\nArgs:\nrotated_box (tuple): a tuple containing (cnt_x, cnt_y, w, h, angle),\nwhere cnt_x and cnt_y are the center coordinates of the box.\nw and h are the width and height of the box. angle represents how\nmany degrees the box is rotated CCW with regard to the 0-degree box.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nlabel (string): label for rotated box. It will not be rendered when set to None.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\ncnt_x, cnt_y, w, h, angle = rotated_box\narea = w * h\n# use thinner lines when the box is small\nlinewidth = self._default_font_size / (\n6 if area < _SMALL_OBJECT_AREA_THRESH * self.output.scale else 3\n)\ntheta = angle * math.pi / 180.0\nc = math.cos(theta)\ns = math.sin(theta)\nrect = [(-w / 2, h / 2), (-w / 2, -h / 2), (w / 2, -h / 2), (w / 2, h / 2)]\n# x: left->right ; y: top->down\nrotated_rect = [(s * yy + c * xx + cnt_x, c * yy - s * xx + cnt_y) for (xx, yy) in rect]\nfor k in range(4):\nj = (k + 1) % 4\nself.draw_line(\n[rotated_rect[k][0], rotated_rect[j][0]],\n[rotated_rect[k][1], rotated_rect[j][1]],\ncolor=edge_color,\nlinestyle=\"--\" if k == 1 else line_style,\nlinewidth=linewidth,\n)\nif label is not None:\ntext_pos = rotated_rect[1]  # topleft corner\nheight_ratio = h / np.sqrt(self.output.height * self.output.width)\nlabel_color = self._change_color_brightness(edge_color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2) * 0.5 * self._default_font_size\n)\nself.draw_text(label, text_pos, color=label_color, font_size=font_size, rotation=angle)\nreturn self.output\ndef draw_circle(self, circle_coord, color, radius=3):\n\"\"\"\nArgs:\ncircle_coord (list(int) or tuple(int)): contains the x and y coordinates\nof the center of the circle.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nradius (int): radius of the circle.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx, y = circle_coord\nself.output.ax.add_patch(\nmpl.patches.Circle(circle_coord, radius=radius, fill=True, color=color)\n)\nreturn self.output\ndef draw_line(self, x_data, y_data, color, linestyle=\"-\", linewidth=None):\n\"\"\"\nArgs:\nx_data (list[int]): a list containing x values of all the points being drawn.\nLength of list should match the length of y_data.\ny_data (list[int]): a list containing y values of all the points being drawn.\nLength of list should match the length of x_data.\ncolor: color of the line. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nlinestyle: style of the line. Refer to `matplotlib.lines.Line2D`\nfor a full list of formats that are accepted.\nlinewidth (float or None): width of the line. When it's None,\na default value will be computed and used.\nReturns:\noutput (VisImage): image object with line drawn.\n\"\"\"\nif linewidth is None:\nlinewidth = self._default_font_size / 3\nlinewidth = max(linewidth, 1)\nself.output.ax.add_line(\nmpl.lines.Line2D(\nx_data,\ny_data,\nlinewidth=linewidth * self.output.scale,\ncolor=color,\nlinestyle=linestyle,\n)\n)\nreturn self.output\ndef draw_binary_mask(\nself, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n):\n\"\"\"\nArgs:\nbinary_mask (ndarray): numpy array of shape (H, W), where H is the image height and\nW is the image width. Each value in the array is either a 0 or 1 value of uint8\ntype.\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\narea_threshold (float): a connected component smaller than this area will not be shown.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nhas_valid_segment = False\nbinary_mask = binary_mask.astype(\"uint8\")  # opencv needs uint8\nmask = GenericMask(binary_mask, self.output.height, self.output.width)\nshape2d = (binary_mask.shape[0], binary_mask.shape[1])\nif not mask.has_holes:\n# draw polygons for regular masks\nfor segment in mask.polygons:\narea = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))\nif area < (area_threshold or 0):\ncontinue\nhas_valid_segment = True\nsegment = segment.reshape(-1, 2)\nself.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)\nelse:\n# TODO: Use Path/PathPatch to draw vector graphics:\n# https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = (mask.mask == 1).astype(\"float32\") * alpha\nhas_valid_segment = True\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None and has_valid_segment:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nself._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output\ndef draw_soft_mask(self, soft_mask, color=None, *, text=None, alpha=0.5):\n\"\"\"\nArgs:\nsoft_mask (ndarray): float array of shape (H, W), each value in [0, 1].\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nshape2d = (soft_mask.shape[0], soft_mask.shape[1])\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = soft_mask * alpha\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nbinary_mask = (soft_mask > 0.5).astype(\"uint8\")\nself._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output\ndef draw_polygon(self, segment, color, edge_color=None, alpha=0.5):\n\"\"\"\nArgs:\nsegment: numpy array of shape Nx2, containing all the points in the polygon.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted. If not provided, a darker shade\nof the polygon color will be used instead.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with polygon drawn.\n\"\"\"\nif edge_color is None:\n# make edge color darker than the polygon color\nif alpha > 0.8:\nedge_color = self._change_color_brightness(color, brightness_factor=-0.7)\nelse:\nedge_color = color\nedge_color = mplc.to_rgb(edge_color) + (1,)\npolygon = mpl.patches.Polygon(\nsegment,\nfill=True,\nfacecolor=mplc.to_rgb(color) + (alpha,),\nedgecolor=edge_color,\nlinewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n)\nself.output.ax.add_patch(polygon)\nreturn self.output\n\"\"\"\nInternal methods:\n\"\"\"\ndef _jitter(self, color):\n\"\"\"\nRandomly modifies given color to produce a slightly different color than the color given.\nArgs:\ncolor (tuple[double]): a tuple of 3 elements, containing the RGB values of the color\npicked. The values in the list are in the [0.0, 1.0] range.\nReturns:\njittered_color (tuple[double]): a tuple of 3 elements, containing the RGB values of the\ncolor after being jittered. The values in the list are in the [0.0, 1.0] range.\n\"\"\"\ncolor = mplc.to_rgb(color)\nvec = np.random.rand(3)\n# better to do it in another color space\nvec = vec / np.linalg.norm(vec) * 0.5\nres = np.clip(vec + color, 0, 1)\nreturn tuple(res)\ndef _create_grayscale_image(self, mask=None):\n\"\"\"\nCreate a grayscale version of the original image.\nThe colors in masked area, if given, will be kept.\n\"\"\"\nimg_bw = self.img.astype(\"f4\").mean(axis=2)\nimg_bw = np.stack([img_bw] * 3, axis=2)\nif mask is not None:\nimg_bw[mask] = self.img[mask]\nreturn img_bw\ndef _change_color_brightness(self, color, brightness_factor):\n\"\"\"\nDepending on the brightness_factor, gives a lighter or darker color i.e. a color with\nless or more saturation than the original color.\nArgs:\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nbrightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of\n0 will correspond to no change, a factor in [-1.0, 0) range will result in\na darker color and a factor in (0, 1.0] range will result in a lighter color.\nReturns:\nmodified_color (tuple[double]): a tuple containing the RGB values of the\nmodified color. Each value in the tuple is in the [0.0, 1.0] range.\n\"\"\"\nassert brightness_factor >= -1.0 and brightness_factor <= 1.0\ncolor = mplc.to_rgb(color)\npolygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))\nmodified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])\nmodified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness\nmodified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness\nmodified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])\nreturn modified_color\ndef _convert_boxes(self, boxes):\n\"\"\"\nConvert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n\"\"\"\nif isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):\nreturn boxes.tensor.detach().numpy()\nelse:\nreturn np.asarray(boxes)\ndef _convert_masks(self, masks_or_polygons):\n\"\"\"\nConvert different format of masks or polygons to a tuple of masks and polygons.\nReturns:\nlist[GenericMask]:\n\"\"\"\nm = masks_or_polygons\nif isinstance(m, PolygonMasks):\nm = m.polygons\nif isinstance(m, BitMasks):\nm = m.tensor.numpy()\nif isinstance(m, torch.Tensor):\nm = m.numpy()\nret = []\nfor x in m:\nif isinstance(x, GenericMask):\nret.append(x)\nelse:\nret.append(GenericMask(x, self.output.height, self.output.width))\nreturn ret\ndef _draw_text_in_mask(self, binary_mask, text, color):\n\"\"\"\nFind proper places to draw text given a binary mask.\n\"\"\"\n# TODO sometimes drawn on wrong objects. the heuristics here can improve.\n_num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, 8)\nif stats[1:, -1].size == 0:\nreturn\nlargest_component_id = np.argmax(stats[1:, -1]) + 1\n# draw text on the largest component, as well as other very large components.\nfor cid in range(1, _num_cc):\nif cid == largest_component_id or stats[cid, -1] > _LARGE_MASK_AREA_THRESH:\n# median is more stable than centroid\n# center = centroids[largest_component_id]\ncenter = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]\nself.draw_text(text, center, color=color)\ndef _convert_keypoints(self, keypoints):\nif isinstance(keypoints, Keypoints):\nkeypoints = keypoints.tensor\nkeypoints = np.asarray(keypoints)\nreturn keypoints\ndef get_output(self):\n\"\"\"\nReturns:\noutput (VisImage): the image output containing the visualizations added\nto the image.\n\"\"\"\nreturn self.output",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 186
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "pretrain_python_data_7524367",
                        "content": "<gh_stars>0\n# Copyright (c) Facebook, Inc. and its affiliates.\nimport colorsys\nimport logging\nimport math\nimport numpy as np\nfrom enum import Enum, unique\nimport cv2\nimport matplotlib as mpl\nimport matplotlib.colors as mplc\nimport matplotlib.figure as mplfigure\nimport pycocotools.mask as mask_util\nimport torch\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\nfrom PIL import Image\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.structures import BitMasks, Boxes, BoxMode, Keypoints, PolygonMasks, RotatedBoxes\nfrom detectron2.utils.file_io import PathManager\nfrom .colormap import random_color\nlogger = logging.getLogger(__name__)\n__all__ = [\"ColorMode\", \"VisImage\", \"Visualizer\"]\n_SMALL_OBJECT_AREA_THRESH = 1000\n_LARGE_MASK_AREA_THRESH = 120000\n_OFF_WHITE = (1.0, 1.0, 240.0 / 255)\n_BLACK = (0, 0, 0)\n_RED = (1.0, 0, 0)\n_KEYPOINT_THRESHOLD = 0.05\n@unique\nclass ColorMode(Enum):\n\"\"\"\nEnum of different color modes to use for instance visualizations.\n\"\"\"\nIMAGE = 0\n\"\"\"\nPicks a random color for every instance and overlay segmentations with low opacity.\n\"\"\"\nSEGMENTATION = 1\n\"\"\"\nLet instances of the same category have similar colors\n(from metadata.thing_colors), and overlay them with\nhigh opacity. This provides more attention on the quality of segmentation.\n\"\"\"\nIMAGE_BW = 2\n\"\"\"\nSame as IMAGE, but convert all areas without masks to gray-scale.\nOnly available for drawing per-instance mask predictions.\n\"\"\"\nclass GenericMask:\n\"\"\"\nAttribute:\npolygons (list[ndarray]): list[ndarray]: polygons for this mask.\nEach ndarray has format [x, y, x, y, ...]\nmask (ndarray): a binary mask\n\"\"\"\ndef __init__(self, mask_or_polygons, height, width):\nself._mask = self._polygons = self._has_holes = None\nself.height = height\nself.width = width\nm = mask_or_polygons\nif isinstance(m, dict):\n# RLEs\nassert \"counts\" in m and \"size\" in m\nif isinstance(m[\"counts\"], list):  # uncompressed RLEs\nh, w = m[\"size\"]\nassert h == height and w == width\nm = mask_util.frPyObjects(m, h, w)\nself._mask = mask_util.decode(m)[:, :]\nreturn\nif isinstance(m, list):  # list[ndarray]\nself._polygons = [np.asarray(x).reshape(-1) for x in m]\nreturn\nif isinstance(m, np.ndarray):  # assumed to be a binary mask\nassert m.shape[1] != 2, m.shape\nassert m.shape == (\nheight,\nwidth,\n), f\"mask shape: {m.shape}, target dims: {height}, {width}\"\nself._mask = m.astype(\"uint8\")\nreturn\nraise ValueError(\"GenericMask cannot handle object {} of type '{}'\".format(m, type(m)))\n@property\ndef mask(self):\nif self._mask is None:\nself._mask = self.polygons_to_mask(self._polygons)\nreturn self._mask\n@property\ndef polygons(self):\nif self._polygons is None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nreturn self._polygons\n@property\ndef has_holes(self):\nif self._has_holes is None:\nif self._mask is not None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nelse:\nself._has_holes = False  # if original format is polygon, does not have holes\nreturn self._has_holes\ndef mask_to_polygons(self, mask):\n# cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level\n# hierarchy. External contours (boundary) of the object are placed in hierarchy-1.\n# Internal contours (holes) are placed in hierarchy-2.\n# cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.\nmask = np.ascontiguousarray(mask)  # some versions of cv2 does not support incontiguous arr\nres = cv2.findContours(mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\nhierarchy = res[-1]\nif hierarchy is None:  # empty mask\nreturn [], False\nhas_holes = (hierarchy.reshape(-1, 4)[:, 3] >= 0).sum() > 0\nres = res[-2]\nres = [x.flatten() for x in res]\n# These coordinates from OpenCV are integers in range [0, W-1 or H-1].\n# We add 0.5 to turn them into real-value coordinate space. A better solution\n# would be to first +0.5 and then dilate the returned polygon by 0.5.\nres = [x + 0.5 for x in res if len(x) >= 6]\nreturn res, has_holes\ndef polygons_to_mask(self, polygons):\nrle = mask_util.frPyObjects(polygons, self.height, self.width)\nrle = mask_util.merge(rle)\nreturn mask_util.decode(rle)[:, :]\ndef area(self):\nreturn self.mask.sum()\ndef bbox(self):\np = mask_util.frPyObjects(self.polygons, self.height, self.width)\np = mask_util.merge(p)\nbbox = mask_util.toBbox(p)\nbbox[2] += bbox[0]\nbbox[3] += bbox[1]\nreturn bbox\nclass _PanopticPrediction:\n\"\"\"\nUnify different panoptic annotation/prediction formats\n\"\"\"\ndef __init__(self, panoptic_seg, segments_info, metadata=None):\nif segments_info is None:\nassert metadata is not None\n# If \"segments_info\" is None, we assume \"panoptic_img\" is a\n# H*W int32 image storing the panoptic_id in the format of\n# category_id * label_divisor + instance_id. We reserve -1 for\n# VOID label.\nlabel_divisor = metadata.label_divisor\nsegments_info = []\nfor panoptic_label in np.unique(panoptic_seg.numpy()):\nif panoptic_label == -1:\n# VOID region.\ncontinue\npred_class = panoptic_label // label_divisor\nisthing = pred_class in metadata.thing_dataset_id_to_contiguous_id.values()\nsegments_info.append(\n{\n\"id\": int(panoptic_label),\n\"category_id\": int(pred_class),\n\"isthing\": bool(isthing),\n}\n)\ndel metadata\nself._seg = panoptic_seg\nself._sinfo = {s[\"id\"]: s for s in segments_info}  # seg id -> seg info\nsegment_ids, areas = torch.unique(panoptic_seg, sorted=True, return_counts=True)\nareas = areas.numpy()\nsorted_idxs = np.argsort(-areas)\nself._seg_ids, self._seg_areas = segment_ids[sorted_idxs], areas[sorted_idxs]\nself._seg_ids = self._seg_ids.tolist()\nfor sid, area in zip(self._seg_ids, self._seg_areas):\nif sid in self._sinfo:\nself._sinfo[sid][\"area\"] = float(area)\ndef non_empty_mask(self):\n\"\"\"\nReturns:\n(H, W) array, a mask for all pixels that have a prediction\n\"\"\"\nempty_ids = []\nfor id in self._seg_ids:\nif id not in self._sinfo:\nempty_ids.append(id)\nif len(empty_ids) == 0:\nreturn np.zeros(self._seg.shape, dtype=np.uint8)\nassert (\nlen(empty_ids) == 1\n), \">1 ids corresponds to no labels. This is currently not supported\"\nreturn (self._seg != empty_ids[0]).numpy().astype(np.bool)\ndef semantic_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or sinfo[\"isthing\"]:\n# Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions.\ncontinue\nyield (self._seg == sid).numpy().astype(np.bool), sinfo\ndef instance_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or not sinfo[\"isthing\"]:\ncontinue\nmask = (self._seg == sid).numpy().astype(np.bool)\nif mask.sum() > 0:\nyield mask, sinfo\ndef _create_text_labels(classes, scores, class_names, is_crowd=None):\n\"\"\"\nArgs:\nclasses (list[int] or None):\nscores (list[float] or None):\nclass_names (list[str] or None):\nis_crowd (list[bool] or None):\nReturns:\nlist[str] or None\n\"\"\"\nlabels = None\nif classes is not None:\nif class_names is not None and len(class_names) > 0:\nlabels = [class_names[i] for i in classes]\nelse:\nlabels = [str(i) for i in classes]\nif scores is not None:\nif labels is None:\nlabels = [\"{:.0f}%\".format(s * 100) for s in scores]\nelse:\nlabels = [\"{} {:.0f}%\".format(l, s * 100) for l, s in zip(labels, scores)]\nif labels is not None and is_crowd is not None:\nlabels = [l + (\"|crowd\" if crowd else \"\") for l, crowd in zip(labels, is_crowd)]\nreturn labels\nclass VisImage:\ndef __init__(self, img, scale=1.0):\n\"\"\"\nArgs:\nimg (ndarray): an RGB image of shape (H, W, 3) in range [0, 255].\nscale (float): scale the input image\n\"\"\"\nself.img = img\nself.scale = scale\nself.width, self.height = img.shape[1], img.shape[0]\nself._setup_figure(img)\ndef _setup_figure(self, img):\n\"\"\"\nArgs:\nSame as in :meth:`__init__()`.\nReturns:\nfig (matplotlib.pyplot.figure): top level container for all the image plot elements.\nax (matplotlib.pyplot.Axes): contains figure elements and sets the coordinate system.\n\"\"\"\nfig = mplfigure.Figure(frameon=False)\nself.dpi = fig.get_dpi()\n# add a small 1e-2 to avoid precision lost due to matplotlib's truncation\n# (https://github.com/matplotlib/matplotlib/issues/15363)\nfig.set_size_inches(\n(self.width * self.scale + 1e-2) / self.dpi,\n(self.height * self.scale + 1e-2) / self.dpi,\n)\nself.canvas = FigureCanvasAgg(fig)\n# self.canvas = mpl.backends.backend_cairo.FigureCanvasCairo(fig)\nax = fig.add_axes([0.0, 0.0, 1.0, 1.0])\nax.axis(\"off\")\nself.fig = fig\nself.ax = ax\nself.reset_image(img)\ndef reset_image(self, img):\n\"\"\"\nArgs:\nimg: same as in __init__\n\"\"\"\nimg = img.astype(\"uint8\")\nself.ax.imshow(img, extent=(0, self.width, self.height, 0), interpolation=\"nearest\")\ndef save(self, filepath):\n\"\"\"\nArgs:\nfilepath (str): a string that contains the absolute path, including the file name, where\nthe visualized image will be saved.\n\"\"\"\nself.fig.savefig(filepath)\ndef get_image(self):\n\"\"\"\nReturns:\nndarray:\nthe visualized image of shape (H, W, 3) (RGB) in uint8 type.\nThe shape is scaled w.r.t the input image using the given `scale` argument.\n\"\"\"\ncanvas = self.canvas\ns, (width, height) = canvas.print_to_buffer()\n# buf = io.BytesIO()  # works for cairo backend\n# canvas.print_rgba(buf)\n# width, height = self.width, self.height\n# s = buf.getvalue()\nbuffer = np.frombuffer(s, dtype=\"uint8\")\nimg_rgba = buffer.reshape(height, width, 4)\nrgb, alpha = np.split(img_rgba, [3], axis=2)\nreturn rgb.astype(\"uint8\")\nclass Visualizer:\n\"\"\"\nVisualizer that draws data about detection/segmentation on images.\nIt contains methods like `draw_{text,box,circle,line,binary_mask,polygon}`\nthat draw primitive objects to images, as well as high-level wrappers like\n`draw_{instance_predictions,sem_seg,panoptic_seg_predictions,dataset_dict}`\nthat draw composite data in some pre-defined style.\nNote that the exact visualization style for the high-level wrappers are subject to change.\nStyle such as color, opacity, label contents, visibility of labels, or even the visibility\nof objects themselves (e.g. when the object is too small) may change according\nto different heuristics, as long as the results still look visually reasonable.\nTo obtain a consistent style, you can implement custom drawing functions with the\nabovementioned primitive methods instead. If you need more customized visualization\nstyles, you can process the data yourself following their format documented in\ntutorials (:doc:`/tutorials/models`, :doc:`/tutorials/datasets`). This class does not\nintend to satisfy everyone's preference on drawing styles.\nThis visualizer focuses on high rendering quality rather than performance. It is not\ndesigned to be used for real-time applications.\n\"\"\"\n# TODO implement a fast, rasterized version using OpenCV\ndef __init__(self, img_rgb, metadata=None, scale=1.0, instance_mode=ColorMode.IMAGE):\n\"\"\"\nArgs:\nimg_rgb: a numpy array of shape (H, W, C), where H and W correspond to\nthe height and width of the image respectively. C is the number of\ncolor channels. The image is required to be in RGB format since that\nis a requirement of the Matplotlib library. The image is also expected\nto be in the range [0, 255].\nmetadata (Metadata): dataset metadata (e.g. class names and colors)\ninstance_mode (ColorMode): defines one of the pre-defined style for drawing\ninstances on an image.\n\"\"\"\nself.img = np.asarray(img_rgb).clip(0, 255).astype(np.uint8)\nif metadata is None:\nmetadata = MetadataCatalog.get(\"__nonexist__\")\nself.metadata = metadata\nself.output = VisImage(self.img, scale=scale)\nself.cpu_device = torch.device(\"cpu\")\n# too small texts are useless, therefore clamp to 9\nself._default_font_size = max(\nnp.sqrt(self.output.height * self.output.width) // 90, 10 // scale\n)\nself._instance_mode = instance_mode\nself.keypoint_threshold = _KEYPOINT_THRESHOLD\ndef draw_instance_predictions(self, predictions):\n\"\"\"\nDraw instance-level prediction results on an image.\nArgs:\npredictions (Instances): the output of an instance detection/segmentation\nmodel. Following fields will be used to draw:\n\"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nboxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\nscores = predictions.scores if predictions.has(\"scores\") else None\nclasses = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\nlabels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\nkeypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\nif predictions.has(\"pred_masks\"):\nmasks = np.asarray(predictions.pred_masks)\nmasks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\nelse:\nmasks = None\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n]\nalpha = 0.8\nelse:\ncolors = None\nalpha = 0.5\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(\nself._create_grayscale_image(\n(predictions.pred_masks.any(dim=0) > 0).numpy()\nif predictions.has(\"pred_masks\")\nelse None\n)\n)\nalpha = 0.3\nself.overlay_instances(\nmasks=masks,\nboxes=boxes,\nlabels=labels,\nkeypoints=keypoints,\nassigned_colors=colors,\nalpha=alpha,\n)\nreturn self.output\ndef draw_sem_seg(self, sem_seg, area_threshold=None, alpha=0.8):\n\"\"\"\nDraw semantic segmentation predictions/labels.\nArgs:\nsem_seg (Tensor or ndarray): the segmentation of shape (H, W).\nEach value is the integer label of the pixel.\narea_threshold (int): segments with less than `area_threshold` are not drawn.\nalpha (float): the larger it is, the more opaque the segmentations are.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nif isinstance(sem_seg, torch.Tensor):\nsem_seg = sem_seg.numpy()\nlabels, areas = np.unique(sem_seg, return_counts=True)\nsorted_idxs = np.argsort(-areas).tolist()\nlabels = labels[sorted_idxs]\nfor label in filter(lambda l: l < len(self.metadata.stuff_classes), labels):\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[label]]\nexcept (AttributeError, IndexError):\nmask_color = None\nbinary_mask = (sem_seg == label).astype(np.uint8)\ntext = self.metadata.stuff_classes[label]\nself.draw_binary_mask(\nbinary_mask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\nreturn self.output\ndef draw_panoptic_seg(self, panoptic_seg, segments_info, area_threshold=None, alpha=0.7):\n\"\"\"\nDraw panoptic prediction annotations or results.\nArgs:\npanoptic_seg (Tensor): of shape (height, width) where the values are ids for each\nsegment.\nsegments_info (list[dict] or None): Describe each segment in `panoptic_seg`.\nIf it is a ``list[dict]``, each dict contains keys \"id\", \"category_id\".\nIf None, category id of each pixel is computed by\n``pixel // metadata.label_divisor``.\narea_threshold (int): stuff segments with less than `area_threshold` are not drawn.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nsemant_vect = []\npred = _PanopticPrediction(panoptic_seg, segments_info, self.metadata)\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(self._create_grayscale_image(pred.non_empty_mask()))\n# draw mask for all semantic segments first i.e. \"stuff\"\nfor mask, sinfo in pred.semantic_masks():\ncategory_idx = sinfo[\"category_id\"]\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]\nexcept AttributeError:\nmask_color = None\ntext = self.metadata.stuff_classes[category_idx]\n_ , pos_semant=self.draw_binary_mask(\nmask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\nsemant_vect.append([pos_semant, text])\n# draw mask for all instances second\nall_instances = list(pred.instance_masks())\nif len(all_instances) == 0:\nreturn self.output, semant_vect\nmasks, sinfo = list(zip(*all_instances))\ncategory_ids = [x[\"category_id\"] for x in sinfo]\ntry:\nscores = [x[\"score\"] for x in sinfo]\nexcept KeyError:\nscores = None\nlabels = _create_text_labels(\ncategory_ids, scores, self.metadata.thing_classes, [x.get(\"iscrowd\", 0) for x in sinfo]\n)\ntry:\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in category_ids\n]\nexcept AttributeError:\ncolors = None\n_, pos_obj_semant= self.overlay_instances(masks=masks, labels=labels, assigned_colors=colors, alpha=alpha)\nsemant_vect.extend(pos_obj_semant)\nreturn self.output, semant_vect\n#return semant_vect.append(pos_obj_semant)\ndraw_panoptic_seg_predictions = draw_panoptic_seg  # backward compatibility\ndef draw_dataset_dict(self, dic):\n\"\"\"\nDraw annotations/segmentaions in Detectron2 Dataset format.\nArgs:\ndic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nannos = dic.get(\"annotations\", None)\nif annos:\nif \"segmentation\" in annos[0]:\nmasks = [x[\"segmentation\"] for x in annos]\nelse:\nmasks = None\nif \"keypoints\" in annos[0]:\nkeypts = [x[\"keypoints\"] for x in annos]\nkeypts = np.array(keypts).reshape(len(annos), -1, 3)\nelse:\nkeypts = None\nboxes = [\nBoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS)\nif len(x[\"bbox\"]) == 4\nelse x[\"bbox\"]\nfor x in annos\n]\ncolors = None\ncategory_ids = [x[\"category_id\"] for x in annos]\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]])\nfor c in category_ids\n]\nnames = self.metadata.get(\"thing_classes\", None)\nlabels = _create_text_labels(\ncategory_ids,\nscores=None,\nclass_names=names,\nis_crowd=[x.get(\"iscrowd\", 0) for x in annos],\n)\nself.overlay_instances(\nlabels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n)\nsem_seg = dic.get(\"sem_seg\", None)\nif sem_seg is None and \"sem_seg_file_name\" in dic:\nwith PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\nsem_seg = Image.open(f)\nsem_seg = np.asarray(sem_seg, dtype=\"uint8\")\nif sem_seg is not None:\nself.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\npan_seg = dic.get(\"pan_seg\", None)\nif pan_seg is None and \"pan_seg_file_name\" in dic:\nwith PathManager.open(dic[\"pan_seg_file_name\"], \"rb\") as f:\npan_seg = Image.open(f)\npan_seg = np.asarray(pan_seg)\nfrom panopticapi.utils import rgb2id\npan_seg = rgb2id(pan_seg)\nif pan_seg is not None:\nsegments_info = dic[\"segments_info\"]\npan_seg = torch.tensor(pan_seg)\nself.draw_panoptic_seg(pan_seg, segments_info, area_threshold=0, alpha=0.5)\nreturn self.output\ndef overlay_instances(\nself,\n*,\nboxes=None,\nlabels=None,\nmasks=None,\nkeypoints=None,\nassigned_colors=None,\nalpha=0.5,\n):\n\"\"\"\nArgs:\nboxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,\nor an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,\nor a :class:`RotatedBoxes`,\nor an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image,\nlabels (list[str]): the text to be displayed for each instance.\nmasks (masks-like object): Supported types are:\n* :class:`detectron2.structures.PolygonMasks`,\n:class:`detectron2.structures.BitMasks`.\n* list[list[ndarray]]: contains the segmentation masks for all objects in one image.\nThe first level of the list corresponds to individual instances. The second\nlevel to all the polygon that compose the instance, and the third level\nto the polygon coordinates. The third level should have the format of\n[x0, y0, x1, y1, ..., xn, yn] (n >= 3).\n* list[ndarray]: each ndarray is a binary mask of shape (H, W).\n* list[dict]: each dict is a COCO-style RLE.\nkeypoints (Keypoint or array like): an array-like object of shape (N, K, 3),\nwhere the N is the number of instances and K is the number of keypoints.\nThe last dimension corresponds to (x, y, visibility or score).\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = 0\nif boxes is not None:\nboxes = self._convert_boxes(boxes)\nnum_instances = len(boxes)\nif masks is not None:\nmasks = self._convert_masks(masks)\nif num_instances:\nassert len(masks) == num_instances\nelse:\nnum_instances = len(masks)\nif keypoints is not None:\nif num_instances:\nassert len(keypoints) == num_instances\nelse:\nnum_instances = len(keypoints)\nkeypoints = self._convert_keypoints(keypoints)\nif labels is not None:\nassert len(labels) == num_instances\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\nif boxes is not None and boxes.shape[1] == 5:\nreturn self.overlay_rotated_instances(\nboxes=boxes, labels=labels, assigned_colors=assigned_colors\n)\n# Display in largest to smallest order to reduce occlusion.\nareas = None\nif boxes is not None:\nareas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\nelif masks is not None:\nareas = np.asarray([x.area() for x in masks])\nif areas is not None:\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs] if boxes is not None else None\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\nmasks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\nassigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\nkeypoints = keypoints[sorted_idxs] if keypoints is not None else None\n#print(num_instances)\n#print(labels)\nobj_vect=[]\nfor i in range(num_instances):\ncolor = assigned_colors[i]\nif boxes is not None:\nself.draw_box(boxes[i], edge_color=color)\nif masks is not None:\nfor segment in masks[i].polygons:\nself.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\nif labels is not None:\n# first get a box\nif boxes is not None:\nx0, y0, x1, y1 = boxes[i]\ntext_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\nhoriz_align = \"left\"\nelif masks is not None:\n# skip small mask without polygon\nif len(masks[i].polygons) == 0:\ncontinue\nx0, y0, x1, y1 = masks[i].bbox()\n# draw text in the center (defined by median) when box is not drawn\n# median is less sensitive to outliers.\ntext_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\nhoriz_align = \"center\"\nelse:\ncontinue  # drawing the box confidence for keypoints isn't very useful.\n# for small objects, draw text at the side to avoid occlusion\ninstance_area = (y1 - y0) * (x1 - x0)\nif (\ninstance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\nor y1 - y0 < 40 * self.output.scale\n):\nif y1 >= self.output.height - 5:\ntext_pos = (x1, y0)\nelse:\ntext_pos = (x0, y1)\nheight_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n* 0.5\n* self._default_font_size\n)\nself.draw_text(\nlabels[i],\ntext_pos,\ncolor=lighter_color,\nhorizontal_alignment=horiz_align,\nfont_size=font_size,\n)\nobj_vect.append([text_pos,labels[i]])\n# draw keypoints\nif keypoints is not None:\nfor keypoints_per_instance in keypoints:\nself.draw_and_connect_keypoints(keypoints_per_instance)\nreturn self.output, obj_vect\ndef overlay_rotated_instances(self, boxes=None, labels=None, assigned_colors=None):\n\"\"\"\nArgs:\nboxes (ndarray): an Nx5 numpy array of\n(x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image.\nlabels (list[str]): the text to be displayed for each instance.\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = len(boxes)\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\n# Display in largest to smallest order to reduce occlusion.\nif boxes is not None:\nareas = boxes[:, 2] * boxes[:, 3]\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs]\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\ncolors = [assigned_colors[idx] for idx in sorted_idxs]\nfor i in range(num_instances):\nself.draw_rotated_box_with_label(\nboxes[i], edge_color=colors[i], label=labels[i] if labels is not None else None\n)\nreturn self.output\ndef draw_and_connect_keypoints(self, keypoints):\n\"\"\"\nDraws keypoints of an instance and follows the rules for keypoint connections\nto draw lines between appropriate keypoints. This follows color heuristics for\nline color.\nArgs:\nkeypoints (Tensor): a tensor of shape (K, 3), where K is the number of keypoints\nand the last dimension corresponds to (x, y, probability).\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nvisible = {}\nkeypoint_names = self.metadata.get(\"keypoint_names\")\nfor idx, keypoint in enumerate(keypoints):\n# draw keypoint\nx, y, prob = keypoint\nif prob > self.keypoint_threshold:\nself.draw_circle((x, y), color=_RED)\nif keypoint_names:\nkeypoint_name = keypoint_names[idx]\nvisible[keypoint_name] = (x, y)\nif self.metadata.get(\"keypoint_connection_rules\"):\nfor kp0, kp1, color in self.metadata.keypoint_connection_rules:\nif kp0 in visible and kp1 in visible:\nx0, y0 = visible[kp0]\nx1, y1 = visible[kp1]\ncolor = tuple(x / 255.0 for x in color)\nself.draw_line([x0, x1], [y0, y1], color=color)\n# draw lines from nose to mid-shoulder and mid-shoulder to mid-hip\n# Note that this strategy is specific to person keypoints.\n# For other keypoints, it should just do nothing\ntry:\nls_x, ls_y = visible[\"left_shoulder\"]\nrs_x, rs_y = visible[\"right_shoulder\"]\nmid_shoulder_x, mid_shoulder_y = (ls_x + rs_x) / 2, (ls_y + rs_y) / 2\nexcept KeyError:\npass\nelse:\n# draw line from nose to mid-shoulder\nnose_x, nose_y = visible.get(\"nose\", (None, None))\nif nose_x is not None:\nself.draw_line([nose_x, mid_shoulder_x], [nose_y, mid_shoulder_y], color=_RED)\ntry:\n# draw line from mid-shoulder to mid-hip\nlh_x, lh_y = visible[\"left_hip\"]\nrh_x, rh_y = visible[\"right_hip\"]\nexcept KeyError:\npass\nelse:\nmid_hip_x, mid_hip_y = (lh_x + rh_x) / 2, (lh_y + rh_y) / 2\nself.draw_line([mid_hip_x, mid_shoulder_x], [mid_hip_y, mid_shoulder_y], color=_RED)\nreturn self.output\n\"\"\"\nPrimitive drawing functions:\n\"\"\"\ndef draw_text(\nself,\ntext,\nposition,\n*,\nfont_size=None,\ncolor=\"g\",\nhorizontal_alignment=\"center\",\nrotation=0,\n):\n\"\"\"\nArgs:\ntext (str): class label\nposition (tuple): a tuple of the x and y coordinates to place text on image.\nfont_size (int, optional): font of the text. If not provided, a font size\nproportional to the image width is calculated and used.\ncolor: color of the text. Refer to `matplotlib.colors` for full list\nof formats that are accepted.\nhorizontal_alignment (str): see `matplotlib.text.Text`\nrotation: rotation angle in degrees CCW\nReturns:\noutput (VisImage): image object with text drawn.\n\"\"\"\nif not font_size:\nfont_size = self._default_font_size\n# since the text background is dark, we don't want the text to be dark\ncolor = np.maximum(list(mplc.to_rgb(color)), 0.2)\ncolor[np.argmax(color)] = max(0.8, np.max(color))\nx, y = position\nself.output.ax.text(\nx,\ny,\ntext,\nsize=font_size * self.output.scale,\nfamily=\"sans-serif\",\nbbox={\"facecolor\": \"black\", \"alpha\": 0.8, \"pad\": 0.7, \"edgecolor\": \"none\"},\nverticalalignment=\"top\",\nhorizontalalignment=horizontal_alignment,\ncolor=color,\nzorder=10,\nrotation=rotation,\n)\nreturn self.output\ndef draw_box(self, box_coord, alpha=0.5, edge_color=\"g\", line_style=\"-\"):\n\"\"\"\nArgs:\nbox_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0\nare the coordinates of the image's top left corner. x1 and y1 are the\ncoordinates of the image's bottom right corner.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx0, y0, x1, y1 = box_coord\nwidth = x1 - x0\nheight = y1 - y0\nlinewidth = max(self._default_font_size / 4, 1)\nself.output.ax.add_patch(\nmpl.patches.Rectangle(\n(x0, y0),\nwidth,\nheight,\nfill=False,\nedgecolor=edge_color,\nlinewidth=linewidth * self.output.scale,\nalpha=alpha,\nlinestyle=line_style,\n)\n)\nreturn self.output\ndef draw_rotated_box_with_label(\nself, rotated_box, alpha=0.5, edge_color=\"g\", line_style=\"-\", label=None\n):\n\"\"\"\nDraw a rotated box with label on its top-left corner.\nArgs:\nrotated_box (tuple): a tuple containing (cnt_x, cnt_y, w, h, angle),\nwhere cnt_x and cnt_y are the center coordinates of the box.\nw and h are the width and height of the box. angle represents how\nmany degrees the box is rotated CCW with regard to the 0-degree box.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nlabel (string): label for rotated box. It will not be rendered when set to None.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\ncnt_x, cnt_y, w, h, angle = rotated_box\narea = w * h\n# use thinner lines when the box is small\nlinewidth = self._default_font_size / (\n6 if area < _SMALL_OBJECT_AREA_THRESH * self.output.scale else 3\n)\ntheta = angle * math.pi / 180.0\nc = math.cos(theta)\ns = math.sin(theta)\nrect = [(-w / 2, h / 2), (-w / 2, -h / 2), (w / 2, -h / 2), (w / 2, h / 2)]\n# x: left->right ; y: top->down\nrotated_rect = [(s * yy + c * xx + cnt_x, c * yy - s * xx + cnt_y) for (xx, yy) in rect]\nfor k in range(4):\nj = (k + 1) % 4\nself.draw_line(\n[rotated_rect[k][0], rotated_rect[j][0]],\n[rotated_rect[k][1], rotated_rect[j][1]],\ncolor=edge_color,\nlinestyle=\"--\" if k == 1 else line_style,\nlinewidth=linewidth,\n)\nif label is not None:\ntext_pos = rotated_rect[1]  # topleft corner\nheight_ratio = h / np.sqrt(self.output.height * self.output.width)\nlabel_color = self._change_color_brightness(edge_color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2) * 0.5 * self._default_font_size\n)\nself.draw_text(label, text_pos, color=label_color, font_size=font_size, rotation=angle)\nreturn self.output\ndef draw_circle(self, circle_coord, color, radius=3):\n\"\"\"\nArgs:\ncircle_coord (list(int) or tuple(int)): contains the x and y coordinates\nof the center of the circle.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nradius (int): radius of the circle.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx, y = circle_coord\nself.output.ax.add_patch(\nmpl.patches.Circle(circle_coord, radius=radius, fill=True, color=color)\n)\nreturn self.output\ndef draw_line(self, x_data, y_data, color, linestyle=\"-\", linewidth=None):\n\"\"\"\nArgs:\nx_data (list[int]): a list containing x values of all the points being drawn.\nLength of list should match the length of y_data.\ny_data (list[int]): a list containing y values of all the points being drawn.\nLength of list should match the length of x_data.\ncolor: color of the line. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nlinestyle: style of the line. Refer to `matplotlib.lines.Line2D`\nfor a full list of formats that are accepted.\nlinewidth (float or None): width of the line. When it's None,\na default value will be computed and used.\nReturns:\noutput (VisImage): image object with line drawn.\n\"\"\"\nif linewidth is None:\nlinewidth = self._default_font_size / 3\nlinewidth = max(linewidth, 1)\nself.output.ax.add_line(\nmpl.lines.Line2D(\nx_data,\ny_data,\nlinewidth=linewidth * self.output.scale,\ncolor=color,\nlinestyle=linestyle,\n)\n)\nreturn self.output\ndef draw_binary_mask(\nself, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n):\n\"\"\"\nArgs:\nbinary_mask (ndarray): numpy array of shape (H, W), where H is the image height and\nW is the image width. Each value in the array is either a 0 or 1 value of uint8\ntype.\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\narea_threshold (float): a connected component smaller than this area will not be shown.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nhas_valid_segment = False\nbinary_mask = binary_mask.astype(\"uint8\")  # opencv needs uint8\nmask = GenericMask(binary_mask, self.output.height, self.output.width)\nshape2d = (binary_mask.shape[0], binary_mask.shape[1])\nif not mask.has_holes:\n# draw polygons for regular masks\nfor segment in mask.polygons:\narea = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))\nif area < (area_threshold or 0):\ncontinue\nhas_valid_segment = True\nsegment = segment.reshape(-1, 2)\nself.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)\nelse:\n# TODO: Use Path/PathPatch to draw vector graphics:\n# https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = (mask.mask == 1).astype(\"float32\") * alpha\nhas_valid_segment = True\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None and has_valid_segment:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\ncenter = self._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output, center\ndef draw_soft_mask(self, soft_mask, color=None, *, text=None, alpha=0.5):\n\"\"\"\nArgs:\nsoft_mask (ndarray): float array of shape (H, W), each value in [0, 1].\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nshape2d = (soft_mask.shape[0], soft_mask.shape[1])\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = soft_mask * alpha\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nbinary_mask = (soft_mask > 0.5).astype(\"uint8\")\nself._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output\ndef draw_polygon(self, segment, color, edge_color=None, alpha=0.5):\n\"\"\"\nArgs:\nsegment: numpy array of shape Nx2, containing all the points in the polygon.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted. If not provided, a darker shade\nof the polygon color will be used instead.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with polygon drawn.\n\"\"\"\nif edge_color is None:\n# make edge color darker than the polygon color\nif alpha > 0.8:\nedge_color = self._change_color_brightness(color, brightness_factor=-0.7)\nelse:\nedge_color = color\nedge_color = mplc.to_rgb(edge_color) + (1,)\npolygon = mpl.patches.Polygon(\nsegment,\nfill=True,\nfacecolor=mplc.to_rgb(color) + (alpha,),\nedgecolor=edge_color,\nlinewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n)\nself.output.ax.add_patch(polygon)\nreturn self.output\n\"\"\"\nInternal methods:\n\"\"\"\ndef _jitter(self, color):\n\"\"\"\nRandomly modifies given color to produce a slightly different color than the color given.\nArgs:\ncolor (tuple[double]): a tuple of 3 elements, containing the RGB values of the color\npicked. The values in the list are in the [0.0, 1.0] range.\nReturns:\njittered_color (tuple[double]): a tuple of 3 elements, containing the RGB values of the\ncolor after being jittered. The values in the list are in the [0.0, 1.0] range.\n\"\"\"\ncolor = mplc.to_rgb(color)\nvec = np.random.rand(3)\n# better to do it in another color space\nvec = vec / np.linalg.norm(vec) * 0.5\nres = np.clip(vec + color, 0, 1)\nreturn tuple(res)\ndef _create_grayscale_image(self, mask=None):\n\"\"\"\nCreate a grayscale version of the original image.\nThe colors in masked area, if given, will be kept.\n\"\"\"\nimg_bw = self.img.astype(\"f4\").mean(axis=2)\nimg_bw = np.stack([img_bw] * 3, axis=2)\nif mask is not None:\nimg_bw[mask] = self.img[mask]\nreturn img_bw\ndef _change_color_brightness(self, color, brightness_factor):\n\"\"\"\nDepending on the brightness_factor, gives a lighter or darker color i.e. a color with\nless or more saturation than the original color.\nArgs:\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nbrightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of\n0 will correspond to no change, a factor in [-1.0, 0) range will result in\na darker color and a factor in (0, 1.0] range will result in a lighter color.\nReturns:\nmodified_color (tuple[double]): a tuple containing the RGB values of the\nmodified color. Each value in the tuple is in the [0.0, 1.0] range.\n\"\"\"\nassert brightness_factor >= -1.0 and brightness_factor <= 1.0\ncolor = mplc.to_rgb(color)\npolygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))\nmodified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])\nmodified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness\nmodified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness\nmodified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])\nreturn modified_color\ndef _convert_boxes(self, boxes):\n\"\"\"\nConvert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n\"\"\"\nif isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):\nreturn boxes.tensor.detach().numpy()\nelse:\nreturn np.asarray(boxes)\ndef _convert_masks(self, masks_or_polygons):\n\"\"\"\nConvert different format of masks or polygons to a tuple of masks and polygons.\nReturns:\nlist[GenericMask]:\n\"\"\"\nm = masks_or_polygons\nif isinstance(m, PolygonMasks):\nm = m.polygons\nif isinstance(m, BitMasks):\nm = m.tensor.numpy()\nif isinstance(m, torch.Tensor):\nm = m.numpy()\nret = []\nfor x in m:\nif isinstance(x, GenericMask):\nret.append(x)\nelse:\nret.append(GenericMask(x, self.output.height, self.output.width))\nreturn ret\ndef _draw_text_in_mask(self, binary_mask, text, color):\n\"\"\"\nFind proper places to draw text given a binary mask.\n\"\"\"\n# TODO sometimes drawn on wrong objects. the heuristics here can improve.\n_num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, 8)\nif stats[1:, -1].size == 0:\nreturn\nlargest_component_id = np.argmax(stats[1:, -1]) + 1\n# draw text on the largest component, as well as other very large components.\nfor cid in range(1, _num_cc):\nif cid == largest_component_id or stats[cid, -1] > _LARGE_MASK_AREA_THRESH:\n# median is more stable than centroid\n# center = centroids[largest_component_id]\ncenter = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]\nself.draw_text(text, center, color=color)\nreturn center\ndef _convert_keypoints(self, keypoints):\nif isinstance(keypoints, Keypoints):\nkeypoints = keypoints.tensor\nkeypoints = np.asarray(keypoints)\nreturn keypoints\ndef get_output(self):\n\"\"\"\nReturns:\noutput (VisImage): the image output containing the visualizations added\nto the image.\n\"\"\"\nreturn self.output",
                        "max_stars_repo_path": "detectron2/utils/visualizer.py",
                        "max_stars_repo_name": "robolableonardo/detectron2",
                        "max_stars_count": 0,
                        "__cluster__": 186
                },
                {
                        "id": "test_evocodebench_data_188",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nimport colorsys\nimport logging\nimport math\nfrom enum import Enum, unique\nimport cv2\nimport matplotlib as mpl\nimport matplotlib.colors as mplc\nimport matplotlib.figure as mplfigure\nimport numpy as np\nimport pycocotools.mask as mask_util\nimport torch\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.structures import (\nBitMasks,\nBoxes,\nBoxMode,\nKeypoints,\nPolygonMasks,\nRotatedBoxes,\n)\nfrom detectron2.utils.file_io import PathManager\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\nfrom PIL import Image\nfrom .colormap import random_color\nlogger = logging.getLogger(__name__)\n__all__ = [\"ColorMode\", \"VisImage\", \"Visualizer\"]\n_SMALL_OBJECT_AREA_THRESH = 1000\n_LARGE_MASK_AREA_THRESH = 120000\n_OFF_WHITE = (1.0, 1.0, 240.0 / 255)\n_BLACK = (0, 0, 0)\n_RED = (1.0, 0, 0)\n_KEYPOINT_THRESHOLD = 0.05\n@unique\nclass ColorMode(Enum):\n\"\"\"\nEnum of different color modes to use for instance visualizations.\n\"\"\"\nIMAGE = 0\n\"\"\"\nPicks a random color for every instance and overlay segmentations with low opacity.\n\"\"\"\nSEGMENTATION = 1\n\"\"\"\nLet instances of the same category have similar colors\n(from metadata.thing_colors), and overlay them with\nhigh opacity. This provides more attention on the quality of segmentation.\n\"\"\"\nIMAGE_BW = 2\n\"\"\"\nSame as IMAGE, but convert all areas without masks to gray-scale.\nOnly available for drawing per-instance mask predictions.\n\"\"\"\nclass GenericMask:\n\"\"\"\nAttribute:\npolygons (list[ndarray]): list[ndarray]: polygons for this mask.\nEach ndarray has format [x, y, x, y, ...]\nmask (ndarray): a binary mask\n\"\"\"\ndef __init__(self, mask_or_polygons, height, width):\nself._mask = self._polygons = self._has_holes = None\nself.height = height\nself.width = width\nm = mask_or_polygons\nif isinstance(m, dict):\n# RLEs\nassert \"counts\" in m and \"size\" in m\nif isinstance(m[\"counts\"], list):  # uncompressed RLEs\nh, w = m[\"size\"]\nassert h == height and w == width\nm = mask_util.frPyObjects(m, h, w)\nself._mask = mask_util.decode(m)[:, :]\nreturn\nif isinstance(m, list):  # list[ndarray]\nself._polygons = [np.asarray(x).reshape(-1) for x in m]\nreturn\nif isinstance(m, np.ndarray):  # assumed to be a binary mask\nassert m.shape[1] != 2, m.shape\nassert m.shape == (\nheight,\nwidth,\n), f\"mask shape: {m.shape}, target dims: {height}, {width}\"\nself._mask = m.astype(\"uint8\")\nreturn\nraise ValueError(\"GenericMask cannot handle object {} of type '{}'\".format(m, type(m)))\n@property\ndef mask(self):\nif self._mask is None:\nself._mask = self.polygons_to_mask(self._polygons)\nreturn self._mask\n@property\ndef polygons(self):\nif self._polygons is None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nreturn self._polygons\n@property\ndef has_holes(self):\nif self._has_holes is None:\nif self._mask is not None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nelse:\nself._has_holes = False  # if original format is polygon, does not have holes\nreturn self._has_holes\ndef mask_to_polygons(self, mask):\n# cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level\n# hierarchy. External contours (boundary) of the object are placed in hierarchy-1.\n# Internal contours (holes) are placed in hierarchy-2.\n# cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.\nmask = np.ascontiguousarray(mask)  # some versions of cv2 does not support incontiguous arr\nres = cv2.findContours(mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\nhierarchy = res[-1]\nif hierarchy is None:  # empty mask\nreturn [], False\nhas_holes = (hierarchy.reshape(-1, 4)[:, 3] >= 0).sum() > 0\nres = res[-2]\nres = [x.flatten() for x in res]\n# These coordinates from OpenCV are integers in range [0, W-1 or H-1].\n# We add 0.5 to turn them into real-value coordinate space. A better solution\n# would be to first +0.5 and then dilate the returned polygon by 0.5.\nres = [x + 0.5 for x in res if len(x) >= 6]\nreturn res, has_holes\ndef polygons_to_mask(self, polygons):\nrle = mask_util.frPyObjects(polygons, self.height, self.width)\nrle = mask_util.merge(rle)\nreturn mask_util.decode(rle)[:, :]\ndef area(self):\nreturn self.mask.sum()\ndef bbox(self):\np = mask_util.frPyObjects(self.polygons, self.height, self.width)\np = mask_util.merge(p)\nbbox = mask_util.toBbox(p)\nbbox[2] += bbox[0]\nbbox[3] += bbox[1]\nreturn bbox\nclass _PanopticPrediction:\n\"\"\"\nUnify different panoptic annotation/prediction formats\n\"\"\"\ndef __init__(self, panoptic_seg, segments_info, metadata=None):\nif segments_info is None:\nassert metadata is not None\n# If \"segments_info\" is None, we assume \"panoptic_img\" is a\n# H*W int32 image storing the panoptic_id in the format of\n# category_id * label_divisor + instance_id. We reserve -1 for\n# VOID label.\nlabel_divisor = metadata.label_divisor\nsegments_info = []\nfor panoptic_label in np.unique(panoptic_seg.numpy()):\nif panoptic_label == -1:\n# VOID region.\ncontinue\npred_class = panoptic_label // label_divisor\nisthing = pred_class in metadata.thing_dataset_id_to_contiguous_id.values()\nsegments_info.append(\n{\n\"id\": int(panoptic_label),\n\"category_id\": int(pred_class),\n\"isthing\": bool(isthing),\n}\n)\ndel metadata\nself._seg = panoptic_seg\nself._sinfo = {s[\"id\"]: s for s in segments_info}  # seg id -> seg info\nsegment_ids, areas = torch.unique(panoptic_seg, sorted=True, return_counts=True)\nareas = areas.numpy()\nsorted_idxs = np.argsort(-areas)\nself._seg_ids, self._seg_areas = segment_ids[sorted_idxs], areas[sorted_idxs]\nself._seg_ids = self._seg_ids.tolist()\nfor sid, area in zip(self._seg_ids, self._seg_areas):\nif sid in self._sinfo:\nself._sinfo[sid][\"area\"] = float(area)\ndef non_empty_mask(self):\n\"\"\"\nReturns:\n(H, W) array, a mask for all pixels that have a prediction\n\"\"\"\nempty_ids = []\nfor id in self._seg_ids:\nif id not in self._sinfo:\nempty_ids.append(id)\nif len(empty_ids) == 0:\nreturn np.zeros(self._seg.shape, dtype=np.uint8)\nassert (\nlen(empty_ids) == 1\n), \">1 ids corresponds to no labels. This is currently not supported\"\nreturn (self._seg != empty_ids[0]).numpy().astype(np.bool)\ndef semantic_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or sinfo[\"isthing\"]:\n# Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions.\ncontinue\nyield (self._seg == sid).numpy().astype(np.bool), sinfo\ndef instance_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or not sinfo[\"isthing\"]:\ncontinue\nmask = (self._seg == sid).numpy().astype(np.bool)\nif mask.sum() > 0:\nyield mask, sinfo\ndef _create_text_labels(classes, scores, class_names, is_crowd=None):\n\"\"\"\nArgs:\nclasses (list[int] or None):\nscores (list[float] or None):\nclass_names (list[str] or None):\nis_crowd (list[bool] or None):\nReturns:\nlist[str] or None\n\"\"\"\nlabels = None\nif classes is not None:\nif class_names is not None and len(class_names) > 0:\nlabels = [class_names[i] for i in classes]\nelse:\nlabels = [str(i) for i in classes]\nif scores is not None:\nif labels is None:\nlabels = [\"{:.0f}%\".format(s * 100) for s in scores]\nelse:\nlabels = [\"{} {:.0f}%\".format(l, s * 100) for l, s in zip(labels, scores)]\nif labels is not None and is_crowd is not None:\nlabels = [l + (\"|crowd\" if crowd else \"\") for l, crowd in zip(labels, is_crowd)]\nreturn labels\nclass VisImage:\ndef __init__(self, img, scale=1.0):\n\"\"\"\nArgs:\nimg (ndarray): an RGB image of shape (H, W, 3) in range [0, 255].\nscale (float): scale the input image\n\"\"\"\nself.img = img\nself.scale = scale\nself.width, self.height = img.shape[1], img.shape[0]\nself._setup_figure(img)\ndef _setup_figure(self, img):\n\"\"\"\nArgs:\nSame as in :meth:`__init__()`.\nReturns:\nfig (matplotlib.pyplot.figure): top level container for all the image plot elements.\nax (matplotlib.pyplot.Axes): contains figure elements and sets the coordinate system.\n\"\"\"\nfig = mplfigure.Figure(frameon=False)\nself.dpi = fig.get_dpi()\n# add a small 1e-2 to avoid precision lost due to matplotlib's truncation\n# (https://github.com/matplotlib/matplotlib/issues/15363)\nfig.set_size_inches(\n(self.width * self.scale + 1e-2) / self.dpi,\n(self.height * self.scale + 1e-2) / self.dpi,\n)\nself.canvas = FigureCanvasAgg(fig)\n# self.canvas = mpl.backends.backend_cairo.FigureCanvasCairo(fig)\nax = fig.add_axes([0.0, 0.0, 1.0, 1.0])\nax.axis(\"off\")\nself.fig = fig\nself.ax = ax\nself.reset_image(img)\ndef reset_image(self, img):\n\"\"\"\nArgs:\nimg: same as in __init__\n\"\"\"\nimg = img.astype(\"uint8\")\nself.ax.imshow(img, extent=(0, self.width, self.height, 0), interpolation=\"nearest\")\ndef save(self, filepath):\n\"\"\"\nArgs:\nfilepath (str): a string that contains the absolute path, including the file name, where\nthe visualized image will be saved.\n\"\"\"\nself.fig.savefig(filepath)\ndef get_image(self):\n\"\"\"\nReturns:\nndarray:\nthe visualized image of shape (H, W, 3) (RGB) in uint8 type.\nThe shape is scaled w.r.t the input image using the given `scale` argument.\n\"\"\"\ncanvas = self.canvas\ns, (width, height) = canvas.print_to_buffer()\n# buf = io.BytesIO()  # works for cairo backend\n# canvas.print_rgba(buf)\n# width, height = self.width, self.height\n# s = buf.getvalue()\nbuffer = np.frombuffer(s, dtype=\"uint8\")\nimg_rgba = buffer.reshape(height, width, 4)\nrgb, alpha = np.split(img_rgba, [3], axis=2)\nreturn rgb.astype(\"uint8\")\nclass Visualizer:\n\"\"\"\nVisualizer that draws data about detection/segmentation on images.\nIt contains methods like `draw_{text,box,circle,line,binary_mask,polygon}`\nthat draw primitive objects to images, as well as high-level wrappers like\n`draw_{instance_predictions,sem_seg,panoptic_seg_predictions,dataset_dict}`\nthat draw composite data in some pre-defined style.\nNote that the exact visualization style for the high-level wrappers are subject to change.\nStyle such as color, opacity, label contents, visibility of labels, or even the visibility\nof objects themselves (e.g. when the object is too small) may change according\nto different heuristics, as long as the results still look visually reasonable.\nTo obtain a consistent style, you can implement custom drawing functions with the\nabovementioned primitive methods instead. If you need more customized visualization\nstyles, you can process the data yourself following their format documented in\ntutorials (:doc:`/tutorials/models`, :doc:`/tutorials/datasets`). This class does not\nintend to satisfy everyone's preference on drawing styles.\nThis visualizer focuses on high rendering quality rather than performance. It is not\ndesigned to be used for real-time applications.\n\"\"\"\n# TODO implement a fast, rasterized version using OpenCV\ndef __init__(self, img_rgb, metadata=None, scale=1.0, instance_mode=ColorMode.IMAGE):\n\"\"\"\nArgs:\nimg_rgb: a numpy array of shape (H, W, C), where H and W correspond to\nthe height and width of the image respectively. C is the number of\ncolor channels. The image is required to be in RGB format since that\nis a requirement of the Matplotlib library. The image is also expected\nto be in the range [0, 255].\nmetadata (Metadata): dataset metadata (e.g. class names and colors)\ninstance_mode (ColorMode): defines one of the pre-defined style for drawing\ninstances on an image.\n\"\"\"\nself.img = np.asarray(img_rgb).clip(0, 255).astype(np.uint8)\nif metadata is None:\nmetadata = MetadataCatalog.get(\"__nonexist__\")\nself.metadata = metadata\nself.output = VisImage(self.img, scale=scale)\nself.cpu_device = torch.device(\"cpu\")\n# too small texts are useless, therefore clamp to 9\nself._default_font_size = max(\nnp.sqrt(self.output.height * self.output.width) // 90, 10 // scale\n)\nself._instance_mode = instance_mode\nself.keypoint_threshold = _KEYPOINT_THRESHOLD\ndef draw_instance_predictions(self, predictions):\n\"\"\"\nDraw instance-level prediction results on an image.\nArgs:\npredictions (Instances): the output of an instance detection/segmentation\nmodel. Following fields will be used to draw:\n\"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nboxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\nscores = predictions.scores if predictions.has(\"scores\") else None\nclasses = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\nlabels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\nkeypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\nif predictions.has(\"pred_masks\"):\nmasks = np.asarray(predictions.pred_masks)\nmasks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\nelse:\nmasks = None\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n]\nalpha = 0.8\nelse:\ncolors = None\nalpha = 0.5\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(\nself._create_grayscale_image(\n(predictions.pred_masks.any(dim=0) > 0).numpy()\nif predictions.has(\"pred_masks\")\nelse None\n)\n)\nalpha = 0.3\nself.overlay_instances(\nmasks=masks,\nboxes=boxes,\nlabels=labels,\nkeypoints=keypoints,\nassigned_colors=colors,\nalpha=alpha,\n)\nreturn self.output\ndef draw_sem_seg(self, sem_seg, area_threshold=None, alpha=0.8):\n\"\"\"\nDraw semantic segmentation predictions/labels.\nArgs:\nsem_seg (Tensor or ndarray): the segmentation of shape (H, W).\nEach value is the integer label of the pixel.\narea_threshold (int): segments with less than `area_threshold` are not drawn.\nalpha (float): the larger it is, the more opaque the segmentations are.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nif isinstance(sem_seg, torch.Tensor):\nsem_seg = sem_seg.numpy()\nlabels, areas = np.unique(sem_seg, return_counts=True)\nsorted_idxs = np.argsort(-areas).tolist()\nlabels = labels[sorted_idxs]\nfor label in filter(lambda l: l < len(self.metadata.stuff_classes), labels):\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[label]]\nexcept (AttributeError, IndexError):\nmask_color = None\nbinary_mask = (sem_seg == label).astype(np.uint8)\ntext = self.metadata.stuff_classes[label]\nself.draw_binary_mask(\nbinary_mask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\nreturn self.output\ndef draw_panoptic_seg(self, panoptic_seg, segments_info, area_threshold=None, alpha=0.7):\n\"\"\"\nDraw panoptic prediction annotations or results.\nArgs:\npanoptic_seg (Tensor): of shape (height, width) where the values are ids for each\nsegment.\nsegments_info (list[dict] or None): Describe each segment in `panoptic_seg`.\nIf it is a ``list[dict]``, each dict contains keys \"id\", \"category_id\".\nIf None, category id of each pixel is computed by\n``pixel // metadata.label_divisor``.\narea_threshold (int): stuff segments with less than `area_threshold` are not drawn.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\npred = _PanopticPrediction(panoptic_seg, segments_info, self.metadata)\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(self._create_grayscale_image(pred.non_empty_mask()))\n# draw mask for all semantic segments first i.e. \"stuff\"\nfor mask, sinfo in pred.semantic_masks():\ncategory_idx = sinfo[\"category_id\"]\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]\nexcept AttributeError:\nmask_color = None\ntext = self.metadata.stuff_classes[category_idx]\nself.draw_binary_mask(\nmask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\n# draw mask for all instances second\nall_instances = list(pred.instance_masks())\nif len(all_instances) == 0:\nreturn self.output\nmasks, sinfo = list(zip(*all_instances))\ncategory_ids = [x[\"category_id\"] for x in sinfo]\ntry:\nscores = [x[\"score\"] for x in sinfo]\nexcept KeyError:\nscores = None\nlabels = _create_text_labels(\ncategory_ids, scores, self.metadata.thing_classes, [x.get(\"iscrowd\", 0) for x in sinfo]\n)\ntry:\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in category_ids\n]\nexcept AttributeError:\ncolors = None\nself.overlay_instances(masks=masks, labels=labels, assigned_colors=colors, alpha=alpha)\nreturn self.output\ndraw_panoptic_seg_predictions = draw_panoptic_seg  # backward compatibility\ndef draw_dataset_dict(self, dic):\n\"\"\"\nDraw annotations/segmentaions in Detectron2 Dataset format.\nArgs:\ndic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nannos = dic.get(\"annotations\", None)\nif annos:\nif \"segmentation\" in annos[0]:\nmasks = [x[\"segmentation\"] for x in annos]\nelse:\nmasks = None\nif \"keypoints\" in annos[0]:\nkeypts = [x[\"keypoints\"] for x in annos]\nkeypts = np.array(keypts).reshape(len(annos), -1, 3)\nelse:\nkeypts = None\nboxes = [\nBoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS)\nif len(x[\"bbox\"]) == 4\nelse x[\"bbox\"]\nfor x in annos\n]\ncolors = None\ncategory_ids = [x[\"category_id\"] for x in annos]\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]])\nfor c in category_ids\n]\nnames = self.metadata.get(\"thing_classes\", None)\nlabels = _create_text_labels(\ncategory_ids,\nscores=None,\nclass_names=names,\nis_crowd=[x.get(\"iscrowd\", 0) for x in annos],\n)\nself.overlay_instances(\nlabels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n)\nsem_seg = dic.get(\"sem_seg\", None)\nif sem_seg is None and \"sem_seg_file_name\" in dic:\nwith PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\nsem_seg = Image.open(f)\nsem_seg = np.asarray(sem_seg, dtype=\"uint8\")\nif sem_seg is not None:\nself.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\npan_seg = dic.get(\"pan_seg\", None)\nif pan_seg is None and \"pan_seg_file_name\" in dic:\nwith PathManager.open(dic[\"pan_seg_file_name\"], \"rb\") as f:\npan_seg = Image.open(f)\npan_seg = np.asarray(pan_seg)\nfrom panopticapi.utils import rgb2id\npan_seg = rgb2id(pan_seg)\nif pan_seg is not None:\nsegments_info = dic[\"segments_info\"]\npan_seg = torch.tensor(pan_seg)\nself.draw_panoptic_seg(pan_seg, segments_info, area_threshold=0, alpha=0.5)\nreturn self.output\ndef overlay_instances(\nself,\n*,\nboxes=None,\nlabels=None,\nmasks=None,\nkeypoints=None,\nassigned_colors=None,\nalpha=0.5,\n):\n\"\"\"\nArgs:\nboxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,\nor an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,\nor a :class:`RotatedBoxes`,\nor an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image,\nlabels (list[str]): the text to be displayed for each instance.\nmasks (masks-like object): Supported types are:\n* :class:`detectron2.structures.PolygonMasks`,\n:class:`detectron2.structures.BitMasks`.\n* list[list[ndarray]]: contains the segmentation masks for all objects in one image.\nThe first level of the list corresponds to individual instances. The second\nlevel to all the polygon that compose the instance, and the third level\nto the polygon coordinates. The third level should have the format of\n[x0, y0, x1, y1, ..., xn, yn] (n >= 3).\n* list[ndarray]: each ndarray is a binary mask of shape (H, W).\n* list[dict]: each dict is a COCO-style RLE.\nkeypoints (Keypoint or array like): an array-like object of shape (N, K, 3),\nwhere the N is the number of instances and K is the number of keypoints.\nThe last dimension corresponds to (x, y, visibility or score).\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = 0\nif boxes is not None:\nboxes = self._convert_boxes(boxes)\nnum_instances = len(boxes)\nif masks is not None:\nmasks = self._convert_masks(masks)\nif num_instances:\nassert len(masks) == num_instances\nelse:\nnum_instances = len(masks)\nif keypoints is not None:\nif num_instances:\nassert len(keypoints) == num_instances\nelse:\nnum_instances = len(keypoints)\nkeypoints = self._convert_keypoints(keypoints)\nif labels is not None:\nassert len(labels) == num_instances\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\nif boxes is not None and boxes.shape[1] == 5:\nreturn self.overlay_rotated_instances(\nboxes=boxes, labels=labels, assigned_colors=assigned_colors\n)\n# Display in largest to smallest order to reduce occlusion.\nareas = None\nif boxes is not None:\nareas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\nelif masks is not None:\nareas = np.asarray([x.area() for x in masks])\nif areas is not None:\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs] if boxes is not None else None\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\nmasks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\nassigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\nkeypoints = keypoints[sorted_idxs] if keypoints is not None else None\nfor i in range(num_instances):\ncolor = assigned_colors[i]\nif boxes is not None:\nself.draw_box(boxes[i], edge_color=color)\nif masks is not None:\nfor segment in masks[i].polygons:\nself.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\nif labels is not None:\n# first get a box\nif boxes is not None:\nx0, y0, x1, y1 = boxes[i]\ntext_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\nhoriz_align = \"left\"\nelif masks is not None:\n# skip small mask without polygon\nif len(masks[i].polygons) == 0:\ncontinue\nx0, y0, x1, y1 = masks[i].bbox()\n# draw text in the center (defined by median) when box is not drawn\n# median is less sensitive to outliers.\ntext_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\nhoriz_align = \"center\"\nelse:\ncontinue  # drawing the box confidence for keypoints isn't very useful.\n# for small objects, draw text at the side to avoid occlusion\ninstance_area = (y1 - y0) * (x1 - x0)\nif (\ninstance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\nor y1 - y0 < 40 * self.output.scale\n):\nif y1 >= self.output.height - 5:\ntext_pos = (x1, y0)\nelse:\ntext_pos = (x0, y1)\nheight_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n* 0.5\n* self._default_font_size\n)\nself.draw_text(\nlabels[i],\ntext_pos,\ncolor=lighter_color,\nhorizontal_alignment=horiz_align,\nfont_size=font_size,\n)\n# draw keypoints\nif keypoints is not None:\nfor keypoints_per_instance in keypoints:\nself.draw_and_connect_keypoints(keypoints_per_instance)\nreturn self.output\ndef overlay_rotated_instances(self, boxes=None, labels=None, assigned_colors=None):\n\"\"\"\nArgs:\nboxes (ndarray): an Nx5 numpy array of\n(x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image.\nlabels (list[str]): the text to be displayed for each instance.\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = len(boxes)\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\n# Display in largest to smallest order to reduce occlusion.\nif boxes is not None:\nareas = boxes[:, 2] * boxes[:, 3]\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs]\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\ncolors = [assigned_colors[idx] for idx in sorted_idxs]\nfor i in range(num_instances):\nself.draw_rotated_box_with_label(\nboxes[i], edge_color=colors[i], label=labels[i] if labels is not None else None\n)\nreturn self.output\ndef draw_and_connect_keypoints(self, keypoints):\n\"\"\"\nDraws keypoints of an instance and follows the rules for keypoint connections\nto draw lines between appropriate keypoints. This follows color heuristics for\nline color.\nArgs:\nkeypoints (Tensor): a tensor of shape (K, 3), where K is the number of keypoints\nand the last dimension corresponds to (x, y, probability).\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nvisible = {}\nkeypoint_names = self.metadata.get(\"keypoint_names\")\nfor idx, keypoint in enumerate(keypoints):\n# draw keypoint\nx, y, prob = keypoint\nif prob > self.keypoint_threshold:\nself.draw_circle((x, y), color=_RED)\nif keypoint_names:\nkeypoint_name = keypoint_names[idx]\nvisible[keypoint_name] = (x, y)\nif self.metadata.get(\"keypoint_connection_rules\"):\nfor kp0, kp1, color in self.metadata.keypoint_connection_rules:\nif kp0 in visible and kp1 in visible:\nx0, y0 = visible[kp0]\nx1, y1 = visible[kp1]\ncolor = tuple(x / 255.0 for x in color)\nself.draw_line([x0, x1], [y0, y1], color=color)\n# draw lines from nose to mid-shoulder and mid-shoulder to mid-hip\n# Note that this strategy is specific to person keypoints.\n# For other keypoints, it should just do nothing\ntry:\nls_x, ls_y = visible[\"left_shoulder\"]\nrs_x, rs_y = visible[\"right_shoulder\"]\nmid_shoulder_x, mid_shoulder_y = (ls_x + rs_x) / 2, (ls_y + rs_y) / 2\nexcept KeyError:\npass\nelse:\n# draw line from nose to mid-shoulder\nnose_x, nose_y = visible.get(\"nose\", (None, None))\nif nose_x is not None:\nself.draw_line([nose_x, mid_shoulder_x], [nose_y, mid_shoulder_y], color=_RED)\ntry:\n# draw line from mid-shoulder to mid-hip\nlh_x, lh_y = visible[\"left_hip\"]\nrh_x, rh_y = visible[\"right_hip\"]\nexcept KeyError:\npass\nelse:\nmid_hip_x, mid_hip_y = (lh_x + rh_x) / 2, (lh_y + rh_y) / 2\nself.draw_line([mid_hip_x, mid_shoulder_x], [mid_hip_y, mid_shoulder_y], color=_RED)\nreturn self.output\n\"\"\"\nPrimitive drawing functions:\n\"\"\"\ndef draw_text(\nself,\ntext,\nposition,\n*,\nfont_size=None,\ncolor=\"g\",\nhorizontal_alignment=\"center\",\nrotation=0,\n):\n\"\"\"\nArgs:\ntext (str): class label\nposition (tuple): a tuple of the x and y coordinates to place text on image.\nfont_size (int, optional): font of the text. If not provided, a font size\nproportional to the image width is calculated and used.\ncolor: color of the text. Refer to `matplotlib.colors` for full list\nof formats that are accepted.\nhorizontal_alignment (str): see `matplotlib.text.Text`\nrotation: rotation angle in degrees CCW\nReturns:\noutput (VisImage): image object with text drawn.\n\"\"\"\nif not font_size:\nfont_size = self._default_font_size\n# since the text background is dark, we don't want the text to be dark\ncolor = np.maximum(list(mplc.to_rgb(color)), 0.2)\ncolor[np.argmax(color)] = max(0.8, np.max(color))\nx, y = position\nself.output.ax.text(\nx,\ny,\ntext,\nsize=font_size * self.output.scale,\nfamily=\"sans-serif\",\nbbox={\"facecolor\": \"black\", \"alpha\": 0.8, \"pad\": 0.7, \"edgecolor\": \"none\"},\nverticalalignment=\"top\",\nhorizontalalignment=horizontal_alignment,\ncolor=color,\nzorder=10,\nrotation=rotation,\n)\nreturn self.output\ndef draw_box(self, box_coord, alpha=0.5, edge_color=\"g\", line_style=\"-\"):\n\"\"\"\nArgs:\nbox_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0\nare the coordinates of the image's top left corner. x1 and y1 are the\ncoordinates of the image's bottom right corner.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx0, y0, x1, y1 = box_coord\nwidth = x1 - x0\nheight = y1 - y0\nlinewidth = max(self._default_font_size / 4, 1)\nself.output.ax.add_patch(\nmpl.patches.Rectangle(\n(x0, y0),\nwidth,\nheight,\nfill=False,\nedgecolor=edge_color,\nlinewidth=linewidth * self.output.scale,\nalpha=alpha,\nlinestyle=line_style,\n)\n)\nreturn self.output\ndef draw_rotated_box_with_label(\nself, rotated_box, alpha=0.5, edge_color=\"g\", line_style=\"-\", label=None\n):\n\"\"\"\nDraw a rotated box with label on its top-left corner.\nArgs:\nrotated_box (tuple): a tuple containing (cnt_x, cnt_y, w, h, angle),\nwhere cnt_x and cnt_y are the center coordinates of the box.\nw and h are the width and height of the box. angle represents how\nmany degrees the box is rotated CCW with regard to the 0-degree box.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nlabel (string): label for rotated box. It will not be rendered when set to None.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\ncnt_x, cnt_y, w, h, angle = rotated_box\narea = w * h\n# use thinner lines when the box is small\nlinewidth = self._default_font_size / (\n6 if area < _SMALL_OBJECT_AREA_THRESH * self.output.scale else 3\n)\ntheta = angle * math.pi / 180.0\nc = math.cos(theta)\ns = math.sin(theta)\nrect = [(-w / 2, h / 2), (-w / 2, -h / 2), (w / 2, -h / 2), (w / 2, h / 2)]\n# x: left->right ; y: top->down\nrotated_rect = [(s * yy + c * xx + cnt_x, c * yy - s * xx + cnt_y) for (xx, yy) in rect]\nfor k in range(4):\nj = (k + 1) % 4\nself.draw_line(\n[rotated_rect[k][0], rotated_rect[j][0]],\n[rotated_rect[k][1], rotated_rect[j][1]],\ncolor=edge_color,\nlinestyle=\"--\" if k == 1 else line_style,\nlinewidth=linewidth,\n)\nif label is not None:\ntext_pos = rotated_rect[1]  # topleft corner\nheight_ratio = h / np.sqrt(self.output.height * self.output.width)\nlabel_color = self._change_color_brightness(edge_color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2) * 0.5 * self._default_font_size\n)\nself.draw_text(label, text_pos, color=label_color, font_size=font_size, rotation=angle)\nreturn self.output\ndef draw_circle(self, circle_coord, color, radius=3):\n\"\"\"\nArgs:\ncircle_coord (list(int) or tuple(int)): contains the x and y coordinates\nof the center of the circle.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nradius (int): radius of the circle.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx, y = circle_coord\nself.output.ax.add_patch(\nmpl.patches.Circle(circle_coord, radius=radius, fill=True, color=color)\n)\nreturn self.output\ndef draw_line(self, x_data, y_data, color, linestyle=\"-\", linewidth=None):\n\"\"\"\nArgs:\nx_data (list[int]): a list containing x values of all the points being drawn.\nLength of list should match the length of y_data.\ny_data (list[int]): a list containing y values of all the points being drawn.\nLength of list should match the length of x_data.\ncolor: color of the line. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nlinestyle: style of the line. Refer to `matplotlib.lines.Line2D`\nfor a full list of formats that are accepted.\nlinewidth (float or None): width of the line. When it's None,\na default value will be computed and used.\nReturns:\noutput (VisImage): image object with line drawn.\n\"\"\"\nif linewidth is None:\nlinewidth = self._default_font_size / 3\nlinewidth = max(linewidth, 1)\nself.output.ax.add_line(\nmpl.lines.Line2D(\nx_data,\ny_data,\nlinewidth=linewidth * self.output.scale,\ncolor=color,\nlinestyle=linestyle,\n)\n)\nreturn self.output\ndef draw_binary_mask(\nself, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n):\n\"\"\"\nArgs:\nbinary_mask (ndarray): numpy array of shape (H, W), where H is the image height and\nW is the image width. Each value in the array is either a 0 or 1 value of uint8\ntype.\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\narea_threshold (float): a connected component smaller than this area will not be shown.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nhas_valid_segment = False\nbinary_mask = binary_mask.astype(\"uint8\")  # opencv needs uint8\nmask = GenericMask(binary_mask, self.output.height, self.output.width)\nshape2d = (binary_mask.shape[0], binary_mask.shape[1])\nif not mask.has_holes:\n# draw polygons for regular masks\nfor segment in mask.polygons:\narea = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))\nif area < (area_threshold or 0):\ncontinue\nhas_valid_segment = True\nsegment = segment.reshape(-1, 2)\nself.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)\nelse:\n# TODO: Use Path/PathPatch to draw vector graphics:\n# https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = (mask.mask == 1).astype(\"float32\") * alpha\nhas_valid_segment = True\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None and has_valid_segment:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nself._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output\ndef draw_soft_mask(self, soft_mask, color=None, *, text=None, alpha=0.5):\n\"\"\"\nArgs:\nsoft_mask (ndarray): float array of shape (H, W), each value in [0, 1].\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nshape2d = (soft_mask.shape[0], soft_mask.shape[1])\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = soft_mask * alpha\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nbinary_mask = (soft_mask > 0.5).astype(\"uint8\")\nself._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output\ndef draw_polygon(self, segment, color, edge_color=None, alpha=0.5):\n\"\"\"\nArgs:\nsegment: numpy array of shape Nx2, containing all the points in the polygon.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted. If not provided, a darker shade\nof the polygon color will be used instead.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with polygon drawn.\n\"\"\"\nif edge_color is None:\n# make edge color darker than the polygon color\nif alpha > 0.8:\nedge_color = self._change_color_brightness(color, brightness_factor=-0.7)\nelse:\nedge_color = color\nedge_color = mplc.to_rgb(edge_color) + (1,)\npolygon = mpl.patches.Polygon(\nsegment,\nfill=True,\nfacecolor=mplc.to_rgb(color) + (alpha,),\nedgecolor=edge_color,\nlinewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n)\nself.output.ax.add_patch(polygon)\nreturn self.output\n\"\"\"\nInternal methods:\n\"\"\"\ndef _jitter(self, color):\n\"\"\"\nRandomly modifies given color to produce a slightly different color than the color given.\nArgs:\ncolor (tuple[double]): a tuple of 3 elements, containing the RGB values of the color\npicked. The values in the list are in the [0.0, 1.0] range.\nReturns:\njittered_color (tuple[double]): a tuple of 3 elements, containing the RGB values of the\ncolor after being jittered. The values in the list are in the [0.0, 1.0] range.\n\"\"\"\ncolor = mplc.to_rgb(color)\nvec = np.random.rand(3)\n# better to do it in another color space\nvec = vec / np.linalg.norm(vec) * 0.5\nres = np.clip(vec + color, 0, 1)\nreturn tuple(res)\ndef _create_grayscale_image(self, mask=None):\n\"\"\"\nCreate a grayscale version of the original image.\nThe colors in masked area, if given, will be kept.\n\"\"\"\nimg_bw = self.img.astype(\"f4\").mean(axis=2)\nimg_bw = np.stack([img_bw] * 3, axis=2)\nif mask is not None:\nimg_bw[mask] = self.img[mask]\nreturn img_bw\ndef _change_color_brightness(self, color, brightness_factor):\n\"\"\"\nDepending on the brightness_factor, gives a lighter or darker color i.e. a color with\nless or more saturation than the original color.\nArgs:\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nbrightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of\n0 will correspond to no change, a factor in [-1.0, 0) range will result in\na darker color and a factor in (0, 1.0] range will result in a lighter color.\nReturns:\nmodified_color (tuple[double]): a tuple containing the RGB values of the\nmodified color. Each value in the tuple is in the [0.0, 1.0] range.\n\"\"\"\nassert brightness_factor >= -1.0 and brightness_factor <= 1.0\ncolor = mplc.to_rgb(color)\npolygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))\nmodified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])\nmodified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness\nmodified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness\nmodified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])\nreturn modified_color\ndef _convert_boxes(self, boxes):\n\"\"\"\nConvert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n\"\"\"\nif isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):\nreturn boxes.tensor.detach().numpy()\nelse:\nreturn np.asarray(boxes)\ndef _convert_masks(self, masks_or_polygons):\n\"\"\"\nConvert different format of masks or polygons to a tuple of masks and polygons.\nReturns:\nlist[GenericMask]:\n\"\"\"\nm = masks_or_polygons\nif isinstance(m, PolygonMasks):\nm = m.polygons\nif isinstance(m, BitMasks):\nm = m.tensor.numpy()\nif isinstance(m, torch.Tensor):\nm = m.numpy()\nret = []\nfor x in m:\nif isinstance(x, GenericMask):\nret.append(x)\nelse:\nret.append(GenericMask(x, self.output.height, self.output.width))\nreturn ret\ndef _draw_text_in_mask(self, binary_mask, text, color):\n\"\"\"\nFind proper places to draw text given a binary mask.\n\"\"\"\n# TODO sometimes drawn on wrong objects. the heuristics here can improve.\n_num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, 8)\nif stats[1:, -1].size == 0:\nreturn\nlargest_component_id = np.argmax(stats[1:, -1]) + 1\n# draw text on the largest component, as well as other very large components.\nfor cid in range(1, _num_cc):\nif cid == largest_component_id or stats[cid, -1] > _LARGE_MASK_AREA_THRESH:\n# median is more stable than centroid\n# center = centroids[largest_component_id]\ncenter = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]\nself.draw_text(text, center, color=color)\ndef _convert_keypoints(self, keypoints):\nif isinstance(keypoints, Keypoints):\nkeypoints = keypoints.tensor\nkeypoints = np.asarray(keypoints)\nreturn keypoints\ndef get_output(self):\n\"\"\"\nReturns:\noutput (VisImage): the image output containing the visualizations added\nto the image.\n\"\"\"\nreturn self.output",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 186
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "pretrain_python_data_7524367",
                        "content": "<gh_stars>0\n# Copyright (c) Facebook, Inc. and its affiliates.\nimport colorsys\nimport logging\nimport math\nimport numpy as np\nfrom enum import Enum, unique\nimport cv2\nimport matplotlib as mpl\nimport matplotlib.colors as mplc\nimport matplotlib.figure as mplfigure\nimport pycocotools.mask as mask_util\nimport torch\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\nfrom PIL import Image\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.structures import BitMasks, Boxes, BoxMode, Keypoints, PolygonMasks, RotatedBoxes\nfrom detectron2.utils.file_io import PathManager\nfrom .colormap import random_color\nlogger = logging.getLogger(__name__)\n__all__ = [\"ColorMode\", \"VisImage\", \"Visualizer\"]\n_SMALL_OBJECT_AREA_THRESH = 1000\n_LARGE_MASK_AREA_THRESH = 120000\n_OFF_WHITE = (1.0, 1.0, 240.0 / 255)\n_BLACK = (0, 0, 0)\n_RED = (1.0, 0, 0)\n_KEYPOINT_THRESHOLD = 0.05\n@unique\nclass ColorMode(Enum):\n\"\"\"\nEnum of different color modes to use for instance visualizations.\n\"\"\"\nIMAGE = 0\n\"\"\"\nPicks a random color for every instance and overlay segmentations with low opacity.\n\"\"\"\nSEGMENTATION = 1\n\"\"\"\nLet instances of the same category have similar colors\n(from metadata.thing_colors), and overlay them with\nhigh opacity. This provides more attention on the quality of segmentation.\n\"\"\"\nIMAGE_BW = 2\n\"\"\"\nSame as IMAGE, but convert all areas without masks to gray-scale.\nOnly available for drawing per-instance mask predictions.\n\"\"\"\nclass GenericMask:\n\"\"\"\nAttribute:\npolygons (list[ndarray]): list[ndarray]: polygons for this mask.\nEach ndarray has format [x, y, x, y, ...]\nmask (ndarray): a binary mask\n\"\"\"\ndef __init__(self, mask_or_polygons, height, width):\nself._mask = self._polygons = self._has_holes = None\nself.height = height\nself.width = width\nm = mask_or_polygons\nif isinstance(m, dict):\n# RLEs\nassert \"counts\" in m and \"size\" in m\nif isinstance(m[\"counts\"], list):  # uncompressed RLEs\nh, w = m[\"size\"]\nassert h == height and w == width\nm = mask_util.frPyObjects(m, h, w)\nself._mask = mask_util.decode(m)[:, :]\nreturn\nif isinstance(m, list):  # list[ndarray]\nself._polygons = [np.asarray(x).reshape(-1) for x in m]\nreturn\nif isinstance(m, np.ndarray):  # assumed to be a binary mask\nassert m.shape[1] != 2, m.shape\nassert m.shape == (\nheight,\nwidth,\n), f\"mask shape: {m.shape}, target dims: {height}, {width}\"\nself._mask = m.astype(\"uint8\")\nreturn\nraise ValueError(\"GenericMask cannot handle object {} of type '{}'\".format(m, type(m)))\n@property\ndef mask(self):\nif self._mask is None:\nself._mask = self.polygons_to_mask(self._polygons)\nreturn self._mask\n@property\ndef polygons(self):\nif self._polygons is None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nreturn self._polygons\n@property\ndef has_holes(self):\nif self._has_holes is None:\nif self._mask is not None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nelse:\nself._has_holes = False  # if original format is polygon, does not have holes\nreturn self._has_holes\ndef mask_to_polygons(self, mask):\n# cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level\n# hierarchy. External contours (boundary) of the object are placed in hierarchy-1.\n# Internal contours (holes) are placed in hierarchy-2.\n# cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.\nmask = np.ascontiguousarray(mask)  # some versions of cv2 does not support incontiguous arr\nres = cv2.findContours(mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\nhierarchy = res[-1]\nif hierarchy is None:  # empty mask\nreturn [], False\nhas_holes = (hierarchy.reshape(-1, 4)[:, 3] >= 0).sum() > 0\nres = res[-2]\nres = [x.flatten() for x in res]\n# These coordinates from OpenCV are integers in range [0, W-1 or H-1].\n# We add 0.5 to turn them into real-value coordinate space. A better solution\n# would be to first +0.5 and then dilate the returned polygon by 0.5.\nres = [x + 0.5 for x in res if len(x) >= 6]\nreturn res, has_holes\ndef polygons_to_mask(self, polygons):\nrle = mask_util.frPyObjects(polygons, self.height, self.width)\nrle = mask_util.merge(rle)\nreturn mask_util.decode(rle)[:, :]\ndef area(self):\nreturn self.mask.sum()\ndef bbox(self):\np = mask_util.frPyObjects(self.polygons, self.height, self.width)\np = mask_util.merge(p)\nbbox = mask_util.toBbox(p)\nbbox[2] += bbox[0]\nbbox[3] += bbox[1]\nreturn bbox\nclass _PanopticPrediction:\n\"\"\"\nUnify different panoptic annotation/prediction formats\n\"\"\"\ndef __init__(self, panoptic_seg, segments_info, metadata=None):\nif segments_info is None:\nassert metadata is not None\n# If \"segments_info\" is None, we assume \"panoptic_img\" is a\n# H*W int32 image storing the panoptic_id in the format of\n# category_id * label_divisor + instance_id. We reserve -1 for\n# VOID label.\nlabel_divisor = metadata.label_divisor\nsegments_info = []\nfor panoptic_label in np.unique(panoptic_seg.numpy()):\nif panoptic_label == -1:\n# VOID region.\ncontinue\npred_class = panoptic_label // label_divisor\nisthing = pred_class in metadata.thing_dataset_id_to_contiguous_id.values()\nsegments_info.append(\n{\n\"id\": int(panoptic_label),\n\"category_id\": int(pred_class),\n\"isthing\": bool(isthing),\n}\n)\ndel metadata\nself._seg = panoptic_seg\nself._sinfo = {s[\"id\"]: s for s in segments_info}  # seg id -> seg info\nsegment_ids, areas = torch.unique(panoptic_seg, sorted=True, return_counts=True)\nareas = areas.numpy()\nsorted_idxs = np.argsort(-areas)\nself._seg_ids, self._seg_areas = segment_ids[sorted_idxs], areas[sorted_idxs]\nself._seg_ids = self._seg_ids.tolist()\nfor sid, area in zip(self._seg_ids, self._seg_areas):\nif sid in self._sinfo:\nself._sinfo[sid][\"area\"] = float(area)\ndef non_empty_mask(self):\n\"\"\"\nReturns:\n(H, W) array, a mask for all pixels that have a prediction\n\"\"\"\nempty_ids = []\nfor id in self._seg_ids:\nif id not in self._sinfo:\nempty_ids.append(id)\nif len(empty_ids) == 0:\nreturn np.zeros(self._seg.shape, dtype=np.uint8)\nassert (\nlen(empty_ids) == 1\n), \">1 ids corresponds to no labels. This is currently not supported\"\nreturn (self._seg != empty_ids[0]).numpy().astype(np.bool)\ndef semantic_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or sinfo[\"isthing\"]:\n# Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions.\ncontinue\nyield (self._seg == sid).numpy().astype(np.bool), sinfo\ndef instance_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or not sinfo[\"isthing\"]:\ncontinue\nmask = (self._seg == sid).numpy().astype(np.bool)\nif mask.sum() > 0:\nyield mask, sinfo\ndef _create_text_labels(classes, scores, class_names, is_crowd=None):\n\"\"\"\nArgs:\nclasses (list[int] or None):\nscores (list[float] or None):\nclass_names (list[str] or None):\nis_crowd (list[bool] or None):\nReturns:\nlist[str] or None\n\"\"\"\nlabels = None\nif classes is not None:\nif class_names is not None and len(class_names) > 0:\nlabels = [class_names[i] for i in classes]\nelse:\nlabels = [str(i) for i in classes]\nif scores is not None:\nif labels is None:\nlabels = [\"{:.0f}%\".format(s * 100) for s in scores]\nelse:\nlabels = [\"{} {:.0f}%\".format(l, s * 100) for l, s in zip(labels, scores)]\nif labels is not None and is_crowd is not None:\nlabels = [l + (\"|crowd\" if crowd else \"\") for l, crowd in zip(labels, is_crowd)]\nreturn labels\nclass VisImage:\ndef __init__(self, img, scale=1.0):\n\"\"\"\nArgs:\nimg (ndarray): an RGB image of shape (H, W, 3) in range [0, 255].\nscale (float): scale the input image\n\"\"\"\nself.img = img\nself.scale = scale\nself.width, self.height = img.shape[1], img.shape[0]\nself._setup_figure(img)\ndef _setup_figure(self, img):\n\"\"\"\nArgs:\nSame as in :meth:`__init__()`.\nReturns:\nfig (matplotlib.pyplot.figure): top level container for all the image plot elements.\nax (matplotlib.pyplot.Axes): contains figure elements and sets the coordinate system.\n\"\"\"\nfig = mplfigure.Figure(frameon=False)\nself.dpi = fig.get_dpi()\n# add a small 1e-2 to avoid precision lost due to matplotlib's truncation\n# (https://github.com/matplotlib/matplotlib/issues/15363)\nfig.set_size_inches(\n(self.width * self.scale + 1e-2) / self.dpi,\n(self.height * self.scale + 1e-2) / self.dpi,\n)\nself.canvas = FigureCanvasAgg(fig)\n# self.canvas = mpl.backends.backend_cairo.FigureCanvasCairo(fig)\nax = fig.add_axes([0.0, 0.0, 1.0, 1.0])\nax.axis(\"off\")\nself.fig = fig\nself.ax = ax\nself.reset_image(img)\ndef reset_image(self, img):\n\"\"\"\nArgs:\nimg: same as in __init__\n\"\"\"\nimg = img.astype(\"uint8\")\nself.ax.imshow(img, extent=(0, self.width, self.height, 0), interpolation=\"nearest\")\ndef save(self, filepath):\n\"\"\"\nArgs:\nfilepath (str): a string that contains the absolute path, including the file name, where\nthe visualized image will be saved.\n\"\"\"\nself.fig.savefig(filepath)\ndef get_image(self):\n\"\"\"\nReturns:\nndarray:\nthe visualized image of shape (H, W, 3) (RGB) in uint8 type.\nThe shape is scaled w.r.t the input image using the given `scale` argument.\n\"\"\"\ncanvas = self.canvas\ns, (width, height) = canvas.print_to_buffer()\n# buf = io.BytesIO()  # works for cairo backend\n# canvas.print_rgba(buf)\n# width, height = self.width, self.height\n# s = buf.getvalue()\nbuffer = np.frombuffer(s, dtype=\"uint8\")\nimg_rgba = buffer.reshape(height, width, 4)\nrgb, alpha = np.split(img_rgba, [3], axis=2)\nreturn rgb.astype(\"uint8\")\nclass Visualizer:\n\"\"\"\nVisualizer that draws data about detection/segmentation on images.\nIt contains methods like `draw_{text,box,circle,line,binary_mask,polygon}`\nthat draw primitive objects to images, as well as high-level wrappers like\n`draw_{instance_predictions,sem_seg,panoptic_seg_predictions,dataset_dict}`\nthat draw composite data in some pre-defined style.\nNote that the exact visualization style for the high-level wrappers are subject to change.\nStyle such as color, opacity, label contents, visibility of labels, or even the visibility\nof objects themselves (e.g. when the object is too small) may change according\nto different heuristics, as long as the results still look visually reasonable.\nTo obtain a consistent style, you can implement custom drawing functions with the\nabovementioned primitive methods instead. If you need more customized visualization\nstyles, you can process the data yourself following their format documented in\ntutorials (:doc:`/tutorials/models`, :doc:`/tutorials/datasets`). This class does not\nintend to satisfy everyone's preference on drawing styles.\nThis visualizer focuses on high rendering quality rather than performance. It is not\ndesigned to be used for real-time applications.\n\"\"\"\n# TODO implement a fast, rasterized version using OpenCV\ndef __init__(self, img_rgb, metadata=None, scale=1.0, instance_mode=ColorMode.IMAGE):\n\"\"\"\nArgs:\nimg_rgb: a numpy array of shape (H, W, C), where H and W correspond to\nthe height and width of the image respectively. C is the number of\ncolor channels. The image is required to be in RGB format since that\nis a requirement of the Matplotlib library. The image is also expected\nto be in the range [0, 255].\nmetadata (Metadata): dataset metadata (e.g. class names and colors)\ninstance_mode (ColorMode): defines one of the pre-defined style for drawing\ninstances on an image.\n\"\"\"\nself.img = np.asarray(img_rgb).clip(0, 255).astype(np.uint8)\nif metadata is None:\nmetadata = MetadataCatalog.get(\"__nonexist__\")\nself.metadata = metadata\nself.output = VisImage(self.img, scale=scale)\nself.cpu_device = torch.device(\"cpu\")\n# too small texts are useless, therefore clamp to 9\nself._default_font_size = max(\nnp.sqrt(self.output.height * self.output.width) // 90, 10 // scale\n)\nself._instance_mode = instance_mode\nself.keypoint_threshold = _KEYPOINT_THRESHOLD\ndef draw_instance_predictions(self, predictions):\n\"\"\"\nDraw instance-level prediction results on an image.\nArgs:\npredictions (Instances): the output of an instance detection/segmentation\nmodel. Following fields will be used to draw:\n\"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nboxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\nscores = predictions.scores if predictions.has(\"scores\") else None\nclasses = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\nlabels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\nkeypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\nif predictions.has(\"pred_masks\"):\nmasks = np.asarray(predictions.pred_masks)\nmasks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\nelse:\nmasks = None\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n]\nalpha = 0.8\nelse:\ncolors = None\nalpha = 0.5\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(\nself._create_grayscale_image(\n(predictions.pred_masks.any(dim=0) > 0).numpy()\nif predictions.has(\"pred_masks\")\nelse None\n)\n)\nalpha = 0.3\nself.overlay_instances(\nmasks=masks,\nboxes=boxes,\nlabels=labels,\nkeypoints=keypoints,\nassigned_colors=colors,\nalpha=alpha,\n)\nreturn self.output\ndef draw_sem_seg(self, sem_seg, area_threshold=None, alpha=0.8):\n\"\"\"\nDraw semantic segmentation predictions/labels.\nArgs:\nsem_seg (Tensor or ndarray): the segmentation of shape (H, W).\nEach value is the integer label of the pixel.\narea_threshold (int): segments with less than `area_threshold` are not drawn.\nalpha (float): the larger it is, the more opaque the segmentations are.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nif isinstance(sem_seg, torch.Tensor):\nsem_seg = sem_seg.numpy()\nlabels, areas = np.unique(sem_seg, return_counts=True)\nsorted_idxs = np.argsort(-areas).tolist()\nlabels = labels[sorted_idxs]\nfor label in filter(lambda l: l < len(self.metadata.stuff_classes), labels):\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[label]]\nexcept (AttributeError, IndexError):\nmask_color = None\nbinary_mask = (sem_seg == label).astype(np.uint8)\ntext = self.metadata.stuff_classes[label]\nself.draw_binary_mask(\nbinary_mask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\nreturn self.output\ndef draw_panoptic_seg(self, panoptic_seg, segments_info, area_threshold=None, alpha=0.7):\n\"\"\"\nDraw panoptic prediction annotations or results.\nArgs:\npanoptic_seg (Tensor): of shape (height, width) where the values are ids for each\nsegment.\nsegments_info (list[dict] or None): Describe each segment in `panoptic_seg`.\nIf it is a ``list[dict]``, each dict contains keys \"id\", \"category_id\".\nIf None, category id of each pixel is computed by\n``pixel // metadata.label_divisor``.\narea_threshold (int): stuff segments with less than `area_threshold` are not drawn.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nsemant_vect = []\npred = _PanopticPrediction(panoptic_seg, segments_info, self.metadata)\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(self._create_grayscale_image(pred.non_empty_mask()))\n# draw mask for all semantic segments first i.e. \"stuff\"\nfor mask, sinfo in pred.semantic_masks():\ncategory_idx = sinfo[\"category_id\"]\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]\nexcept AttributeError:\nmask_color = None\ntext = self.metadata.stuff_classes[category_idx]\n_ , pos_semant=self.draw_binary_mask(\nmask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\nsemant_vect.append([pos_semant, text])\n# draw mask for all instances second\nall_instances = list(pred.instance_masks())\nif len(all_instances) == 0:\nreturn self.output, semant_vect\nmasks, sinfo = list(zip(*all_instances))\ncategory_ids = [x[\"category_id\"] for x in sinfo]\ntry:\nscores = [x[\"score\"] for x in sinfo]\nexcept KeyError:\nscores = None\nlabels = _create_text_labels(\ncategory_ids, scores, self.metadata.thing_classes, [x.get(\"iscrowd\", 0) for x in sinfo]\n)\ntry:\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in category_ids\n]\nexcept AttributeError:\ncolors = None\n_, pos_obj_semant= self.overlay_instances(masks=masks, labels=labels, assigned_colors=colors, alpha=alpha)\nsemant_vect.extend(pos_obj_semant)\nreturn self.output, semant_vect\n#return semant_vect.append(pos_obj_semant)\ndraw_panoptic_seg_predictions = draw_panoptic_seg  # backward compatibility\ndef draw_dataset_dict(self, dic):\n\"\"\"\nDraw annotations/segmentaions in Detectron2 Dataset format.\nArgs:\ndic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nannos = dic.get(\"annotations\", None)\nif annos:\nif \"segmentation\" in annos[0]:\nmasks = [x[\"segmentation\"] for x in annos]\nelse:\nmasks = None\nif \"keypoints\" in annos[0]:\nkeypts = [x[\"keypoints\"] for x in annos]\nkeypts = np.array(keypts).reshape(len(annos), -1, 3)\nelse:\nkeypts = None\nboxes = [\nBoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS)\nif len(x[\"bbox\"]) == 4\nelse x[\"bbox\"]\nfor x in annos\n]\ncolors = None\ncategory_ids = [x[\"category_id\"] for x in annos]\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]])\nfor c in category_ids\n]\nnames = self.metadata.get(\"thing_classes\", None)\nlabels = _create_text_labels(\ncategory_ids,\nscores=None,\nclass_names=names,\nis_crowd=[x.get(\"iscrowd\", 0) for x in annos],\n)\nself.overlay_instances(\nlabels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n)\nsem_seg = dic.get(\"sem_seg\", None)\nif sem_seg is None and \"sem_seg_file_name\" in dic:\nwith PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\nsem_seg = Image.open(f)\nsem_seg = np.asarray(sem_seg, dtype=\"uint8\")\nif sem_seg is not None:\nself.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\npan_seg = dic.get(\"pan_seg\", None)\nif pan_seg is None and \"pan_seg_file_name\" in dic:\nwith PathManager.open(dic[\"pan_seg_file_name\"], \"rb\") as f:\npan_seg = Image.open(f)\npan_seg = np.asarray(pan_seg)\nfrom panopticapi.utils import rgb2id\npan_seg = rgb2id(pan_seg)\nif pan_seg is not None:\nsegments_info = dic[\"segments_info\"]\npan_seg = torch.tensor(pan_seg)\nself.draw_panoptic_seg(pan_seg, segments_info, area_threshold=0, alpha=0.5)\nreturn self.output\ndef overlay_instances(\nself,\n*,\nboxes=None,\nlabels=None,\nmasks=None,\nkeypoints=None,\nassigned_colors=None,\nalpha=0.5,\n):\n\"\"\"\nArgs:\nboxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,\nor an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,\nor a :class:`RotatedBoxes`,\nor an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image,\nlabels (list[str]): the text to be displayed for each instance.\nmasks (masks-like object): Supported types are:\n* :class:`detectron2.structures.PolygonMasks`,\n:class:`detectron2.structures.BitMasks`.\n* list[list[ndarray]]: contains the segmentation masks for all objects in one image.\nThe first level of the list corresponds to individual instances. The second\nlevel to all the polygon that compose the instance, and the third level\nto the polygon coordinates. The third level should have the format of\n[x0, y0, x1, y1, ..., xn, yn] (n >= 3).\n* list[ndarray]: each ndarray is a binary mask of shape (H, W).\n* list[dict]: each dict is a COCO-style RLE.\nkeypoints (Keypoint or array like): an array-like object of shape (N, K, 3),\nwhere the N is the number of instances and K is the number of keypoints.\nThe last dimension corresponds to (x, y, visibility or score).\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = 0\nif boxes is not None:\nboxes = self._convert_boxes(boxes)\nnum_instances = len(boxes)\nif masks is not None:\nmasks = self._convert_masks(masks)\nif num_instances:\nassert len(masks) == num_instances\nelse:\nnum_instances = len(masks)\nif keypoints is not None:\nif num_instances:\nassert len(keypoints) == num_instances\nelse:\nnum_instances = len(keypoints)\nkeypoints = self._convert_keypoints(keypoints)\nif labels is not None:\nassert len(labels) == num_instances\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\nif boxes is not None and boxes.shape[1] == 5:\nreturn self.overlay_rotated_instances(\nboxes=boxes, labels=labels, assigned_colors=assigned_colors\n)\n# Display in largest to smallest order to reduce occlusion.\nareas = None\nif boxes is not None:\nareas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\nelif masks is not None:\nareas = np.asarray([x.area() for x in masks])\nif areas is not None:\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs] if boxes is not None else None\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\nmasks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\nassigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\nkeypoints = keypoints[sorted_idxs] if keypoints is not None else None\n#print(num_instances)\n#print(labels)\nobj_vect=[]\nfor i in range(num_instances):\ncolor = assigned_colors[i]\nif boxes is not None:\nself.draw_box(boxes[i], edge_color=color)\nif masks is not None:\nfor segment in masks[i].polygons:\nself.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\nif labels is not None:\n# first get a box\nif boxes is not None:\nx0, y0, x1, y1 = boxes[i]\ntext_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\nhoriz_align = \"left\"\nelif masks is not None:\n# skip small mask without polygon\nif len(masks[i].polygons) == 0:\ncontinue\nx0, y0, x1, y1 = masks[i].bbox()\n# draw text in the center (defined by median) when box is not drawn\n# median is less sensitive to outliers.\ntext_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\nhoriz_align = \"center\"\nelse:\ncontinue  # drawing the box confidence for keypoints isn't very useful.\n# for small objects, draw text at the side to avoid occlusion\ninstance_area = (y1 - y0) * (x1 - x0)\nif (\ninstance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\nor y1 - y0 < 40 * self.output.scale\n):\nif y1 >= self.output.height - 5:\ntext_pos = (x1, y0)\nelse:\ntext_pos = (x0, y1)\nheight_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n* 0.5\n* self._default_font_size\n)\nself.draw_text(\nlabels[i],\ntext_pos,\ncolor=lighter_color,\nhorizontal_alignment=horiz_align,\nfont_size=font_size,\n)\nobj_vect.append([text_pos,labels[i]])\n# draw keypoints\nif keypoints is not None:\nfor keypoints_per_instance in keypoints:\nself.draw_and_connect_keypoints(keypoints_per_instance)\nreturn self.output, obj_vect\ndef overlay_rotated_instances(self, boxes=None, labels=None, assigned_colors=None):\n\"\"\"\nArgs:\nboxes (ndarray): an Nx5 numpy array of\n(x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image.\nlabels (list[str]): the text to be displayed for each instance.\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = len(boxes)\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\n# Display in largest to smallest order to reduce occlusion.\nif boxes is not None:\nareas = boxes[:, 2] * boxes[:, 3]\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs]\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\ncolors = [assigned_colors[idx] for idx in sorted_idxs]\nfor i in range(num_instances):\nself.draw_rotated_box_with_label(\nboxes[i], edge_color=colors[i], label=labels[i] if labels is not None else None\n)\nreturn self.output\ndef draw_and_connect_keypoints(self, keypoints):\n\"\"\"\nDraws keypoints of an instance and follows the rules for keypoint connections\nto draw lines between appropriate keypoints. This follows color heuristics for\nline color.\nArgs:\nkeypoints (Tensor): a tensor of shape (K, 3), where K is the number of keypoints\nand the last dimension corresponds to (x, y, probability).\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nvisible = {}\nkeypoint_names = self.metadata.get(\"keypoint_names\")\nfor idx, keypoint in enumerate(keypoints):\n# draw keypoint\nx, y, prob = keypoint\nif prob > self.keypoint_threshold:\nself.draw_circle((x, y), color=_RED)\nif keypoint_names:\nkeypoint_name = keypoint_names[idx]\nvisible[keypoint_name] = (x, y)\nif self.metadata.get(\"keypoint_connection_rules\"):\nfor kp0, kp1, color in self.metadata.keypoint_connection_rules:\nif kp0 in visible and kp1 in visible:\nx0, y0 = visible[kp0]\nx1, y1 = visible[kp1]\ncolor = tuple(x / 255.0 for x in color)\nself.draw_line([x0, x1], [y0, y1], color=color)\n# draw lines from nose to mid-shoulder and mid-shoulder to mid-hip\n# Note that this strategy is specific to person keypoints.\n# For other keypoints, it should just do nothing\ntry:\nls_x, ls_y = visible[\"left_shoulder\"]\nrs_x, rs_y = visible[\"right_shoulder\"]\nmid_shoulder_x, mid_shoulder_y = (ls_x + rs_x) / 2, (ls_y + rs_y) / 2\nexcept KeyError:\npass\nelse:\n# draw line from nose to mid-shoulder\nnose_x, nose_y = visible.get(\"nose\", (None, None))\nif nose_x is not None:\nself.draw_line([nose_x, mid_shoulder_x], [nose_y, mid_shoulder_y], color=_RED)\ntry:\n# draw line from mid-shoulder to mid-hip\nlh_x, lh_y = visible[\"left_hip\"]\nrh_x, rh_y = visible[\"right_hip\"]\nexcept KeyError:\npass\nelse:\nmid_hip_x, mid_hip_y = (lh_x + rh_x) / 2, (lh_y + rh_y) / 2\nself.draw_line([mid_hip_x, mid_shoulder_x], [mid_hip_y, mid_shoulder_y], color=_RED)\nreturn self.output\n\"\"\"\nPrimitive drawing functions:\n\"\"\"\ndef draw_text(\nself,\ntext,\nposition,\n*,\nfont_size=None,\ncolor=\"g\",\nhorizontal_alignment=\"center\",\nrotation=0,\n):\n\"\"\"\nArgs:\ntext (str): class label\nposition (tuple): a tuple of the x and y coordinates to place text on image.\nfont_size (int, optional): font of the text. If not provided, a font size\nproportional to the image width is calculated and used.\ncolor: color of the text. Refer to `matplotlib.colors` for full list\nof formats that are accepted.\nhorizontal_alignment (str): see `matplotlib.text.Text`\nrotation: rotation angle in degrees CCW\nReturns:\noutput (VisImage): image object with text drawn.\n\"\"\"\nif not font_size:\nfont_size = self._default_font_size\n# since the text background is dark, we don't want the text to be dark\ncolor = np.maximum(list(mplc.to_rgb(color)), 0.2)\ncolor[np.argmax(color)] = max(0.8, np.max(color))\nx, y = position\nself.output.ax.text(\nx,\ny,\ntext,\nsize=font_size * self.output.scale,\nfamily=\"sans-serif\",\nbbox={\"facecolor\": \"black\", \"alpha\": 0.8, \"pad\": 0.7, \"edgecolor\": \"none\"},\nverticalalignment=\"top\",\nhorizontalalignment=horizontal_alignment,\ncolor=color,\nzorder=10,\nrotation=rotation,\n)\nreturn self.output\ndef draw_box(self, box_coord, alpha=0.5, edge_color=\"g\", line_style=\"-\"):\n\"\"\"\nArgs:\nbox_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0\nare the coordinates of the image's top left corner. x1 and y1 are the\ncoordinates of the image's bottom right corner.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx0, y0, x1, y1 = box_coord\nwidth = x1 - x0\nheight = y1 - y0\nlinewidth = max(self._default_font_size / 4, 1)\nself.output.ax.add_patch(\nmpl.patches.Rectangle(\n(x0, y0),\nwidth,\nheight,\nfill=False,\nedgecolor=edge_color,\nlinewidth=linewidth * self.output.scale,\nalpha=alpha,\nlinestyle=line_style,\n)\n)\nreturn self.output\ndef draw_rotated_box_with_label(\nself, rotated_box, alpha=0.5, edge_color=\"g\", line_style=\"-\", label=None\n):\n\"\"\"\nDraw a rotated box with label on its top-left corner.\nArgs:\nrotated_box (tuple): a tuple containing (cnt_x, cnt_y, w, h, angle),\nwhere cnt_x and cnt_y are the center coordinates of the box.\nw and h are the width and height of the box. angle represents how\nmany degrees the box is rotated CCW with regard to the 0-degree box.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nlabel (string): label for rotated box. It will not be rendered when set to None.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\ncnt_x, cnt_y, w, h, angle = rotated_box\narea = w * h\n# use thinner lines when the box is small\nlinewidth = self._default_font_size / (\n6 if area < _SMALL_OBJECT_AREA_THRESH * self.output.scale else 3\n)\ntheta = angle * math.pi / 180.0\nc = math.cos(theta)\ns = math.sin(theta)\nrect = [(-w / 2, h / 2), (-w / 2, -h / 2), (w / 2, -h / 2), (w / 2, h / 2)]\n# x: left->right ; y: top->down\nrotated_rect = [(s * yy + c * xx + cnt_x, c * yy - s * xx + cnt_y) for (xx, yy) in rect]\nfor k in range(4):\nj = (k + 1) % 4\nself.draw_line(\n[rotated_rect[k][0], rotated_rect[j][0]],\n[rotated_rect[k][1], rotated_rect[j][1]],\ncolor=edge_color,\nlinestyle=\"--\" if k == 1 else line_style,\nlinewidth=linewidth,\n)\nif label is not None:\ntext_pos = rotated_rect[1]  # topleft corner\nheight_ratio = h / np.sqrt(self.output.height * self.output.width)\nlabel_color = self._change_color_brightness(edge_color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2) * 0.5 * self._default_font_size\n)\nself.draw_text(label, text_pos, color=label_color, font_size=font_size, rotation=angle)\nreturn self.output\ndef draw_circle(self, circle_coord, color, radius=3):\n\"\"\"\nArgs:\ncircle_coord (list(int) or tuple(int)): contains the x and y coordinates\nof the center of the circle.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nradius (int): radius of the circle.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx, y = circle_coord\nself.output.ax.add_patch(\nmpl.patches.Circle(circle_coord, radius=radius, fill=True, color=color)\n)\nreturn self.output\ndef draw_line(self, x_data, y_data, color, linestyle=\"-\", linewidth=None):\n\"\"\"\nArgs:\nx_data (list[int]): a list containing x values of all the points being drawn.\nLength of list should match the length of y_data.\ny_data (list[int]): a list containing y values of all the points being drawn.\nLength of list should match the length of x_data.\ncolor: color of the line. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nlinestyle: style of the line. Refer to `matplotlib.lines.Line2D`\nfor a full list of formats that are accepted.\nlinewidth (float or None): width of the line. When it's None,\na default value will be computed and used.\nReturns:\noutput (VisImage): image object with line drawn.\n\"\"\"\nif linewidth is None:\nlinewidth = self._default_font_size / 3\nlinewidth = max(linewidth, 1)\nself.output.ax.add_line(\nmpl.lines.Line2D(\nx_data,\ny_data,\nlinewidth=linewidth * self.output.scale,\ncolor=color,\nlinestyle=linestyle,\n)\n)\nreturn self.output\ndef draw_binary_mask(\nself, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n):\n\"\"\"\nArgs:\nbinary_mask (ndarray): numpy array of shape (H, W), where H is the image height and\nW is the image width. Each value in the array is either a 0 or 1 value of uint8\ntype.\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\narea_threshold (float): a connected component smaller than this area will not be shown.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nhas_valid_segment = False\nbinary_mask = binary_mask.astype(\"uint8\")  # opencv needs uint8\nmask = GenericMask(binary_mask, self.output.height, self.output.width)\nshape2d = (binary_mask.shape[0], binary_mask.shape[1])\nif not mask.has_holes:\n# draw polygons for regular masks\nfor segment in mask.polygons:\narea = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))\nif area < (area_threshold or 0):\ncontinue\nhas_valid_segment = True\nsegment = segment.reshape(-1, 2)\nself.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)\nelse:\n# TODO: Use Path/PathPatch to draw vector graphics:\n# https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = (mask.mask == 1).astype(\"float32\") * alpha\nhas_valid_segment = True\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None and has_valid_segment:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\ncenter = self._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output, center\ndef draw_soft_mask(self, soft_mask, color=None, *, text=None, alpha=0.5):\n\"\"\"\nArgs:\nsoft_mask (ndarray): float array of shape (H, W), each value in [0, 1].\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nshape2d = (soft_mask.shape[0], soft_mask.shape[1])\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = soft_mask * alpha\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nbinary_mask = (soft_mask > 0.5).astype(\"uint8\")\nself._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output\ndef draw_polygon(self, segment, color, edge_color=None, alpha=0.5):\n\"\"\"\nArgs:\nsegment: numpy array of shape Nx2, containing all the points in the polygon.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted. If not provided, a darker shade\nof the polygon color will be used instead.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with polygon drawn.\n\"\"\"\nif edge_color is None:\n# make edge color darker than the polygon color\nif alpha > 0.8:\nedge_color = self._change_color_brightness(color, brightness_factor=-0.7)\nelse:\nedge_color = color\nedge_color = mplc.to_rgb(edge_color) + (1,)\npolygon = mpl.patches.Polygon(\nsegment,\nfill=True,\nfacecolor=mplc.to_rgb(color) + (alpha,),\nedgecolor=edge_color,\nlinewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n)\nself.output.ax.add_patch(polygon)\nreturn self.output\n\"\"\"\nInternal methods:\n\"\"\"\ndef _jitter(self, color):\n\"\"\"\nRandomly modifies given color to produce a slightly different color than the color given.\nArgs:\ncolor (tuple[double]): a tuple of 3 elements, containing the RGB values of the color\npicked. The values in the list are in the [0.0, 1.0] range.\nReturns:\njittered_color (tuple[double]): a tuple of 3 elements, containing the RGB values of the\ncolor after being jittered. The values in the list are in the [0.0, 1.0] range.\n\"\"\"\ncolor = mplc.to_rgb(color)\nvec = np.random.rand(3)\n# better to do it in another color space\nvec = vec / np.linalg.norm(vec) * 0.5\nres = np.clip(vec + color, 0, 1)\nreturn tuple(res)\ndef _create_grayscale_image(self, mask=None):\n\"\"\"\nCreate a grayscale version of the original image.\nThe colors in masked area, if given, will be kept.\n\"\"\"\nimg_bw = self.img.astype(\"f4\").mean(axis=2)\nimg_bw = np.stack([img_bw] * 3, axis=2)\nif mask is not None:\nimg_bw[mask] = self.img[mask]\nreturn img_bw\ndef _change_color_brightness(self, color, brightness_factor):\n\"\"\"\nDepending on the brightness_factor, gives a lighter or darker color i.e. a color with\nless or more saturation than the original color.\nArgs:\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nbrightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of\n0 will correspond to no change, a factor in [-1.0, 0) range will result in\na darker color and a factor in (0, 1.0] range will result in a lighter color.\nReturns:\nmodified_color (tuple[double]): a tuple containing the RGB values of the\nmodified color. Each value in the tuple is in the [0.0, 1.0] range.\n\"\"\"\nassert brightness_factor >= -1.0 and brightness_factor <= 1.0\ncolor = mplc.to_rgb(color)\npolygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))\nmodified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])\nmodified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness\nmodified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness\nmodified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])\nreturn modified_color\ndef _convert_boxes(self, boxes):\n\"\"\"\nConvert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n\"\"\"\nif isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):\nreturn boxes.tensor.detach().numpy()\nelse:\nreturn np.asarray(boxes)\ndef _convert_masks(self, masks_or_polygons):\n\"\"\"\nConvert different format of masks or polygons to a tuple of masks and polygons.\nReturns:\nlist[GenericMask]:\n\"\"\"\nm = masks_or_polygons\nif isinstance(m, PolygonMasks):\nm = m.polygons\nif isinstance(m, BitMasks):\nm = m.tensor.numpy()\nif isinstance(m, torch.Tensor):\nm = m.numpy()\nret = []\nfor x in m:\nif isinstance(x, GenericMask):\nret.append(x)\nelse:\nret.append(GenericMask(x, self.output.height, self.output.width))\nreturn ret\ndef _draw_text_in_mask(self, binary_mask, text, color):\n\"\"\"\nFind proper places to draw text given a binary mask.\n\"\"\"\n# TODO sometimes drawn on wrong objects. the heuristics here can improve.\n_num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, 8)\nif stats[1:, -1].size == 0:\nreturn\nlargest_component_id = np.argmax(stats[1:, -1]) + 1\n# draw text on the largest component, as well as other very large components.\nfor cid in range(1, _num_cc):\nif cid == largest_component_id or stats[cid, -1] > _LARGE_MASK_AREA_THRESH:\n# median is more stable than centroid\n# center = centroids[largest_component_id]\ncenter = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]\nself.draw_text(text, center, color=color)\nreturn center\ndef _convert_keypoints(self, keypoints):\nif isinstance(keypoints, Keypoints):\nkeypoints = keypoints.tensor\nkeypoints = np.asarray(keypoints)\nreturn keypoints\ndef get_output(self):\n\"\"\"\nReturns:\noutput (VisImage): the image output containing the visualizations added\nto the image.\n\"\"\"\nreturn self.output",
                        "max_stars_repo_path": "detectron2/utils/visualizer.py",
                        "max_stars_repo_name": "robolableonardo/detectron2",
                        "max_stars_count": 0,
                        "__cluster__": 186
                },
                {
                        "id": "test_evocodebench_data_189",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nimport colorsys\nimport logging\nimport math\nfrom enum import Enum, unique\nimport cv2\nimport matplotlib as mpl\nimport matplotlib.colors as mplc\nimport matplotlib.figure as mplfigure\nimport numpy as np\nimport pycocotools.mask as mask_util\nimport torch\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.structures import (\nBitMasks,\nBoxes,\nBoxMode,\nKeypoints,\nPolygonMasks,\nRotatedBoxes,\n)\nfrom detectron2.utils.file_io import PathManager\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\nfrom PIL import Image\nfrom .colormap import random_color\nlogger = logging.getLogger(__name__)\n__all__ = [\"ColorMode\", \"VisImage\", \"Visualizer\"]\n_SMALL_OBJECT_AREA_THRESH = 1000\n_LARGE_MASK_AREA_THRESH = 120000\n_OFF_WHITE = (1.0, 1.0, 240.0 / 255)\n_BLACK = (0, 0, 0)\n_RED = (1.0, 0, 0)\n_KEYPOINT_THRESHOLD = 0.05\n@unique\nclass ColorMode(Enum):\n\"\"\"\nEnum of different color modes to use for instance visualizations.\n\"\"\"\nIMAGE = 0\n\"\"\"\nPicks a random color for every instance and overlay segmentations with low opacity.\n\"\"\"\nSEGMENTATION = 1\n\"\"\"\nLet instances of the same category have similar colors\n(from metadata.thing_colors), and overlay them with\nhigh opacity. This provides more attention on the quality of segmentation.\n\"\"\"\nIMAGE_BW = 2\n\"\"\"\nSame as IMAGE, but convert all areas without masks to gray-scale.\nOnly available for drawing per-instance mask predictions.\n\"\"\"\nclass GenericMask:\n\"\"\"\nAttribute:\npolygons (list[ndarray]): list[ndarray]: polygons for this mask.\nEach ndarray has format [x, y, x, y, ...]\nmask (ndarray): a binary mask\n\"\"\"\ndef __init__(self, mask_or_polygons, height, width):\nself._mask = self._polygons = self._has_holes = None\nself.height = height\nself.width = width\nm = mask_or_polygons\nif isinstance(m, dict):\n# RLEs\nassert \"counts\" in m and \"size\" in m\nif isinstance(m[\"counts\"], list):  # uncompressed RLEs\nh, w = m[\"size\"]\nassert h == height and w == width\nm = mask_util.frPyObjects(m, h, w)\nself._mask = mask_util.decode(m)[:, :]\nreturn\nif isinstance(m, list):  # list[ndarray]\nself._polygons = [np.asarray(x).reshape(-1) for x in m]\nreturn\nif isinstance(m, np.ndarray):  # assumed to be a binary mask\nassert m.shape[1] != 2, m.shape\nassert m.shape == (\nheight,\nwidth,\n), f\"mask shape: {m.shape}, target dims: {height}, {width}\"\nself._mask = m.astype(\"uint8\")\nreturn\nraise ValueError(\"GenericMask cannot handle object {} of type '{}'\".format(m, type(m)))\n@property\ndef mask(self):\nif self._mask is None:\nself._mask = self.polygons_to_mask(self._polygons)\nreturn self._mask\n@property\ndef polygons(self):\nif self._polygons is None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nreturn self._polygons\n@property\ndef has_holes(self):\nif self._has_holes is None:\nif self._mask is not None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nelse:\nself._has_holes = False  # if original format is polygon, does not have holes\nreturn self._has_holes\ndef mask_to_polygons(self, mask):\n# cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level\n# hierarchy. External contours (boundary) of the object are placed in hierarchy-1.\n# Internal contours (holes) are placed in hierarchy-2.\n# cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.\nmask = np.ascontiguousarray(mask)  # some versions of cv2 does not support incontiguous arr\nres = cv2.findContours(mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\nhierarchy = res[-1]\nif hierarchy is None:  # empty mask\nreturn [], False\nhas_holes = (hierarchy.reshape(-1, 4)[:, 3] >= 0).sum() > 0\nres = res[-2]\nres = [x.flatten() for x in res]\n# These coordinates from OpenCV are integers in range [0, W-1 or H-1].\n# We add 0.5 to turn them into real-value coordinate space. A better solution\n# would be to first +0.5 and then dilate the returned polygon by 0.5.\nres = [x + 0.5 for x in res if len(x) >= 6]\nreturn res, has_holes\ndef polygons_to_mask(self, polygons):\nrle = mask_util.frPyObjects(polygons, self.height, self.width)\nrle = mask_util.merge(rle)\nreturn mask_util.decode(rle)[:, :]\ndef area(self):\nreturn self.mask.sum()\ndef bbox(self):\np = mask_util.frPyObjects(self.polygons, self.height, self.width)\np = mask_util.merge(p)\nbbox = mask_util.toBbox(p)\nbbox[2] += bbox[0]\nbbox[3] += bbox[1]\nreturn bbox\nclass _PanopticPrediction:\n\"\"\"\nUnify different panoptic annotation/prediction formats\n\"\"\"\ndef __init__(self, panoptic_seg, segments_info, metadata=None):\nif segments_info is None:\nassert metadata is not None\n# If \"segments_info\" is None, we assume \"panoptic_img\" is a\n# H*W int32 image storing the panoptic_id in the format of\n# category_id * label_divisor + instance_id. We reserve -1 for\n# VOID label.\nlabel_divisor = metadata.label_divisor\nsegments_info = []\nfor panoptic_label in np.unique(panoptic_seg.numpy()):\nif panoptic_label == -1:\n# VOID region.\ncontinue\npred_class = panoptic_label // label_divisor\nisthing = pred_class in metadata.thing_dataset_id_to_contiguous_id.values()\nsegments_info.append(\n{\n\"id\": int(panoptic_label),\n\"category_id\": int(pred_class),\n\"isthing\": bool(isthing),\n}\n)\ndel metadata\nself._seg = panoptic_seg\nself._sinfo = {s[\"id\"]: s for s in segments_info}  # seg id -> seg info\nsegment_ids, areas = torch.unique(panoptic_seg, sorted=True, return_counts=True)\nareas = areas.numpy()\nsorted_idxs = np.argsort(-areas)\nself._seg_ids, self._seg_areas = segment_ids[sorted_idxs], areas[sorted_idxs]\nself._seg_ids = self._seg_ids.tolist()\nfor sid, area in zip(self._seg_ids, self._seg_areas):\nif sid in self._sinfo:\nself._sinfo[sid][\"area\"] = float(area)\ndef non_empty_mask(self):\n\"\"\"\nReturns:\n(H, W) array, a mask for all pixels that have a prediction\n\"\"\"\nempty_ids = []\nfor id in self._seg_ids:\nif id not in self._sinfo:\nempty_ids.append(id)\nif len(empty_ids) == 0:\nreturn np.zeros(self._seg.shape, dtype=np.uint8)\nassert (\nlen(empty_ids) == 1\n), \">1 ids corresponds to no labels. This is currently not supported\"\nreturn (self._seg != empty_ids[0]).numpy().astype(np.bool)\ndef semantic_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or sinfo[\"isthing\"]:\n# Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions.\ncontinue\nyield (self._seg == sid).numpy().astype(np.bool), sinfo\ndef instance_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or not sinfo[\"isthing\"]:\ncontinue\nmask = (self._seg == sid).numpy().astype(np.bool)\nif mask.sum() > 0:\nyield mask, sinfo\ndef _create_text_labels(classes, scores, class_names, is_crowd=None):\n\"\"\"\nArgs:\nclasses (list[int] or None):\nscores (list[float] or None):\nclass_names (list[str] or None):\nis_crowd (list[bool] or None):\nReturns:\nlist[str] or None\n\"\"\"\nlabels = None\nif classes is not None:\nif class_names is not None and len(class_names) > 0:\nlabels = [class_names[i] for i in classes]\nelse:\nlabels = [str(i) for i in classes]\nif scores is not None:\nif labels is None:\nlabels = [\"{:.0f}%\".format(s * 100) for s in scores]\nelse:\nlabels = [\"{} {:.0f}%\".format(l, s * 100) for l, s in zip(labels, scores)]\nif labels is not None and is_crowd is not None:\nlabels = [l + (\"|crowd\" if crowd else \"\") for l, crowd in zip(labels, is_crowd)]\nreturn labels\nclass VisImage:\ndef __init__(self, img, scale=1.0):\n\"\"\"\nArgs:\nimg (ndarray): an RGB image of shape (H, W, 3) in range [0, 255].\nscale (float): scale the input image\n\"\"\"\nself.img = img\nself.scale = scale\nself.width, self.height = img.shape[1], img.shape[0]\nself._setup_figure(img)\ndef _setup_figure(self, img):\n\"\"\"\nArgs:\nSame as in :meth:`__init__()`.\nReturns:\nfig (matplotlib.pyplot.figure): top level container for all the image plot elements.\nax (matplotlib.pyplot.Axes): contains figure elements and sets the coordinate system.\n\"\"\"\nfig = mplfigure.Figure(frameon=False)\nself.dpi = fig.get_dpi()\n# add a small 1e-2 to avoid precision lost due to matplotlib's truncation\n# (https://github.com/matplotlib/matplotlib/issues/15363)\nfig.set_size_inches(\n(self.width * self.scale + 1e-2) / self.dpi,\n(self.height * self.scale + 1e-2) / self.dpi,\n)\nself.canvas = FigureCanvasAgg(fig)\n# self.canvas = mpl.backends.backend_cairo.FigureCanvasCairo(fig)\nax = fig.add_axes([0.0, 0.0, 1.0, 1.0])\nax.axis(\"off\")\nself.fig = fig\nself.ax = ax\nself.reset_image(img)\ndef reset_image(self, img):\n\"\"\"\nArgs:\nimg: same as in __init__\n\"\"\"\nimg = img.astype(\"uint8\")\nself.ax.imshow(img, extent=(0, self.width, self.height, 0), interpolation=\"nearest\")\ndef save(self, filepath):\n\"\"\"\nArgs:\nfilepath (str): a string that contains the absolute path, including the file name, where\nthe visualized image will be saved.\n\"\"\"\nself.fig.savefig(filepath)\ndef get_image(self):\n\"\"\"\nReturns:\nndarray:\nthe visualized image of shape (H, W, 3) (RGB) in uint8 type.\nThe shape is scaled w.r.t the input image using the given `scale` argument.\n\"\"\"\ncanvas = self.canvas\ns, (width, height) = canvas.print_to_buffer()\n# buf = io.BytesIO()  # works for cairo backend\n# canvas.print_rgba(buf)\n# width, height = self.width, self.height\n# s = buf.getvalue()\nbuffer = np.frombuffer(s, dtype=\"uint8\")\nimg_rgba = buffer.reshape(height, width, 4)\nrgb, alpha = np.split(img_rgba, [3], axis=2)\nreturn rgb.astype(\"uint8\")\nclass Visualizer:\n\"\"\"\nVisualizer that draws data about detection/segmentation on images.\nIt contains methods like `draw_{text,box,circle,line,binary_mask,polygon}`\nthat draw primitive objects to images, as well as high-level wrappers like\n`draw_{instance_predictions,sem_seg,panoptic_seg_predictions,dataset_dict}`\nthat draw composite data in some pre-defined style.\nNote that the exact visualization style for the high-level wrappers are subject to change.\nStyle such as color, opacity, label contents, visibility of labels, or even the visibility\nof objects themselves (e.g. when the object is too small) may change according\nto different heuristics, as long as the results still look visually reasonable.\nTo obtain a consistent style, you can implement custom drawing functions with the\nabovementioned primitive methods instead. If you need more customized visualization\nstyles, you can process the data yourself following their format documented in\ntutorials (:doc:`/tutorials/models`, :doc:`/tutorials/datasets`). This class does not\nintend to satisfy everyone's preference on drawing styles.\nThis visualizer focuses on high rendering quality rather than performance. It is not\ndesigned to be used for real-time applications.\n\"\"\"\n# TODO implement a fast, rasterized version using OpenCV\ndef __init__(self, img_rgb, metadata=None, scale=1.0, instance_mode=ColorMode.IMAGE):\n\"\"\"\nArgs:\nimg_rgb: a numpy array of shape (H, W, C), where H and W correspond to\nthe height and width of the image respectively. C is the number of\ncolor channels. The image is required to be in RGB format since that\nis a requirement of the Matplotlib library. The image is also expected\nto be in the range [0, 255].\nmetadata (Metadata): dataset metadata (e.g. class names and colors)\ninstance_mode (ColorMode): defines one of the pre-defined style for drawing\ninstances on an image.\n\"\"\"\nself.img = np.asarray(img_rgb).clip(0, 255).astype(np.uint8)\nif metadata is None:\nmetadata = MetadataCatalog.get(\"__nonexist__\")\nself.metadata = metadata\nself.output = VisImage(self.img, scale=scale)\nself.cpu_device = torch.device(\"cpu\")\n# too small texts are useless, therefore clamp to 9\nself._default_font_size = max(\nnp.sqrt(self.output.height * self.output.width) // 90, 10 // scale\n)\nself._instance_mode = instance_mode\nself.keypoint_threshold = _KEYPOINT_THRESHOLD\ndef draw_instance_predictions(self, predictions):\n\"\"\"\nDraw instance-level prediction results on an image.\nArgs:\npredictions (Instances): the output of an instance detection/segmentation\nmodel. Following fields will be used to draw:\n\"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nboxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\nscores = predictions.scores if predictions.has(\"scores\") else None\nclasses = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\nlabels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\nkeypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\nif predictions.has(\"pred_masks\"):\nmasks = np.asarray(predictions.pred_masks)\nmasks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\nelse:\nmasks = None\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n]\nalpha = 0.8\nelse:\ncolors = None\nalpha = 0.5\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(\nself._create_grayscale_image(\n(predictions.pred_masks.any(dim=0) > 0).numpy()\nif predictions.has(\"pred_masks\")\nelse None\n)\n)\nalpha = 0.3\nself.overlay_instances(\nmasks=masks,\nboxes=boxes,\nlabels=labels,\nkeypoints=keypoints,\nassigned_colors=colors,\nalpha=alpha,\n)\nreturn self.output\ndef draw_sem_seg(self, sem_seg, area_threshold=None, alpha=0.8):\n\"\"\"\nDraw semantic segmentation predictions/labels.\nArgs:\nsem_seg (Tensor or ndarray): the segmentation of shape (H, W).\nEach value is the integer label of the pixel.\narea_threshold (int): segments with less than `area_threshold` are not drawn.\nalpha (float): the larger it is, the more opaque the segmentations are.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nif isinstance(sem_seg, torch.Tensor):\nsem_seg = sem_seg.numpy()\nlabels, areas = np.unique(sem_seg, return_counts=True)\nsorted_idxs = np.argsort(-areas).tolist()\nlabels = labels[sorted_idxs]\nfor label in filter(lambda l: l < len(self.metadata.stuff_classes), labels):\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[label]]\nexcept (AttributeError, IndexError):\nmask_color = None\nbinary_mask = (sem_seg == label).astype(np.uint8)\ntext = self.metadata.stuff_classes[label]\nself.draw_binary_mask(\nbinary_mask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\nreturn self.output\ndef draw_panoptic_seg(self, panoptic_seg, segments_info, area_threshold=None, alpha=0.7):\n\"\"\"\nDraw panoptic prediction annotations or results.\nArgs:\npanoptic_seg (Tensor): of shape (height, width) where the values are ids for each\nsegment.\nsegments_info (list[dict] or None): Describe each segment in `panoptic_seg`.\nIf it is a ``list[dict]``, each dict contains keys \"id\", \"category_id\".\nIf None, category id of each pixel is computed by\n``pixel // metadata.label_divisor``.\narea_threshold (int): stuff segments with less than `area_threshold` are not drawn.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\npred = _PanopticPrediction(panoptic_seg, segments_info, self.metadata)\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(self._create_grayscale_image(pred.non_empty_mask()))\n# draw mask for all semantic segments first i.e. \"stuff\"\nfor mask, sinfo in pred.semantic_masks():\ncategory_idx = sinfo[\"category_id\"]\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]\nexcept AttributeError:\nmask_color = None\ntext = self.metadata.stuff_classes[category_idx]\nself.draw_binary_mask(\nmask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\n# draw mask for all instances second\nall_instances = list(pred.instance_masks())\nif len(all_instances) == 0:\nreturn self.output\nmasks, sinfo = list(zip(*all_instances))\ncategory_ids = [x[\"category_id\"] for x in sinfo]\ntry:\nscores = [x[\"score\"] for x in sinfo]\nexcept KeyError:\nscores = None\nlabels = _create_text_labels(\ncategory_ids, scores, self.metadata.thing_classes, [x.get(\"iscrowd\", 0) for x in sinfo]\n)\ntry:\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in category_ids\n]\nexcept AttributeError:\ncolors = None\nself.overlay_instances(masks=masks, labels=labels, assigned_colors=colors, alpha=alpha)\nreturn self.output\ndraw_panoptic_seg_predictions = draw_panoptic_seg  # backward compatibility\ndef draw_dataset_dict(self, dic):\n\"\"\"\nDraw annotations/segmentaions in Detectron2 Dataset format.\nArgs:\ndic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nannos = dic.get(\"annotations\", None)\nif annos:\nif \"segmentation\" in annos[0]:\nmasks = [x[\"segmentation\"] for x in annos]\nelse:\nmasks = None\nif \"keypoints\" in annos[0]:\nkeypts = [x[\"keypoints\"] for x in annos]\nkeypts = np.array(keypts).reshape(len(annos), -1, 3)\nelse:\nkeypts = None\nboxes = [\nBoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS)\nif len(x[\"bbox\"]) == 4\nelse x[\"bbox\"]\nfor x in annos\n]\ncolors = None\ncategory_ids = [x[\"category_id\"] for x in annos]\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]])\nfor c in category_ids\n]\nnames = self.metadata.get(\"thing_classes\", None)\nlabels = _create_text_labels(\ncategory_ids,\nscores=None,\nclass_names=names,\nis_crowd=[x.get(\"iscrowd\", 0) for x in annos],\n)\nself.overlay_instances(\nlabels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n)\nsem_seg = dic.get(\"sem_seg\", None)\nif sem_seg is None and \"sem_seg_file_name\" in dic:\nwith PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\nsem_seg = Image.open(f)\nsem_seg = np.asarray(sem_seg, dtype=\"uint8\")\nif sem_seg is not None:\nself.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\npan_seg = dic.get(\"pan_seg\", None)\nif pan_seg is None and \"pan_seg_file_name\" in dic:\nwith PathManager.open(dic[\"pan_seg_file_name\"], \"rb\") as f:\npan_seg = Image.open(f)\npan_seg = np.asarray(pan_seg)\nfrom panopticapi.utils import rgb2id\npan_seg = rgb2id(pan_seg)\nif pan_seg is not None:\nsegments_info = dic[\"segments_info\"]\npan_seg = torch.tensor(pan_seg)\nself.draw_panoptic_seg(pan_seg, segments_info, area_threshold=0, alpha=0.5)\nreturn self.output\ndef overlay_instances(\nself,\n*,\nboxes=None,\nlabels=None,\nmasks=None,\nkeypoints=None,\nassigned_colors=None,\nalpha=0.5,\n):\n\"\"\"\nArgs:\nboxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,\nor an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,\nor a :class:`RotatedBoxes`,\nor an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image,\nlabels (list[str]): the text to be displayed for each instance.\nmasks (masks-like object): Supported types are:\n* :class:`detectron2.structures.PolygonMasks`,\n:class:`detectron2.structures.BitMasks`.\n* list[list[ndarray]]: contains the segmentation masks for all objects in one image.\nThe first level of the list corresponds to individual instances. The second\nlevel to all the polygon that compose the instance, and the third level\nto the polygon coordinates. The third level should have the format of\n[x0, y0, x1, y1, ..., xn, yn] (n >= 3).\n* list[ndarray]: each ndarray is a binary mask of shape (H, W).\n* list[dict]: each dict is a COCO-style RLE.\nkeypoints (Keypoint or array like): an array-like object of shape (N, K, 3),\nwhere the N is the number of instances and K is the number of keypoints.\nThe last dimension corresponds to (x, y, visibility or score).\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = 0\nif boxes is not None:\nboxes = self._convert_boxes(boxes)\nnum_instances = len(boxes)\nif masks is not None:\nmasks = self._convert_masks(masks)\nif num_instances:\nassert len(masks) == num_instances\nelse:\nnum_instances = len(masks)\nif keypoints is not None:\nif num_instances:\nassert len(keypoints) == num_instances\nelse:\nnum_instances = len(keypoints)\nkeypoints = self._convert_keypoints(keypoints)\nif labels is not None:\nassert len(labels) == num_instances\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\nif boxes is not None and boxes.shape[1] == 5:\nreturn self.overlay_rotated_instances(\nboxes=boxes, labels=labels, assigned_colors=assigned_colors\n)\n# Display in largest to smallest order to reduce occlusion.\nareas = None\nif boxes is not None:\nareas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\nelif masks is not None:\nareas = np.asarray([x.area() for x in masks])\nif areas is not None:\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs] if boxes is not None else None\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\nmasks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\nassigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\nkeypoints = keypoints[sorted_idxs] if keypoints is not None else None\nfor i in range(num_instances):\ncolor = assigned_colors[i]\nif boxes is not None:\nself.draw_box(boxes[i], edge_color=color)\nif masks is not None:\nfor segment in masks[i].polygons:\nself.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\nif labels is not None:\n# first get a box\nif boxes is not None:\nx0, y0, x1, y1 = boxes[i]\ntext_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\nhoriz_align = \"left\"\nelif masks is not None:\n# skip small mask without polygon\nif len(masks[i].polygons) == 0:\ncontinue\nx0, y0, x1, y1 = masks[i].bbox()\n# draw text in the center (defined by median) when box is not drawn\n# median is less sensitive to outliers.\ntext_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\nhoriz_align = \"center\"\nelse:\ncontinue  # drawing the box confidence for keypoints isn't very useful.\n# for small objects, draw text at the side to avoid occlusion\ninstance_area = (y1 - y0) * (x1 - x0)\nif (\ninstance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\nor y1 - y0 < 40 * self.output.scale\n):\nif y1 >= self.output.height - 5:\ntext_pos = (x1, y0)\nelse:\ntext_pos = (x0, y1)\nheight_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n* 0.5\n* self._default_font_size\n)\nself.draw_text(\nlabels[i],\ntext_pos,\ncolor=lighter_color,\nhorizontal_alignment=horiz_align,\nfont_size=font_size,\n)\n# draw keypoints\nif keypoints is not None:\nfor keypoints_per_instance in keypoints:\nself.draw_and_connect_keypoints(keypoints_per_instance)\nreturn self.output\ndef overlay_rotated_instances(self, boxes=None, labels=None, assigned_colors=None):\n\"\"\"\nArgs:\nboxes (ndarray): an Nx5 numpy array of\n(x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image.\nlabels (list[str]): the text to be displayed for each instance.\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = len(boxes)\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\n# Display in largest to smallest order to reduce occlusion.\nif boxes is not None:\nareas = boxes[:, 2] * boxes[:, 3]\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs]\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\ncolors = [assigned_colors[idx] for idx in sorted_idxs]\nfor i in range(num_instances):\nself.draw_rotated_box_with_label(\nboxes[i], edge_color=colors[i], label=labels[i] if labels is not None else None\n)\nreturn self.output\ndef draw_and_connect_keypoints(self, keypoints):\n\"\"\"\nDraws keypoints of an instance and follows the rules for keypoint connections\nto draw lines between appropriate keypoints. This follows color heuristics for\nline color.\nArgs:\nkeypoints (Tensor): a tensor of shape (K, 3), where K is the number of keypoints\nand the last dimension corresponds to (x, y, probability).\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nvisible = {}\nkeypoint_names = self.metadata.get(\"keypoint_names\")\nfor idx, keypoint in enumerate(keypoints):\n# draw keypoint\nx, y, prob = keypoint\nif prob > self.keypoint_threshold:\nself.draw_circle((x, y), color=_RED)\nif keypoint_names:\nkeypoint_name = keypoint_names[idx]\nvisible[keypoint_name] = (x, y)\nif self.metadata.get(\"keypoint_connection_rules\"):\nfor kp0, kp1, color in self.metadata.keypoint_connection_rules:\nif kp0 in visible and kp1 in visible:\nx0, y0 = visible[kp0]\nx1, y1 = visible[kp1]\ncolor = tuple(x / 255.0 for x in color)\nself.draw_line([x0, x1], [y0, y1], color=color)\n# draw lines from nose to mid-shoulder and mid-shoulder to mid-hip\n# Note that this strategy is specific to person keypoints.\n# For other keypoints, it should just do nothing\ntry:\nls_x, ls_y = visible[\"left_shoulder\"]\nrs_x, rs_y = visible[\"right_shoulder\"]\nmid_shoulder_x, mid_shoulder_y = (ls_x + rs_x) / 2, (ls_y + rs_y) / 2\nexcept KeyError:\npass\nelse:\n# draw line from nose to mid-shoulder\nnose_x, nose_y = visible.get(\"nose\", (None, None))\nif nose_x is not None:\nself.draw_line([nose_x, mid_shoulder_x], [nose_y, mid_shoulder_y], color=_RED)\ntry:\n# draw line from mid-shoulder to mid-hip\nlh_x, lh_y = visible[\"left_hip\"]\nrh_x, rh_y = visible[\"right_hip\"]\nexcept KeyError:\npass\nelse:\nmid_hip_x, mid_hip_y = (lh_x + rh_x) / 2, (lh_y + rh_y) / 2\nself.draw_line([mid_hip_x, mid_shoulder_x], [mid_hip_y, mid_shoulder_y], color=_RED)\nreturn self.output\n\"\"\"\nPrimitive drawing functions:\n\"\"\"\ndef draw_text(\nself,\ntext,\nposition,\n*,\nfont_size=None,\ncolor=\"g\",\nhorizontal_alignment=\"center\",\nrotation=0,\n):\n\"\"\"\nArgs:\ntext (str): class label\nposition (tuple): a tuple of the x and y coordinates to place text on image.\nfont_size (int, optional): font of the text. If not provided, a font size\nproportional to the image width is calculated and used.\ncolor: color of the text. Refer to `matplotlib.colors` for full list\nof formats that are accepted.\nhorizontal_alignment (str): see `matplotlib.text.Text`\nrotation: rotation angle in degrees CCW\nReturns:\noutput (VisImage): image object with text drawn.\n\"\"\"\nif not font_size:\nfont_size = self._default_font_size\n# since the text background is dark, we don't want the text to be dark\ncolor = np.maximum(list(mplc.to_rgb(color)), 0.2)\ncolor[np.argmax(color)] = max(0.8, np.max(color))\nx, y = position\nself.output.ax.text(\nx,\ny,\ntext,\nsize=font_size * self.output.scale,\nfamily=\"sans-serif\",\nbbox={\"facecolor\": \"black\", \"alpha\": 0.8, \"pad\": 0.7, \"edgecolor\": \"none\"},\nverticalalignment=\"top\",\nhorizontalalignment=horizontal_alignment,\ncolor=color,\nzorder=10,\nrotation=rotation,\n)\nreturn self.output\ndef draw_box(self, box_coord, alpha=0.5, edge_color=\"g\", line_style=\"-\"):\n\"\"\"\nArgs:\nbox_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0\nare the coordinates of the image's top left corner. x1 and y1 are the\ncoordinates of the image's bottom right corner.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx0, y0, x1, y1 = box_coord\nwidth = x1 - x0\nheight = y1 - y0\nlinewidth = max(self._default_font_size / 4, 1)\nself.output.ax.add_patch(\nmpl.patches.Rectangle(\n(x0, y0),\nwidth,\nheight,\nfill=False,\nedgecolor=edge_color,\nlinewidth=linewidth * self.output.scale,\nalpha=alpha,\nlinestyle=line_style,\n)\n)\nreturn self.output\ndef draw_rotated_box_with_label(\nself, rotated_box, alpha=0.5, edge_color=\"g\", line_style=\"-\", label=None\n):\n\"\"\"\nDraw a rotated box with label on its top-left corner.\nArgs:\nrotated_box (tuple): a tuple containing (cnt_x, cnt_y, w, h, angle),\nwhere cnt_x and cnt_y are the center coordinates of the box.\nw and h are the width and height of the box. angle represents how\nmany degrees the box is rotated CCW with regard to the 0-degree box.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nlabel (string): label for rotated box. It will not be rendered when set to None.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\ncnt_x, cnt_y, w, h, angle = rotated_box\narea = w * h\n# use thinner lines when the box is small\nlinewidth = self._default_font_size / (\n6 if area < _SMALL_OBJECT_AREA_THRESH * self.output.scale else 3\n)\ntheta = angle * math.pi / 180.0\nc = math.cos(theta)\ns = math.sin(theta)\nrect = [(-w / 2, h / 2), (-w / 2, -h / 2), (w / 2, -h / 2), (w / 2, h / 2)]\n# x: left->right ; y: top->down\nrotated_rect = [(s * yy + c * xx + cnt_x, c * yy - s * xx + cnt_y) for (xx, yy) in rect]\nfor k in range(4):\nj = (k + 1) % 4\nself.draw_line(\n[rotated_rect[k][0], rotated_rect[j][0]],\n[rotated_rect[k][1], rotated_rect[j][1]],\ncolor=edge_color,\nlinestyle=\"--\" if k == 1 else line_style,\nlinewidth=linewidth,\n)\nif label is not None:\ntext_pos = rotated_rect[1]  # topleft corner\nheight_ratio = h / np.sqrt(self.output.height * self.output.width)\nlabel_color = self._change_color_brightness(edge_color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2) * 0.5 * self._default_font_size\n)\nself.draw_text(label, text_pos, color=label_color, font_size=font_size, rotation=angle)\nreturn self.output\ndef draw_circle(self, circle_coord, color, radius=3):\n\"\"\"\nArgs:\ncircle_coord (list(int) or tuple(int)): contains the x and y coordinates\nof the center of the circle.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nradius (int): radius of the circle.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx, y = circle_coord\nself.output.ax.add_patch(\nmpl.patches.Circle(circle_coord, radius=radius, fill=True, color=color)\n)\nreturn self.output\ndef draw_line(self, x_data, y_data, color, linestyle=\"-\", linewidth=None):\n\"\"\"\nArgs:\nx_data (list[int]): a list containing x values of all the points being drawn.\nLength of list should match the length of y_data.\ny_data (list[int]): a list containing y values of all the points being drawn.\nLength of list should match the length of x_data.\ncolor: color of the line. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nlinestyle: style of the line. Refer to `matplotlib.lines.Line2D`\nfor a full list of formats that are accepted.\nlinewidth (float or None): width of the line. When it's None,\na default value will be computed and used.\nReturns:\noutput (VisImage): image object with line drawn.\n\"\"\"\nif linewidth is None:\nlinewidth = self._default_font_size / 3\nlinewidth = max(linewidth, 1)\nself.output.ax.add_line(\nmpl.lines.Line2D(\nx_data,\ny_data,\nlinewidth=linewidth * self.output.scale,\ncolor=color,\nlinestyle=linestyle,\n)\n)\nreturn self.output\ndef draw_binary_mask(\nself, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n):\n\"\"\"\nArgs:\nbinary_mask (ndarray): numpy array of shape (H, W), where H is the image height and\nW is the image width. Each value in the array is either a 0 or 1 value of uint8\ntype.\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\narea_threshold (float): a connected component smaller than this area will not be shown.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nhas_valid_segment = False\nbinary_mask = binary_mask.astype(\"uint8\")  # opencv needs uint8\nmask = GenericMask(binary_mask, self.output.height, self.output.width)\nshape2d = (binary_mask.shape[0], binary_mask.shape[1])\nif not mask.has_holes:\n# draw polygons for regular masks\nfor segment in mask.polygons:\narea = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))\nif area < (area_threshold or 0):\ncontinue\nhas_valid_segment = True\nsegment = segment.reshape(-1, 2)\nself.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)\nelse:\n# TODO: Use Path/PathPatch to draw vector graphics:\n# https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = (mask.mask == 1).astype(\"float32\") * alpha\nhas_valid_segment = True\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None and has_valid_segment:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nself._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output\ndef draw_soft_mask(self, soft_mask, color=None, *, text=None, alpha=0.5):\n\"\"\"\nArgs:\nsoft_mask (ndarray): float array of shape (H, W), each value in [0, 1].\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nshape2d = (soft_mask.shape[0], soft_mask.shape[1])\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = soft_mask * alpha\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nbinary_mask = (soft_mask > 0.5).astype(\"uint8\")\nself._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output\ndef draw_polygon(self, segment, color, edge_color=None, alpha=0.5):\n\"\"\"\nArgs:\nsegment: numpy array of shape Nx2, containing all the points in the polygon.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted. If not provided, a darker shade\nof the polygon color will be used instead.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with polygon drawn.\n\"\"\"\nif edge_color is None:\n# make edge color darker than the polygon color\nif alpha > 0.8:\nedge_color = self._change_color_brightness(color, brightness_factor=-0.7)\nelse:\nedge_color = color\nedge_color = mplc.to_rgb(edge_color) + (1,)\npolygon = mpl.patches.Polygon(\nsegment,\nfill=True,\nfacecolor=mplc.to_rgb(color) + (alpha,),\nedgecolor=edge_color,\nlinewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n)\nself.output.ax.add_patch(polygon)\nreturn self.output\n\"\"\"\nInternal methods:\n\"\"\"\ndef _jitter(self, color):\n\"\"\"\nRandomly modifies given color to produce a slightly different color than the color given.\nArgs:\ncolor (tuple[double]): a tuple of 3 elements, containing the RGB values of the color\npicked. The values in the list are in the [0.0, 1.0] range.\nReturns:\njittered_color (tuple[double]): a tuple of 3 elements, containing the RGB values of the\ncolor after being jittered. The values in the list are in the [0.0, 1.0] range.\n\"\"\"\ncolor = mplc.to_rgb(color)\nvec = np.random.rand(3)\n# better to do it in another color space\nvec = vec / np.linalg.norm(vec) * 0.5\nres = np.clip(vec + color, 0, 1)\nreturn tuple(res)\ndef _create_grayscale_image(self, mask=None):\n\"\"\"\nCreate a grayscale version of the original image.\nThe colors in masked area, if given, will be kept.\n\"\"\"\nimg_bw = self.img.astype(\"f4\").mean(axis=2)\nimg_bw = np.stack([img_bw] * 3, axis=2)\nif mask is not None:\nimg_bw[mask] = self.img[mask]\nreturn img_bw\ndef _change_color_brightness(self, color, brightness_factor):\n\"\"\"\nDepending on the brightness_factor, gives a lighter or darker color i.e. a color with\nless or more saturation than the original color.\nArgs:\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nbrightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of\n0 will correspond to no change, a factor in [-1.0, 0) range will result in\na darker color and a factor in (0, 1.0] range will result in a lighter color.\nReturns:\nmodified_color (tuple[double]): a tuple containing the RGB values of the\nmodified color. Each value in the tuple is in the [0.0, 1.0] range.\n\"\"\"\nassert brightness_factor >= -1.0 and brightness_factor <= 1.0\ncolor = mplc.to_rgb(color)\npolygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))\nmodified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])\nmodified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness\nmodified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness\nmodified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])\nreturn modified_color\ndef _convert_boxes(self, boxes):\n\"\"\"\nConvert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n\"\"\"\nif isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):\nreturn boxes.tensor.detach().numpy()\nelse:\nreturn np.asarray(boxes)\ndef _convert_masks(self, masks_or_polygons):\n\"\"\"\nConvert different format of masks or polygons to a tuple of masks and polygons.\nReturns:\nlist[GenericMask]:\n\"\"\"\nm = masks_or_polygons\nif isinstance(m, PolygonMasks):\nm = m.polygons\nif isinstance(m, BitMasks):\nm = m.tensor.numpy()\nif isinstance(m, torch.Tensor):\nm = m.numpy()\nret = []\nfor x in m:\nif isinstance(x, GenericMask):\nret.append(x)\nelse:\nret.append(GenericMask(x, self.output.height, self.output.width))\nreturn ret\ndef _draw_text_in_mask(self, binary_mask, text, color):\n\"\"\"\nFind proper places to draw text given a binary mask.\n\"\"\"\n# TODO sometimes drawn on wrong objects. the heuristics here can improve.\n_num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, 8)\nif stats[1:, -1].size == 0:\nreturn\nlargest_component_id = np.argmax(stats[1:, -1]) + 1\n# draw text on the largest component, as well as other very large components.\nfor cid in range(1, _num_cc):\nif cid == largest_component_id or stats[cid, -1] > _LARGE_MASK_AREA_THRESH:\n# median is more stable than centroid\n# center = centroids[largest_component_id]\ncenter = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]\nself.draw_text(text, center, color=color)\ndef _convert_keypoints(self, keypoints):\nif isinstance(keypoints, Keypoints):\nkeypoints = keypoints.tensor\nkeypoints = np.asarray(keypoints)\nreturn keypoints\ndef get_output(self):\n\"\"\"\nReturns:\noutput (VisImage): the image output containing the visualizations added\nto the image.\n\"\"\"\nreturn self.output",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 186
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "pretrain_python_data_7524367",
                        "content": "<gh_stars>0\n# Copyright (c) Facebook, Inc. and its affiliates.\nimport colorsys\nimport logging\nimport math\nimport numpy as np\nfrom enum import Enum, unique\nimport cv2\nimport matplotlib as mpl\nimport matplotlib.colors as mplc\nimport matplotlib.figure as mplfigure\nimport pycocotools.mask as mask_util\nimport torch\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\nfrom PIL import Image\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.structures import BitMasks, Boxes, BoxMode, Keypoints, PolygonMasks, RotatedBoxes\nfrom detectron2.utils.file_io import PathManager\nfrom .colormap import random_color\nlogger = logging.getLogger(__name__)\n__all__ = [\"ColorMode\", \"VisImage\", \"Visualizer\"]\n_SMALL_OBJECT_AREA_THRESH = 1000\n_LARGE_MASK_AREA_THRESH = 120000\n_OFF_WHITE = (1.0, 1.0, 240.0 / 255)\n_BLACK = (0, 0, 0)\n_RED = (1.0, 0, 0)\n_KEYPOINT_THRESHOLD = 0.05\n@unique\nclass ColorMode(Enum):\n\"\"\"\nEnum of different color modes to use for instance visualizations.\n\"\"\"\nIMAGE = 0\n\"\"\"\nPicks a random color for every instance and overlay segmentations with low opacity.\n\"\"\"\nSEGMENTATION = 1\n\"\"\"\nLet instances of the same category have similar colors\n(from metadata.thing_colors), and overlay them with\nhigh opacity. This provides more attention on the quality of segmentation.\n\"\"\"\nIMAGE_BW = 2\n\"\"\"\nSame as IMAGE, but convert all areas without masks to gray-scale.\nOnly available for drawing per-instance mask predictions.\n\"\"\"\nclass GenericMask:\n\"\"\"\nAttribute:\npolygons (list[ndarray]): list[ndarray]: polygons for this mask.\nEach ndarray has format [x, y, x, y, ...]\nmask (ndarray): a binary mask\n\"\"\"\ndef __init__(self, mask_or_polygons, height, width):\nself._mask = self._polygons = self._has_holes = None\nself.height = height\nself.width = width\nm = mask_or_polygons\nif isinstance(m, dict):\n# RLEs\nassert \"counts\" in m and \"size\" in m\nif isinstance(m[\"counts\"], list):  # uncompressed RLEs\nh, w = m[\"size\"]\nassert h == height and w == width\nm = mask_util.frPyObjects(m, h, w)\nself._mask = mask_util.decode(m)[:, :]\nreturn\nif isinstance(m, list):  # list[ndarray]\nself._polygons = [np.asarray(x).reshape(-1) for x in m]\nreturn\nif isinstance(m, np.ndarray):  # assumed to be a binary mask\nassert m.shape[1] != 2, m.shape\nassert m.shape == (\nheight,\nwidth,\n), f\"mask shape: {m.shape}, target dims: {height}, {width}\"\nself._mask = m.astype(\"uint8\")\nreturn\nraise ValueError(\"GenericMask cannot handle object {} of type '{}'\".format(m, type(m)))\n@property\ndef mask(self):\nif self._mask is None:\nself._mask = self.polygons_to_mask(self._polygons)\nreturn self._mask\n@property\ndef polygons(self):\nif self._polygons is None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nreturn self._polygons\n@property\ndef has_holes(self):\nif self._has_holes is None:\nif self._mask is not None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nelse:\nself._has_holes = False  # if original format is polygon, does not have holes\nreturn self._has_holes\ndef mask_to_polygons(self, mask):\n# cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level\n# hierarchy. External contours (boundary) of the object are placed in hierarchy-1.\n# Internal contours (holes) are placed in hierarchy-2.\n# cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.\nmask = np.ascontiguousarray(mask)  # some versions of cv2 does not support incontiguous arr\nres = cv2.findContours(mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\nhierarchy = res[-1]\nif hierarchy is None:  # empty mask\nreturn [], False\nhas_holes = (hierarchy.reshape(-1, 4)[:, 3] >= 0).sum() > 0\nres = res[-2]\nres = [x.flatten() for x in res]\n# These coordinates from OpenCV are integers in range [0, W-1 or H-1].\n# We add 0.5 to turn them into real-value coordinate space. A better solution\n# would be to first +0.5 and then dilate the returned polygon by 0.5.\nres = [x + 0.5 for x in res if len(x) >= 6]\nreturn res, has_holes\ndef polygons_to_mask(self, polygons):\nrle = mask_util.frPyObjects(polygons, self.height, self.width)\nrle = mask_util.merge(rle)\nreturn mask_util.decode(rle)[:, :]\ndef area(self):\nreturn self.mask.sum()\ndef bbox(self):\np = mask_util.frPyObjects(self.polygons, self.height, self.width)\np = mask_util.merge(p)\nbbox = mask_util.toBbox(p)\nbbox[2] += bbox[0]\nbbox[3] += bbox[1]\nreturn bbox\nclass _PanopticPrediction:\n\"\"\"\nUnify different panoptic annotation/prediction formats\n\"\"\"\ndef __init__(self, panoptic_seg, segments_info, metadata=None):\nif segments_info is None:\nassert metadata is not None\n# If \"segments_info\" is None, we assume \"panoptic_img\" is a\n# H*W int32 image storing the panoptic_id in the format of\n# category_id * label_divisor + instance_id. We reserve -1 for\n# VOID label.\nlabel_divisor = metadata.label_divisor\nsegments_info = []\nfor panoptic_label in np.unique(panoptic_seg.numpy()):\nif panoptic_label == -1:\n# VOID region.\ncontinue\npred_class = panoptic_label // label_divisor\nisthing = pred_class in metadata.thing_dataset_id_to_contiguous_id.values()\nsegments_info.append(\n{\n\"id\": int(panoptic_label),\n\"category_id\": int(pred_class),\n\"isthing\": bool(isthing),\n}\n)\ndel metadata\nself._seg = panoptic_seg\nself._sinfo = {s[\"id\"]: s for s in segments_info}  # seg id -> seg info\nsegment_ids, areas = torch.unique(panoptic_seg, sorted=True, return_counts=True)\nareas = areas.numpy()\nsorted_idxs = np.argsort(-areas)\nself._seg_ids, self._seg_areas = segment_ids[sorted_idxs], areas[sorted_idxs]\nself._seg_ids = self._seg_ids.tolist()\nfor sid, area in zip(self._seg_ids, self._seg_areas):\nif sid in self._sinfo:\nself._sinfo[sid][\"area\"] = float(area)\ndef non_empty_mask(self):\n\"\"\"\nReturns:\n(H, W) array, a mask for all pixels that have a prediction\n\"\"\"\nempty_ids = []\nfor id in self._seg_ids:\nif id not in self._sinfo:\nempty_ids.append(id)\nif len(empty_ids) == 0:\nreturn np.zeros(self._seg.shape, dtype=np.uint8)\nassert (\nlen(empty_ids) == 1\n), \">1 ids corresponds to no labels. This is currently not supported\"\nreturn (self._seg != empty_ids[0]).numpy().astype(np.bool)\ndef semantic_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or sinfo[\"isthing\"]:\n# Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions.\ncontinue\nyield (self._seg == sid).numpy().astype(np.bool), sinfo\ndef instance_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or not sinfo[\"isthing\"]:\ncontinue\nmask = (self._seg == sid).numpy().astype(np.bool)\nif mask.sum() > 0:\nyield mask, sinfo\ndef _create_text_labels(classes, scores, class_names, is_crowd=None):\n\"\"\"\nArgs:\nclasses (list[int] or None):\nscores (list[float] or None):\nclass_names (list[str] or None):\nis_crowd (list[bool] or None):\nReturns:\nlist[str] or None\n\"\"\"\nlabels = None\nif classes is not None:\nif class_names is not None and len(class_names) > 0:\nlabels = [class_names[i] for i in classes]\nelse:\nlabels = [str(i) for i in classes]\nif scores is not None:\nif labels is None:\nlabels = [\"{:.0f}%\".format(s * 100) for s in scores]\nelse:\nlabels = [\"{} {:.0f}%\".format(l, s * 100) for l, s in zip(labels, scores)]\nif labels is not None and is_crowd is not None:\nlabels = [l + (\"|crowd\" if crowd else \"\") for l, crowd in zip(labels, is_crowd)]\nreturn labels\nclass VisImage:\ndef __init__(self, img, scale=1.0):\n\"\"\"\nArgs:\nimg (ndarray): an RGB image of shape (H, W, 3) in range [0, 255].\nscale (float): scale the input image\n\"\"\"\nself.img = img\nself.scale = scale\nself.width, self.height = img.shape[1], img.shape[0]\nself._setup_figure(img)\ndef _setup_figure(self, img):\n\"\"\"\nArgs:\nSame as in :meth:`__init__()`.\nReturns:\nfig (matplotlib.pyplot.figure): top level container for all the image plot elements.\nax (matplotlib.pyplot.Axes): contains figure elements and sets the coordinate system.\n\"\"\"\nfig = mplfigure.Figure(frameon=False)\nself.dpi = fig.get_dpi()\n# add a small 1e-2 to avoid precision lost due to matplotlib's truncation\n# (https://github.com/matplotlib/matplotlib/issues/15363)\nfig.set_size_inches(\n(self.width * self.scale + 1e-2) / self.dpi,\n(self.height * self.scale + 1e-2) / self.dpi,\n)\nself.canvas = FigureCanvasAgg(fig)\n# self.canvas = mpl.backends.backend_cairo.FigureCanvasCairo(fig)\nax = fig.add_axes([0.0, 0.0, 1.0, 1.0])\nax.axis(\"off\")\nself.fig = fig\nself.ax = ax\nself.reset_image(img)\ndef reset_image(self, img):\n\"\"\"\nArgs:\nimg: same as in __init__\n\"\"\"\nimg = img.astype(\"uint8\")\nself.ax.imshow(img, extent=(0, self.width, self.height, 0), interpolation=\"nearest\")\ndef save(self, filepath):\n\"\"\"\nArgs:\nfilepath (str): a string that contains the absolute path, including the file name, where\nthe visualized image will be saved.\n\"\"\"\nself.fig.savefig(filepath)\ndef get_image(self):\n\"\"\"\nReturns:\nndarray:\nthe visualized image of shape (H, W, 3) (RGB) in uint8 type.\nThe shape is scaled w.r.t the input image using the given `scale` argument.\n\"\"\"\ncanvas = self.canvas\ns, (width, height) = canvas.print_to_buffer()\n# buf = io.BytesIO()  # works for cairo backend\n# canvas.print_rgba(buf)\n# width, height = self.width, self.height\n# s = buf.getvalue()\nbuffer = np.frombuffer(s, dtype=\"uint8\")\nimg_rgba = buffer.reshape(height, width, 4)\nrgb, alpha = np.split(img_rgba, [3], axis=2)\nreturn rgb.astype(\"uint8\")\nclass Visualizer:\n\"\"\"\nVisualizer that draws data about detection/segmentation on images.\nIt contains methods like `draw_{text,box,circle,line,binary_mask,polygon}`\nthat draw primitive objects to images, as well as high-level wrappers like\n`draw_{instance_predictions,sem_seg,panoptic_seg_predictions,dataset_dict}`\nthat draw composite data in some pre-defined style.\nNote that the exact visualization style for the high-level wrappers are subject to change.\nStyle such as color, opacity, label contents, visibility of labels, or even the visibility\nof objects themselves (e.g. when the object is too small) may change according\nto different heuristics, as long as the results still look visually reasonable.\nTo obtain a consistent style, you can implement custom drawing functions with the\nabovementioned primitive methods instead. If you need more customized visualization\nstyles, you can process the data yourself following their format documented in\ntutorials (:doc:`/tutorials/models`, :doc:`/tutorials/datasets`). This class does not\nintend to satisfy everyone's preference on drawing styles.\nThis visualizer focuses on high rendering quality rather than performance. It is not\ndesigned to be used for real-time applications.\n\"\"\"\n# TODO implement a fast, rasterized version using OpenCV\ndef __init__(self, img_rgb, metadata=None, scale=1.0, instance_mode=ColorMode.IMAGE):\n\"\"\"\nArgs:\nimg_rgb: a numpy array of shape (H, W, C), where H and W correspond to\nthe height and width of the image respectively. C is the number of\ncolor channels. The image is required to be in RGB format since that\nis a requirement of the Matplotlib library. The image is also expected\nto be in the range [0, 255].\nmetadata (Metadata): dataset metadata (e.g. class names and colors)\ninstance_mode (ColorMode): defines one of the pre-defined style for drawing\ninstances on an image.\n\"\"\"\nself.img = np.asarray(img_rgb).clip(0, 255).astype(np.uint8)\nif metadata is None:\nmetadata = MetadataCatalog.get(\"__nonexist__\")\nself.metadata = metadata\nself.output = VisImage(self.img, scale=scale)\nself.cpu_device = torch.device(\"cpu\")\n# too small texts are useless, therefore clamp to 9\nself._default_font_size = max(\nnp.sqrt(self.output.height * self.output.width) // 90, 10 // scale\n)\nself._instance_mode = instance_mode\nself.keypoint_threshold = _KEYPOINT_THRESHOLD\ndef draw_instance_predictions(self, predictions):\n\"\"\"\nDraw instance-level prediction results on an image.\nArgs:\npredictions (Instances): the output of an instance detection/segmentation\nmodel. Following fields will be used to draw:\n\"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nboxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\nscores = predictions.scores if predictions.has(\"scores\") else None\nclasses = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\nlabels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\nkeypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\nif predictions.has(\"pred_masks\"):\nmasks = np.asarray(predictions.pred_masks)\nmasks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\nelse:\nmasks = None\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n]\nalpha = 0.8\nelse:\ncolors = None\nalpha = 0.5\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(\nself._create_grayscale_image(\n(predictions.pred_masks.any(dim=0) > 0).numpy()\nif predictions.has(\"pred_masks\")\nelse None\n)\n)\nalpha = 0.3\nself.overlay_instances(\nmasks=masks,\nboxes=boxes,\nlabels=labels,\nkeypoints=keypoints,\nassigned_colors=colors,\nalpha=alpha,\n)\nreturn self.output\ndef draw_sem_seg(self, sem_seg, area_threshold=None, alpha=0.8):\n\"\"\"\nDraw semantic segmentation predictions/labels.\nArgs:\nsem_seg (Tensor or ndarray): the segmentation of shape (H, W).\nEach value is the integer label of the pixel.\narea_threshold (int): segments with less than `area_threshold` are not drawn.\nalpha (float): the larger it is, the more opaque the segmentations are.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nif isinstance(sem_seg, torch.Tensor):\nsem_seg = sem_seg.numpy()\nlabels, areas = np.unique(sem_seg, return_counts=True)\nsorted_idxs = np.argsort(-areas).tolist()\nlabels = labels[sorted_idxs]\nfor label in filter(lambda l: l < len(self.metadata.stuff_classes), labels):\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[label]]\nexcept (AttributeError, IndexError):\nmask_color = None\nbinary_mask = (sem_seg == label).astype(np.uint8)\ntext = self.metadata.stuff_classes[label]\nself.draw_binary_mask(\nbinary_mask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\nreturn self.output\ndef draw_panoptic_seg(self, panoptic_seg, segments_info, area_threshold=None, alpha=0.7):\n\"\"\"\nDraw panoptic prediction annotations or results.\nArgs:\npanoptic_seg (Tensor): of shape (height, width) where the values are ids for each\nsegment.\nsegments_info (list[dict] or None): Describe each segment in `panoptic_seg`.\nIf it is a ``list[dict]``, each dict contains keys \"id\", \"category_id\".\nIf None, category id of each pixel is computed by\n``pixel // metadata.label_divisor``.\narea_threshold (int): stuff segments with less than `area_threshold` are not drawn.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nsemant_vect = []\npred = _PanopticPrediction(panoptic_seg, segments_info, self.metadata)\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(self._create_grayscale_image(pred.non_empty_mask()))\n# draw mask for all semantic segments first i.e. \"stuff\"\nfor mask, sinfo in pred.semantic_masks():\ncategory_idx = sinfo[\"category_id\"]\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]\nexcept AttributeError:\nmask_color = None\ntext = self.metadata.stuff_classes[category_idx]\n_ , pos_semant=self.draw_binary_mask(\nmask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\nsemant_vect.append([pos_semant, text])\n# draw mask for all instances second\nall_instances = list(pred.instance_masks())\nif len(all_instances) == 0:\nreturn self.output, semant_vect\nmasks, sinfo = list(zip(*all_instances))\ncategory_ids = [x[\"category_id\"] for x in sinfo]\ntry:\nscores = [x[\"score\"] for x in sinfo]\nexcept KeyError:\nscores = None\nlabels = _create_text_labels(\ncategory_ids, scores, self.metadata.thing_classes, [x.get(\"iscrowd\", 0) for x in sinfo]\n)\ntry:\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in category_ids\n]\nexcept AttributeError:\ncolors = None\n_, pos_obj_semant= self.overlay_instances(masks=masks, labels=labels, assigned_colors=colors, alpha=alpha)\nsemant_vect.extend(pos_obj_semant)\nreturn self.output, semant_vect\n#return semant_vect.append(pos_obj_semant)\ndraw_panoptic_seg_predictions = draw_panoptic_seg  # backward compatibility\ndef draw_dataset_dict(self, dic):\n\"\"\"\nDraw annotations/segmentaions in Detectron2 Dataset format.\nArgs:\ndic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nannos = dic.get(\"annotations\", None)\nif annos:\nif \"segmentation\" in annos[0]:\nmasks = [x[\"segmentation\"] for x in annos]\nelse:\nmasks = None\nif \"keypoints\" in annos[0]:\nkeypts = [x[\"keypoints\"] for x in annos]\nkeypts = np.array(keypts).reshape(len(annos), -1, 3)\nelse:\nkeypts = None\nboxes = [\nBoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS)\nif len(x[\"bbox\"]) == 4\nelse x[\"bbox\"]\nfor x in annos\n]\ncolors = None\ncategory_ids = [x[\"category_id\"] for x in annos]\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]])\nfor c in category_ids\n]\nnames = self.metadata.get(\"thing_classes\", None)\nlabels = _create_text_labels(\ncategory_ids,\nscores=None,\nclass_names=names,\nis_crowd=[x.get(\"iscrowd\", 0) for x in annos],\n)\nself.overlay_instances(\nlabels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n)\nsem_seg = dic.get(\"sem_seg\", None)\nif sem_seg is None and \"sem_seg_file_name\" in dic:\nwith PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\nsem_seg = Image.open(f)\nsem_seg = np.asarray(sem_seg, dtype=\"uint8\")\nif sem_seg is not None:\nself.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\npan_seg = dic.get(\"pan_seg\", None)\nif pan_seg is None and \"pan_seg_file_name\" in dic:\nwith PathManager.open(dic[\"pan_seg_file_name\"], \"rb\") as f:\npan_seg = Image.open(f)\npan_seg = np.asarray(pan_seg)\nfrom panopticapi.utils import rgb2id\npan_seg = rgb2id(pan_seg)\nif pan_seg is not None:\nsegments_info = dic[\"segments_info\"]\npan_seg = torch.tensor(pan_seg)\nself.draw_panoptic_seg(pan_seg, segments_info, area_threshold=0, alpha=0.5)\nreturn self.output\ndef overlay_instances(\nself,\n*,\nboxes=None,\nlabels=None,\nmasks=None,\nkeypoints=None,\nassigned_colors=None,\nalpha=0.5,\n):\n\"\"\"\nArgs:\nboxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,\nor an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,\nor a :class:`RotatedBoxes`,\nor an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image,\nlabels (list[str]): the text to be displayed for each instance.\nmasks (masks-like object): Supported types are:\n* :class:`detectron2.structures.PolygonMasks`,\n:class:`detectron2.structures.BitMasks`.\n* list[list[ndarray]]: contains the segmentation masks for all objects in one image.\nThe first level of the list corresponds to individual instances. The second\nlevel to all the polygon that compose the instance, and the third level\nto the polygon coordinates. The third level should have the format of\n[x0, y0, x1, y1, ..., xn, yn] (n >= 3).\n* list[ndarray]: each ndarray is a binary mask of shape (H, W).\n* list[dict]: each dict is a COCO-style RLE.\nkeypoints (Keypoint or array like): an array-like object of shape (N, K, 3),\nwhere the N is the number of instances and K is the number of keypoints.\nThe last dimension corresponds to (x, y, visibility or score).\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = 0\nif boxes is not None:\nboxes = self._convert_boxes(boxes)\nnum_instances = len(boxes)\nif masks is not None:\nmasks = self._convert_masks(masks)\nif num_instances:\nassert len(masks) == num_instances\nelse:\nnum_instances = len(masks)\nif keypoints is not None:\nif num_instances:\nassert len(keypoints) == num_instances\nelse:\nnum_instances = len(keypoints)\nkeypoints = self._convert_keypoints(keypoints)\nif labels is not None:\nassert len(labels) == num_instances\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\nif boxes is not None and boxes.shape[1] == 5:\nreturn self.overlay_rotated_instances(\nboxes=boxes, labels=labels, assigned_colors=assigned_colors\n)\n# Display in largest to smallest order to reduce occlusion.\nareas = None\nif boxes is not None:\nareas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\nelif masks is not None:\nareas = np.asarray([x.area() for x in masks])\nif areas is not None:\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs] if boxes is not None else None\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\nmasks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\nassigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\nkeypoints = keypoints[sorted_idxs] if keypoints is not None else None\n#print(num_instances)\n#print(labels)\nobj_vect=[]\nfor i in range(num_instances):\ncolor = assigned_colors[i]\nif boxes is not None:\nself.draw_box(boxes[i], edge_color=color)\nif masks is not None:\nfor segment in masks[i].polygons:\nself.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\nif labels is not None:\n# first get a box\nif boxes is not None:\nx0, y0, x1, y1 = boxes[i]\ntext_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\nhoriz_align = \"left\"\nelif masks is not None:\n# skip small mask without polygon\nif len(masks[i].polygons) == 0:\ncontinue\nx0, y0, x1, y1 = masks[i].bbox()\n# draw text in the center (defined by median) when box is not drawn\n# median is less sensitive to outliers.\ntext_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\nhoriz_align = \"center\"\nelse:\ncontinue  # drawing the box confidence for keypoints isn't very useful.\n# for small objects, draw text at the side to avoid occlusion\ninstance_area = (y1 - y0) * (x1 - x0)\nif (\ninstance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\nor y1 - y0 < 40 * self.output.scale\n):\nif y1 >= self.output.height - 5:\ntext_pos = (x1, y0)\nelse:\ntext_pos = (x0, y1)\nheight_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n* 0.5\n* self._default_font_size\n)\nself.draw_text(\nlabels[i],\ntext_pos,\ncolor=lighter_color,\nhorizontal_alignment=horiz_align,\nfont_size=font_size,\n)\nobj_vect.append([text_pos,labels[i]])\n# draw keypoints\nif keypoints is not None:\nfor keypoints_per_instance in keypoints:\nself.draw_and_connect_keypoints(keypoints_per_instance)\nreturn self.output, obj_vect\ndef overlay_rotated_instances(self, boxes=None, labels=None, assigned_colors=None):\n\"\"\"\nArgs:\nboxes (ndarray): an Nx5 numpy array of\n(x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image.\nlabels (list[str]): the text to be displayed for each instance.\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = len(boxes)\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\n# Display in largest to smallest order to reduce occlusion.\nif boxes is not None:\nareas = boxes[:, 2] * boxes[:, 3]\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs]\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\ncolors = [assigned_colors[idx] for idx in sorted_idxs]\nfor i in range(num_instances):\nself.draw_rotated_box_with_label(\nboxes[i], edge_color=colors[i], label=labels[i] if labels is not None else None\n)\nreturn self.output\ndef draw_and_connect_keypoints(self, keypoints):\n\"\"\"\nDraws keypoints of an instance and follows the rules for keypoint connections\nto draw lines between appropriate keypoints. This follows color heuristics for\nline color.\nArgs:\nkeypoints (Tensor): a tensor of shape (K, 3), where K is the number of keypoints\nand the last dimension corresponds to (x, y, probability).\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nvisible = {}\nkeypoint_names = self.metadata.get(\"keypoint_names\")\nfor idx, keypoint in enumerate(keypoints):\n# draw keypoint\nx, y, prob = keypoint\nif prob > self.keypoint_threshold:\nself.draw_circle((x, y), color=_RED)\nif keypoint_names:\nkeypoint_name = keypoint_names[idx]\nvisible[keypoint_name] = (x, y)\nif self.metadata.get(\"keypoint_connection_rules\"):\nfor kp0, kp1, color in self.metadata.keypoint_connection_rules:\nif kp0 in visible and kp1 in visible:\nx0, y0 = visible[kp0]\nx1, y1 = visible[kp1]\ncolor = tuple(x / 255.0 for x in color)\nself.draw_line([x0, x1], [y0, y1], color=color)\n# draw lines from nose to mid-shoulder and mid-shoulder to mid-hip\n# Note that this strategy is specific to person keypoints.\n# For other keypoints, it should just do nothing\ntry:\nls_x, ls_y = visible[\"left_shoulder\"]\nrs_x, rs_y = visible[\"right_shoulder\"]\nmid_shoulder_x, mid_shoulder_y = (ls_x + rs_x) / 2, (ls_y + rs_y) / 2\nexcept KeyError:\npass\nelse:\n# draw line from nose to mid-shoulder\nnose_x, nose_y = visible.get(\"nose\", (None, None))\nif nose_x is not None:\nself.draw_line([nose_x, mid_shoulder_x], [nose_y, mid_shoulder_y], color=_RED)\ntry:\n# draw line from mid-shoulder to mid-hip\nlh_x, lh_y = visible[\"left_hip\"]\nrh_x, rh_y = visible[\"right_hip\"]\nexcept KeyError:\npass\nelse:\nmid_hip_x, mid_hip_y = (lh_x + rh_x) / 2, (lh_y + rh_y) / 2\nself.draw_line([mid_hip_x, mid_shoulder_x], [mid_hip_y, mid_shoulder_y], color=_RED)\nreturn self.output\n\"\"\"\nPrimitive drawing functions:\n\"\"\"\ndef draw_text(\nself,\ntext,\nposition,\n*,\nfont_size=None,\ncolor=\"g\",\nhorizontal_alignment=\"center\",\nrotation=0,\n):\n\"\"\"\nArgs:\ntext (str): class label\nposition (tuple): a tuple of the x and y coordinates to place text on image.\nfont_size (int, optional): font of the text. If not provided, a font size\nproportional to the image width is calculated and used.\ncolor: color of the text. Refer to `matplotlib.colors` for full list\nof formats that are accepted.\nhorizontal_alignment (str): see `matplotlib.text.Text`\nrotation: rotation angle in degrees CCW\nReturns:\noutput (VisImage): image object with text drawn.\n\"\"\"\nif not font_size:\nfont_size = self._default_font_size\n# since the text background is dark, we don't want the text to be dark\ncolor = np.maximum(list(mplc.to_rgb(color)), 0.2)\ncolor[np.argmax(color)] = max(0.8, np.max(color))\nx, y = position\nself.output.ax.text(\nx,\ny,\ntext,\nsize=font_size * self.output.scale,\nfamily=\"sans-serif\",\nbbox={\"facecolor\": \"black\", \"alpha\": 0.8, \"pad\": 0.7, \"edgecolor\": \"none\"},\nverticalalignment=\"top\",\nhorizontalalignment=horizontal_alignment,\ncolor=color,\nzorder=10,\nrotation=rotation,\n)\nreturn self.output\ndef draw_box(self, box_coord, alpha=0.5, edge_color=\"g\", line_style=\"-\"):\n\"\"\"\nArgs:\nbox_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0\nare the coordinates of the image's top left corner. x1 and y1 are the\ncoordinates of the image's bottom right corner.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx0, y0, x1, y1 = box_coord\nwidth = x1 - x0\nheight = y1 - y0\nlinewidth = max(self._default_font_size / 4, 1)\nself.output.ax.add_patch(\nmpl.patches.Rectangle(\n(x0, y0),\nwidth,\nheight,\nfill=False,\nedgecolor=edge_color,\nlinewidth=linewidth * self.output.scale,\nalpha=alpha,\nlinestyle=line_style,\n)\n)\nreturn self.output\ndef draw_rotated_box_with_label(\nself, rotated_box, alpha=0.5, edge_color=\"g\", line_style=\"-\", label=None\n):\n\"\"\"\nDraw a rotated box with label on its top-left corner.\nArgs:\nrotated_box (tuple): a tuple containing (cnt_x, cnt_y, w, h, angle),\nwhere cnt_x and cnt_y are the center coordinates of the box.\nw and h are the width and height of the box. angle represents how\nmany degrees the box is rotated CCW with regard to the 0-degree box.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nlabel (string): label for rotated box. It will not be rendered when set to None.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\ncnt_x, cnt_y, w, h, angle = rotated_box\narea = w * h\n# use thinner lines when the box is small\nlinewidth = self._default_font_size / (\n6 if area < _SMALL_OBJECT_AREA_THRESH * self.output.scale else 3\n)\ntheta = angle * math.pi / 180.0\nc = math.cos(theta)\ns = math.sin(theta)\nrect = [(-w / 2, h / 2), (-w / 2, -h / 2), (w / 2, -h / 2), (w / 2, h / 2)]\n# x: left->right ; y: top->down\nrotated_rect = [(s * yy + c * xx + cnt_x, c * yy - s * xx + cnt_y) for (xx, yy) in rect]\nfor k in range(4):\nj = (k + 1) % 4\nself.draw_line(\n[rotated_rect[k][0], rotated_rect[j][0]],\n[rotated_rect[k][1], rotated_rect[j][1]],\ncolor=edge_color,\nlinestyle=\"--\" if k == 1 else line_style,\nlinewidth=linewidth,\n)\nif label is not None:\ntext_pos = rotated_rect[1]  # topleft corner\nheight_ratio = h / np.sqrt(self.output.height * self.output.width)\nlabel_color = self._change_color_brightness(edge_color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2) * 0.5 * self._default_font_size\n)\nself.draw_text(label, text_pos, color=label_color, font_size=font_size, rotation=angle)\nreturn self.output\ndef draw_circle(self, circle_coord, color, radius=3):\n\"\"\"\nArgs:\ncircle_coord (list(int) or tuple(int)): contains the x and y coordinates\nof the center of the circle.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nradius (int): radius of the circle.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx, y = circle_coord\nself.output.ax.add_patch(\nmpl.patches.Circle(circle_coord, radius=radius, fill=True, color=color)\n)\nreturn self.output\ndef draw_line(self, x_data, y_data, color, linestyle=\"-\", linewidth=None):\n\"\"\"\nArgs:\nx_data (list[int]): a list containing x values of all the points being drawn.\nLength of list should match the length of y_data.\ny_data (list[int]): a list containing y values of all the points being drawn.\nLength of list should match the length of x_data.\ncolor: color of the line. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nlinestyle: style of the line. Refer to `matplotlib.lines.Line2D`\nfor a full list of formats that are accepted.\nlinewidth (float or None): width of the line. When it's None,\na default value will be computed and used.\nReturns:\noutput (VisImage): image object with line drawn.\n\"\"\"\nif linewidth is None:\nlinewidth = self._default_font_size / 3\nlinewidth = max(linewidth, 1)\nself.output.ax.add_line(\nmpl.lines.Line2D(\nx_data,\ny_data,\nlinewidth=linewidth * self.output.scale,\ncolor=color,\nlinestyle=linestyle,\n)\n)\nreturn self.output\ndef draw_binary_mask(\nself, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n):\n\"\"\"\nArgs:\nbinary_mask (ndarray): numpy array of shape (H, W), where H is the image height and\nW is the image width. Each value in the array is either a 0 or 1 value of uint8\ntype.\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\narea_threshold (float): a connected component smaller than this area will not be shown.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nhas_valid_segment = False\nbinary_mask = binary_mask.astype(\"uint8\")  # opencv needs uint8\nmask = GenericMask(binary_mask, self.output.height, self.output.width)\nshape2d = (binary_mask.shape[0], binary_mask.shape[1])\nif not mask.has_holes:\n# draw polygons for regular masks\nfor segment in mask.polygons:\narea = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))\nif area < (area_threshold or 0):\ncontinue\nhas_valid_segment = True\nsegment = segment.reshape(-1, 2)\nself.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)\nelse:\n# TODO: Use Path/PathPatch to draw vector graphics:\n# https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = (mask.mask == 1).astype(\"float32\") * alpha\nhas_valid_segment = True\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None and has_valid_segment:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\ncenter = self._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output, center\ndef draw_soft_mask(self, soft_mask, color=None, *, text=None, alpha=0.5):\n\"\"\"\nArgs:\nsoft_mask (ndarray): float array of shape (H, W), each value in [0, 1].\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nshape2d = (soft_mask.shape[0], soft_mask.shape[1])\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = soft_mask * alpha\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nbinary_mask = (soft_mask > 0.5).astype(\"uint8\")\nself._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output\ndef draw_polygon(self, segment, color, edge_color=None, alpha=0.5):\n\"\"\"\nArgs:\nsegment: numpy array of shape Nx2, containing all the points in the polygon.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted. If not provided, a darker shade\nof the polygon color will be used instead.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with polygon drawn.\n\"\"\"\nif edge_color is None:\n# make edge color darker than the polygon color\nif alpha > 0.8:\nedge_color = self._change_color_brightness(color, brightness_factor=-0.7)\nelse:\nedge_color = color\nedge_color = mplc.to_rgb(edge_color) + (1,)\npolygon = mpl.patches.Polygon(\nsegment,\nfill=True,\nfacecolor=mplc.to_rgb(color) + (alpha,),\nedgecolor=edge_color,\nlinewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n)\nself.output.ax.add_patch(polygon)\nreturn self.output\n\"\"\"\nInternal methods:\n\"\"\"\ndef _jitter(self, color):\n\"\"\"\nRandomly modifies given color to produce a slightly different color than the color given.\nArgs:\ncolor (tuple[double]): a tuple of 3 elements, containing the RGB values of the color\npicked. The values in the list are in the [0.0, 1.0] range.\nReturns:\njittered_color (tuple[double]): a tuple of 3 elements, containing the RGB values of the\ncolor after being jittered. The values in the list are in the [0.0, 1.0] range.\n\"\"\"\ncolor = mplc.to_rgb(color)\nvec = np.random.rand(3)\n# better to do it in another color space\nvec = vec / np.linalg.norm(vec) * 0.5\nres = np.clip(vec + color, 0, 1)\nreturn tuple(res)\ndef _create_grayscale_image(self, mask=None):\n\"\"\"\nCreate a grayscale version of the original image.\nThe colors in masked area, if given, will be kept.\n\"\"\"\nimg_bw = self.img.astype(\"f4\").mean(axis=2)\nimg_bw = np.stack([img_bw] * 3, axis=2)\nif mask is not None:\nimg_bw[mask] = self.img[mask]\nreturn img_bw\ndef _change_color_brightness(self, color, brightness_factor):\n\"\"\"\nDepending on the brightness_factor, gives a lighter or darker color i.e. a color with\nless or more saturation than the original color.\nArgs:\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nbrightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of\n0 will correspond to no change, a factor in [-1.0, 0) range will result in\na darker color and a factor in (0, 1.0] range will result in a lighter color.\nReturns:\nmodified_color (tuple[double]): a tuple containing the RGB values of the\nmodified color. Each value in the tuple is in the [0.0, 1.0] range.\n\"\"\"\nassert brightness_factor >= -1.0 and brightness_factor <= 1.0\ncolor = mplc.to_rgb(color)\npolygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))\nmodified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])\nmodified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness\nmodified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness\nmodified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])\nreturn modified_color\ndef _convert_boxes(self, boxes):\n\"\"\"\nConvert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n\"\"\"\nif isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):\nreturn boxes.tensor.detach().numpy()\nelse:\nreturn np.asarray(boxes)\ndef _convert_masks(self, masks_or_polygons):\n\"\"\"\nConvert different format of masks or polygons to a tuple of masks and polygons.\nReturns:\nlist[GenericMask]:\n\"\"\"\nm = masks_or_polygons\nif isinstance(m, PolygonMasks):\nm = m.polygons\nif isinstance(m, BitMasks):\nm = m.tensor.numpy()\nif isinstance(m, torch.Tensor):\nm = m.numpy()\nret = []\nfor x in m:\nif isinstance(x, GenericMask):\nret.append(x)\nelse:\nret.append(GenericMask(x, self.output.height, self.output.width))\nreturn ret\ndef _draw_text_in_mask(self, binary_mask, text, color):\n\"\"\"\nFind proper places to draw text given a binary mask.\n\"\"\"\n# TODO sometimes drawn on wrong objects. the heuristics here can improve.\n_num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, 8)\nif stats[1:, -1].size == 0:\nreturn\nlargest_component_id = np.argmax(stats[1:, -1]) + 1\n# draw text on the largest component, as well as other very large components.\nfor cid in range(1, _num_cc):\nif cid == largest_component_id or stats[cid, -1] > _LARGE_MASK_AREA_THRESH:\n# median is more stable than centroid\n# center = centroids[largest_component_id]\ncenter = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]\nself.draw_text(text, center, color=color)\nreturn center\ndef _convert_keypoints(self, keypoints):\nif isinstance(keypoints, Keypoints):\nkeypoints = keypoints.tensor\nkeypoints = np.asarray(keypoints)\nreturn keypoints\ndef get_output(self):\n\"\"\"\nReturns:\noutput (VisImage): the image output containing the visualizations added\nto the image.\n\"\"\"\nreturn self.output",
                        "max_stars_repo_path": "detectron2/utils/visualizer.py",
                        "max_stars_repo_name": "robolableonardo/detectron2",
                        "max_stars_count": 0,
                        "__cluster__": 186
                },
                {
                        "id": "test_evocodebench_data_190",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nimport colorsys\nimport logging\nimport math\nfrom enum import Enum, unique\nimport cv2\nimport matplotlib as mpl\nimport matplotlib.colors as mplc\nimport matplotlib.figure as mplfigure\nimport numpy as np\nimport pycocotools.mask as mask_util\nimport torch\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.structures import (\nBitMasks,\nBoxes,\nBoxMode,\nKeypoints,\nPolygonMasks,\nRotatedBoxes,\n)\nfrom detectron2.utils.file_io import PathManager\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\nfrom PIL import Image\nfrom .colormap import random_color\nlogger = logging.getLogger(__name__)\n__all__ = [\"ColorMode\", \"VisImage\", \"Visualizer\"]\n_SMALL_OBJECT_AREA_THRESH = 1000\n_LARGE_MASK_AREA_THRESH = 120000\n_OFF_WHITE = (1.0, 1.0, 240.0 / 255)\n_BLACK = (0, 0, 0)\n_RED = (1.0, 0, 0)\n_KEYPOINT_THRESHOLD = 0.05\n@unique\nclass ColorMode(Enum):\n\"\"\"\nEnum of different color modes to use for instance visualizations.\n\"\"\"\nIMAGE = 0\n\"\"\"\nPicks a random color for every instance and overlay segmentations with low opacity.\n\"\"\"\nSEGMENTATION = 1\n\"\"\"\nLet instances of the same category have similar colors\n(from metadata.thing_colors), and overlay them with\nhigh opacity. This provides more attention on the quality of segmentation.\n\"\"\"\nIMAGE_BW = 2\n\"\"\"\nSame as IMAGE, but convert all areas without masks to gray-scale.\nOnly available for drawing per-instance mask predictions.\n\"\"\"\nclass GenericMask:\n\"\"\"\nAttribute:\npolygons (list[ndarray]): list[ndarray]: polygons for this mask.\nEach ndarray has format [x, y, x, y, ...]\nmask (ndarray): a binary mask\n\"\"\"\ndef __init__(self, mask_or_polygons, height, width):\nself._mask = self._polygons = self._has_holes = None\nself.height = height\nself.width = width\nm = mask_or_polygons\nif isinstance(m, dict):\n# RLEs\nassert \"counts\" in m and \"size\" in m\nif isinstance(m[\"counts\"], list):  # uncompressed RLEs\nh, w = m[\"size\"]\nassert h == height and w == width\nm = mask_util.frPyObjects(m, h, w)\nself._mask = mask_util.decode(m)[:, :]\nreturn\nif isinstance(m, list):  # list[ndarray]\nself._polygons = [np.asarray(x).reshape(-1) for x in m]\nreturn\nif isinstance(m, np.ndarray):  # assumed to be a binary mask\nassert m.shape[1] != 2, m.shape\nassert m.shape == (\nheight,\nwidth,\n), f\"mask shape: {m.shape}, target dims: {height}, {width}\"\nself._mask = m.astype(\"uint8\")\nreturn\nraise ValueError(\"GenericMask cannot handle object {} of type '{}'\".format(m, type(m)))\n@property\ndef mask(self):\nif self._mask is None:\nself._mask = self.polygons_to_mask(self._polygons)\nreturn self._mask\n@property\ndef polygons(self):\nif self._polygons is None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nreturn self._polygons\n@property\ndef has_holes(self):\nif self._has_holes is None:\nif self._mask is not None:\nself._polygons, self._has_holes = self.mask_to_polygons(self._mask)\nelse:\nself._has_holes = False  # if original format is polygon, does not have holes\nreturn self._has_holes\ndef mask_to_polygons(self, mask):\n# cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level\n# hierarchy. External contours (boundary) of the object are placed in hierarchy-1.\n# Internal contours (holes) are placed in hierarchy-2.\n# cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.\nmask = np.ascontiguousarray(mask)  # some versions of cv2 does not support incontiguous arr\nres = cv2.findContours(mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\nhierarchy = res[-1]\nif hierarchy is None:  # empty mask\nreturn [], False\nhas_holes = (hierarchy.reshape(-1, 4)[:, 3] >= 0).sum() > 0\nres = res[-2]\nres = [x.flatten() for x in res]\n# These coordinates from OpenCV are integers in range [0, W-1 or H-1].\n# We add 0.5 to turn them into real-value coordinate space. A better solution\n# would be to first +0.5 and then dilate the returned polygon by 0.5.\nres = [x + 0.5 for x in res if len(x) >= 6]\nreturn res, has_holes\ndef polygons_to_mask(self, polygons):\nrle = mask_util.frPyObjects(polygons, self.height, self.width)\nrle = mask_util.merge(rle)\nreturn mask_util.decode(rle)[:, :]\ndef area(self):\nreturn self.mask.sum()\ndef bbox(self):\np = mask_util.frPyObjects(self.polygons, self.height, self.width)\np = mask_util.merge(p)\nbbox = mask_util.toBbox(p)\nbbox[2] += bbox[0]\nbbox[3] += bbox[1]\nreturn bbox\nclass _PanopticPrediction:\n\"\"\"\nUnify different panoptic annotation/prediction formats\n\"\"\"\ndef __init__(self, panoptic_seg, segments_info, metadata=None):\nif segments_info is None:\nassert metadata is not None\n# If \"segments_info\" is None, we assume \"panoptic_img\" is a\n# H*W int32 image storing the panoptic_id in the format of\n# category_id * label_divisor + instance_id. We reserve -1 for\n# VOID label.\nlabel_divisor = metadata.label_divisor\nsegments_info = []\nfor panoptic_label in np.unique(panoptic_seg.numpy()):\nif panoptic_label == -1:\n# VOID region.\ncontinue\npred_class = panoptic_label // label_divisor\nisthing = pred_class in metadata.thing_dataset_id_to_contiguous_id.values()\nsegments_info.append(\n{\n\"id\": int(panoptic_label),\n\"category_id\": int(pred_class),\n\"isthing\": bool(isthing),\n}\n)\ndel metadata\nself._seg = panoptic_seg\nself._sinfo = {s[\"id\"]: s for s in segments_info}  # seg id -> seg info\nsegment_ids, areas = torch.unique(panoptic_seg, sorted=True, return_counts=True)\nareas = areas.numpy()\nsorted_idxs = np.argsort(-areas)\nself._seg_ids, self._seg_areas = segment_ids[sorted_idxs], areas[sorted_idxs]\nself._seg_ids = self._seg_ids.tolist()\nfor sid, area in zip(self._seg_ids, self._seg_areas):\nif sid in self._sinfo:\nself._sinfo[sid][\"area\"] = float(area)\ndef non_empty_mask(self):\n\"\"\"\nReturns:\n(H, W) array, a mask for all pixels that have a prediction\n\"\"\"\nempty_ids = []\nfor id in self._seg_ids:\nif id not in self._sinfo:\nempty_ids.append(id)\nif len(empty_ids) == 0:\nreturn np.zeros(self._seg.shape, dtype=np.uint8)\nassert (\nlen(empty_ids) == 1\n), \">1 ids corresponds to no labels. This is currently not supported\"\nreturn (self._seg != empty_ids[0]).numpy().astype(np.bool)\ndef semantic_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or sinfo[\"isthing\"]:\n# Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions.\ncontinue\nyield (self._seg == sid).numpy().astype(np.bool), sinfo\ndef instance_masks(self):\nfor sid in self._seg_ids:\nsinfo = self._sinfo.get(sid)\nif sinfo is None or not sinfo[\"isthing\"]:\ncontinue\nmask = (self._seg == sid).numpy().astype(np.bool)\nif mask.sum() > 0:\nyield mask, sinfo\ndef _create_text_labels(classes, scores, class_names, is_crowd=None):\n\"\"\"\nArgs:\nclasses (list[int] or None):\nscores (list[float] or None):\nclass_names (list[str] or None):\nis_crowd (list[bool] or None):\nReturns:\nlist[str] or None\n\"\"\"\nlabels = None\nif classes is not None:\nif class_names is not None and len(class_names) > 0:\nlabels = [class_names[i] for i in classes]\nelse:\nlabels = [str(i) for i in classes]\nif scores is not None:\nif labels is None:\nlabels = [\"{:.0f}%\".format(s * 100) for s in scores]\nelse:\nlabels = [\"{} {:.0f}%\".format(l, s * 100) for l, s in zip(labels, scores)]\nif labels is not None and is_crowd is not None:\nlabels = [l + (\"|crowd\" if crowd else \"\") for l, crowd in zip(labels, is_crowd)]\nreturn labels\nclass VisImage:\ndef __init__(self, img, scale=1.0):\n\"\"\"\nArgs:\nimg (ndarray): an RGB image of shape (H, W, 3) in range [0, 255].\nscale (float): scale the input image\n\"\"\"\nself.img = img\nself.scale = scale\nself.width, self.height = img.shape[1], img.shape[0]\nself._setup_figure(img)\ndef _setup_figure(self, img):\n\"\"\"\nArgs:\nSame as in :meth:`__init__()`.\nReturns:\nfig (matplotlib.pyplot.figure): top level container for all the image plot elements.\nax (matplotlib.pyplot.Axes): contains figure elements and sets the coordinate system.\n\"\"\"\nfig = mplfigure.Figure(frameon=False)\nself.dpi = fig.get_dpi()\n# add a small 1e-2 to avoid precision lost due to matplotlib's truncation\n# (https://github.com/matplotlib/matplotlib/issues/15363)\nfig.set_size_inches(\n(self.width * self.scale + 1e-2) / self.dpi,\n(self.height * self.scale + 1e-2) / self.dpi,\n)\nself.canvas = FigureCanvasAgg(fig)\n# self.canvas = mpl.backends.backend_cairo.FigureCanvasCairo(fig)\nax = fig.add_axes([0.0, 0.0, 1.0, 1.0])\nax.axis(\"off\")\nself.fig = fig\nself.ax = ax\nself.reset_image(img)\ndef reset_image(self, img):\n\"\"\"\nArgs:\nimg: same as in __init__\n\"\"\"\nimg = img.astype(\"uint8\")\nself.ax.imshow(img, extent=(0, self.width, self.height, 0), interpolation=\"nearest\")\ndef save(self, filepath):\n\"\"\"\nArgs:\nfilepath (str): a string that contains the absolute path, including the file name, where\nthe visualized image will be saved.\n\"\"\"\nself.fig.savefig(filepath)\ndef get_image(self):\n\"\"\"\nReturns:\nndarray:\nthe visualized image of shape (H, W, 3) (RGB) in uint8 type.\nThe shape is scaled w.r.t the input image using the given `scale` argument.\n\"\"\"\ncanvas = self.canvas\ns, (width, height) = canvas.print_to_buffer()\n# buf = io.BytesIO()  # works for cairo backend\n# canvas.print_rgba(buf)\n# width, height = self.width, self.height\n# s = buf.getvalue()\nbuffer = np.frombuffer(s, dtype=\"uint8\")\nimg_rgba = buffer.reshape(height, width, 4)\nrgb, alpha = np.split(img_rgba, [3], axis=2)\nreturn rgb.astype(\"uint8\")\nclass Visualizer:\n\"\"\"\nVisualizer that draws data about detection/segmentation on images.\nIt contains methods like `draw_{text,box,circle,line,binary_mask,polygon}`\nthat draw primitive objects to images, as well as high-level wrappers like\n`draw_{instance_predictions,sem_seg,panoptic_seg_predictions,dataset_dict}`\nthat draw composite data in some pre-defined style.\nNote that the exact visualization style for the high-level wrappers are subject to change.\nStyle such as color, opacity, label contents, visibility of labels, or even the visibility\nof objects themselves (e.g. when the object is too small) may change according\nto different heuristics, as long as the results still look visually reasonable.\nTo obtain a consistent style, you can implement custom drawing functions with the\nabovementioned primitive methods instead. If you need more customized visualization\nstyles, you can process the data yourself following their format documented in\ntutorials (:doc:`/tutorials/models`, :doc:`/tutorials/datasets`). This class does not\nintend to satisfy everyone's preference on drawing styles.\nThis visualizer focuses on high rendering quality rather than performance. It is not\ndesigned to be used for real-time applications.\n\"\"\"\n# TODO implement a fast, rasterized version using OpenCV\ndef __init__(self, img_rgb, metadata=None, scale=1.0, instance_mode=ColorMode.IMAGE):\n\"\"\"\nArgs:\nimg_rgb: a numpy array of shape (H, W, C), where H and W correspond to\nthe height and width of the image respectively. C is the number of\ncolor channels. The image is required to be in RGB format since that\nis a requirement of the Matplotlib library. The image is also expected\nto be in the range [0, 255].\nmetadata (Metadata): dataset metadata (e.g. class names and colors)\ninstance_mode (ColorMode): defines one of the pre-defined style for drawing\ninstances on an image.\n\"\"\"\nself.img = np.asarray(img_rgb).clip(0, 255).astype(np.uint8)\nif metadata is None:\nmetadata = MetadataCatalog.get(\"__nonexist__\")\nself.metadata = metadata\nself.output = VisImage(self.img, scale=scale)\nself.cpu_device = torch.device(\"cpu\")\n# too small texts are useless, therefore clamp to 9\nself._default_font_size = max(\nnp.sqrt(self.output.height * self.output.width) // 90, 10 // scale\n)\nself._instance_mode = instance_mode\nself.keypoint_threshold = _KEYPOINT_THRESHOLD\ndef draw_instance_predictions(self, predictions):\n\"\"\"\nDraw instance-level prediction results on an image.\nArgs:\npredictions (Instances): the output of an instance detection/segmentation\nmodel. Following fields will be used to draw:\n\"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nboxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\nscores = predictions.scores if predictions.has(\"scores\") else None\nclasses = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\nlabels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\nkeypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\nif predictions.has(\"pred_masks\"):\nmasks = np.asarray(predictions.pred_masks)\nmasks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\nelse:\nmasks = None\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n]\nalpha = 0.8\nelse:\ncolors = None\nalpha = 0.5\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(\nself._create_grayscale_image(\n(predictions.pred_masks.any(dim=0) > 0).numpy()\nif predictions.has(\"pred_masks\")\nelse None\n)\n)\nalpha = 0.3\nself.overlay_instances(\nmasks=masks,\nboxes=boxes,\nlabels=labels,\nkeypoints=keypoints,\nassigned_colors=colors,\nalpha=alpha,\n)\nreturn self.output\ndef draw_sem_seg(self, sem_seg, area_threshold=None, alpha=0.8):\n\"\"\"\nDraw semantic segmentation predictions/labels.\nArgs:\nsem_seg (Tensor or ndarray): the segmentation of shape (H, W).\nEach value is the integer label of the pixel.\narea_threshold (int): segments with less than `area_threshold` are not drawn.\nalpha (float): the larger it is, the more opaque the segmentations are.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nif isinstance(sem_seg, torch.Tensor):\nsem_seg = sem_seg.numpy()\nlabels, areas = np.unique(sem_seg, return_counts=True)\nsorted_idxs = np.argsort(-areas).tolist()\nlabels = labels[sorted_idxs]\nfor label in filter(lambda l: l < len(self.metadata.stuff_classes), labels):\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[label]]\nexcept (AttributeError, IndexError):\nmask_color = None\nbinary_mask = (sem_seg == label).astype(np.uint8)\ntext = self.metadata.stuff_classes[label]\nself.draw_binary_mask(\nbinary_mask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\nreturn self.output\ndef draw_panoptic_seg(self, panoptic_seg, segments_info, area_threshold=None, alpha=0.7):\n\"\"\"\nDraw panoptic prediction annotations or results.\nArgs:\npanoptic_seg (Tensor): of shape (height, width) where the values are ids for each\nsegment.\nsegments_info (list[dict] or None): Describe each segment in `panoptic_seg`.\nIf it is a ``list[dict]``, each dict contains keys \"id\", \"category_id\".\nIf None, category id of each pixel is computed by\n``pixel // metadata.label_divisor``.\narea_threshold (int): stuff segments with less than `area_threshold` are not drawn.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\npred = _PanopticPrediction(panoptic_seg, segments_info, self.metadata)\nif self._instance_mode == ColorMode.IMAGE_BW:\nself.output.reset_image(self._create_grayscale_image(pred.non_empty_mask()))\n# draw mask for all semantic segments first i.e. \"stuff\"\nfor mask, sinfo in pred.semantic_masks():\ncategory_idx = sinfo[\"category_id\"]\ntry:\nmask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]\nexcept AttributeError:\nmask_color = None\ntext = self.metadata.stuff_classes[category_idx]\nself.draw_binary_mask(\nmask,\ncolor=mask_color,\nedge_color=_OFF_WHITE,\ntext=text,\nalpha=alpha,\narea_threshold=area_threshold,\n)\n# draw mask for all instances second\nall_instances = list(pred.instance_masks())\nif len(all_instances) == 0:\nreturn self.output\nmasks, sinfo = list(zip(*all_instances))\ncategory_ids = [x[\"category_id\"] for x in sinfo]\ntry:\nscores = [x[\"score\"] for x in sinfo]\nexcept KeyError:\nscores = None\nlabels = _create_text_labels(\ncategory_ids, scores, self.metadata.thing_classes, [x.get(\"iscrowd\", 0) for x in sinfo]\n)\ntry:\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in category_ids\n]\nexcept AttributeError:\ncolors = None\nself.overlay_instances(masks=masks, labels=labels, assigned_colors=colors, alpha=alpha)\nreturn self.output\ndraw_panoptic_seg_predictions = draw_panoptic_seg  # backward compatibility\ndef draw_dataset_dict(self, dic):\n\"\"\"\nDraw annotations/segmentaions in Detectron2 Dataset format.\nArgs:\ndic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nannos = dic.get(\"annotations\", None)\nif annos:\nif \"segmentation\" in annos[0]:\nmasks = [x[\"segmentation\"] for x in annos]\nelse:\nmasks = None\nif \"keypoints\" in annos[0]:\nkeypts = [x[\"keypoints\"] for x in annos]\nkeypts = np.array(keypts).reshape(len(annos), -1, 3)\nelse:\nkeypts = None\nboxes = [\nBoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS)\nif len(x[\"bbox\"]) == 4\nelse x[\"bbox\"]\nfor x in annos\n]\ncolors = None\ncategory_ids = [x[\"category_id\"] for x in annos]\nif self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\ncolors = [\nself._jitter([x / 255 for x in self.metadata.thing_colors[c]])\nfor c in category_ids\n]\nnames = self.metadata.get(\"thing_classes\", None)\nlabels = _create_text_labels(\ncategory_ids,\nscores=None,\nclass_names=names,\nis_crowd=[x.get(\"iscrowd\", 0) for x in annos],\n)\nself.overlay_instances(\nlabels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n)\nsem_seg = dic.get(\"sem_seg\", None)\nif sem_seg is None and \"sem_seg_file_name\" in dic:\nwith PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\nsem_seg = Image.open(f)\nsem_seg = np.asarray(sem_seg, dtype=\"uint8\")\nif sem_seg is not None:\nself.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\npan_seg = dic.get(\"pan_seg\", None)\nif pan_seg is None and \"pan_seg_file_name\" in dic:\nwith PathManager.open(dic[\"pan_seg_file_name\"], \"rb\") as f:\npan_seg = Image.open(f)\npan_seg = np.asarray(pan_seg)\nfrom panopticapi.utils import rgb2id\npan_seg = rgb2id(pan_seg)\nif pan_seg is not None:\nsegments_info = dic[\"segments_info\"]\npan_seg = torch.tensor(pan_seg)\nself.draw_panoptic_seg(pan_seg, segments_info, area_threshold=0, alpha=0.5)\nreturn self.output\ndef overlay_instances(\nself,\n*,\nboxes=None,\nlabels=None,\nmasks=None,\nkeypoints=None,\nassigned_colors=None,\nalpha=0.5,\n):\n\"\"\"\nArgs:\nboxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,\nor an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,\nor a :class:`RotatedBoxes`,\nor an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image,\nlabels (list[str]): the text to be displayed for each instance.\nmasks (masks-like object): Supported types are:\n* :class:`detectron2.structures.PolygonMasks`,\n:class:`detectron2.structures.BitMasks`.\n* list[list[ndarray]]: contains the segmentation masks for all objects in one image.\nThe first level of the list corresponds to individual instances. The second\nlevel to all the polygon that compose the instance, and the third level\nto the polygon coordinates. The third level should have the format of\n[x0, y0, x1, y1, ..., xn, yn] (n >= 3).\n* list[ndarray]: each ndarray is a binary mask of shape (H, W).\n* list[dict]: each dict is a COCO-style RLE.\nkeypoints (Keypoint or array like): an array-like object of shape (N, K, 3),\nwhere the N is the number of instances and K is the number of keypoints.\nThe last dimension corresponds to (x, y, visibility or score).\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = 0\nif boxes is not None:\nboxes = self._convert_boxes(boxes)\nnum_instances = len(boxes)\nif masks is not None:\nmasks = self._convert_masks(masks)\nif num_instances:\nassert len(masks) == num_instances\nelse:\nnum_instances = len(masks)\nif keypoints is not None:\nif num_instances:\nassert len(keypoints) == num_instances\nelse:\nnum_instances = len(keypoints)\nkeypoints = self._convert_keypoints(keypoints)\nif labels is not None:\nassert len(labels) == num_instances\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\nif boxes is not None and boxes.shape[1] == 5:\nreturn self.overlay_rotated_instances(\nboxes=boxes, labels=labels, assigned_colors=assigned_colors\n)\n# Display in largest to smallest order to reduce occlusion.\nareas = None\nif boxes is not None:\nareas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\nelif masks is not None:\nareas = np.asarray([x.area() for x in masks])\nif areas is not None:\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs] if boxes is not None else None\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\nmasks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\nassigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\nkeypoints = keypoints[sorted_idxs] if keypoints is not None else None\nfor i in range(num_instances):\ncolor = assigned_colors[i]\nif boxes is not None:\nself.draw_box(boxes[i], edge_color=color)\nif masks is not None:\nfor segment in masks[i].polygons:\nself.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\nif labels is not None:\n# first get a box\nif boxes is not None:\nx0, y0, x1, y1 = boxes[i]\ntext_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\nhoriz_align = \"left\"\nelif masks is not None:\n# skip small mask without polygon\nif len(masks[i].polygons) == 0:\ncontinue\nx0, y0, x1, y1 = masks[i].bbox()\n# draw text in the center (defined by median) when box is not drawn\n# median is less sensitive to outliers.\ntext_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\nhoriz_align = \"center\"\nelse:\ncontinue  # drawing the box confidence for keypoints isn't very useful.\n# for small objects, draw text at the side to avoid occlusion\ninstance_area = (y1 - y0) * (x1 - x0)\nif (\ninstance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\nor y1 - y0 < 40 * self.output.scale\n):\nif y1 >= self.output.height - 5:\ntext_pos = (x1, y0)\nelse:\ntext_pos = (x0, y1)\nheight_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n* 0.5\n* self._default_font_size\n)\nself.draw_text(\nlabels[i],\ntext_pos,\ncolor=lighter_color,\nhorizontal_alignment=horiz_align,\nfont_size=font_size,\n)\n# draw keypoints\nif keypoints is not None:\nfor keypoints_per_instance in keypoints:\nself.draw_and_connect_keypoints(keypoints_per_instance)\nreturn self.output\ndef overlay_rotated_instances(self, boxes=None, labels=None, assigned_colors=None):\n\"\"\"\nArgs:\nboxes (ndarray): an Nx5 numpy array of\n(x_center, y_center, width, height, angle_degrees) format\nfor the N objects in a single image.\nlabels (list[str]): the text to be displayed for each instance.\nassigned_colors (list[matplotlib.colors]): a list of colors, where each color\ncorresponds to each mask or box in the image. Refer to 'matplotlib.colors'\nfor full list of formats that the colors are accepted in.\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nnum_instances = len(boxes)\nif assigned_colors is None:\nassigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\nif num_instances == 0:\nreturn self.output\n# Display in largest to smallest order to reduce occlusion.\nif boxes is not None:\nareas = boxes[:, 2] * boxes[:, 3]\nsorted_idxs = np.argsort(-areas).tolist()\n# Re-order overlapped instances in descending order.\nboxes = boxes[sorted_idxs]\nlabels = [labels[k] for k in sorted_idxs] if labels is not None else None\ncolors = [assigned_colors[idx] for idx in sorted_idxs]\nfor i in range(num_instances):\nself.draw_rotated_box_with_label(\nboxes[i], edge_color=colors[i], label=labels[i] if labels is not None else None\n)\nreturn self.output\ndef draw_and_connect_keypoints(self, keypoints):\n\"\"\"\nDraws keypoints of an instance and follows the rules for keypoint connections\nto draw lines between appropriate keypoints. This follows color heuristics for\nline color.\nArgs:\nkeypoints (Tensor): a tensor of shape (K, 3), where K is the number of keypoints\nand the last dimension corresponds to (x, y, probability).\nReturns:\noutput (VisImage): image object with visualizations.\n\"\"\"\nvisible = {}\nkeypoint_names = self.metadata.get(\"keypoint_names\")\nfor idx, keypoint in enumerate(keypoints):\n# draw keypoint\nx, y, prob = keypoint\nif prob > self.keypoint_threshold:\nself.draw_circle((x, y), color=_RED)\nif keypoint_names:\nkeypoint_name = keypoint_names[idx]\nvisible[keypoint_name] = (x, y)\nif self.metadata.get(\"keypoint_connection_rules\"):\nfor kp0, kp1, color in self.metadata.keypoint_connection_rules:\nif kp0 in visible and kp1 in visible:\nx0, y0 = visible[kp0]\nx1, y1 = visible[kp1]\ncolor = tuple(x / 255.0 for x in color)\nself.draw_line([x0, x1], [y0, y1], color=color)\n# draw lines from nose to mid-shoulder and mid-shoulder to mid-hip\n# Note that this strategy is specific to person keypoints.\n# For other keypoints, it should just do nothing\ntry:\nls_x, ls_y = visible[\"left_shoulder\"]\nrs_x, rs_y = visible[\"right_shoulder\"]\nmid_shoulder_x, mid_shoulder_y = (ls_x + rs_x) / 2, (ls_y + rs_y) / 2\nexcept KeyError:\npass\nelse:\n# draw line from nose to mid-shoulder\nnose_x, nose_y = visible.get(\"nose\", (None, None))\nif nose_x is not None:\nself.draw_line([nose_x, mid_shoulder_x], [nose_y, mid_shoulder_y], color=_RED)\ntry:\n# draw line from mid-shoulder to mid-hip\nlh_x, lh_y = visible[\"left_hip\"]\nrh_x, rh_y = visible[\"right_hip\"]\nexcept KeyError:\npass\nelse:\nmid_hip_x, mid_hip_y = (lh_x + rh_x) / 2, (lh_y + rh_y) / 2\nself.draw_line([mid_hip_x, mid_shoulder_x], [mid_hip_y, mid_shoulder_y], color=_RED)\nreturn self.output\n\"\"\"\nPrimitive drawing functions:\n\"\"\"\ndef draw_text(\nself,\ntext,\nposition,\n*,\nfont_size=None,\ncolor=\"g\",\nhorizontal_alignment=\"center\",\nrotation=0,\n):\n\"\"\"\nArgs:\ntext (str): class label\nposition (tuple): a tuple of the x and y coordinates to place text on image.\nfont_size (int, optional): font of the text. If not provided, a font size\nproportional to the image width is calculated and used.\ncolor: color of the text. Refer to `matplotlib.colors` for full list\nof formats that are accepted.\nhorizontal_alignment (str): see `matplotlib.text.Text`\nrotation: rotation angle in degrees CCW\nReturns:\noutput (VisImage): image object with text drawn.\n\"\"\"\nif not font_size:\nfont_size = self._default_font_size\n# since the text background is dark, we don't want the text to be dark\ncolor = np.maximum(list(mplc.to_rgb(color)), 0.2)\ncolor[np.argmax(color)] = max(0.8, np.max(color))\nx, y = position\nself.output.ax.text(\nx,\ny,\ntext,\nsize=font_size * self.output.scale,\nfamily=\"sans-serif\",\nbbox={\"facecolor\": \"black\", \"alpha\": 0.8, \"pad\": 0.7, \"edgecolor\": \"none\"},\nverticalalignment=\"top\",\nhorizontalalignment=horizontal_alignment,\ncolor=color,\nzorder=10,\nrotation=rotation,\n)\nreturn self.output\ndef draw_box(self, box_coord, alpha=0.5, edge_color=\"g\", line_style=\"-\"):\n\"\"\"\nArgs:\nbox_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0\nare the coordinates of the image's top left corner. x1 and y1 are the\ncoordinates of the image's bottom right corner.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx0, y0, x1, y1 = box_coord\nwidth = x1 - x0\nheight = y1 - y0\nlinewidth = max(self._default_font_size / 4, 1)\nself.output.ax.add_patch(\nmpl.patches.Rectangle(\n(x0, y0),\nwidth,\nheight,\nfill=False,\nedgecolor=edge_color,\nlinewidth=linewidth * self.output.scale,\nalpha=alpha,\nlinestyle=line_style,\n)\n)\nreturn self.output\ndef draw_rotated_box_with_label(\nself, rotated_box, alpha=0.5, edge_color=\"g\", line_style=\"-\", label=None\n):\n\"\"\"\nDraw a rotated box with label on its top-left corner.\nArgs:\nrotated_box (tuple): a tuple containing (cnt_x, cnt_y, w, h, angle),\nwhere cnt_x and cnt_y are the center coordinates of the box.\nw and h are the width and height of the box. angle represents how\nmany degrees the box is rotated CCW with regard to the 0-degree box.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nedge_color: color of the outline of the box. Refer to `matplotlib.colors`\nfor full list of formats that are accepted.\nline_style (string): the string to use to create the outline of the boxes.\nlabel (string): label for rotated box. It will not be rendered when set to None.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\ncnt_x, cnt_y, w, h, angle = rotated_box\narea = w * h\n# use thinner lines when the box is small\nlinewidth = self._default_font_size / (\n6 if area < _SMALL_OBJECT_AREA_THRESH * self.output.scale else 3\n)\ntheta = angle * math.pi / 180.0\nc = math.cos(theta)\ns = math.sin(theta)\nrect = [(-w / 2, h / 2), (-w / 2, -h / 2), (w / 2, -h / 2), (w / 2, h / 2)]\n# x: left->right ; y: top->down\nrotated_rect = [(s * yy + c * xx + cnt_x, c * yy - s * xx + cnt_y) for (xx, yy) in rect]\nfor k in range(4):\nj = (k + 1) % 4\nself.draw_line(\n[rotated_rect[k][0], rotated_rect[j][0]],\n[rotated_rect[k][1], rotated_rect[j][1]],\ncolor=edge_color,\nlinestyle=\"--\" if k == 1 else line_style,\nlinewidth=linewidth,\n)\nif label is not None:\ntext_pos = rotated_rect[1]  # topleft corner\nheight_ratio = h / np.sqrt(self.output.height * self.output.width)\nlabel_color = self._change_color_brightness(edge_color, brightness_factor=0.7)\nfont_size = (\nnp.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2) * 0.5 * self._default_font_size\n)\nself.draw_text(label, text_pos, color=label_color, font_size=font_size, rotation=angle)\nreturn self.output\ndef draw_circle(self, circle_coord, color, radius=3):\n\"\"\"\nArgs:\ncircle_coord (list(int) or tuple(int)): contains the x and y coordinates\nof the center of the circle.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nradius (int): radius of the circle.\nReturns:\noutput (VisImage): image object with box drawn.\n\"\"\"\nx, y = circle_coord\nself.output.ax.add_patch(\nmpl.patches.Circle(circle_coord, radius=radius, fill=True, color=color)\n)\nreturn self.output\ndef draw_line(self, x_data, y_data, color, linestyle=\"-\", linewidth=None):\n\"\"\"\nArgs:\nx_data (list[int]): a list containing x values of all the points being drawn.\nLength of list should match the length of y_data.\ny_data (list[int]): a list containing y values of all the points being drawn.\nLength of list should match the length of x_data.\ncolor: color of the line. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nlinestyle: style of the line. Refer to `matplotlib.lines.Line2D`\nfor a full list of formats that are accepted.\nlinewidth (float or None): width of the line. When it's None,\na default value will be computed and used.\nReturns:\noutput (VisImage): image object with line drawn.\n\"\"\"\nif linewidth is None:\nlinewidth = self._default_font_size / 3\nlinewidth = max(linewidth, 1)\nself.output.ax.add_line(\nmpl.lines.Line2D(\nx_data,\ny_data,\nlinewidth=linewidth * self.output.scale,\ncolor=color,\nlinestyle=linestyle,\n)\n)\nreturn self.output\ndef draw_binary_mask(\nself, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n):\n\"\"\"\nArgs:\nbinary_mask (ndarray): numpy array of shape (H, W), where H is the image height and\nW is the image width. Each value in the array is either a 0 or 1 value of uint8\ntype.\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\narea_threshold (float): a connected component smaller than this area will not be shown.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nhas_valid_segment = False\nbinary_mask = binary_mask.astype(\"uint8\")  # opencv needs uint8\nmask = GenericMask(binary_mask, self.output.height, self.output.width)\nshape2d = (binary_mask.shape[0], binary_mask.shape[1])\nif not mask.has_holes:\n# draw polygons for regular masks\nfor segment in mask.polygons:\narea = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))\nif area < (area_threshold or 0):\ncontinue\nhas_valid_segment = True\nsegment = segment.reshape(-1, 2)\nself.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)\nelse:\n# TODO: Use Path/PathPatch to draw vector graphics:\n# https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = (mask.mask == 1).astype(\"float32\") * alpha\nhas_valid_segment = True\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None and has_valid_segment:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nself._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output\ndef draw_soft_mask(self, soft_mask, color=None, *, text=None, alpha=0.5):\n\"\"\"\nArgs:\nsoft_mask (ndarray): float array of shape (H, W), each value in [0, 1].\ncolor: color of the mask. Refer to `matplotlib.colors` for a full list of\nformats that are accepted. If None, will pick a random color.\ntext (str): if None, will be drawn on the object\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with mask drawn.\n\"\"\"\nif color is None:\ncolor = random_color(rgb=True, maximum=1)\ncolor = mplc.to_rgb(color)\nshape2d = (soft_mask.shape[0], soft_mask.shape[1])\nrgba = np.zeros(shape2d + (4,), dtype=\"float32\")\nrgba[:, :, :3] = color\nrgba[:, :, 3] = soft_mask * alpha\nself.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\nif text is not None:\nlighter_color = self._change_color_brightness(color, brightness_factor=0.7)\nbinary_mask = (soft_mask > 0.5).astype(\"uint8\")\nself._draw_text_in_mask(binary_mask, text, lighter_color)\nreturn self.output\ndef draw_polygon(self, segment, color, edge_color=None, alpha=0.5):\n\"\"\"\nArgs:\nsegment: numpy array of shape Nx2, containing all the points in the polygon.\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nedge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\nfull list of formats that are accepted. If not provided, a darker shade\nof the polygon color will be used instead.\nalpha (float): blending efficient. Smaller values lead to more transparent masks.\nReturns:\noutput (VisImage): image object with polygon drawn.\n\"\"\"\nif edge_color is None:\n# make edge color darker than the polygon color\nif alpha > 0.8:\nedge_color = self._change_color_brightness(color, brightness_factor=-0.7)\nelse:\nedge_color = color\nedge_color = mplc.to_rgb(edge_color) + (1,)\npolygon = mpl.patches.Polygon(\nsegment,\nfill=True,\nfacecolor=mplc.to_rgb(color) + (alpha,),\nedgecolor=edge_color,\nlinewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n)\nself.output.ax.add_patch(polygon)\nreturn self.output\n\"\"\"\nInternal methods:\n\"\"\"\ndef _jitter(self, color):\n\"\"\"\nRandomly modifies given color to produce a slightly different color than the color given.\nArgs:\ncolor (tuple[double]): a tuple of 3 elements, containing the RGB values of the color\npicked. The values in the list are in the [0.0, 1.0] range.\nReturns:\njittered_color (tuple[double]): a tuple of 3 elements, containing the RGB values of the\ncolor after being jittered. The values in the list are in the [0.0, 1.0] range.\n\"\"\"\ncolor = mplc.to_rgb(color)\nvec = np.random.rand(3)\n# better to do it in another color space\nvec = vec / np.linalg.norm(vec) * 0.5\nres = np.clip(vec + color, 0, 1)\nreturn tuple(res)\ndef _create_grayscale_image(self, mask=None):\n\"\"\"\nCreate a grayscale version of the original image.\nThe colors in masked area, if given, will be kept.\n\"\"\"\nimg_bw = self.img.astype(\"f4\").mean(axis=2)\nimg_bw = np.stack([img_bw] * 3, axis=2)\nif mask is not None:\nimg_bw[mask] = self.img[mask]\nreturn img_bw\ndef _change_color_brightness(self, color, brightness_factor):\n\"\"\"\nDepending on the brightness_factor, gives a lighter or darker color i.e. a color with\nless or more saturation than the original color.\nArgs:\ncolor: color of the polygon. Refer to `matplotlib.colors` for a full list of\nformats that are accepted.\nbrightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of\n0 will correspond to no change, a factor in [-1.0, 0) range will result in\na darker color and a factor in (0, 1.0] range will result in a lighter color.\nReturns:\nmodified_color (tuple[double]): a tuple containing the RGB values of the\nmodified color. Each value in the tuple is in the [0.0, 1.0] range.\n\"\"\"\nassert brightness_factor >= -1.0 and brightness_factor <= 1.0\ncolor = mplc.to_rgb(color)\npolygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))\nmodified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])\nmodified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness\nmodified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness\nmodified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])\nreturn modified_color\ndef _convert_boxes(self, boxes):\n\"\"\"\nConvert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n\"\"\"\nif isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):\nreturn boxes.tensor.detach().numpy()\nelse:\nreturn np.asarray(boxes)\ndef _convert_masks(self, masks_or_polygons):\n\"\"\"\nConvert different format of masks or polygons to a tuple of masks and polygons.\nReturns:\nlist[GenericMask]:\n\"\"\"\nm = masks_or_polygons\nif isinstance(m, PolygonMasks):\nm = m.polygons\nif isinstance(m, BitMasks):\nm = m.tensor.numpy()\nif isinstance(m, torch.Tensor):\nm = m.numpy()\nret = []\nfor x in m:\nif isinstance(x, GenericMask):\nret.append(x)\nelse:\nret.append(GenericMask(x, self.output.height, self.output.width))\nreturn ret\ndef _draw_text_in_mask(self, binary_mask, text, color):\n\"\"\"\nFind proper places to draw text given a binary mask.\n\"\"\"\n# TODO sometimes drawn on wrong objects. the heuristics here can improve.\n_num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, 8)\nif stats[1:, -1].size == 0:\nreturn\nlargest_component_id = np.argmax(stats[1:, -1]) + 1\n# draw text on the largest component, as well as other very large components.\nfor cid in range(1, _num_cc):\nif cid == largest_component_id or stats[cid, -1] > _LARGE_MASK_AREA_THRESH:\n# median is more stable than centroid\n# center = centroids[largest_component_id]\ncenter = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]\nself.draw_text(text, center, color=color)\ndef _convert_keypoints(self, keypoints):\nif isinstance(keypoints, Keypoints):\nkeypoints = keypoints.tensor\nkeypoints = np.asarray(keypoints)\nreturn keypoints\ndef get_output(self):\n\"\"\"\nReturns:\noutput (VisImage): the image output containing the visualizations added\nto the image.\n\"\"\"\nreturn self.output",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 186
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "test_evocodebench_data_179",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nimport collections\nfrom dataclasses import dataclass\nfrom typing import Callable, List, Optional, Tuple\nimport torch\nfrom torch import nn\nfrom detectron2.structures import Boxes, Instances, ROIMasks\nfrom detectron2.utils.registry import _convert_target_to_string, locate\nfrom .torchscript_patch import patch_builtin_len\n@dataclass\nclass Schema:\n\"\"\"\nA Schema defines how to flatten a possibly hierarchical object into tuple of\nprimitive objects, so it can be used as inputs/outputs of PyTorch's tracing.\nPyTorch does not support tracing a function that produces rich output\nstructures (e.g. dict, Instances, Boxes). To trace such a function, we\nflatten the rich object into tuple of tensors, and return this tuple of tensors\ninstead. Meanwhile, we also need to know how to \"rebuild\" the original object\nfrom the flattened results, so we can evaluate the flattened results.\nA Schema defines how to flatten an object, and while flattening it, it records\nnecessary schemas so that the object can be rebuilt using the flattened outputs.\nThe flattened object and the schema object is returned by ``.flatten`` classmethod.\nThen the original object can be rebuilt with the ``__call__`` method of schema.\nA Schema is a dataclass that can be serialized easily.\n\"\"\"\n# inspired by FetchMapper in tensorflow/python/client/session.py\n@classmethod\ndef flatten(cls, obj):\nraise NotImplementedError\ndef __call__(self, values):\nraise NotImplementedError\n@staticmethod\ndef _concat(values):\nret = ()\nsizes = []\nfor v in values:\nassert isinstance(v, tuple), \"Flattened results must be a tuple\"\nret = ret + v\nsizes.append(len(v))\nreturn ret, sizes\n@staticmethod\ndef _split(values, sizes):\nif len(sizes):\nexpected_len = sum(sizes)\nassert (\nlen(values) == expected_len\n), f\"Values has length {len(values)} but expect length {expected_len}.\"\nret = []\nfor k in range(len(sizes)):\nbegin, end = sum(sizes[:k]), sum(sizes[: k + 1])\nret.append(values[begin:end])\nreturn ret\n@dataclass\nclass ListSchema(Schema):\nschemas: List[Schema]  # the schemas that define how to flatten each element in the list\nsizes: List[int]  # the flattened length of each element\ndef __call__(self, values):\nvalues = self._split(values, self.sizes)\nif len(values) != len(self.schemas):\nraise ValueError(\nf\"Values has length {len(values)} but schemas \" f\"has length {len(self.schemas)}!\"\n)\nvalues = [m(v) for m, v in zip(self.schemas, values)]\nreturn list(values)\n@classmethod\ndef flatten(cls, obj):\nres = [flatten_to_tuple(k) for k in obj]\nvalues, sizes = cls._concat([k[0] for k in res])\nreturn values, cls([k[1] for k in res], sizes)\n@dataclass\nclass TupleSchema(ListSchema):\ndef __call__(self, values):\nreturn tuple(super().__call__(values))\n@dataclass\nclass IdentitySchema(Schema):\ndef __call__(self, values):\nreturn values[0]\n@classmethod\ndef flatten(cls, obj):\nreturn (obj,), cls()\n@dataclass\nclass DictSchema(ListSchema):\nkeys: List[str]\ndef __call__(self, values):\nvalues = super().__call__(values)\nreturn dict(zip(self.keys, values))\n@classmethod\ndef flatten(cls, obj):\nfor k in obj.keys():\nif not isinstance(k, str):\nraise KeyError(\"Only support flattening dictionaries if keys are str.\")\nkeys = sorted(obj.keys())\nvalues = [obj[k] for k in keys]\nret, schema = ListSchema.flatten(values)\nreturn ret, cls(schema.schemas, schema.sizes, keys)\n@dataclass\nclass InstancesSchema(DictSchema):\ndef __call__(self, values):\nimage_size, fields = values[-1], values[:-1]\nfields = super().__call__(fields)\nreturn Instances(image_size, **fields)\n@classmethod\ndef flatten(cls, obj):\nret, schema = super().flatten(obj.get_fields())\nsize = obj.image_size\nif not isinstance(size, torch.Tensor):\nsize = torch.tensor(size)\nreturn ret + (size,), schema\n@dataclass\nclass TensorWrapSchema(Schema):\n\"\"\"\nFor classes that are simple wrapper of tensors, e.g.\nBoxes, RotatedBoxes, BitMasks\n\"\"\"\nclass_name: str\ndef __call__(self, values):\nreturn locate(self.class_name)(values[0])\n@classmethod\ndef flatten(cls, obj):\nreturn (obj.tensor,), cls(_convert_target_to_string(type(obj)))\n# if more custom structures needed in the future, can allow\n# passing in extra schemas for custom types\ndef flatten_to_tuple(obj):\n\"\"\"\nFlatten an object so it can be used for PyTorch tracing.\nAlso returns how to rebuild the original object from the flattened outputs.\nReturns:\nres (tuple): the flattened results that can be used as tracing outputs\nschema: an object with a ``__call__`` method such that ``schema(res) == obj``.\nIt is a pure dataclass that can be serialized.\n\"\"\"\nschemas = [\n((str, bytes), IdentitySchema),\n(list, ListSchema),\n(tuple, TupleSchema),\n(collections.abc.Mapping, DictSchema),\n(Instances, InstancesSchema),\n((Boxes, ROIMasks), TensorWrapSchema),\n]\nfor klass, schema in schemas:\nif isinstance(obj, klass):\nF = schema\nbreak\nelse:\nF = IdentitySchema\nreturn F.flatten(obj)\nclass TracingAdapter(nn.Module):\n\"\"\"\nA model may take rich input/output format (e.g. dict or custom classes),\nbut `torch.jit.trace` requires tuple of tensors as input/output.\nThis adapter flattens input/output format of a model so it becomes traceable.\nIt also records the necessary schema to rebuild model's inputs/outputs from flattened\ninputs/outputs.\nExample:\n::\noutputs = model(inputs)   # inputs/outputs may be rich structure\nadapter = TracingAdapter(model, inputs)\n# can now trace the model, with adapter.flattened_inputs, or another\n# tuple of tensors with the same length and meaning\ntraced = torch.jit.trace(adapter, adapter.flattened_inputs)\n# traced model can only produce flattened outputs (tuple of tensors)\nflattened_outputs = traced(*adapter.flattened_inputs)\n# adapter knows the schema to convert it back (new_outputs == outputs)\nnew_outputs = adapter.outputs_schema(flattened_outputs)\n\"\"\"\nflattened_inputs: Tuple[torch.Tensor] = None\n\"\"\"\nFlattened version of inputs given to this class's constructor.\n\"\"\"\ninputs_schema: Schema = None\n\"\"\"\nSchema of the inputs given to this class's constructor.\n\"\"\"\noutputs_schema: Schema = None\n\"\"\"\nSchema of the output produced by calling the given model with inputs.\n\"\"\"\ndef __init__(\nself,\nmodel: nn.Module,\ninputs,\ninference_func: Optional[Callable] = None,\nallow_non_tensor: bool = False,\n):\n\"\"\"\nArgs:\nmodel: an nn.Module\ninputs: An input argument or a tuple of input arguments used to call model.\nAfter flattening, it has to only consist of tensors.\ninference_func: a callable that takes (model, *inputs), calls the\nmodel with inputs, and return outputs. By default it\nis ``lambda model, *inputs: model(*inputs)``. Can be override\nif you need to call the model differently.\nallow_non_tensor: allow inputs/outputs to contain non-tensor objects.\nThis option will filter out non-tensor objects to make the\nmodel traceable, but ``inputs_schema``/``outputs_schema`` cannot be\nused anymore because inputs/outputs cannot be rebuilt from pure tensors.\nThis is useful when you're only interested in the single trace of\nexecution (e.g. for flop count), but not interested in\ngeneralizing the traced graph to new inputs.\n\"\"\"\nsuper().__init__()\nif isinstance(model, (nn.parallel.distributed.DistributedDataParallel, nn.DataParallel)):\nmodel = model.module\nself.model = model\nif not isinstance(inputs, tuple):\ninputs = (inputs,)\nself.inputs = inputs\nself.allow_non_tensor = allow_non_tensor\nif inference_func is None:\ninference_func = lambda model, *inputs: model(*inputs)  # noqa\nself.inference_func = inference_func\nself.flattened_inputs, self.inputs_schema = flatten_to_tuple(inputs)\nif all(isinstance(x, torch.Tensor) for x in self.flattened_inputs):\nreturn\nif self.allow_non_tensor:\nself.flattened_inputs = tuple(\n[x for x in self.flattened_inputs if isinstance(x, torch.Tensor)]\n)\nself.inputs_schema = None\nelse:\nfor input in self.flattened_inputs:\nif not isinstance(input, torch.Tensor):\nraise ValueError(\n\"Inputs for tracing must only contain tensors. \"\nf\"Got a {type(input)} instead.\"\n)\ndef forward(self, *args: torch.Tensor):\nwith torch.no_grad(), patch_builtin_len():\nif self.inputs_schema is not None:\ninputs_orig_format = self.inputs_schema(args)\nelse:\nif len(args) != len(self.flattened_inputs) or any(\nx is not y for x, y in zip(args, self.flattened_inputs)\n):\nraise ValueError(\n\"TracingAdapter does not contain valid inputs_schema.\"\n\" So it cannot generalize to other inputs and must be\"\n\" traced with `.flattened_inputs`.\"\n)\ninputs_orig_format = self.inputs\noutputs = self.inference_func(self.model, *inputs_orig_format)\nflattened_outputs, schema = flatten_to_tuple(outputs)\nflattened_output_tensors = tuple(\n[x for x in flattened_outputs if isinstance(x, torch.Tensor)]\n)\nif len(flattened_output_tensors) < len(flattened_outputs):\nif self.allow_non_tensor:\nflattened_outputs = flattened_output_tensors\nself.outputs_schema = None\nelse:\nraise ValueError(\n\"Model cannot be traced because some model outputs \"\n\"cannot flatten to tensors.\"\n)\nelse:  # schema is valid\nif self.outputs_schema is None:\nself.outputs_schema = schema\nelse:\nassert self.outputs_schema == schema, (\n\"Model should always return outputs with the same \"\n\"structure so it can be traced!\"\n)\nreturn flattened_outputs\ndef _create_wrapper(self, traced_model):\n\"\"\"\nReturn a function that has an input/output interface the same as the\noriginal model, but it calls the given traced model under the hood.\n\"\"\"\ndef forward(*args):\nflattened_inputs, _ = flatten_to_tuple(args)\nflattened_outputs = traced_model(*flattened_inputs)\nreturn self.outputs_schema(flattened_outputs)\nreturn forward",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 178
                },
                {
                        "id": "pretrain_python_data_9071656",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nimport collections\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import Callable, List, Optional, Tuple\nimport torch\nfrom torch import nn\nfrom detectron2.structures import Boxes, Instances, ROIMasks\nfrom detectron2.utils.registry import _convert_target_to_string, locate\nfrom .torchscript_patch import patch_builtin_len\n@dataclass\nclass Schema:\n\"\"\"\nA Schema defines how to flatten a possibly hierarchical object into tuple of\nprimitive objects, so it can be used as inputs/outputs of PyTorch's tracing.\nThe flatten representation can be used as inputs/outputs of PyTorch's tracing.\nPyTorch does not support tracing a function that produces rich output\nstructures (e.g. dict, Instances, Boxes). To trace such a function, we\nflatten the rich object into tuple of tensors, and return this tuple of tensors\ninstead. Meanwhile, we also need to know how to \"rebuild\" the original object\nfrom the flattened results, so we can evaluate the flattened results.\nA Schema defines how to flatten an object, and while flattening it, it records\nnecessary schemas so that the object can be rebuilt using the flattened outputs.\nThe flattened object and the schema object is returned by ``.flatten`` classmethod.\nThen the original object can be rebuilt with the ``__call__`` method of schema.\nA Schema is a dataclass that can be serialized easily.\n\"\"\"\n# inspired by FetchMapper in tensorflow/python/client/session.py\n@classmethod\ndef flatten(cls, obj):\nraise NotImplementedError\ndef __call__(self, values):\nraise NotImplementedError\n@staticmethod\ndef _concat(values):\nret = ()\nsizes = []\nfor v in values:\nassert isinstance(v, tuple), \"Flattened results must be a tuple\"\nret = ret + v\nsizes.append(len(v))\nreturn ret, sizes\n@staticmethod\ndef _split(values, sizes):\nif len(sizes):\nexpected_len = sum(sizes)\nassert (\nlen(values) == expected_len\n), f\"Values has length {len(values)} but expect length {expected_len}.\"\nret = []\nfor k in range(len(sizes)):\nbegin, end = sum(sizes[:k]), sum(sizes[: k + 1])\nret.append(values[begin:end])\nreturn ret\n@dataclass\nclass ListSchema(Schema):\nschemas: List[Schema]  # the schemas that define how to flatten each element in the list\nsizes: List[int]  # the flattened length of each element\ndef __call__(self, values):\nvalues = self._split(values, self.sizes)\nif len(values) != len(self.schemas):\nraise ValueError(\nf\"Values has length {len(values)} but schemas \" f\"has length {len(self.schemas)}!\"\n)\nvalues = [m(v) for m, v in zip(self.schemas, values)]\nreturn list(values)\n@classmethod\ndef flatten(cls, obj):\nres = [flatten_to_tuple(k) for k in obj]\nvalues, sizes = cls._concat([k[0] for k in res])\nreturn values, cls([k[1] for k in res], sizes)\n@dataclass\nclass TupleSchema(ListSchema):\ndef __call__(self, values):\nreturn tuple(super().__call__(values))\n@dataclass\nclass IdentitySchema(Schema):\ndef __call__(self, values):\nreturn values[0]\n@classmethod\ndef flatten(cls, obj):\nreturn (obj,), cls()\n@dataclass\nclass DictSchema(ListSchema):\nkeys: List[str]\ndef __call__(self, values):\nvalues = super().__call__(values)\nreturn dict(zip(self.keys, values))\n@classmethod\ndef flatten(cls, obj):\nfor k in obj.keys():\nif not isinstance(k, str):\nraise KeyError(\"Only support flattening dictionaries if keys are str.\")\nkeys = sorted(obj.keys())\nvalues = [obj[k] for k in keys]\nret, schema = ListSchema.flatten(values)\nreturn ret, cls(schema.schemas, schema.sizes, keys)\n@dataclass\nclass InstancesSchema(DictSchema):\ndef __call__(self, values):\nimage_size, fields = values[-1], values[:-1]\nfields = super().__call__(fields)\nreturn Instances(image_size, **fields)\n@classmethod\ndef flatten(cls, obj):\nret, schema = super().flatten(obj.get_fields())\nsize = obj.image_size\nif not isinstance(size, torch.Tensor):\nsize = torch.tensor(size)\nreturn ret + (size,), schema\n@dataclass\nclass TensorWrapSchema(Schema):\n\"\"\"\nFor classes that are simple wrapper of tensors, e.g.\nBoxes, RotatedBoxes, BitMasks\n\"\"\"\nclass_name: str\ndef __call__(self, values):\nreturn locate(self.class_name)(values[0])\n@classmethod\ndef flatten(cls, obj):\nreturn (obj.tensor,), cls(_convert_target_to_string(type(obj)))\n# if more custom structures needed in the future, can allow\n# passing in extra schemas for custom types\ndef flatten_to_tuple(obj):\n\"\"\"\nFlatten an object so it can be used for PyTorch tracing.\nAlso returns how to rebuild the original object from the flattened outputs.\nReturns:\nres (tuple): the flattened results that can be used as tracing outputs\nschema: an object with a ``__call__`` method such that ``schema(res) == obj``.\nIt is a pure dataclass that can be serialized.\n\"\"\"\nschemas = [\n((str, bytes), IdentitySchema),\n(list, ListSchema),\n(tuple, TupleSchema),\n(collections.abc.Mapping, DictSchema),\n(Instances, InstancesSchema),\n((Boxes, ROIMasks), TensorWrapSchema),\n]\nfor klass, schema in schemas:\nif isinstance(obj, klass):\nF = schema\nbreak\nelse:\nF = IdentitySchema\nreturn F.flatten(obj)\nclass TracingAdapter(nn.Module):\n\"\"\"\nA model may take rich input/output format (e.g. dict or custom classes),\nbut `torch.jit.trace` requires tuple of tensors as input/output.\nThis adapter flattens input/output format of a model so it becomes traceable.\nIt also records the necessary schema to rebuild model's inputs/outputs from flattened\ninputs/outputs.\nExample:\n::\noutputs = model(inputs)   # inputs/outputs may be rich structure\nadapter = TracingAdapter(model, inputs)\n# can now trace the model, with adapter.flattened_inputs, or another\n# tuple of tensors with the same length and meaning\ntraced = torch.jit.trace(adapter, adapter.flattened_inputs)\n# traced model can only produce flattened outputs (tuple of tensors)\nflattened_outputs = traced(*adapter.flattened_inputs)\n# adapter knows the schema to convert it back (new_outputs == outputs)\nnew_outputs = adapter.outputs_schema(flattened_outputs)\n\"\"\"\nflattened_inputs: Tuple[torch.Tensor] = None\n\"\"\"\nFlattened version of inputs given to this class's constructor.\n\"\"\"\ninputs_schema: Schema = None\n\"\"\"\nSchema of the inputs given to this class's constructor.\n\"\"\"\noutputs_schema: Schema = None\n\"\"\"\nSchema of the output produced by calling the given model with inputs.\n\"\"\"\ndef __init__(\nself,\nmodel: nn.Module,\ninputs,\ninference_func: Optional[Callable] = None,\nallow_non_tensor: bool = False,\n):\n\"\"\"\nArgs:\nmodel: an nn.Module\ninputs: An input argument or a tuple of input arguments used to call model.\nAfter flattening, it has to only consist of tensors.\ninference_func: a callable that takes (model, *inputs), calls the\nmodel with inputs, and return outputs. By default it\nis ``lambda model, *inputs: model(*inputs)``. Can be override\nif you need to call the model differently.\nallow_non_tensor: allow inputs/outputs to contain non-tensor objects.\nThis option will filter out non-tensor objects to make the\nmodel traceable, but ``inputs_schema``/``outputs_schema`` cannot be\nused anymore because inputs/outputs cannot be rebuilt from pure tensors.\nThis is useful when you're only interested in the single trace of\nexecution (e.g. for flop count), but not interested in\ngeneralizing the traced graph to new inputs.\n\"\"\"\nsuper().__init__()\nif isinstance(model, (nn.parallel.distributed.DistributedDataParallel, nn.DataParallel)):\nmodel = model.module\nself.model = model\nif not isinstance(inputs, tuple):\ninputs = (inputs,)\nself.inputs = inputs\nself.allow_non_tensor = allow_non_tensor\nself._is_in_torch_onnx_export = torch.onnx.is_in_onnx_export()\nif inference_func is None:\ninference_func = lambda model, *inputs: model(*inputs)  # noqa\nself.inference_func = inference_func\nself.flattened_inputs, self.inputs_schema = flatten_to_tuple(inputs)\nif all(isinstance(x, torch.Tensor) for x in self.flattened_inputs):\nreturn\nif self.allow_non_tensor:\nself.flattened_inputs = tuple(\n[x for x in self.flattened_inputs if isinstance(x, torch.Tensor)]\n)\nself.inputs_schema = None\nelse:\nfor input in self.flattened_inputs:\nif not isinstance(input, torch.Tensor):\nraise ValueError(\n\"Inputs for tracing must only contain tensors. \"\nf\"Got a {type(input)} instead.\"\n)\ndef forward(self, *args: torch.Tensor):\nwith torch.no_grad(), patch_builtin_len():\nif self.inputs_schema is not None:\ninputs_orig_format = self.inputs_schema(args)\nelse:\nif len(args) != len(self.flattened_inputs) or any(\nx is not y for x, y in zip(args, self.flattened_inputs)\n):\nraise ValueError(\n\"TracingAdapter does not contain valid inputs_schema.\"\n\" So it cannot generalize to other inputs and must be\"\n\" traced with `.flattened_inputs`.\"\n)\ninputs_orig_format = self.inputs\noutputs = self.inference_func(self.model, *inputs_orig_format)\nflattened_outputs, schema = flatten_to_tuple(outputs)\nflattened_output_tensors = tuple(\n[x for x in flattened_outputs if isinstance(x, torch.Tensor)]\n)\nif len(flattened_output_tensors) < len(flattened_outputs):\nif self.allow_non_tensor:\nflattened_outputs = flattened_output_tensors\nself.outputs_schema = None\nelse:\nraise ValueError(\n\"Model cannot be traced because some model outputs \"\n\"cannot flatten to tensors.\"\n)\nelse:  # schema is valid\n# During torch.onnx.export(), extra outputs that `Schema` implementations\n# can generate (e.g. `InstancesSchema`) are dropped from final ONNX graph.\n# This is OK because ONNX graphs do not need to rebuild original data.\nif (\nlen(flattened_output_tensors) > len(flattened_outputs)\nand self.__is_in_torch_onnx_export\n):\nwarnings.warn(\n\"PyTorch ONNX export (`torch.onnx.export`) detected!\"\n\" To prevent extra outputs in the ONNX graph, the original\"\n\" model output cannot be reconstructed through\"\n\" `adapter.outputs_schema(flattened_outputs)`.\"\n\" For results evaluation, use `torch.jit.trace` instead.\"\n)\nflattened_outputs = flattened_outputs[: len(outputs)]\nif self.outputs_schema is None:\nself.outputs_schema = schema\nelse:\nassert self.outputs_schema == schema, (\n\"Model should always return outputs with the same \"\n\"structure so it can be traced!\"\n)\nreturn flattened_outputs\ndef _create_wrapper(self, traced_model):\n\"\"\"\nReturn a function that has an input/output interface the same as the\noriginal model, but it calls the given traced model under the hood.\n\"\"\"\ndef forward(*args):\nif traced_model._is_in_torch_onnx_export:\nwarnings.warn(\n\"PyTorch ONNX export (`torch.onnx.export`) detected!\"\n\" To prevent extra outputs in the ONNX graph, the original\"\n\" model output cannot be reconstructed through\"\n\" `adapter.outputs_schema(flattened_outputs)`.\"\n\" For results evaluation, use `torch.jit.trace` instead.\"\n)\nflattened_inputs, _ = flatten_to_tuple(args)\nflattened_outputs = traced_model(*flattened_inputs)\nreturn self.outputs_schema(flattened_outputs)\nreturn forward",
                        "max_stars_repo_path": "detectron2/export/flatten.py",
                        "max_stars_repo_name": "thiagocrepaldi/detectron2",
                        "max_stars_count": 0,
                        "__cluster__": 178
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "test_evocodebench_data_49",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nimport io\nimport numpy as np\nimport torch\nfrom detectron2 import model_zoo\nfrom detectron2.config import CfgNode, instantiate\nfrom detectron2.data import DatasetCatalog\nfrom detectron2.data.detection_utils import read_image\nfrom detectron2.modeling import build_model\nfrom detectron2.structures import Boxes, Instances, ROIMasks\nfrom detectron2.utils.file_io import PathManager\n\"\"\"\nInternal utilities for tests. Don't use except for writing tests.\n\"\"\"\ndef get_model_no_weights(config_path):\n\"\"\"\nLike model_zoo.get, but do not load any weights (even pretrained)\n\"\"\"\ncfg = model_zoo.get_config(config_path)\nif isinstance(cfg, CfgNode):\nif not torch.cuda.is_available():\ncfg.MODEL.DEVICE = \"cpu\"\nreturn build_model(cfg)\nelse:\nreturn instantiate(cfg.model)\ndef random_boxes(num_boxes, max_coord=100, device=\"cpu\"):\n\"\"\"\nCreate a random Nx4 boxes tensor, with coordinates < max_coord.\n\"\"\"\nboxes = torch.rand(num_boxes, 4, device=device) * (max_coord * 0.5)\nboxes.clamp_(min=1.0)  # tiny boxes cause numerical instability in box regression\n# Note: the implementation of this function in torchvision is:\n# boxes[:, 2:] += torch.rand(N, 2) * 100\n# but it does not guarantee non-negative widths/heights constraints:\n# boxes[:, 2] >= boxes[:, 0] and boxes[:, 3] >= boxes[:, 1]:\nboxes[:, 2:] += boxes[:, :2]\nreturn boxes\ndef get_sample_coco_image(tensor=True):\n\"\"\"\nArgs:\ntensor (bool): if True, returns 3xHxW tensor.\nelse, returns a HxWx3 numpy array.\nReturns:\nan image, in BGR color.\n\"\"\"\ntry:\nfile_name = DatasetCatalog.get(\"coco_2017_val_100\")[0][\"file_name\"]\nif not PathManager.exists(file_name):\nraise FileNotFoundError()\nexcept IOError:\n# for public CI to run\nfile_name = PathManager.get_local_path(\n\"http://images.cocodataset.org/train2017/000000000009.jpg\"\n)\nret = read_image(file_name, format=\"BGR\")\nif tensor:\nret = torch.from_numpy(np.ascontiguousarray(ret.transpose(2, 0, 1)))\nreturn ret\ndef convert_scripted_instances(instances):\n\"\"\"\nConvert a scripted Instances object to a regular :class:`Instances` object\n\"\"\"\nassert hasattr(\ninstances, \"image_size\"\n), f\"Expect an Instances object, but got {type(instances)}!\"\nret = Instances(instances.image_size)\nfor name in instances._field_names:\nval = getattr(instances, \"_\" + name, None)\nif val is not None:\nret.set(name, val)\nreturn ret\ndef assert_instances_allclose(input, other, *, rtol=1e-5, msg=\"\", size_as_tensor=False):\n\"\"\"\nArgs:\ninput, other (Instances):\nsize_as_tensor: compare image_size of the Instances as tensors (instead of tuples).\nUseful for comparing outputs of tracing.\n\"\"\"\nif not isinstance(input, Instances):\ninput = convert_scripted_instances(input)\nif not isinstance(other, Instances):\nother = convert_scripted_instances(other)\nif not msg:\nmsg = \"Two Instances are different! \"\nelse:\nmsg = msg.rstrip() + \" \"\nsize_error_msg = msg + f\"image_size is {input.image_size} vs. {other.image_size}!\"\nif size_as_tensor:\nassert torch.equal(\ntorch.tensor(input.image_size), torch.tensor(other.image_size)\n), size_error_msg\nelse:\nassert input.image_size == other.image_size, size_error_msg\nfields = sorted(input.get_fields().keys())\nfields_other = sorted(other.get_fields().keys())\nassert fields == fields_other, msg + f\"Fields are {fields} vs {fields_other}!\"\nfor f in fields:\nval1, val2 = input.get(f), other.get(f)\nif isinstance(val1, (Boxes, ROIMasks)):\n# boxes in the range of O(100) and can have a larger tolerance\nassert torch.allclose(val1.tensor, val2.tensor, atol=100 * rtol), (\nmsg + f\"Field {f} differs too much!\"\n)\nelif isinstance(val1, torch.Tensor):\nif val1.dtype.is_floating_point:\nmag = torch.abs(val1).max().cpu().item()\nassert torch.allclose(val1, val2, atol=mag * rtol), (\nmsg + f\"Field {f} differs too much!\"\n)\nelse:\nassert torch.equal(val1, val2), msg + f\"Field {f} is different!\"\nelse:\nraise ValueError(f\"Don't know how to compare type {type(val1)}\")\ndef reload_script_model(module):\n\"\"\"\nSave a jit module and load it back.\nSimilar to the `getExportImportCopy` function in torch/testing/\n\"\"\"\nbuffer = io.BytesIO()\ntorch.jit.save(module, buffer)\nbuffer.seek(0)\nreturn torch.jit.load(buffer)",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 48
                },
                {
                        "id": "pretrain_python_data_7657539",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nimport io\nimport numpy as np\nimport torch\nfrom detectron2 import model_zoo\nfrom detectron2.data import DatasetCatalog\nfrom detectron2.data.detection_utils import read_image\nfrom detectron2.modeling import build_model\nfrom detectron2.structures import Boxes, Instances, ROIMasks\nfrom detectron2.utils.file_io import PathManager\n\"\"\"\nInternal utilities for tests. Don't use except for writing tests.\n\"\"\"\ndef get_model_no_weights(config_path):\n\"\"\"\nLike model_zoo.get, but do not load any weights (even pretrained)\n\"\"\"\ncfg = model_zoo.get_config(config_path)\nif not torch.cuda.is_available():\ncfg.MODEL.DEVICE = \"cpu\"\nreturn build_model(cfg)\ndef random_boxes(num_boxes, max_coord=100, device=\"cpu\"):\n\"\"\"\nCreate a random Nx4 boxes tensor, with coordinates < max_coord.\n\"\"\"\nboxes = torch.rand(num_boxes, 4, device=device) * (max_coord * 0.5)\nboxes.clamp_(min=1.0)  # tiny boxes cause numerical instability in box regression\n# Note: the implementation of this function in torchvision is:\n# boxes[:, 2:] += torch.rand(N, 2) * 100\n# but it does not guarantee non-negative widths/heights constraints:\n# boxes[:, 2] >= boxes[:, 0] and boxes[:, 3] >= boxes[:, 1]:\nboxes[:, 2:] += boxes[:, :2]\nreturn boxes\ndef get_sample_coco_image(tensor=True):\n\"\"\"\nArgs:\ntensor (bool): if True, returns 3xHxW tensor.\nelse, returns a HxWx3 numpy array.\nReturns:\nan image, in BGR color.\n\"\"\"\ntry:\nfile_name = DatasetCatalog.get(\"coco_2017_val_100\")[0][\"file_name\"]\nif not PathManager.exists(file_name):\nraise FileNotFoundError()\nexcept IOError:\n# for public CI to run\nfile_name = PathManager.get_local_path(\n\"http://images.cocodataset.org/train2017/000000000009.jpg\"\n)\nret = read_image(file_name, format=\"BGR\")\nif tensor:\nret = torch.from_numpy(np.ascontiguousarray(ret.transpose(2, 0, 1)))\nreturn ret\ndef convert_scripted_instances(instances):\n\"\"\"\nConvert a scripted Instances object to a regular :class:`Instances` object\n\"\"\"\nassert hasattr(\ninstances, \"image_size\"\n), f\"Expect an Instances object, but got {type(instances)}!\"\nret = Instances(instances.image_size)\nfor name in instances._field_names:\nval = getattr(instances, \"_\" + name, None)\nif val is not None:\nret.set(name, val)\nreturn ret\ndef assert_instances_allclose(input, other, *, rtol=1e-5, msg=\"\", size_as_tensor=False):\n\"\"\"\nArgs:\ninput, other (Instances):\nsize_as_tensor: compare image_size of the Instances as tensors (instead of tuples).\nUseful for comparing outputs of tracing.\n\"\"\"\nif not isinstance(input, Instances):\ninput = convert_scripted_instances(input)\nif not isinstance(other, Instances):\nother = convert_scripted_instances(other)\nif not msg:\nmsg = \"Two Instances are different! \"\nelse:\nmsg = msg.rstrip() + \" \"\nsize_error_msg = msg + f\"image_size is {input.image_size} vs. {other.image_size}!\"\nif size_as_tensor:\nassert torch.equal(\ntorch.tensor(input.image_size), torch.tensor(other.image_size)\n), size_error_msg\nelse:\nassert input.image_size == other.image_size, size_error_msg\nfields = sorted(input.get_fields().keys())\nfields_other = sorted(other.get_fields().keys())\nassert fields == fields_other, msg + f\"Fields are {fields} vs {fields_other}!\"\nfor f in fields:\nval1, val2 = input.get(f), other.get(f)\nif isinstance(val1, (Boxes, ROIMasks)):\n# boxes in the range of O(100) and can have a larger tolerance\nassert torch.allclose(val1.tensor, val2.tensor, atol=100 * rtol), (\nmsg + f\"Field {f} differs too much!\"\n)\nelif isinstance(val1, torch.Tensor):\nif val1.dtype.is_floating_point:\nmag = torch.abs(val1).max().cpu().item()\nassert torch.allclose(val1, val2, atol=mag * rtol), (\nmsg + f\"Field {f} differs too much!\"\n)\nelse:\nassert torch.equal(val1, val2), msg + f\"Field {f} is different!\"\nelse:\nraise ValueError(f\"Don't know how to compare type {type(val1)}\")\ndef reload_script_model(module):\n\"\"\"\nSave a jit module and load it back.\nSimilar to the `getExportImportCopy` function in torch/testing/\n\"\"\"\nbuffer = io.BytesIO()\ntorch.jit.save(module, buffer)\nbuffer.seek(0)\nreturn torch.jit.load(buffer)",
                        "max_stars_repo_path": "detectron2/utils/testing.py",
                        "max_stars_repo_name": "nijkah/detectron2",
                        "max_stars_count": 21274,
                        "__cluster__": 48
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "pretrain_python_data_7657539",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nimport io\nimport numpy as np\nimport torch\nfrom detectron2 import model_zoo\nfrom detectron2.data import DatasetCatalog\nfrom detectron2.data.detection_utils import read_image\nfrom detectron2.modeling import build_model\nfrom detectron2.structures import Boxes, Instances, ROIMasks\nfrom detectron2.utils.file_io import PathManager\n\"\"\"\nInternal utilities for tests. Don't use except for writing tests.\n\"\"\"\ndef get_model_no_weights(config_path):\n\"\"\"\nLike model_zoo.get, but do not load any weights (even pretrained)\n\"\"\"\ncfg = model_zoo.get_config(config_path)\nif not torch.cuda.is_available():\ncfg.MODEL.DEVICE = \"cpu\"\nreturn build_model(cfg)\ndef random_boxes(num_boxes, max_coord=100, device=\"cpu\"):\n\"\"\"\nCreate a random Nx4 boxes tensor, with coordinates < max_coord.\n\"\"\"\nboxes = torch.rand(num_boxes, 4, device=device) * (max_coord * 0.5)\nboxes.clamp_(min=1.0)  # tiny boxes cause numerical instability in box regression\n# Note: the implementation of this function in torchvision is:\n# boxes[:, 2:] += torch.rand(N, 2) * 100\n# but it does not guarantee non-negative widths/heights constraints:\n# boxes[:, 2] >= boxes[:, 0] and boxes[:, 3] >= boxes[:, 1]:\nboxes[:, 2:] += boxes[:, :2]\nreturn boxes\ndef get_sample_coco_image(tensor=True):\n\"\"\"\nArgs:\ntensor (bool): if True, returns 3xHxW tensor.\nelse, returns a HxWx3 numpy array.\nReturns:\nan image, in BGR color.\n\"\"\"\ntry:\nfile_name = DatasetCatalog.get(\"coco_2017_val_100\")[0][\"file_name\"]\nif not PathManager.exists(file_name):\nraise FileNotFoundError()\nexcept IOError:\n# for public CI to run\nfile_name = PathManager.get_local_path(\n\"http://images.cocodataset.org/train2017/000000000009.jpg\"\n)\nret = read_image(file_name, format=\"BGR\")\nif tensor:\nret = torch.from_numpy(np.ascontiguousarray(ret.transpose(2, 0, 1)))\nreturn ret\ndef convert_scripted_instances(instances):\n\"\"\"\nConvert a scripted Instances object to a regular :class:`Instances` object\n\"\"\"\nassert hasattr(\ninstances, \"image_size\"\n), f\"Expect an Instances object, but got {type(instances)}!\"\nret = Instances(instances.image_size)\nfor name in instances._field_names:\nval = getattr(instances, \"_\" + name, None)\nif val is not None:\nret.set(name, val)\nreturn ret\ndef assert_instances_allclose(input, other, *, rtol=1e-5, msg=\"\", size_as_tensor=False):\n\"\"\"\nArgs:\ninput, other (Instances):\nsize_as_tensor: compare image_size of the Instances as tensors (instead of tuples).\nUseful for comparing outputs of tracing.\n\"\"\"\nif not isinstance(input, Instances):\ninput = convert_scripted_instances(input)\nif not isinstance(other, Instances):\nother = convert_scripted_instances(other)\nif not msg:\nmsg = \"Two Instances are different! \"\nelse:\nmsg = msg.rstrip() + \" \"\nsize_error_msg = msg + f\"image_size is {input.image_size} vs. {other.image_size}!\"\nif size_as_tensor:\nassert torch.equal(\ntorch.tensor(input.image_size), torch.tensor(other.image_size)\n), size_error_msg\nelse:\nassert input.image_size == other.image_size, size_error_msg\nfields = sorted(input.get_fields().keys())\nfields_other = sorted(other.get_fields().keys())\nassert fields == fields_other, msg + f\"Fields are {fields} vs {fields_other}!\"\nfor f in fields:\nval1, val2 = input.get(f), other.get(f)\nif isinstance(val1, (Boxes, ROIMasks)):\n# boxes in the range of O(100) and can have a larger tolerance\nassert torch.allclose(val1.tensor, val2.tensor, atol=100 * rtol), (\nmsg + f\"Field {f} differs too much!\"\n)\nelif isinstance(val1, torch.Tensor):\nif val1.dtype.is_floating_point:\nmag = torch.abs(val1).max().cpu().item()\nassert torch.allclose(val1, val2, atol=mag * rtol), (\nmsg + f\"Field {f} differs too much!\"\n)\nelse:\nassert torch.equal(val1, val2), msg + f\"Field {f} is different!\"\nelse:\nraise ValueError(f\"Don't know how to compare type {type(val1)}\")\ndef reload_script_model(module):\n\"\"\"\nSave a jit module and load it back.\nSimilar to the `getExportImportCopy` function in torch/testing/\n\"\"\"\nbuffer = io.BytesIO()\ntorch.jit.save(module, buffer)\nbuffer.seek(0)\nreturn torch.jit.load(buffer)",
                        "max_stars_repo_path": "detectron2/utils/testing.py",
                        "max_stars_repo_name": "nijkah/detectron2",
                        "max_stars_count": 21274,
                        "__cluster__": 48
                },
                {
                        "id": "test_evocodebench_data_191",
                        "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nimport io\nimport numpy as np\nimport torch\nfrom detectron2 import model_zoo\nfrom detectron2.config import CfgNode, instantiate\nfrom detectron2.data import DatasetCatalog\nfrom detectron2.data.detection_utils import read_image\nfrom detectron2.modeling import build_model\nfrom detectron2.structures import Boxes, Instances, ROIMasks\nfrom detectron2.utils.file_io import PathManager\n\"\"\"\nInternal utilities for tests. Don't use except for writing tests.\n\"\"\"\ndef get_model_no_weights(config_path):\n\"\"\"\nLike model_zoo.get, but do not load any weights (even pretrained)\n\"\"\"\ncfg = model_zoo.get_config(config_path)\nif isinstance(cfg, CfgNode):\nif not torch.cuda.is_available():\ncfg.MODEL.DEVICE = \"cpu\"\nreturn build_model(cfg)\nelse:\nreturn instantiate(cfg.model)\ndef random_boxes(num_boxes, max_coord=100, device=\"cpu\"):\n\"\"\"\nCreate a random Nx4 boxes tensor, with coordinates < max_coord.\n\"\"\"\nboxes = torch.rand(num_boxes, 4, device=device) * (max_coord * 0.5)\nboxes.clamp_(min=1.0)  # tiny boxes cause numerical instability in box regression\n# Note: the implementation of this function in torchvision is:\n# boxes[:, 2:] += torch.rand(N, 2) * 100\n# but it does not guarantee non-negative widths/heights constraints:\n# boxes[:, 2] >= boxes[:, 0] and boxes[:, 3] >= boxes[:, 1]:\nboxes[:, 2:] += boxes[:, :2]\nreturn boxes\ndef get_sample_coco_image(tensor=True):\n\"\"\"\nArgs:\ntensor (bool): if True, returns 3xHxW tensor.\nelse, returns a HxWx3 numpy array.\nReturns:\nan image, in BGR color.\n\"\"\"\ntry:\nfile_name = DatasetCatalog.get(\"coco_2017_val_100\")[0][\"file_name\"]\nif not PathManager.exists(file_name):\nraise FileNotFoundError()\nexcept IOError:\n# for public CI to run\nfile_name = PathManager.get_local_path(\n\"http://images.cocodataset.org/train2017/000000000009.jpg\"\n)\nret = read_image(file_name, format=\"BGR\")\nif tensor:\nret = torch.from_numpy(np.ascontiguousarray(ret.transpose(2, 0, 1)))\nreturn ret\ndef convert_scripted_instances(instances):\n\"\"\"\nConvert a scripted Instances object to a regular :class:`Instances` object\n\"\"\"\nassert hasattr(\ninstances, \"image_size\"\n), f\"Expect an Instances object, but got {type(instances)}!\"\nret = Instances(instances.image_size)\nfor name in instances._field_names:\nval = getattr(instances, \"_\" + name, None)\nif val is not None:\nret.set(name, val)\nreturn ret\ndef assert_instances_allclose(input, other, *, rtol=1e-5, msg=\"\", size_as_tensor=False):\n\"\"\"\nArgs:\ninput, other (Instances):\nsize_as_tensor: compare image_size of the Instances as tensors (instead of tuples).\nUseful for comparing outputs of tracing.\n\"\"\"\nif not isinstance(input, Instances):\ninput = convert_scripted_instances(input)\nif not isinstance(other, Instances):\nother = convert_scripted_instances(other)\nif not msg:\nmsg = \"Two Instances are different! \"\nelse:\nmsg = msg.rstrip() + \" \"\nsize_error_msg = msg + f\"image_size is {input.image_size} vs. {other.image_size}!\"\nif size_as_tensor:\nassert torch.equal(\ntorch.tensor(input.image_size), torch.tensor(other.image_size)\n), size_error_msg\nelse:\nassert input.image_size == other.image_size, size_error_msg\nfields = sorted(input.get_fields().keys())\nfields_other = sorted(other.get_fields().keys())\nassert fields == fields_other, msg + f\"Fields are {fields} vs {fields_other}!\"\nfor f in fields:\nval1, val2 = input.get(f), other.get(f)\nif isinstance(val1, (Boxes, ROIMasks)):\n# boxes in the range of O(100) and can have a larger tolerance\nassert torch.allclose(val1.tensor, val2.tensor, atol=100 * rtol), (\nmsg + f\"Field {f} differs too much!\"\n)\nelif isinstance(val1, torch.Tensor):\nif val1.dtype.is_floating_point:\nmag = torch.abs(val1).max().cpu().item()\nassert torch.allclose(val1, val2, atol=mag * rtol), (\nmsg + f\"Field {f} differs too much!\"\n)\nelse:\nassert torch.equal(val1, val2), msg + f\"Field {f} is different!\"\nelse:\nraise ValueError(f\"Don't know how to compare type {type(val1)}\")\ndef reload_script_model(module):\n\"\"\"\nSave a jit module and load it back.\nSimilar to the `getExportImportCopy` function in torch/testing/\n\"\"\"\nbuffer = io.BytesIO()\ntorch.jit.save(module, buffer)\nbuffer.seek(0)\nreturn torch.jit.load(buffer)",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 48
                },
                {
                        "real_dup": "3"
                }
        ],


        [
                {
                        "id": "test_evocodebench_data_186",
                        "content": "# -*- coding: utf-8 -*-\n# Copyright (c) Facebook, Inc. and its affiliates.\n\"\"\"\nSee \"Data Augmentation\" tutorial for an overview of the system:\nhttps://detectron2.readthedocs.io/tutorials/augmentation.html\n\"\"\"\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom fvcore.transforms.transform import (\nCropTransform,\nHFlipTransform,\nNoOpTransform,\nTransform,\nTransformList,\n)\nfrom PIL import Image\ntry:\nimport cv2  # noqa\nexcept ImportError:\n# OpenCV is an optional dependency at the moment\npass\n__all__ = [\n\"ExtentTransform\",\n\"ResizeTransform\",\n\"RotationTransform\",\n\"ColorTransform\",\n\"PILColorTransform\",\n]\nclass ExtentTransform(Transform):\n\"\"\"\nExtracts a subregion from the source image and scales it to the output size.\nThe fill color is used to map pixels from the source rect that fall outside\nthe source image.\nSee: https://pillow.readthedocs.io/en/latest/PIL.html#PIL.ImageTransform.ExtentTransform\n\"\"\"\ndef __init__(self, src_rect, output_size, interp=Image.LINEAR, fill=0):\n\"\"\"\nArgs:\nsrc_rect (x0, y0, x1, y1): src coordinates\noutput_size (h, w): dst image size\ninterp: PIL interpolation methods\nfill: Fill color used when src_rect extends outside image\n\"\"\"\nsuper().__init__()\nself._set_attributes(locals())\ndef apply_image(self, img, interp=None):\nh, w = self.output_size\nif len(img.shape) > 2 and img.shape[2] == 1:\npil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\nelse:\npil_image = Image.fromarray(img)\npil_image = pil_image.transform(\nsize=(w, h),\nmethod=Image.EXTENT,\ndata=self.src_rect,\nresample=interp if interp else self.interp,\nfill=self.fill,\n)\nret = np.asarray(pil_image)\nif len(img.shape) > 2 and img.shape[2] == 1:\nret = np.expand_dims(ret, -1)\nreturn ret\ndef apply_coords(self, coords):\n# Transform image center from source coordinates into output coordinates\n# and then map the new origin to the corner of the output image.\nh, w = self.output_size\nx0, y0, x1, y1 = self.src_rect\nnew_coords = coords.astype(np.float32)\nnew_coords[:, 0] -= 0.5 * (x0 + x1)\nnew_coords[:, 1] -= 0.5 * (y0 + y1)\nnew_coords[:, 0] *= w / (x1 - x0)\nnew_coords[:, 1] *= h / (y1 - y0)\nnew_coords[:, 0] += 0.5 * w\nnew_coords[:, 1] += 0.5 * h\nreturn new_coords\ndef apply_segmentation(self, segmentation):\nsegmentation = self.apply_image(segmentation, interp=Image.NEAREST)\nreturn segmentation\nclass ResizeTransform(Transform):\n\"\"\"\nResize the image to a target size.\n\"\"\"\ndef __init__(self, h, w, new_h, new_w, interp=None):\n\"\"\"\nArgs:\nh, w (int): original image size\nnew_h, new_w (int): new image size\ninterp: PIL interpolation methods, defaults to bilinear.\n\"\"\"\n# TODO decide on PIL vs opencv\nsuper().__init__()\nif interp is None:\ninterp = Image.BILINEAR\nself._set_attributes(locals())\ndef apply_image(self, img, interp=None):\nassert img.shape[:2] == (self.h, self.w)\nassert len(img.shape) <= 4\ninterp_method = interp if interp is not None else self.interp\nif img.dtype == np.uint8:\nif len(img.shape) > 2 and img.shape[2] == 1:\npil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\nelse:\npil_image = Image.fromarray(img)\npil_image = pil_image.resize((self.new_w, self.new_h), interp_method)\nret = np.asarray(pil_image)\nif len(img.shape) > 2 and img.shape[2] == 1:\nret = np.expand_dims(ret, -1)\nelse:\n# PIL only supports uint8\nif any(x < 0 for x in img.strides):\nimg = np.ascontiguousarray(img)\nimg = torch.from_numpy(img)\nshape = list(img.shape)\nshape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\nimg = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n_PIL_RESIZE_TO_INTERPOLATE_MODE = {\nImage.NEAREST: \"nearest\",\nImage.BILINEAR: \"bilinear\",\nImage.BICUBIC: \"bicubic\",\n}\nmode = _PIL_RESIZE_TO_INTERPOLATE_MODE[interp_method]\nalign_corners = None if mode == \"nearest\" else False\nimg = F.interpolate(\nimg, (self.new_h, self.new_w), mode=mode, align_corners=align_corners\n)\nshape[:2] = (self.new_h, self.new_w)\nret = img.permute(2, 3, 0, 1).view(shape).numpy()  # nchw -> hw(c)\nreturn ret\ndef apply_coords(self, coords):\ncoords[:, 0] = coords[:, 0] * (self.new_w * 1.0 / self.w)\ncoords[:, 1] = coords[:, 1] * (self.new_h * 1.0 / self.h)\nreturn coords\ndef apply_segmentation(self, segmentation):\nsegmentation = self.apply_image(segmentation, interp=Image.NEAREST)\nreturn segmentation\ndef inverse(self):\nreturn ResizeTransform(self.new_h, self.new_w, self.h, self.w, self.interp)\nclass RotationTransform(Transform):\n\"\"\"\nThis method returns a copy of this image, rotated the given\nnumber of degrees counter clockwise around its center.\n\"\"\"\ndef __init__(self, h, w, angle, expand=True, center=None, interp=None):\n\"\"\"\nArgs:\nh, w (int): original image size\nangle (float): degrees for rotation\nexpand (bool): choose if the image should be resized to fit the whole\nrotated image (default), or simply cropped\ncenter (tuple (width, height)): coordinates of the rotation center\nif left to None, the center will be fit to the center of each image\ncenter has no effect if expand=True because it only affects shifting\ninterp: cv2 interpolation method, default cv2.INTER_LINEAR\n\"\"\"\nsuper().__init__()\nimage_center = np.array((w / 2, h / 2))\nif center is None:\ncenter = image_center\nif interp is None:\ninterp = cv2.INTER_LINEAR\nabs_cos, abs_sin = (abs(np.cos(np.deg2rad(angle))), abs(np.sin(np.deg2rad(angle))))\nif expand:\n# find the new width and height bounds\nbound_w, bound_h = np.rint(\n[h * abs_sin + w * abs_cos, h * abs_cos + w * abs_sin]\n).astype(int)\nelse:\nbound_w, bound_h = w, h\nself._set_attributes(locals())\nself.rm_coords = self.create_rotation_matrix()\n# Needed because of this problem https://github.com/opencv/opencv/issues/11784\nself.rm_image = self.create_rotation_matrix(offset=-0.5)\ndef apply_image(self, img, interp=None):\n\"\"\"\nimg should be a numpy array, formatted as Height * Width * Nchannels\n\"\"\"\nif len(img) == 0 or self.angle % 360 == 0:\nreturn img\nassert img.shape[:2] == (self.h, self.w)\ninterp = interp if interp is not None else self.interp\nreturn cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\ndef apply_coords(self, coords):\n\"\"\"\ncoords should be a N * 2 array-like, containing N couples of (x, y) points\n\"\"\"\ncoords = np.asarray(coords, dtype=float)\nif len(coords) == 0 or self.angle % 360 == 0:\nreturn coords\nreturn cv2.transform(coords[:, np.newaxis, :], self.rm_coords)[:, 0, :]\ndef apply_segmentation(self, segmentation):\nsegmentation = self.apply_image(segmentation, interp=cv2.INTER_NEAREST)\nreturn segmentation\ndef create_rotation_matrix(self, offset=0):\ncenter = (self.center[0] + offset, self.center[1] + offset)\nrm = cv2.getRotationMatrix2D(tuple(center), self.angle, 1)\nif self.expand:\n# Find the coordinates of the center of rotation in the new image\n# The only point for which we know the future coordinates is the center of the image\nrot_im_center = cv2.transform(self.image_center[None, None, :] + offset, rm)[0, 0, :]\nnew_center = np.array([self.bound_w / 2, self.bound_h / 2]) + offset - rot_im_center\n# shift the rotation center to the new coordinates\nrm[:, 2] += new_center\nreturn rm\ndef inverse(self):\n\"\"\"\nThe inverse is to rotate it back with expand, and crop to get the original shape.\n\"\"\"\nif not self.expand:  # Not possible to inverse if a part of the image is lost\nraise NotImplementedError()\nrotation = RotationTransform(\nself.bound_h, self.bound_w, -self.angle, True, None, self.interp\n)\ncrop = CropTransform(\n(rotation.bound_w - self.w) // 2, (rotation.bound_h - self.h) // 2, self.w, self.h\n)\nreturn TransformList([rotation, crop])\nclass ColorTransform(Transform):\n\"\"\"\nGeneric wrapper for any photometric transforms.\nThese transformations should only affect the color space and\nnot the coordinate space of the image (e.g. annotation\ncoordinates such as bounding boxes should not be changed)\n\"\"\"\ndef __init__(self, op):\n\"\"\"\nArgs:\nop (Callable): operation to be applied to the image,\nwhich takes in an ndarray and returns an ndarray.\n\"\"\"\nif not callable(op):\nraise ValueError(\"op parameter should be callable\")\nsuper().__init__()\nself._set_attributes(locals())\ndef apply_image(self, img):\nreturn self.op(img)\ndef apply_coords(self, coords):\nreturn coords\ndef inverse(self):\nreturn NoOpTransform()\ndef apply_segmentation(self, segmentation):\nreturn segmentation\nclass PILColorTransform(ColorTransform):\n\"\"\"\nGeneric wrapper for PIL Photometric image transforms,\nwhich affect the color space and not the coordinate\nspace of the image\n\"\"\"\ndef __init__(self, op):\n\"\"\"\nArgs:\nop (Callable): operation to be applied to the image,\nwhich takes in a PIL Image and returns a transformed\nPIL Image.\nFor reference on possible operations see:\n- https://pillow.readthedocs.io/en/stable/\n\"\"\"\nif not callable(op):\nraise ValueError(\"op parameter should be callable\")\nsuper().__init__(op)\ndef apply_image(self, img):\nimg = Image.fromarray(img)\nreturn np.asarray(super().apply_image(img))\ndef HFlip_rotated_box(transform, rotated_boxes):\n\"\"\"\nApply the horizontal flip transform on rotated boxes.\nArgs:\nrotated_boxes (ndarray): Nx5 floating point array of\n(x_center, y_center, width, height, angle_degrees) format\nin absolute coordinates.\n\"\"\"\n# Transform x_center\nrotated_boxes[:, 0] = transform.width - rotated_boxes[:, 0]\n# Transform angle\nrotated_boxes[:, 4] = -rotated_boxes[:, 4]\nreturn rotated_boxes\ndef Resize_rotated_box(transform, rotated_boxes):\n\"\"\"\nApply the resizing transform on rotated boxes. For details of how these (approximation)\nformulas are derived, please refer to :meth:`RotatedBoxes.scale`.\nArgs:\nrotated_boxes (ndarray): Nx5 floating point array of\n(x_center, y_center, width, height, angle_degrees) format\nin absolute coordinates.\n\"\"\"\nscale_factor_x = transform.new_w * 1.0 / transform.w\nscale_factor_y = transform.new_h * 1.0 / transform.h\nrotated_boxes[:, 0] *= scale_factor_x\nrotated_boxes[:, 1] *= scale_factor_y\ntheta = rotated_boxes[:, 4] * np.pi / 180.0\nc = np.cos(theta)\ns = np.sin(theta)\nrotated_boxes[:, 2] *= np.sqrt(np.square(scale_factor_x * c) + np.square(scale_factor_y * s))\nrotated_boxes[:, 3] *= np.sqrt(np.square(scale_factor_x * s) + np.square(scale_factor_y * c))\nrotated_boxes[:, 4] = np.arctan2(scale_factor_x * s, scale_factor_y * c) * 180 / np.pi\nreturn rotated_boxes\nHFlipTransform.register_type(\"rotated_box\", HFlip_rotated_box)\nResizeTransform.register_type(\"rotated_box\", Resize_rotated_box)\n# not necessary any more with latest fvcore\nNoOpTransform.register_type(\"rotated_box\", lambda t, x: x)",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 183
                },
                {
                        "id": "pretrain_python_data_7846898",
                        "content": "<filename>detectron2/data/transforms/transform.py\n# -*- coding: utf-8 -*-\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n# File: transform.py\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom fvcore.transforms.transform import HFlipTransform, NoOpTransform, Transform\nfrom PIL import Image\ntry:\nimport cv2  # noqa\nexcept ImportError:\n# OpenCV is an optional dependency at the moment\npass\n__all__ = [\"ExtentTransform\", \"ResizeTransform\", \"RotationTransform\"]\nclass ExtentTransform(Transform):\n\"\"\"\nExtracts a subregion from the source image and scales it to the output size.\nThe fill color is used to map pixels from the source rect that fall outside\nthe source image.\nSee: https://pillow.readthedocs.io/en/latest/PIL.html#PIL.ImageTransform.ExtentTransform\n\"\"\"\ndef __init__(self, src_rect, output_size, interp=Image.LINEAR, fill=0):\n\"\"\"\nArgs:\nsrc_rect (x0, y0, x1, y1): src coordinates\noutput_size (h, w): dst image size\ninterp: PIL interpolation methods\nfill: Fill color used when src_rect extends outside image\n\"\"\"\nsuper().__init__()\nself._set_attributes(locals())\ndef apply_image(self, img, interp=None):\nh, w = self.output_size\nret = Image.fromarray(img).transform(\nsize=(w, h),\nmethod=Image.EXTENT,\ndata=self.src_rect,\nresample=interp if interp else self.interp,\nfill=self.fill,\n)\nreturn np.asarray(ret)\ndef apply_coords(self, coords):\n# Transform image center from source coordinates into output coordinates\n# and then map the new origin to the corner of the output image.\nh, w = self.output_size\nx0, y0, x1, y1 = self.src_rect\nnew_coords = coords.astype(np.float32)\nnew_coords[:, 0] -= 0.5 * (x0 + x1)\nnew_coords[:, 1] -= 0.5 * (y0 + y1)\nnew_coords[:, 0] *= w / (x1 - x0)\nnew_coords[:, 1] *= h / (y1 - y0)\nnew_coords[:, 0] += 0.5 * w\nnew_coords[:, 1] += 0.5 * h\nreturn new_coords\ndef apply_segmentation(self, segmentation):\nsegmentation = self.apply_image(segmentation, interp=Image.NEAREST)\nreturn segmentation\nclass ResizeTransform(Transform):\n\"\"\"\nResize the image to a target size.\n\"\"\"\ndef __init__(self, h, w, new_h, new_w, interp):\n\"\"\"\nArgs:\nh, w (int): original image size\nnew_h, new_w (int): new image size\ninterp: PIL interpolation methods\n\"\"\"\n# TODO decide on PIL vs opencv\nsuper().__init__()\nself._set_attributes(locals())\ndef apply_image(self, img, interp=None):\nassert img.shape[:2] == (self.h, self.w)\nassert len(img.shape) <= 4\nif img.dtype == np.uint8:\npil_image = Image.fromarray(img)\ninterp_method = interp if interp is not None else self.interp\npil_image = pil_image.resize((self.new_w, self.new_h), interp_method)\nret = np.asarray(pil_image)\nelse:\n# PIL only supports uint8\nimg = torch.from_numpy(img)\nshape = list(img.shape)\nshape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\nimg = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n_PIL_RESIZE_TO_INTERPOLATE_MODE = {Image.BILINEAR: \"bilinear\", Image.BICUBIC: \"bicubic\"}\nmode = _PIL_RESIZE_TO_INTERPOLATE_MODE[self.interp]\nimg = F.interpolate(img, (self.new_h, self.new_w), mode=mode, align_corners=False)\nshape[:2] = (self.new_h, self.new_w)\nret = img.permute(2, 3, 0, 1).view(shape).numpy()  # nchw -> hw(c)\nreturn ret\ndef apply_coords(self, coords):\ncoords[:, 0] = coords[:, 0] * (self.new_w * 1.0 / self.w)\ncoords[:, 1] = coords[:, 1] * (self.new_h * 1.0 / self.h)\nreturn coords\ndef apply_segmentation(self, segmentation):\nsegmentation = self.apply_image(segmentation, interp=Image.NEAREST)\nreturn segmentation\nclass RotationTransform(Transform):\n\"\"\"\nThis method returns a copy of this image, rotated the given\nnumber of degrees counter clockwise around its center.\n\"\"\"\ndef __init__(self, h, w, angle, expand=True, center=None, interp=None):\n\"\"\"\nArgs:\nh, w (int): original image size\nangle (float): degrees for rotation\nexpand (bool): choose if the image should be resized to fit the whole\nrotated image (default), or simply cropped\ncenter (tuple (width, height)): coordinates of the rotation center\nif left to None, the center will be fit to the center of each image\ncenter has no effect if expand=True because it only affects shifting\ninterp: cv2 interpolation method, default cv2.INTER_LINEAR\n\"\"\"\nsuper().__init__()\nimage_center = np.array((w / 2, h / 2))\nif center is None:\ncenter = image_center\nif interp is None:\ninterp = cv2.INTER_LINEAR\nabs_cos, abs_sin = abs(np.cos(np.deg2rad(angle))), abs(np.sin(np.deg2rad(angle)))\nif expand:\n# find the new width and height bounds\nbound_w, bound_h = np.rint(\n[h * abs_sin + w * abs_cos, h * abs_cos + w * abs_sin]\n).astype(int)\nelse:\nbound_w, bound_h = w, h\nself._set_attributes(locals())\nself.rm_coords = self.create_rotation_matrix()\n# Needed because of this problem https://github.com/opencv/opencv/issues/11784\nself.rm_image = self.create_rotation_matrix(offset=-0.5)\ndef apply_image(self, img, interp=None):\n\"\"\"\nimg should be a numpy array, formatted as Height * Width * Nchannels\n\"\"\"\nif len(img) == 0 or self.angle % 360 == 0:\nreturn img\nassert img.shape[:2] == (self.h, self.w)\ninterp = interp if interp is not None else self.interp\nreturn cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\ndef apply_coords(self, coords):\n\"\"\"\ncoords should be a N * 2 array-like, containing N couples of (x, y) points\n\"\"\"\ncoords = np.asarray(coords, dtype=float)\nif len(coords) == 0 or self.angle % 360 == 0:\nreturn coords\nreturn cv2.transform(coords[:, np.newaxis, :], self.rm_coords)[:, 0, :]\ndef apply_segmentation(self, segmentation):\nsegmentation = self.apply_image(segmentation, interp=cv2.INTER_NEAREST)\nreturn segmentation\ndef create_rotation_matrix(self, offset=0):\ncenter = (self.center[0] + offset, self.center[1] + offset)\nrm = cv2.getRotationMatrix2D(tuple(center), self.angle, 1)\nif self.expand:\n# Find the coordinates of the center of rotation in the new image\n# The only point for which we know the future coordinates is the center of the image\nrot_im_center = cv2.transform(self.image_center[None, None, :] + offset, rm)[0, 0, :]\nnew_center = np.array([self.bound_w / 2, self.bound_h / 2]) + offset - rot_im_center\n# shift the rotation center to the new coordinates\nrm[:, 2] += new_center\nreturn rm\ndef HFlip_rotated_box(transform, rotated_boxes):\n\"\"\"\nApply the horizontal flip transform on rotated boxes.\nArgs:\nrotated_boxes (ndarray): Nx5 floating point array of\n(x_center, y_center, width, height, angle_degrees) format\nin absolute coordinates.\n\"\"\"\n# Transform x_center\nrotated_boxes[:, 0] = transform.width - rotated_boxes[:, 0]\n# Transform angle\nrotated_boxes[:, 4] = -rotated_boxes[:, 4]\nreturn rotated_boxes\ndef Resize_rotated_box(transform, rotated_boxes):\n\"\"\"\nApply the resizing transform on rotated boxes. For details of how these (approximation)\nformulas are derived, please refer to :meth:`RotatedBoxes.scale`.\nArgs:\nrotated_boxes (ndarray): Nx5 floating point array of\n(x_center, y_center, width, height, angle_degrees) format\nin absolute coordinates.\n\"\"\"\nscale_factor_x = transform.new_w * 1.0 / transform.w\nscale_factor_y = transform.new_h * 1.0 / transform.h\nrotated_boxes[:, 0] *= scale_factor_x\nrotated_boxes[:, 1] *= scale_factor_y\ntheta = rotated_boxes[:, 4] * np.pi / 180.0\nc = np.cos(theta)\ns = np.sin(theta)\nrotated_boxes[:, 2] *= np.sqrt(np.square(scale_factor_x * c) + np.square(scale_factor_y * s))\nrotated_boxes[:, 3] *= np.sqrt(np.square(scale_factor_x * s) + np.square(scale_factor_y * c))\nrotated_boxes[:, 4] = np.arctan2(scale_factor_x * s, scale_factor_y * c) * 180 / np.pi\nreturn rotated_boxes\nHFlipTransform.register_type(\"rotated_box\", HFlip_rotated_box)\nNoOpTransform.register_type(\"rotated_box\", lambda t, x: x)\nResizeTransform.register_type(\"rotated_box\", Resize_rotated_box)",
                        "max_stars_repo_path": "detectron2/data/transforms/transform.py",
                        "max_stars_repo_name": "hanshuobest/detectron2",
                        "max_stars_count": 2,
                        "__cluster__": 183
                },
                {
                        "real_dup": "1"
                }
        ],


        [
                {
                        "id": "test_evocodebench_data_184",
                        "content": "# -*- coding: utf-8 -*-\n# Copyright (c) Facebook, Inc. and its affiliates.\n\"\"\"\nSee \"Data Augmentation\" tutorial for an overview of the system:\nhttps://detectron2.readthedocs.io/tutorials/augmentation.html\n\"\"\"\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom fvcore.transforms.transform import (\nCropTransform,\nHFlipTransform,\nNoOpTransform,\nTransform,\nTransformList,\n)\nfrom PIL import Image\ntry:\nimport cv2  # noqa\nexcept ImportError:\n# OpenCV is an optional dependency at the moment\npass\n__all__ = [\n\"ExtentTransform\",\n\"ResizeTransform\",\n\"RotationTransform\",\n\"ColorTransform\",\n\"PILColorTransform\",\n]\nclass ExtentTransform(Transform):\n\"\"\"\nExtracts a subregion from the source image and scales it to the output size.\nThe fill color is used to map pixels from the source rect that fall outside\nthe source image.\nSee: https://pillow.readthedocs.io/en/latest/PIL.html#PIL.ImageTransform.ExtentTransform\n\"\"\"\ndef __init__(self, src_rect, output_size, interp=Image.LINEAR, fill=0):\n\"\"\"\nArgs:\nsrc_rect (x0, y0, x1, y1): src coordinates\noutput_size (h, w): dst image size\ninterp: PIL interpolation methods\nfill: Fill color used when src_rect extends outside image\n\"\"\"\nsuper().__init__()\nself._set_attributes(locals())\ndef apply_image(self, img, interp=None):\nh, w = self.output_size\nif len(img.shape) > 2 and img.shape[2] == 1:\npil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\nelse:\npil_image = Image.fromarray(img)\npil_image = pil_image.transform(\nsize=(w, h),\nmethod=Image.EXTENT,\ndata=self.src_rect,\nresample=interp if interp else self.interp,\nfill=self.fill,\n)\nret = np.asarray(pil_image)\nif len(img.shape) > 2 and img.shape[2] == 1:\nret = np.expand_dims(ret, -1)\nreturn ret\ndef apply_coords(self, coords):\n# Transform image center from source coordinates into output coordinates\n# and then map the new origin to the corner of the output image.\nh, w = self.output_size\nx0, y0, x1, y1 = self.src_rect\nnew_coords = coords.astype(np.float32)\nnew_coords[:, 0] -= 0.5 * (x0 + x1)\nnew_coords[:, 1] -= 0.5 * (y0 + y1)\nnew_coords[:, 0] *= w / (x1 - x0)\nnew_coords[:, 1] *= h / (y1 - y0)\nnew_coords[:, 0] += 0.5 * w\nnew_coords[:, 1] += 0.5 * h\nreturn new_coords\ndef apply_segmentation(self, segmentation):\nsegmentation = self.apply_image(segmentation, interp=Image.NEAREST)\nreturn segmentation\nclass ResizeTransform(Transform):\n\"\"\"\nResize the image to a target size.\n\"\"\"\ndef __init__(self, h, w, new_h, new_w, interp=None):\n\"\"\"\nArgs:\nh, w (int): original image size\nnew_h, new_w (int): new image size\ninterp: PIL interpolation methods, defaults to bilinear.\n\"\"\"\n# TODO decide on PIL vs opencv\nsuper().__init__()\nif interp is None:\ninterp = Image.BILINEAR\nself._set_attributes(locals())\ndef apply_image(self, img, interp=None):\nassert img.shape[:2] == (self.h, self.w)\nassert len(img.shape) <= 4\ninterp_method = interp if interp is not None else self.interp\nif img.dtype == np.uint8:\nif len(img.shape) > 2 and img.shape[2] == 1:\npil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\nelse:\npil_image = Image.fromarray(img)\npil_image = pil_image.resize((self.new_w, self.new_h), interp_method)\nret = np.asarray(pil_image)\nif len(img.shape) > 2 and img.shape[2] == 1:\nret = np.expand_dims(ret, -1)\nelse:\n# PIL only supports uint8\nif any(x < 0 for x in img.strides):\nimg = np.ascontiguousarray(img)\nimg = torch.from_numpy(img)\nshape = list(img.shape)\nshape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\nimg = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n_PIL_RESIZE_TO_INTERPOLATE_MODE = {\nImage.NEAREST: \"nearest\",\nImage.BILINEAR: \"bilinear\",\nImage.BICUBIC: \"bicubic\",\n}\nmode = _PIL_RESIZE_TO_INTERPOLATE_MODE[interp_method]\nalign_corners = None if mode == \"nearest\" else False\nimg = F.interpolate(\nimg, (self.new_h, self.new_w), mode=mode, align_corners=align_corners\n)\nshape[:2] = (self.new_h, self.new_w)\nret = img.permute(2, 3, 0, 1).view(shape).numpy()  # nchw -> hw(c)\nreturn ret\ndef apply_coords(self, coords):\ncoords[:, 0] = coords[:, 0] * (self.new_w * 1.0 / self.w)\ncoords[:, 1] = coords[:, 1] * (self.new_h * 1.0 / self.h)\nreturn coords\ndef apply_segmentation(self, segmentation):\nsegmentation = self.apply_image(segmentation, interp=Image.NEAREST)\nreturn segmentation\ndef inverse(self):\nreturn ResizeTransform(self.new_h, self.new_w, self.h, self.w, self.interp)\nclass RotationTransform(Transform):\n\"\"\"\nThis method returns a copy of this image, rotated the given\nnumber of degrees counter clockwise around its center.\n\"\"\"\ndef __init__(self, h, w, angle, expand=True, center=None, interp=None):\n\"\"\"\nArgs:\nh, w (int): original image size\nangle (float): degrees for rotation\nexpand (bool): choose if the image should be resized to fit the whole\nrotated image (default), or simply cropped\ncenter (tuple (width, height)): coordinates of the rotation center\nif left to None, the center will be fit to the center of each image\ncenter has no effect if expand=True because it only affects shifting\ninterp: cv2 interpolation method, default cv2.INTER_LINEAR\n\"\"\"\nsuper().__init__()\nimage_center = np.array((w / 2, h / 2))\nif center is None:\ncenter = image_center\nif interp is None:\ninterp = cv2.INTER_LINEAR\nabs_cos, abs_sin = (abs(np.cos(np.deg2rad(angle))), abs(np.sin(np.deg2rad(angle))))\nif expand:\n# find the new width and height bounds\nbound_w, bound_h = np.rint(\n[h * abs_sin + w * abs_cos, h * abs_cos + w * abs_sin]\n).astype(int)\nelse:\nbound_w, bound_h = w, h\nself._set_attributes(locals())\nself.rm_coords = self.create_rotation_matrix()\n# Needed because of this problem https://github.com/opencv/opencv/issues/11784\nself.rm_image = self.create_rotation_matrix(offset=-0.5)\ndef apply_image(self, img, interp=None):\n\"\"\"\nimg should be a numpy array, formatted as Height * Width * Nchannels\n\"\"\"\nif len(img) == 0 or self.angle % 360 == 0:\nreturn img\nassert img.shape[:2] == (self.h, self.w)\ninterp = interp if interp is not None else self.interp\nreturn cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\ndef apply_coords(self, coords):\n\"\"\"\ncoords should be a N * 2 array-like, containing N couples of (x, y) points\n\"\"\"\ncoords = np.asarray(coords, dtype=float)\nif len(coords) == 0 or self.angle % 360 == 0:\nreturn coords\nreturn cv2.transform(coords[:, np.newaxis, :], self.rm_coords)[:, 0, :]\ndef apply_segmentation(self, segmentation):\nsegmentation = self.apply_image(segmentation, interp=cv2.INTER_NEAREST)\nreturn segmentation\ndef create_rotation_matrix(self, offset=0):\ncenter = (self.center[0] + offset, self.center[1] + offset)\nrm = cv2.getRotationMatrix2D(tuple(center), self.angle, 1)\nif self.expand:\n# Find the coordinates of the center of rotation in the new image\n# The only point for which we know the future coordinates is the center of the image\nrot_im_center = cv2.transform(self.image_center[None, None, :] + offset, rm)[0, 0, :]\nnew_center = np.array([self.bound_w / 2, self.bound_h / 2]) + offset - rot_im_center\n# shift the rotation center to the new coordinates\nrm[:, 2] += new_center\nreturn rm\ndef inverse(self):\n\"\"\"\nThe inverse is to rotate it back with expand, and crop to get the original shape.\n\"\"\"\nif not self.expand:  # Not possible to inverse if a part of the image is lost\nraise NotImplementedError()\nrotation = RotationTransform(\nself.bound_h, self.bound_w, -self.angle, True, None, self.interp\n)\ncrop = CropTransform(\n(rotation.bound_w - self.w) // 2, (rotation.bound_h - self.h) // 2, self.w, self.h\n)\nreturn TransformList([rotation, crop])\nclass ColorTransform(Transform):\n\"\"\"\nGeneric wrapper for any photometric transforms.\nThese transformations should only affect the color space and\nnot the coordinate space of the image (e.g. annotation\ncoordinates such as bounding boxes should not be changed)\n\"\"\"\ndef __init__(self, op):\n\"\"\"\nArgs:\nop (Callable): operation to be applied to the image,\nwhich takes in an ndarray and returns an ndarray.\n\"\"\"\nif not callable(op):\nraise ValueError(\"op parameter should be callable\")\nsuper().__init__()\nself._set_attributes(locals())\ndef apply_image(self, img):\nreturn self.op(img)\ndef apply_coords(self, coords):\nreturn coords\ndef inverse(self):\nreturn NoOpTransform()\ndef apply_segmentation(self, segmentation):\nreturn segmentation\nclass PILColorTransform(ColorTransform):\n\"\"\"\nGeneric wrapper for PIL Photometric image transforms,\nwhich affect the color space and not the coordinate\nspace of the image\n\"\"\"\ndef __init__(self, op):\n\"\"\"\nArgs:\nop (Callable): operation to be applied to the image,\nwhich takes in a PIL Image and returns a transformed\nPIL Image.\nFor reference on possible operations see:\n- https://pillow.readthedocs.io/en/stable/\n\"\"\"\nif not callable(op):\nraise ValueError(\"op parameter should be callable\")\nsuper().__init__(op)\ndef apply_image(self, img):\nimg = Image.fromarray(img)\nreturn np.asarray(super().apply_image(img))\ndef HFlip_rotated_box(transform, rotated_boxes):\n\"\"\"\nApply the horizontal flip transform on rotated boxes.\nArgs:\nrotated_boxes (ndarray): Nx5 floating point array of\n(x_center, y_center, width, height, angle_degrees) format\nin absolute coordinates.\n\"\"\"\n# Transform x_center\nrotated_boxes[:, 0] = transform.width - rotated_boxes[:, 0]\n# Transform angle\nrotated_boxes[:, 4] = -rotated_boxes[:, 4]\nreturn rotated_boxes\ndef Resize_rotated_box(transform, rotated_boxes):\n\"\"\"\nApply the resizing transform on rotated boxes. For details of how these (approximation)\nformulas are derived, please refer to :meth:`RotatedBoxes.scale`.\nArgs:\nrotated_boxes (ndarray): Nx5 floating point array of\n(x_center, y_center, width, height, angle_degrees) format\nin absolute coordinates.\n\"\"\"\nscale_factor_x = transform.new_w * 1.0 / transform.w\nscale_factor_y = transform.new_h * 1.0 / transform.h\nrotated_boxes[:, 0] *= scale_factor_x\nrotated_boxes[:, 1] *= scale_factor_y\ntheta = rotated_boxes[:, 4] * np.pi / 180.0\nc = np.cos(theta)\ns = np.sin(theta)\nrotated_boxes[:, 2] *= np.sqrt(np.square(scale_factor_x * c) + np.square(scale_factor_y * s))\nrotated_boxes[:, 3] *= np.sqrt(np.square(scale_factor_x * s) + np.square(scale_factor_y * c))\nrotated_boxes[:, 4] = np.arctan2(scale_factor_x * s, scale_factor_y * c) * 180 / np.pi\nreturn rotated_boxes\nHFlipTransform.register_type(\"rotated_box\", HFlip_rotated_box)\nResizeTransform.register_type(\"rotated_box\", Resize_rotated_box)\n# not necessary any more with latest fvcore\nNoOpTransform.register_type(\"rotated_box\", lambda t, x: x)",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 183
                },
                {
                        "id": "pretrain_python_data_7846898",
                        "content": "<filename>detectron2/data/transforms/transform.py\n# -*- coding: utf-8 -*-\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n# File: transform.py\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom fvcore.transforms.transform import HFlipTransform, NoOpTransform, Transform\nfrom PIL import Image\ntry:\nimport cv2  # noqa\nexcept ImportError:\n# OpenCV is an optional dependency at the moment\npass\n__all__ = [\"ExtentTransform\", \"ResizeTransform\", \"RotationTransform\"]\nclass ExtentTransform(Transform):\n\"\"\"\nExtracts a subregion from the source image and scales it to the output size.\nThe fill color is used to map pixels from the source rect that fall outside\nthe source image.\nSee: https://pillow.readthedocs.io/en/latest/PIL.html#PIL.ImageTransform.ExtentTransform\n\"\"\"\ndef __init__(self, src_rect, output_size, interp=Image.LINEAR, fill=0):\n\"\"\"\nArgs:\nsrc_rect (x0, y0, x1, y1): src coordinates\noutput_size (h, w): dst image size\ninterp: PIL interpolation methods\nfill: Fill color used when src_rect extends outside image\n\"\"\"\nsuper().__init__()\nself._set_attributes(locals())\ndef apply_image(self, img, interp=None):\nh, w = self.output_size\nret = Image.fromarray(img).transform(\nsize=(w, h),\nmethod=Image.EXTENT,\ndata=self.src_rect,\nresample=interp if interp else self.interp,\nfill=self.fill,\n)\nreturn np.asarray(ret)\ndef apply_coords(self, coords):\n# Transform image center from source coordinates into output coordinates\n# and then map the new origin to the corner of the output image.\nh, w = self.output_size\nx0, y0, x1, y1 = self.src_rect\nnew_coords = coords.astype(np.float32)\nnew_coords[:, 0] -= 0.5 * (x0 + x1)\nnew_coords[:, 1] -= 0.5 * (y0 + y1)\nnew_coords[:, 0] *= w / (x1 - x0)\nnew_coords[:, 1] *= h / (y1 - y0)\nnew_coords[:, 0] += 0.5 * w\nnew_coords[:, 1] += 0.5 * h\nreturn new_coords\ndef apply_segmentation(self, segmentation):\nsegmentation = self.apply_image(segmentation, interp=Image.NEAREST)\nreturn segmentation\nclass ResizeTransform(Transform):\n\"\"\"\nResize the image to a target size.\n\"\"\"\ndef __init__(self, h, w, new_h, new_w, interp):\n\"\"\"\nArgs:\nh, w (int): original image size\nnew_h, new_w (int): new image size\ninterp: PIL interpolation methods\n\"\"\"\n# TODO decide on PIL vs opencv\nsuper().__init__()\nself._set_attributes(locals())\ndef apply_image(self, img, interp=None):\nassert img.shape[:2] == (self.h, self.w)\nassert len(img.shape) <= 4\nif img.dtype == np.uint8:\npil_image = Image.fromarray(img)\ninterp_method = interp if interp is not None else self.interp\npil_image = pil_image.resize((self.new_w, self.new_h), interp_method)\nret = np.asarray(pil_image)\nelse:\n# PIL only supports uint8\nimg = torch.from_numpy(img)\nshape = list(img.shape)\nshape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\nimg = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n_PIL_RESIZE_TO_INTERPOLATE_MODE = {Image.BILINEAR: \"bilinear\", Image.BICUBIC: \"bicubic\"}\nmode = _PIL_RESIZE_TO_INTERPOLATE_MODE[self.interp]\nimg = F.interpolate(img, (self.new_h, self.new_w), mode=mode, align_corners=False)\nshape[:2] = (self.new_h, self.new_w)\nret = img.permute(2, 3, 0, 1).view(shape).numpy()  # nchw -> hw(c)\nreturn ret\ndef apply_coords(self, coords):\ncoords[:, 0] = coords[:, 0] * (self.new_w * 1.0 / self.w)\ncoords[:, 1] = coords[:, 1] * (self.new_h * 1.0 / self.h)\nreturn coords\ndef apply_segmentation(self, segmentation):\nsegmentation = self.apply_image(segmentation, interp=Image.NEAREST)\nreturn segmentation\nclass RotationTransform(Transform):\n\"\"\"\nThis method returns a copy of this image, rotated the given\nnumber of degrees counter clockwise around its center.\n\"\"\"\ndef __init__(self, h, w, angle, expand=True, center=None, interp=None):\n\"\"\"\nArgs:\nh, w (int): original image size\nangle (float): degrees for rotation\nexpand (bool): choose if the image should be resized to fit the whole\nrotated image (default), or simply cropped\ncenter (tuple (width, height)): coordinates of the rotation center\nif left to None, the center will be fit to the center of each image\ncenter has no effect if expand=True because it only affects shifting\ninterp: cv2 interpolation method, default cv2.INTER_LINEAR\n\"\"\"\nsuper().__init__()\nimage_center = np.array((w / 2, h / 2))\nif center is None:\ncenter = image_center\nif interp is None:\ninterp = cv2.INTER_LINEAR\nabs_cos, abs_sin = abs(np.cos(np.deg2rad(angle))), abs(np.sin(np.deg2rad(angle)))\nif expand:\n# find the new width and height bounds\nbound_w, bound_h = np.rint(\n[h * abs_sin + w * abs_cos, h * abs_cos + w * abs_sin]\n).astype(int)\nelse:\nbound_w, bound_h = w, h\nself._set_attributes(locals())\nself.rm_coords = self.create_rotation_matrix()\n# Needed because of this problem https://github.com/opencv/opencv/issues/11784\nself.rm_image = self.create_rotation_matrix(offset=-0.5)\ndef apply_image(self, img, interp=None):\n\"\"\"\nimg should be a numpy array, formatted as Height * Width * Nchannels\n\"\"\"\nif len(img) == 0 or self.angle % 360 == 0:\nreturn img\nassert img.shape[:2] == (self.h, self.w)\ninterp = interp if interp is not None else self.interp\nreturn cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\ndef apply_coords(self, coords):\n\"\"\"\ncoords should be a N * 2 array-like, containing N couples of (x, y) points\n\"\"\"\ncoords = np.asarray(coords, dtype=float)\nif len(coords) == 0 or self.angle % 360 == 0:\nreturn coords\nreturn cv2.transform(coords[:, np.newaxis, :], self.rm_coords)[:, 0, :]\ndef apply_segmentation(self, segmentation):\nsegmentation = self.apply_image(segmentation, interp=cv2.INTER_NEAREST)\nreturn segmentation\ndef create_rotation_matrix(self, offset=0):\ncenter = (self.center[0] + offset, self.center[1] + offset)\nrm = cv2.getRotationMatrix2D(tuple(center), self.angle, 1)\nif self.expand:\n# Find the coordinates of the center of rotation in the new image\n# The only point for which we know the future coordinates is the center of the image\nrot_im_center = cv2.transform(self.image_center[None, None, :] + offset, rm)[0, 0, :]\nnew_center = np.array([self.bound_w / 2, self.bound_h / 2]) + offset - rot_im_center\n# shift the rotation center to the new coordinates\nrm[:, 2] += new_center\nreturn rm\ndef HFlip_rotated_box(transform, rotated_boxes):\n\"\"\"\nApply the horizontal flip transform on rotated boxes.\nArgs:\nrotated_boxes (ndarray): Nx5 floating point array of\n(x_center, y_center, width, height, angle_degrees) format\nin absolute coordinates.\n\"\"\"\n# Transform x_center\nrotated_boxes[:, 0] = transform.width - rotated_boxes[:, 0]\n# Transform angle\nrotated_boxes[:, 4] = -rotated_boxes[:, 4]\nreturn rotated_boxes\ndef Resize_rotated_box(transform, rotated_boxes):\n\"\"\"\nApply the resizing transform on rotated boxes. For details of how these (approximation)\nformulas are derived, please refer to :meth:`RotatedBoxes.scale`.\nArgs:\nrotated_boxes (ndarray): Nx5 floating point array of\n(x_center, y_center, width, height, angle_degrees) format\nin absolute coordinates.\n\"\"\"\nscale_factor_x = transform.new_w * 1.0 / transform.w\nscale_factor_y = transform.new_h * 1.0 / transform.h\nrotated_boxes[:, 0] *= scale_factor_x\nrotated_boxes[:, 1] *= scale_factor_y\ntheta = rotated_boxes[:, 4] * np.pi / 180.0\nc = np.cos(theta)\ns = np.sin(theta)\nrotated_boxes[:, 2] *= np.sqrt(np.square(scale_factor_x * c) + np.square(scale_factor_y * s))\nrotated_boxes[:, 3] *= np.sqrt(np.square(scale_factor_x * s) + np.square(scale_factor_y * c))\nrotated_boxes[:, 4] = np.arctan2(scale_factor_x * s, scale_factor_y * c) * 180 / np.pi\nreturn rotated_boxes\nHFlipTransform.register_type(\"rotated_box\", HFlip_rotated_box)\nNoOpTransform.register_type(\"rotated_box\", lambda t, x: x)\nResizeTransform.register_type(\"rotated_box\", Resize_rotated_box)",
                        "max_stars_repo_path": "detectron2/data/transforms/transform.py",
                        "max_stars_repo_name": "hanshuobest/detectron2",
                        "max_stars_count": 2,
                        "__cluster__": 183
                },
                {
                        "real_dup": "1"
                }
        ],


        [
                {
                        "id": "pretrain_python_data_12675138",
                        "content": "import numpy as np\nimport torch\nfrom mmcv.utils import print_log\nfrom terminaltables import AsciiTable\ndef average_precision(recalls, precisions, mode='area'):\n\"\"\"Calculate average precision (for single or multiple scales).\nArgs:\nrecalls (np.ndarray): Recalls with shape of (num_scales, num_dets) \\\nor (num_dets, ).\nprecisions (np.ndarray): Precisions with shape of \\\n(num_scales, num_dets) or (num_dets, ).\nmode (str): 'area' or '11points', 'area' means calculating the area\nunder precision-recall curve, '11points' means calculating\nthe average precision of recalls at [0, 0.1, ..., 1]\nReturns:\nfloat or np.ndarray: Calculated average precision.\n\"\"\"\nif recalls.ndim == 1:\nrecalls = recalls[np.newaxis, :]\nprecisions = precisions[np.newaxis, :]\nassert recalls.shape == precisions.shape\nassert recalls.ndim == 2\nnum_scales = recalls.shape[0]\nap = np.zeros(num_scales, dtype=np.float32)\nif mode == 'area':\nzeros = np.zeros((num_scales, 1), dtype=recalls.dtype)\nones = np.ones((num_scales, 1), dtype=recalls.dtype)\nmrec = np.hstack((zeros, recalls, ones))\nmpre = np.hstack((zeros, precisions, zeros))\nfor i in range(mpre.shape[1] - 1, 0, -1):\nmpre[:, i - 1] = np.maximum(mpre[:, i - 1], mpre[:, i])\nfor i in range(num_scales):\nind = np.where(mrec[i, 1:] != mrec[i, :-1])[0]\nap[i] = np.sum(\n(mrec[i, ind + 1] - mrec[i, ind]) * mpre[i, ind + 1])\nelif mode == '11points':\nfor i in range(num_scales):\nfor thr in np.arange(0, 1 + 1e-3, 0.1):\nprecs = precisions[i, recalls[i, :] >= thr]\nprec = precs.max() if precs.size > 0 else 0\nap[i] += prec\nap /= 11\nelse:\nraise ValueError(\n'Unrecognized mode, only \"area\" and \"11points\" are supported')\nreturn ap\ndef eval_det_cls(pred, gt, iou_thr=None):\n\"\"\"Generic functions to compute precision/recall for object detection for a\nsingle class.\nArgs:\npred (dict): Predictions mapping from image id to bounding boxes \\\nand scores.\ngt (dict): Ground truths mapping from image id to bounding boxes.\niou_thr (list[float]): A list of iou thresholds.\nReturn:\ntuple (np.ndarray, np.ndarray, float): Recalls, precisions and \\\naverage precision.\n\"\"\"\n# {img_id: {'bbox': box structure, 'det': matched list}}\nclass_recs = {}\nnpos = 0\nimg_id_npos = {}\nfor img_id in gt.keys():\ncur_gt_num = len(gt[img_id])\nif cur_gt_num != 0:\ngt_cur = torch.zeros([cur_gt_num, 7], dtype=torch.float32)\nfor i in range(cur_gt_num):\ngt_cur[i] = gt[img_id][i].tensor\nbbox = gt[img_id][0].new_box(gt_cur)\nelse:\nbbox = gt[img_id]\ndet = [[False] * len(bbox) for i in iou_thr]\nnpos += len(bbox)\nimg_id_npos[img_id] = img_id_npos.get(img_id, 0) + len(bbox)\nclass_recs[img_id] = {'bbox': bbox, 'det': det}\n# construct dets\nimage_ids = []\nconfidence = []\nious = []\nfor img_id in pred.keys():\ncur_num = len(pred[img_id])\nif cur_num == 0:\ncontinue\npred_cur = torch.zeros((cur_num, 7), dtype=torch.float32)\nbox_idx = 0\nfor box, score in pred[img_id]:\nimage_ids.append(img_id)\nconfidence.append(score)\npred_cur[box_idx] = box.tensor\nbox_idx += 1\npred_cur = box.new_box(pred_cur)\ngt_cur = class_recs[img_id]['bbox']\nif len(gt_cur) > 0:\n# calculate iou in each image\niou_cur = pred_cur.overlaps(pred_cur, gt_cur)\nfor i in range(cur_num):\nious.append(iou_cur[i])\nelse:\nfor i in range(cur_num):\nious.append(np.zeros(1))\nconfidence = np.array(confidence)\n# sort by confidence\nsorted_ind = np.argsort(-confidence)\nimage_ids = [image_ids[x] for x in sorted_ind]\nious = [ious[x] for x in sorted_ind]\n# go down dets and mark TPs and FPs\nnd = len(image_ids)\ntp_thr = [np.zeros(nd) for i in iou_thr]\nfp_thr = [np.zeros(nd) for i in iou_thr]\nfor d in range(nd):\nR = class_recs[image_ids[d]]\niou_max = -np.inf\nBBGT = R['bbox']\ncur_iou = ious[d]\nif len(BBGT) > 0:\n# compute overlaps\nfor j in range(len(BBGT)):\n# iou = get_iou_main(get_iou_func, (bb, BBGT[j,...]))\niou = cur_iou[j]\nif iou > iou_max:\niou_max = iou\njmax = j\nfor iou_idx, thresh in enumerate(iou_thr):\nif iou_max > thresh:\nif not R['det'][iou_idx][jmax]:\ntp_thr[iou_idx][d] = 1.\nR['det'][iou_idx][jmax] = 1\nelse:\nfp_thr[iou_idx][d] = 1.\nelse:\nfp_thr[iou_idx][d] = 1.\nret = []\n# Return additional information for custom metrics.\nnew_ret = {}\nnew_ret[\"image_ids\"] = image_ids\nnew_ret[\"iou_thr\"] = iou_thr\nnew_ret[\"ious\"] = [max(x.tolist()) for x in ious]\nnew_ret[\"fp_thr\"] = [x.tolist() for x in fp_thr]\nnew_ret[\"tp_thr\"] = [x.tolist() for x in tp_thr]\nnew_ret[\"img_id_npos\"] = img_id_npos\nfor iou_idx, thresh in enumerate(iou_thr):\n# compute precision recall\nfp = np.cumsum(fp_thr[iou_idx])\ntp = np.cumsum(tp_thr[iou_idx])\nrecall = tp / float(npos)\n# avoid divide by zero in case the first detection matches a difficult\n# ground truth\nprecision = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\nap = average_precision(recall, precision)\nret.append((recall, precision, ap))\nreturn ret, new_ret\ndef eval_map_recall(pred, gt, ovthresh=None):\n\"\"\"Evaluate mAP and recall.\nGeneric functions to compute precision/recall for object detection\nfor multiple classes.\nArgs:\npred (dict): Information of detection results,\nwhich maps class_id and predictions.\ngt (dict): Information of ground truths, which maps class_id and \\\nground truths.\novthresh (list[float]): iou threshold.\nDefault: None.\nReturn:\ntuple[dict]: dict results of recall, AP, and precision for all classes.\n\"\"\"\nret_values = {}\nnew_ret_values = {}\nfor classname in gt.keys():\nif classname in pred:\nret_values[classname], new_ret_values[classname] = eval_det_cls(pred[classname],\ngt[classname], ovthresh)\nrecall = [{} for i in ovthresh]\nprecision = [{} for i in ovthresh]\nap = [{} for i in ovthresh]\nfor label in gt.keys():\nfor iou_idx, thresh in enumerate(ovthresh):\nif label in pred:\nrecall[iou_idx][label], precision[iou_idx][label], ap[iou_idx][\nlabel] = ret_values[label][iou_idx]\nelse:\nrecall[iou_idx][label] = np.zeros(1)\nprecision[iou_idx][label] = np.zeros(1)\nap[iou_idx][label] = np.zeros(1)\nreturn recall, precision, ap, new_ret_values\ndef indoor_eval(pts_paths,\ngt_annos,\ndt_annos,\nmetric,\nlabel2cat,\nlogger=None,\nbox_type_3d=None,\nbox_mode_3d=None):\n\"\"\"Indoor Evaluation.\nEvaluate the result of the detection.\nArgs:\ngt_annos (list[dict]): Ground truth annotations.\ndt_annos (list[dict]): Detection annotations. the dict\nincludes the following keys\n- labels_3d (torch.Tensor): Labels of boxes.\n- boxes_3d (:obj:`BaseInstance3DBoxes`): \\\n3D bounding boxes in Depth coordinate.\n- scores_3d (torch.Tensor): Scores of boxes.\nmetric (list[float]): IoU thresholds for computing average precisions.\nlabel2cat (dict): Map from label to category.\nlogger (logging.Logger | str | None): The way to print the mAP\nsummary. See `mmdet.utils.print_log()` for details. Default: None.\nReturn:\ndict[str, float]: Dict of results.\n\"\"\"\nassert len(dt_annos) == len(gt_annos)\npred = {}  # map {class_id: pred}\ngt = {}  # map {class_id: gt}\nfor img_id in range(len(dt_annos)):\n# parse detected annotations\ndet_anno = dt_annos[img_id]\nfor i in range(len(det_anno['labels_3d'])):\nlabel = det_anno['labels_3d'].numpy()[i]\nbbox = det_anno['boxes_3d'].convert_to(box_mode_3d)[i]\nscore = det_anno['scores_3d'].numpy()[i]\nif label not in pred:\npred[int(label)] = {}\nif img_id not in pred[label]:\npred[int(label)][img_id] = []\nif label not in gt:\ngt[int(label)] = {}\nif img_id not in gt[label]:\ngt[int(label)][img_id] = []\npred[int(label)][img_id].append((bbox, score))\n# parse gt annotations\ngt_anno = gt_annos[img_id]\nif gt_anno['gt_num'] != 0:\ngt_boxes = box_type_3d(\ngt_anno['gt_boxes_upright_depth'],\nbox_dim=gt_anno['gt_boxes_upright_depth'].shape[-1],\norigin=(0.5, 0.5, 0.5)).convert_to(box_mode_3d)\nlabels_3d = gt_anno['class']\nelse:\ngt_boxes = box_type_3d(np.array([], dtype=np.float32))\nlabels_3d = np.array([], dtype=np.int64)\nfor i in range(len(labels_3d)):\nlabel = labels_3d[i]\nbbox = gt_boxes[i]\nif label not in gt:\ngt[label] = {}\nif img_id not in gt[label]:\ngt[label][img_id] = []\ngt[label][img_id].append(bbox)\nrec, prec, ap, new_ret_dict = eval_map_recall(pred, gt, metric)\nret_dict = dict()\n# Export additional information for custom metrics calculation.\nret_dict[\"pts_paths\"] = pts_paths\nret_dict[\"new_ret_dict\"] = {label2cat[label] : metrics for label, metrics in new_ret_dict.items()}\nheader = ['classes']\ntable_columns = [[label2cat[label]\nfor label in ap[0].keys()] + ['Overall']]\nfor i, iou_thresh in enumerate(metric):\nheader.append(f'AP_{iou_thresh:.2f}')\nheader.append(f'AR_{iou_thresh:.2f}')\nrec_list = []\nfor label in ap[i].keys():\nret_dict[f'{label2cat[label]}_AP_{iou_thresh:.2f}'] = float(\nap[i][label][0])\nret_dict[f'mAP_{iou_thresh:.2f}'] = float(\nnp.mean(list(ap[i].values())))\ntable_columns.append(list(map(float, list(ap[i].values()))))\ntable_columns[-1] += [ret_dict[f'mAP_{iou_thresh:.2f}']]\ntable_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\nfor label in rec[i].keys():\nret_dict[f'{label2cat[label]}_rec_{iou_thresh:.2f}'] = float(\nrec[i][label][-1])\nrec_list.append(rec[i][label][-1])\nret_dict[f'mAR_{iou_thresh:.2f}'] = float(np.mean(rec_list))\ntable_columns.append(list(map(float, rec_list)))\ntable_columns[-1] += [ret_dict[f'mAR_{iou_thresh:.2f}']]\ntable_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\ntable_data = [header]\ntable_rows = list(zip(*table_columns))\ntable_data += table_rows\ntable = AsciiTable(table_data)\ntable.inner_footing_row_border = True\nprint_log('\\n' + table.table, logger=logger)\nreturn ret_dict",
                        "max_stars_repo_path": "mmdet3d/core/evaluation/indoor_eval.py",
                        "max_stars_repo_name": "mjlbach/fcaf3d",
                        "max_stars_count": 0,
                        "__cluster__": 206
                },
                {
                        "id": "test_evocodebench_data_207",
                        "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport numpy as np\nimport torch\nfrom mmcv.utils import print_log\nfrom terminaltables import AsciiTable\ndef average_precision(recalls, precisions, mode='area'):\n\"\"\"Calculate average precision (for single or multiple scales).\nArgs:\nrecalls (np.ndarray): Recalls with shape of (num_scales, num_dets)\nor (num_dets, ).\nprecisions (np.ndarray): Precisions with shape of\n(num_scales, num_dets) or (num_dets, ).\nmode (str): 'area' or '11points', 'area' means calculating the area\nunder precision-recall curve, '11points' means calculating\nthe average precision of recalls at [0, 0.1, ..., 1]\nReturns:\nfloat or np.ndarray: Calculated average precision.\n\"\"\"\nif recalls.ndim == 1:\nrecalls = recalls[np.newaxis, :]\nprecisions = precisions[np.newaxis, :]\nassert recalls.shape == precisions.shape\nassert recalls.ndim == 2\nnum_scales = recalls.shape[0]\nap = np.zeros(num_scales, dtype=np.float32)\nif mode == 'area':\nzeros = np.zeros((num_scales, 1), dtype=recalls.dtype)\nones = np.ones((num_scales, 1), dtype=recalls.dtype)\nmrec = np.hstack((zeros, recalls, ones))\nmpre = np.hstack((zeros, precisions, zeros))\nfor i in range(mpre.shape[1] - 1, 0, -1):\nmpre[:, i - 1] = np.maximum(mpre[:, i - 1], mpre[:, i])\nfor i in range(num_scales):\nind = np.where(mrec[i, 1:] != mrec[i, :-1])[0]\nap[i] = np.sum(\n(mrec[i, ind + 1] - mrec[i, ind]) * mpre[i, ind + 1])\nelif mode == '11points':\nfor i in range(num_scales):\nfor thr in np.arange(0, 1 + 1e-3, 0.1):\nprecs = precisions[i, recalls[i, :] >= thr]\nprec = precs.max() if precs.size > 0 else 0\nap[i] += prec\nap /= 11\nelse:\nraise ValueError(\n'Unrecognized mode, only \"area\" and \"11points\" are supported')\nreturn ap\ndef eval_det_cls(pred, gt, iou_thr=None):\n\"\"\"Generic functions to compute precision/recall for object detection for a\nsingle class.\nArgs:\npred (dict): Predictions mapping from image id to bounding boxes\nand scores.\ngt (dict): Ground truths mapping from image id to bounding boxes.\niou_thr (list[float]): A list of iou thresholds.\nReturn:\ntuple (np.ndarray, np.ndarray, float): Recalls, precisions and\naverage precision.\n\"\"\"\n# {img_id: {'bbox': box structure, 'det': matched list}}\nclass_recs = {}\nnpos = 0\nfor img_id in gt.keys():\ncur_gt_num = len(gt[img_id])\nif cur_gt_num != 0:\ngt_cur = torch.zeros([cur_gt_num, 7], dtype=torch.float32)\nfor i in range(cur_gt_num):\ngt_cur[i] = gt[img_id][i].tensor\nbbox = gt[img_id][0].new_box(gt_cur)\nelse:\nbbox = gt[img_id]\ndet = [[False] * len(bbox) for i in iou_thr]\nnpos += len(bbox)\nclass_recs[img_id] = {'bbox': bbox, 'det': det}\n# construct dets\nimage_ids = []\nconfidence = []\nious = []\nfor img_id in pred.keys():\ncur_num = len(pred[img_id])\nif cur_num == 0:\ncontinue\npred_cur = torch.zeros((cur_num, 7), dtype=torch.float32)\nbox_idx = 0\nfor box, score in pred[img_id]:\nimage_ids.append(img_id)\nconfidence.append(score)\npred_cur[box_idx] = box.tensor\nbox_idx += 1\npred_cur = box.new_box(pred_cur)\ngt_cur = class_recs[img_id]['bbox']\nif len(gt_cur) > 0:\n# calculate iou in each image\niou_cur = pred_cur.overlaps(pred_cur, gt_cur)\nfor i in range(cur_num):\nious.append(iou_cur[i])\nelse:\nfor i in range(cur_num):\nious.append(np.zeros(1))\nconfidence = np.array(confidence)\n# sort by confidence\nsorted_ind = np.argsort(-confidence)\nimage_ids = [image_ids[x] for x in sorted_ind]\nious = [ious[x] for x in sorted_ind]\n# go down dets and mark TPs and FPs\nnd = len(image_ids)\ntp_thr = [np.zeros(nd) for i in iou_thr]\nfp_thr = [np.zeros(nd) for i in iou_thr]\nfor d in range(nd):\nR = class_recs[image_ids[d]]\niou_max = -np.inf\nBBGT = R['bbox']\ncur_iou = ious[d]\nif len(BBGT) > 0:\n# compute overlaps\nfor j in range(len(BBGT)):\n# iou = get_iou_main(get_iou_func, (bb, BBGT[j,...]))\niou = cur_iou[j]\nif iou > iou_max:\niou_max = iou\njmax = j\nfor iou_idx, thresh in enumerate(iou_thr):\nif iou_max > thresh:\nif not R['det'][iou_idx][jmax]:\ntp_thr[iou_idx][d] = 1.\nR['det'][iou_idx][jmax] = 1\nelse:\nfp_thr[iou_idx][d] = 1.\nelse:\nfp_thr[iou_idx][d] = 1.\nret = []\nfor iou_idx, thresh in enumerate(iou_thr):\n# compute precision recall\nfp = np.cumsum(fp_thr[iou_idx])\ntp = np.cumsum(tp_thr[iou_idx])\nrecall = tp / float(npos)\n# avoid divide by zero in case the first detection matches a difficult\n# ground truth\nprecision = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\nap = average_precision(recall, precision)\nret.append((recall, precision, ap))\nreturn ret\ndef eval_map_recall(pred, gt, ovthresh=None):\n\"\"\"Evaluate mAP and recall.\nGeneric functions to compute precision/recall for object detection\nfor multiple classes.\nArgs:\npred (dict): Information of detection results,\nwhich maps class_id and predictions.\ngt (dict): Information of ground truths, which maps class_id and\nground truths.\novthresh (list[float], optional): iou threshold. Default: None.\nReturn:\ntuple[dict]: dict results of recall, AP, and precision for all classes.\n\"\"\"\nret_values = {}\nfor classname in gt.keys():\nif classname in pred:\nret_values[classname] = eval_det_cls(pred[classname],\ngt[classname], ovthresh)\nrecall = [{} for i in ovthresh]\nprecision = [{} for i in ovthresh]\nap = [{} for i in ovthresh]\nfor label in gt.keys():\nfor iou_idx, thresh in enumerate(ovthresh):\nif label in pred:\nrecall[iou_idx][label], precision[iou_idx][label], ap[iou_idx][\nlabel] = ret_values[label][iou_idx]\nelse:\nrecall[iou_idx][label] = np.zeros(1)\nprecision[iou_idx][label] = np.zeros(1)\nap[iou_idx][label] = np.zeros(1)\nreturn recall, precision, ap\ndef indoor_eval(gt_annos,\ndt_annos,\nmetric,\nlabel2cat,\nlogger=None,\nbox_type_3d=None,\nbox_mode_3d=None):\n\"\"\"Indoor Evaluation.\nEvaluate the result of the detection.\nArgs:\ngt_annos (list[dict]): Ground truth annotations.\ndt_annos (list[dict]): Detection annotations. the dict\nincludes the following keys\n- labels_3d (torch.Tensor): Labels of boxes.\n- boxes_3d (:obj:`BaseInstance3DBoxes`):\n3D bounding boxes in Depth coordinate.\n- scores_3d (torch.Tensor): Scores of boxes.\nmetric (list[float]): IoU thresholds for computing average precisions.\nlabel2cat (dict): Map from label to category.\nlogger (logging.Logger | str, optional): The way to print the mAP\nsummary. See `mmdet.utils.print_log()` for details. Default: None.\nReturn:\ndict[str, float]: Dict of results.\n\"\"\"\nassert len(dt_annos) == len(gt_annos)\npred = {}  # map {class_id: pred}\ngt = {}  # map {class_id: gt}\nfor img_id in range(len(dt_annos)):\n# parse detected annotations\ndet_anno = dt_annos[img_id]\nfor i in range(len(det_anno['labels_3d'])):\nlabel = det_anno['labels_3d'].numpy()[i]\nbbox = det_anno['boxes_3d'].convert_to(box_mode_3d)[i]\nscore = det_anno['scores_3d'].numpy()[i]\nif label not in pred:\npred[int(label)] = {}\nif img_id not in pred[label]:\npred[int(label)][img_id] = []\nif label not in gt:\ngt[int(label)] = {}\nif img_id not in gt[label]:\ngt[int(label)][img_id] = []\npred[int(label)][img_id].append((bbox, score))\n# parse gt annotations\ngt_anno = gt_annos[img_id]\nif gt_anno['gt_num'] != 0:\ngt_boxes = box_type_3d(\ngt_anno['gt_boxes_upright_depth'],\nbox_dim=gt_anno['gt_boxes_upright_depth'].shape[-1],\norigin=(0.5, 0.5, 0.5)).convert_to(box_mode_3d)\nlabels_3d = gt_anno['class']\nelse:\ngt_boxes = box_type_3d(np.array([], dtype=np.float32))\nlabels_3d = np.array([], dtype=np.int64)\nfor i in range(len(labels_3d)):\nlabel = labels_3d[i]\nbbox = gt_boxes[i]\nif label not in gt:\ngt[label] = {}\nif img_id not in gt[label]:\ngt[label][img_id] = []\ngt[label][img_id].append(bbox)\nrec, prec, ap = eval_map_recall(pred, gt, metric)\nret_dict = dict()\nheader = ['classes']\ntable_columns = [[label2cat[label]\nfor label in ap[0].keys()] + ['Overall']]\nfor i, iou_thresh in enumerate(metric):\nheader.append(f'AP_{iou_thresh:.2f}')\nheader.append(f'AR_{iou_thresh:.2f}')\nrec_list = []\nfor label in ap[i].keys():\nret_dict[f'{label2cat[label]}_AP_{iou_thresh:.2f}'] = float(\nap[i][label][0])\nret_dict[f'mAP_{iou_thresh:.2f}'] = float(\nnp.mean(list(ap[i].values())))\ntable_columns.append(list(map(float, list(ap[i].values()))))\ntable_columns[-1] += [ret_dict[f'mAP_{iou_thresh:.2f}']]\ntable_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\nfor label in rec[i].keys():\nret_dict[f'{label2cat[label]}_rec_{iou_thresh:.2f}'] = float(\nrec[i][label][-1])\nrec_list.append(rec[i][label][-1])\nret_dict[f'mAR_{iou_thresh:.2f}'] = float(np.mean(rec_list))\ntable_columns.append(list(map(float, rec_list)))\ntable_columns[-1] += [ret_dict[f'mAR_{iou_thresh:.2f}']]\ntable_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\ntable_data = [header]\ntable_rows = list(zip(*table_columns))\ntable_data += table_rows\ntable = AsciiTable(table_data)\ntable.inner_footing_row_border = True\nprint_log('\\n' + table.table, logger=logger)\nreturn ret_dict",
                        "max_stars_repo_path": "NA",
                        "max_stars_repo_name": "NA",
                        "max_stars_count": 0,
                        "__cluster__": 206
                },
                {
                        "real_dup": "1"
                }
        ]
]