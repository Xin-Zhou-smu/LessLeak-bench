[
    [],
    [],
    [],
    [
        [
            {
                "id": "test_keras-bug-3",
                "content": "\"\"\"Model-related utilities.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom . import backend as K\nfrom .utils.generic_utils import has_arg\nfrom .utils.generic_utils import to_list\nfrom .engine.input_layer import Input\nfrom .engine.input_layer import InputLayer\nfrom .engine.training import Model\nfrom .engine.sequential import Sequential\nfrom .engine.saving import save_model\nfrom .engine.saving import load_model\nfrom .engine.saving import model_from_config\nfrom .engine.saving import model_from_yaml\nfrom .engine.saving import model_from_json\n\ntry:\n    import h5py\nexcept ImportError:\n    h5py = None\n\n\ndef _clone_functional_model(model, input_tensors=None):\n    \"\"\"Clone a functional `Model` instance.\n\n    Model cloning is similar to calling a model on new inputs,\n    except that it creates new layers (and thus new weights) instead\n    of sharing the weights of the existing layers.\n\n    # Arguments\n        model: Instance of `Model`.\n        input_tensors: optional list of input tensors\n            to build the model upon. If not provided,\n            placeholders will be created.\n\n    # Returns\n        An instance of `Model` reproducing the behavior\n        of the original model, on top of new inputs tensors,\n        using newly instantiated weights.\n\n    # Raises\n        ValueError: in case of invalid `model` argument value.\n    \"\"\"\n    if not isinstance(model, Model):\n        raise ValueError('Expected `model` argument '\n                         'to be a `Model` instance, got ', model)\n    if isinstance(model, Sequential):\n        raise ValueError('Expected `model` argument '\n                         'to be a functional `Model` instance, '\n                         'got a `Sequential` instance instead:', model)\n\n    layer_map = {}  # Cache for created layers.\n    tensor_map = {}  # Map {reference_tensor: (corresponding_tensor, mask)}\n    if input_tensors is None:\n        # Create placeholders to build the model on top of.\n        input_layers = []\n        input_tensors = []\n        for layer in model._input_layers:\n            input_tensor = Input(batch_shape=layer.batch_input_shape,\n                                 dtype=layer.dtype,\n                                 sparse=layer.sparse,\n                                 name=layer.name)\n            input_tensors.append(input_tensor)\n            # Cache newly created input layer.\n            newly_created_input_layer = input_tensor._keras_history[0]\n            layer_map[layer] = newly_created_input_layer\n        for _original, _cloned in zip(model._input_layers, input_layers):\n            layer_map[_original] = _cloned\n    else:\n        # Make sure that all input tensors come from a Keras layer.\n        # If tensor comes from an input layer: cache the input layer.\n        input_tensors = to_list(input_tensors)\n        _input_tensors = []\n        for i, x in enumerate(input_tensors):\n            if not K.is_keras_tensor(x):\n                name = model._input_layers[i].name\n                input_tensor = Input(tensor=x,\n                                     name='input_wrapper_for_' + name)\n                _input_tensors.append(input_tensor)\n                # Cache newly created input layer.\n                original_input_layer = x._keras_history[0]\n                newly_created_input_layer = input_tensor._keras_history[0]\n                layer_map[original_input_layer] = newly_created_input_layer\n            else:\n                _input_tensors.append(x)\n        input_tensors = _input_tensors\n\n    for x, y in zip(model.inputs, input_tensors):\n        tensor_map[x] = (y, None)  # tensor, mask\n\n    # Iterated over every node in the reference model, in depth order.\n    depth_keys = list(model._nodes_by_depth.keys())\n    depth_keys.sort(reverse=True)\n    for depth in depth_keys:\n        nodes = model._nodes_by_depth[depth]\n        for node in nodes:\n            # Recover the corresponding layer.\n            layer = node.outbound_layer\n\n            # Get or create layer.\n            if layer not in layer_map:\n                # Clone layer.\n                new_layer = layer.__class__.from_config(layer.get_config())\n                layer_map[layer] = new_layer\n                layer = new_layer\n            else:\n                # Reuse previously cloned layer.\n                layer = layer_map[layer]\n                # Don't call InputLayer multiple times.\n                if isinstance(layer, InputLayer):\n                    continue\n\n            # Gather inputs to call the new layer.\n            reference_input_tensors = node.input_tensors\n            reference_output_tensors = node.output_tensors\n\n            # If all previous input tensors are available in tensor_map,\n            # then call node.inbound_layer on them.\n            computed_data = []  # List of tuples (input, mask).\n            for x in reference_input_tensors:\n                if x in tensor_map:\n                    computed_data.append(tensor_map[x])\n\n            if len(computed_data) == len(reference_input_tensors):\n                # Call layer.\n                if node.arguments:\n                    kwargs = node.arguments\n                else:\n                    kwargs = {}\n                if len(computed_data) == 1:\n                    computed_tensor, computed_mask = computed_data[0]\n                    if has_arg(layer.call, 'mask'):\n                        if 'mask' not in kwargs:\n                            kwargs['mask'] = computed_mask\n                    output_tensors = to_list(\n                        layer(computed_tensor, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensor,\n                                           computed_mask))\n                    computed_tensors = [computed_tensor]\n                    computed_masks = [computed_mask]\n                else:\n                    computed_tensors = [x[0] for x in computed_data]\n                    computed_masks = [x[1] for x in computed_data]\n                    if has_arg(layer.call, 'mask'):\n                        if 'mask' not in kwargs:\n                            kwargs['mask'] = computed_masks\n                    output_tensors = to_list(\n                        layer(computed_tensors, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensors,\n                                           computed_masks))\n                # Update tensor_map.\n                for x, y, mask in zip(reference_output_tensors,\n                                      output_tensors,\n                                      output_masks):\n                    tensor_map[x] = (y, mask)\n\n    # Check that we did compute the model outputs,\n    # then instantiate a new model from inputs and outputs.\n    output_tensors = []\n    for x in model.outputs:\n        assert x in tensor_map, 'Could not compute output ' + str(x)\n        tensor, _ = tensor_map[x]\n        output_tensors.append(tensor)\n    return Model(input_tensors, output_tensors, name=model.name)\n\n\ndef _clone_sequential_model(model, input_tensors=None):\n    \"\"\"Clone a `Sequential` model instance.\n\n    Model cloning is similar to calling a model on new inputs,\n    except that it creates new layers (and thus new weights) instead\n    of sharing the weights of the existing layers.\n\n    # Arguments\n        model: Instance of `Sequential`.\n        input_tensors: optional list of input tensors\n            to build the model upon. If not provided,\n            placeholders will be created.\n\n    # Returns\n        An instance of `Sequential` reproducing the behavior\n        of the original model, on top of new inputs tensors,\n        using newly instantiated weights.\n\n    # Raises\n        ValueError: in case of invalid `model` argument value.\n    \"\"\"\n    if not isinstance(model, Sequential):\n        raise ValueError('Expected `model` argument '\n                         'to be a `Sequential` model instance, '\n                         'but got:', model)\n\n    def clone(layer):\n        return layer.__class__.from_config(layer.get_config())\n\n    layers = [clone(layer) for layer in model.layers]\n    if input_tensors is None:\n        return Sequential(layers=layers, name=model.name)\n    else:\n        if len(to_list(input_tensors)) != 1:\n            raise ValueError('To clone a `Sequential` model, we expect '\n                             ' at most one tensor '\n                             'as part of `input_tensors`.')\n        x = to_list(input_tensors)[0]\n        if K.is_keras_tensor(x):\n            origin_layer = x._keras_history[0]\n            if isinstance(origin_layer, InputLayer):\n                return Sequential(layers=[origin_layer] + layers,\n                                  name=model.name)\n            else:\n                raise ValueError('Cannot clone a `Sequential` model on top '\n                                 'of a tensor that comes from a Keras layer '\n                                 'other than an `InputLayer`. '\n                                 'Use the functional API instead.')\n        input_tensor = Input(tensor=x,\n                             name='input_wrapper_for_' + str(x.name))\n        input_layer = input_tensor._keras_history[0]\n        return Sequential(layers=[input_layer] + layers, name=model.name)\n\n\ndef clone_model(model, input_tensors=None):\n    \"\"\"Clone any `Model` instance.\n\n    Model cloning is similar to calling a model on new inputs,\n    except that it creates new layers (and thus new weights) instead\n    of sharing the weights of the existing layers.\n\n    # Arguments\n        model: Instance of `Model`\n            (could be a functional model or a Sequential model).\n        input_tensors: optional list of input tensors\n            to build the model upon. If not provided,\n            placeholders will be created.\n\n    # Returns\n        An instance of `Model` reproducing the behavior\n        of the original model, on top of new inputs tensors,\n        using newly instantiated weights.\n\n    # Raises\n        ValueError: in case of invalid `model` argument value.\n    \"\"\"\n    if isinstance(model, Sequential):\n        return _clone_sequential_model(model, input_tensors=input_tensors)\n    else:\n        return _clone_functional_model(model, input_tensors=input_tensors)\n\n\n\"\"\"Model-related utilities.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom . import backend as K\nfrom .utils.generic_utils import has_arg\nfrom .utils.generic_utils import to_list\nfrom .engine.input_layer import Input\nfrom .engine.input_layer import InputLayer\nfrom .engine.training import Model\nfrom .engine.sequential import Sequential\nfrom .engine.saving import save_model\nfrom .engine.saving import load_model\nfrom .engine.saving import model_from_config\nfrom .engine.saving import model_from_yaml\nfrom .engine.saving import model_from_json\n\ntry:\n    import h5py\nexcept ImportError:\n    h5py = None\n\n\ndef _clone_functional_model(model, input_tensors=None):\n    \"\"\"Clone a functional `Model` instance.\n\n    Model cloning is similar to calling a model on new inputs,\n    except that it creates new layers (and thus new weights) instead\n    of sharing the weights of the existing layers.\n\n    # Arguments\n        model: Instance of `Model`.\n        input_tensors: optional list of input tensors\n            to build the model upon. If not provided,\n            placeholders will be created.\n\n    # Returns\n        An instance of `Model` reproducing the behavior\n        of the original model, on top of new inputs tensors,\n        using newly instantiated weights.\n\n    # Raises\n        ValueError: in case of invalid `model` argument value.\n    \"\"\"\n    if not isinstance(model, Model):\n        raise ValueError('Expected `model` argument '\n                         'to be a `Model` instance, got ', model)\n    if isinstance(model, Sequential):\n        raise ValueError('Expected `model` argument '\n                         'to be a functional `Model` instance, '\n                         'got a `Sequential` instance instead:', model)\n\n    layer_map = {}  # Cache for created layers.\n    tensor_map = {}  # Map {reference_tensor: (corresponding_tensor, mask)}\n    if input_tensors is None:\n        # Create placeholders to build the model on top of.\n        input_layers = []\n        input_tensors = []\n        for layer in model._input_layers:\n            input_tensor = Input(batch_shape=layer.batch_input_shape,\n                                 dtype=layer.dtype,\n                                 sparse=layer.sparse,\n                                 name=layer.name)\n            input_tensors.append(input_tensor)\n            # Cache newly created input layer.\n            newly_created_input_layer = input_tensor._keras_history[0]\n            layer_map[layer] = newly_created_input_layer\n        for _original, _cloned in zip(model._input_layers, input_layers):\n            layer_map[_original] = _cloned\n    else:\n        # Make sure that all input tensors come from a Keras layer.\n        # If tensor comes from an input layer: cache the input layer.\n        input_tensors = to_list(input_tensors)\n        _input_tensors = []\n        for i, x in enumerate(input_tensors):\n            if not K.is_keras_tensor(x):\n                name = model._input_layers[i].name\n                input_tensor = Input(tensor=x,\n                                     name='input_wrapper_for_' + name)\n                _input_tensors.append(input_tensor)\n                # Cache newly created input layer.\n                original_input_layer = x._keras_history[0]\n                newly_created_input_layer = input_tensor._keras_history[0]\n                layer_map[original_input_layer] = newly_created_input_layer\n            else:\n                _input_tensors.append(x)\n        input_tensors = _input_tensors\n\n    for x, y in zip(model.inputs, input_tensors):\n        tensor_map[x] = (y, None)  # tensor, mask\n\n    # Iterated over every node in the reference model, in depth order.\n    depth_keys = list(model._nodes_by_depth.keys())\n    depth_keys.sort(reverse=True)\n    for depth in depth_keys:\n        nodes = model._nodes_by_depth[depth]\n        for node in nodes:\n            # Recover the corresponding layer.\n            layer = node.outbound_layer\n\n            # Get or create layer.\n            if layer not in layer_map:\n                # Clone layer.\n                new_layer = layer.__class__.from_config(layer.get_config())\n                layer_map[layer] = new_layer\n                layer = new_layer\n            else:\n                # Reuse previously cloned layer.\n                layer = layer_map[layer]\n                # Don't call InputLayer multiple times.\n                if isinstance(layer, InputLayer):\n                    continue\n\n            # Gather inputs to call the new layer.\n            reference_input_tensors = node.input_tensors\n            reference_output_tensors = node.output_tensors\n\n            # If all previous input tensors are available in tensor_map,\n            # then call node.inbound_layer on them.\n            computed_data = []  # List of tuples (input, mask).\n            for x in reference_input_tensors:\n                if x in tensor_map:\n                    computed_data.append(tensor_map[x])\n\n            if len(computed_data) == len(reference_input_tensors):\n                # Call layer.\n                if node.arguments:\n                    kwargs = node.arguments\n                else:\n                    kwargs = {}\n                if len(computed_data) == 1:\n                    computed_tensor, computed_mask = computed_data[0]\n                    if has_arg(layer.call, 'mask'):\n                        if 'mask' not in kwargs:\n                            kwargs['mask'] = computed_mask\n                    output_tensors = to_list(\n                        layer(computed_tensor, **kwargs))\n                    if layer.supports_masking:\n                        output_masks = to_list(\n                            layer.compute_mask(computed_tensor,\n                                               computed_mask))\n                    else:\n                        output_masks = [None] * len(output_tensors)\n                    computed_tensors = [computed_tensor]\n                    computed_masks = [computed_mask]\n                else:\n                    computed_tensors = [x[0] for x in computed_data]\n                    computed_masks = [x[1] for x in computed_data]\n                    if has_arg(layer.call, 'mask'):\n                        if 'mask' not in kwargs:\n                            kwargs['mask'] = computed_masks\n                    output_tensors = to_list(\n                        layer(computed_tensors, **kwargs))\n                    if layer.supports_masking:\n                        output_masks = to_list(\n                            layer.compute_mask(computed_tensors,\n                                               computed_masks))\n                    else:\n                        output_masks = [None] * len(output_tensors)\n                # Update tensor_map.\n                for x, y, mask in zip(reference_output_tensors,\n                                      output_tensors,\n                                      output_masks):\n                    tensor_map[x] = (y, mask)\n\n    # Check that we did compute the model outputs,\n    # then instantiate a new model from inputs and outputs.\n    output_tensors = []\n    for x in model.outputs:\n        assert x in tensor_map, 'Could not compute output ' + str(x)\n        tensor, _ = tensor_map[x]\n        output_tensors.append(tensor)\n    return Model(input_tensors, output_tensors, name=model.name)\n\n\ndef _clone_sequential_model(model, input_tensors=None):\n    \"\"\"Clone a `Sequential` model instance.\n\n    Model cloning is similar to calling a model on new inputs,\n    except that it creates new layers (and thus new weights) instead\n    of sharing the weights of the existing layers.\n\n    # Arguments\n        model: Instance of `Sequential`.\n        input_tensors: optional list of input tensors\n            to build the model upon. If not provided,\n            placeholders will be created.\n\n    # Returns\n        An instance of `Sequential` reproducing the behavior\n        of the original model, on top of new inputs tensors,\n        using newly instantiated weights.\n\n    # Raises\n        ValueError: in case of invalid `model` argument value.\n    \"\"\"\n    if not isinstance(model, Sequential):\n        raise ValueError('Expected `model` argument '\n                         'to be a `Sequential` model instance, '\n                         'but got:', model)\n\n    def clone(layer):\n        return layer.__class__.from_config(layer.get_config())\n\n    layers = [clone(layer) for layer in model.layers]\n    if input_tensors is None:\n        return Sequential(layers=layers, name=model.name)\n    else:\n        if len(to_list(input_tensors)) != 1:\n            raise ValueError('To clone a `Sequential` model, we expect '\n                             ' at most one tensor '\n                             'as part of `input_tensors`.')\n        x = to_list(input_tensors)[0]\n        if K.is_keras_tensor(x):\n            origin_layer = x._keras_history[0]\n            if isinstance(origin_layer, InputLayer):\n                return Sequential(layers=[origin_layer] + layers,\n                                  name=model.name)\n            else:\n                raise ValueError('Cannot clone a `Sequential` model on top '\n                                 'of a tensor that comes from a Keras layer '\n                                 'other than an `InputLayer`. '\n                                 'Use the functional API instead.')\n        input_tensor = Input(tensor=x,\n                             name='input_wrapper_for_' + str(x.name))\n        input_layer = input_tensor._keras_history[0]\n        return Sequential(layers=[input_layer] + layers, name=model.name)\n\n\ndef clone_model(model, input_tensors=None):\n    \"\"\"Clone any `Model` instance.\n\n    Model cloning is similar to calling a model on new inputs,\n    except that it creates new layers (and thus new weights) instead\n    of sharing the weights of the existing layers.\n\n    # Arguments\n        model: Instance of `Model`\n            (could be a functional model or a Sequential model).\n        input_tensors: optional list of input tensors\n            to build the model upon. If not provided,\n            placeholders will be created.\n\n    # Returns\n        An instance of `Model` reproducing the behavior\n        of the original model, on top of new inputs tensors,\n        using newly instantiated weights.\n\n    # Raises\n        ValueError: in case of invalid `model` argument value.\n    \"\"\"\n    if isinstance(model, Sequential):\n        return _clone_sequential_model(model, input_tensors=input_tensors)\n    else:\n        return _clone_functional_model(model, input_tensors=input_tensors)\n",
                "max_stars_repo_path": "BugsInPy/BugsInPy/temp/projects/keras/bug-3-fixed/keras/keras/models.py,BugsInPy/BugsInPy/temp/projects/keras/bug-3-buggy/keras/keras/models.py",
                "max_stars_repo_name": "NA",
                "max_stars_count": 0,
                "__cluster__": 17
            },
            {
                "id": "pretrain_python_data_5455",
                "content": "<filename>keras/models.py\n\"\"\"Model-related utilities.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom . import backend as K\nfrom .utils.generic_utils import has_arg\nfrom .utils.generic_utils import to_list\nfrom .engine.input_layer import Input\nfrom .engine.input_layer import InputLayer\nfrom .engine.training import Model\nfrom .engine.sequential import Sequential\nfrom .engine.saving import save_model\nfrom .engine.saving import load_model\nfrom .engine.saving import model_from_config\nfrom .engine.saving import model_from_yaml\nfrom .engine.saving import model_from_json\nfrom .engine.saving import save_mxnet_model\n\ntry:\n    import h5py\nexcept ImportError:\n    h5py = None\n\n\ndef _clone_functional_model(model, input_tensors=None):\n    \"\"\"Clone a functional `Model` instance.\n\n    Model cloning is similar to calling a model on new inputs,\n    except that it creates new layers (and thus new weights) instead\n    of sharing the weights of the existing layers.\n\n    # Arguments\n        model: Instance of `Model`.\n        input_tensors: optional list of input tensors\n            to build the model upon. If not provided,\n            placeholders will be created.\n\n    # Returns\n        An instance of `Model` reproducing the behavior\n        of the original model, on top of new inputs tensors,\n        using newly instantiated weights.\n\n    # Raises\n        ValueError: in case of invalid `model` argument value.\n    \"\"\"\n    if not isinstance(model, Model):\n        raise ValueError('Expected `model` argument '\n                         'to be a `Model` instance, got ', model)\n    if isinstance(model, Sequential):\n        raise ValueError('Expected `model` argument '\n                         'to be a functional `Model` instance, '\n                         'got a `Sequential` instance instead:', model)\n\n    layer_map = {}  # Cache for created layers.\n    tensor_map = {}  # Map {reference_tensor: (corresponding_tensor, mask)}\n    if input_tensors is None:\n        # Create placeholders to build the model on top of.\n        input_layers = []\n        input_tensors = []\n        for layer in model._input_layers:\n            input_tensor = Input(batch_shape=layer.batch_input_shape,\n                                 dtype=layer.dtype,\n                                 sparse=layer.sparse,\n                                 name=layer.name)\n            input_tensors.append(input_tensor)\n            # Cache newly created input layer.\n            newly_created_input_layer = input_tensor._keras_history[0]\n            layer_map[layer] = newly_created_input_layer\n        for _original, _cloned in zip(model._input_layers, input_layers):\n            layer_map[_original] = _cloned\n    else:\n        # Make sure that all input tensors come from a Keras layer.\n        # If tensor comes from an input layer: cache the input layer.\n        input_tensors = to_list(input_tensors)\n        _input_tensors = []\n        for i, x in enumerate(input_tensors):\n            if not K.is_keras_tensor(x):\n                name = model._input_layers[i].name\n                input_tensor = Input(tensor=x,\n                                     name='input_wrapper_for_' + name)\n                _input_tensors.append(input_tensor)\n                # Cache newly created input layer.\n                original_input_layer = x._keras_history[0]\n                newly_created_input_layer = input_tensor._keras_history[0]\n                layer_map[original_input_layer] = newly_created_input_layer\n            else:\n                _input_tensors.append(x)\n        input_tensors = _input_tensors\n\n    for x, y in zip(model.inputs, input_tensors):\n        tensor_map[x] = (y, None)  # tensor, mask\n\n    # Iterated over every node in the reference model, in depth order.\n    depth_keys = list(model._nodes_by_depth.keys())\n    depth_keys.sort(reverse=True)\n    for depth in depth_keys:\n        nodes = model._nodes_by_depth[depth]\n        for node in nodes:\n            # Recover the corresponding layer.\n            layer = node.outbound_layer\n\n            # Get or create layer.\n            if layer not in layer_map:\n                # Clone layer.\n                new_layer = layer.__class__.from_config(layer.get_config())\n                layer_map[layer] = new_layer\n                layer = new_layer\n            else:\n                # Reuse previously cloned layer.\n                layer = layer_map[layer]\n                # Don't call InputLayer multiple times.\n                if isinstance(layer, InputLayer):\n                    continue\n\n            # Gather inputs to call the new layer.\n            reference_input_tensors = node.input_tensors\n            reference_output_tensors = node.output_tensors\n\n            # If all previous input tensors are available in tensor_map,\n            # then call node.inbound_layer on them.\n            computed_data = []  # List of tuples (input, mask).\n            for x in reference_input_tensors:\n                if x in tensor_map:\n                    computed_data.append(tensor_map[x])\n\n            if len(computed_data) == len(reference_input_tensors):\n                # Call layer.\n                if node.arguments:\n                    kwargs = node.arguments\n                else:\n                    kwargs = {}\n                if len(computed_data) == 1:\n                    computed_tensor, computed_mask = computed_data[0]\n                    if has_arg(layer.call, 'mask'):\n                        if 'mask' not in kwargs:\n                            kwargs['mask'] = computed_mask\n                    output_tensors = to_list(\n                        layer(computed_tensor, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensor,\n                                           computed_mask))\n                    computed_tensors = [computed_tensor]\n                    computed_masks = [computed_mask]\n                else:\n                    computed_tensors = [x[0] for x in computed_data]\n                    computed_masks = [x[1] for x in computed_data]\n                    if has_arg(layer.call, 'mask'):\n                        if 'mask' not in kwargs:\n                            kwargs['mask'] = computed_masks\n                    output_tensors = to_list(\n                        layer(computed_tensors, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensors,\n                                           computed_masks))\n                # Update tensor_map.\n                for x, y, mask in zip(reference_output_tensors,\n                                      output_tensors,\n                                      output_masks):\n                    tensor_map[x] = (y, mask)\n\n    # Check that we did compute the model outputs,\n    # then instantiate a new model from inputs and outputs.\n    output_tensors = []\n    for x in model.outputs:\n        assert x in tensor_map, 'Could not compute output ' + str(x)\n        tensor, _ = tensor_map[x]\n        output_tensors.append(tensor)\n    return Model(input_tensors, output_tensors, name=model.name)\n\n\ndef _clone_sequential_model(model, input_tensors=None):\n    \"\"\"Clone a `Sequential` model instance.\n\n    Model cloning is similar to calling a model on new inputs,\n    except that it creates new layers (and thus new weights) instead\n    of sharing the weights of the existing layers.\n\n    # Arguments\n        model: Instance of `Sequential`.\n        input_tensors: optional list of input tensors\n            to build the model upon. If not provided,\n            placeholders will be created.\n\n    # Returns\n        An instance of `Sequential` reproducing the behavior\n        of the original model, on top of new inputs tensors,\n        using newly instantiated weights.\n\n    # Raises\n        ValueError: in case of invalid `model` argument value.\n    \"\"\"\n    if not isinstance(model, Sequential):\n        raise ValueError('Expected `model` argument '\n                         'to be a `Sequential` model instance, '\n                         'but got:', model)\n\n    def clone(layer):\n        return layer.__class__.from_config(layer.get_config())\n\n    layers = [clone(layer) for layer in model.layers]\n    if input_tensors is None:\n        return Sequential(layers=layers, name=model.name)\n    else:\n        if len(to_list(input_tensors)) != 1:\n            raise ValueError('To clone a `Sequential` model, we expect '\n                             ' at most one tensor '\n                             'as part of `input_tensors`.')\n        x = to_list(input_tensors)[0]\n        if K.is_keras_tensor(x):\n            origin_layer = x._keras_history[0]\n            if isinstance(origin_layer, InputLayer):\n                return Sequential(layers=[origin_layer] + layers,\n                                  name=model.name)\n            else:\n                raise ValueError('Cannot clone a `Sequential` model on top '\n                                 'of a tensor that comes from a Keras layer '\n                                 'other than an `InputLayer`. '\n                                 'Use the functional API instead.')\n        input_tensor = Input(tensor=x,\n                             name='input_wrapper_for_' + str(x.name))\n        input_layer = input_tensor._keras_history[0]\n        return Sequential(layers=[input_layer] + layers, name=model.name)\n\n\ndef clone_model(model, input_tensors=None):\n    \"\"\"Clone any `Model` instance.\n\n    Model cloning is similar to calling a model on new inputs,\n    except that it creates new layers (and thus new weights) instead\n    of sharing the weights of the existing layers.\n\n    # Arguments\n        model: Instance of `Model`\n            (could be a functional model or a Sequential model).\n        input_tensors: optional list of input tensors\n            to build the model upon. If not provided,\n            placeholders will be created.\n\n    # Returns\n        An instance of `Model` reproducing the behavior\n        of the original model, on top of new inputs tensors,\n        using newly instantiated weights.\n\n    # Raises\n        ValueError: in case of invalid `model` argument value.\n    \"\"\"\n    if isinstance(model, Sequential):\n        return _clone_sequential_model(model, input_tensors=input_tensors)\n    else:\n        return _clone_functional_model(model, input_tensors=input_tensors)\n",
                "max_stars_repo_path": "keras/models.py",
                "max_stars_repo_name": "kalyc/keras-apache-mxnet",
                "max_stars_count": 300,
                "__cluster__": 17
            }
        ]
    ],
    [],
    [],
    [],
    [],
    [],
    [
        [
            {
                "id": "test_keras-bug-19",
                "content": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cntk as C\nimport numpy as np\nfrom .common import floatx\nfrom .common import epsilon\nfrom .common import image_data_format\nfrom .common import normalize_data_format\nfrom ..utils.generic_utils import transpose_shape\nfrom collections import defaultdict\nfrom contextlib import contextmanager\nimport warnings\n\n\nC.set_global_option('align_axis', 1)\n\nb_any = any\npy_slice = slice\n\n\ndev = C.device.use_default_device()\nif dev.type() == 0:\n    warnings.warn(\n        'CNTK backend warning: GPU is not detected. '\n        'CNTK\\'s CPU version is not fully optimized,'\n        'please run with GPU to get better performance.')\n\n# A learning phase is a bool tensor used to run Keras models in\n# either train mode (learning_phase == 1) or test mode (learning_phase == 0).\n# LEARNING_PHASE_PLACEHOLDER is the placeholder for dynamic learning phase\n_LEARNING_PHASE_PLACEHOLDER = C.constant(shape=(), dtype=np.float32, value=1.0, name='_keras_learning_phase')\n# static learning phase flag, if it is not 0 or 1, we will go with dynamic learning phase tensor.\n_LEARNING_PHASE = -1\n_UID_PREFIXES = defaultdict(int)\n\n# cntk doesn't support gradient as symbolic op, to hook up with keras model,\n# we will create gradient as a constant placeholder, here use this global\n# map to keep the mapping from grad placeholder to parameter\ngrad_parameter_dict = {}\n\nNAME_SCOPE_STACK = []\n\n\n@contextmanager\ndef name_scope(name):\n    global NAME_SCOPE_STACK\n    NAME_SCOPE_STACK.append(name)\n    yield\n    NAME_SCOPE_STACK.pop()\n\n\ndef get_uid(prefix=''):\n    _UID_PREFIXES[prefix] += 1\n    return _UID_PREFIXES[prefix]\n\n\ndef learning_phase():\n    # If _LEARNING_PHASE is not 0 or 1, return dynamic learning phase tensor\n    return _LEARNING_PHASE if _LEARNING_PHASE in {0, 1} else _LEARNING_PHASE_PLACEHOLDER\n\n\ndef set_learning_phase(value):\n    global _LEARNING_PHASE\n    if value not in {0, 1}:\n        raise ValueError('CNTK Backend: Set learning phase '\n                         'with value %s is not supported, '\n                         'expected 0 or 1.' % value)\n    _LEARNING_PHASE = value\n\n\ndef clear_session():\n    \"\"\"Reset learning phase flag for cntk backend.\n    \"\"\"\n    global _LEARNING_PHASE\n    global _LEARNING_PHASE_PLACEHOLDER\n    _LEARNING_PHASE = -1\n    _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(1.0)\n\n\ndef in_train_phase(x, alt, training=None):\n    global _LEARNING_PHASE\n    if training is None:\n        training = learning_phase()\n        uses_learning_phase = True\n    else:\n        uses_learning_phase = False\n\n    # CNTK currently don't support cond op, so here we use\n    # element_select approach as workaround. It may have\n    # perf issue, will resolve it later with cntk cond op.\n    if callable(x) and isinstance(x, C.cntk_py.Function) is False:\n        x = x()\n    if callable(alt) and isinstance(alt, C.cntk_py.Function) is False:\n        alt = alt()\n\n    if training is True:\n        x._uses_learning_phase = uses_learning_phase\n        return x\n    else:\n        # if _LEARNING_PHASE is static\n        if isinstance(training, int) or isinstance(training, bool):\n            result = x if training == 1 or training is True else alt\n        else:\n            result = C.element_select(training, x, alt)\n        result._uses_learning_phase = uses_learning_phase\n        return result\n\n\ndef in_test_phase(x, alt, training=None):\n    return in_train_phase(alt, x, training=training)\n\n\ndef _convert_string_dtype(dtype):\n    if dtype == 'float32':\n        return np.float32\n    elif dtype == 'float64':\n        return np.float64\n    elif dtype == 'float16':\n        return np.float16\n    else:\n        # cntk only running with float,\n        # try to cast to float to run the model\n        return np.float32\n\n\ndef _convert_dtype_string(dtype):\n    if dtype == np.float32:\n        return 'float32'\n    elif dtype == np.float64:\n        return 'float64'\n    elif dtype == np.float16:\n        return 'float16'\n    else:\n        raise ValueError('CNTK Backend: Unsupported dtype: %s. '\n                         'CNTK only supports float32, float64, and '\n                         'float16.' % dtype)\n\n\ndef variable(value, dtype=None, name=None, constraint=None):\n    \"\"\"Instantiates a variable and returns it.\n\n    # Arguments\n        value: Numpy array, initial value of the tensor.\n        dtype: Tensor type.\n        name: Optional name string for the tensor.\n        constraint: Optional projection function to be\n            applied to the variable after an optimizer update.\n\n    # Returns\n        A variable instance (with Keras metadata included).\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n\n    if name is None:\n        name = ''\n\n    if isinstance(\n            value,\n            C.variables.Constant) or isinstance(\n            value,\n            C.variables.Parameter):\n        value = value.value\n\n    # we don't support init parameter with symbolic op, so eval it first as\n    # workaround\n    if isinstance(value, C.cntk_py.Function):\n        value = eval(value)\n\n    shape = value.shape if hasattr(value, 'shape') else ()\n    if hasattr(value, 'dtype') and value.dtype != dtype and len(shape) > 0:\n        value = value.astype(dtype)\n\n    # TODO: remove the conversion when cntk supports int32, int64\n    # https://docs.microsoft.com/en-us/python/api/cntk.variables.parameter\n    dtype = 'float32' if 'int' in str(dtype) else dtype\n\n    v = C.parameter(shape=shape,\n                    init=value,\n                    dtype=dtype,\n                    name=_prepare_name(name, 'variable'))\n    v._keras_shape = v.shape\n    v._uses_learning_phase = False\n    v.constraint = constraint\n    return v\n\n\ndef bias_add(x, bias, data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    dims = len(x.shape)\n    if dims > 0 and x.shape[0] == C.InferredDimension:\n        dims -= 1\n\n    bias_dims = len(bias.shape)\n    if bias_dims != 1 and bias_dims != dims:\n        raise ValueError('Unexpected bias dimensions %d, '\n                         'expected 1 or %d dimensions' % (bias_dims, dims))\n\n    if dims == 4:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1, 1, 1)\n            else:\n                shape = (bias.shape[3],) + bias.shape[:3]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, 1, 1, bias.shape[0])\n            else:\n                shape = bias.shape\n    elif dims == 3:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1, 1)\n            else:\n                shape = (bias.shape[2],) + bias.shape[:2]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, 1, bias.shape[0])\n            else:\n                shape = bias.shape\n    elif dims == 2:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1)\n            else:\n                shape = (bias.shape[1],) + bias.shape[:1]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, bias.shape[0])\n            else:\n                shape = bias.shape\n    else:\n        shape = bias.shape\n    return x + reshape(bias, shape)\n\n\ndef eval(x):\n    if isinstance(x, C.cntk_py.Function):\n        return x.eval()\n    elif isinstance(x, C.variables.Constant) or isinstance(x, C.variables.Parameter):\n        return x.value\n    else:\n        raise ValueError('CNTK Backend: `eval` method on '\n                         '`%s` type is not supported. '\n                         'CNTK only supports `eval` with '\n                         '`Function`, `Constant` or '\n                         '`Parameter`.' % type(x))\n\n\ndef placeholder(\n        shape=None,\n        ndim=None,\n        dtype=None,\n        sparse=False,\n        name=None,\n        dynamic_axis_num=1):\n    if dtype is None:\n        dtype = floatx()\n    if not shape:\n        if ndim:\n            shape = tuple([None for _ in range(ndim)])\n\n    dynamic_dimension = C.FreeDimension if _get_cntk_version() >= 2.2 else C.InferredDimension\n    cntk_shape = [dynamic_dimension if s is None else s for s in shape]\n    cntk_shape = tuple(cntk_shape)\n\n    if dynamic_axis_num > len(cntk_shape):\n        raise ValueError('CNTK backend: creating placeholder with '\n                         '%d dimension is not supported, at least '\n                         '%d dimensions are needed.'\n                         % (len(cntk_shape), dynamic_axis_num))\n\n    if name is None:\n        name = ''\n\n    cntk_shape = cntk_shape[dynamic_axis_num:]\n\n    x = C.input(\n        shape=cntk_shape,\n        dtype=_convert_string_dtype(dtype),\n        is_sparse=sparse,\n        name=name)\n    x._keras_shape = shape\n    x._uses_learning_phase = False\n    x._cntk_placeholder = True\n    return x\n\n\ndef is_placeholder(x):\n    \"\"\"Returns whether `x` is a placeholder.\n\n    # Arguments\n        x: A candidate placeholder.\n\n    # Returns\n        Boolean.\n    \"\"\"\n    return hasattr(x, '_cntk_placeholder') and x._cntk_placeholder\n\n\ndef is_keras_tensor(x):\n    if not is_tensor(x):\n        raise ValueError('Unexpectedly found an instance of type `' +\n                         str(type(x)) + '`. '\n                         'Expected a symbolic tensor instance.')\n    return hasattr(x, '_keras_history')\n\n\ndef is_tensor(x):\n    return isinstance(x, (C.variables.Constant,\n                          C.variables.Variable,\n                          C.variables.Parameter,\n                          C.ops.functions.Function))\n\n\ndef shape(x):\n    shape = list(int_shape(x))\n    num_dynamic = _get_dynamic_axis_num(x)\n    non_dyn_shape = []\n    for i in range(len(x.shape)):\n        if shape[i + num_dynamic] is None:\n            non_dyn_shape.append(x.shape[i])\n        else:\n            non_dyn_shape.append(shape[i + num_dynamic])\n    return shape[:num_dynamic] + non_dyn_shape\n\n\ndef is_sparse(tensor):\n    return tensor.is_sparse\n\n\ndef int_shape(x):\n    if hasattr(x, '_keras_shape'):\n        return x._keras_shape\n\n    shape = x.shape\n    if hasattr(x, 'dynamic_axes'):\n        dynamic_shape = [None for a in x.dynamic_axes]\n        shape = tuple(dynamic_shape) + shape\n    return shape\n\n\ndef ndim(x):\n    shape = int_shape(x)\n    return len(shape)\n\n\ndef _prepare_name(name, default):\n    prefix = '_'.join(NAME_SCOPE_STACK)\n    if name is None or name == '':\n        return prefix + '/' + default\n    return prefix + '/' + name\n\n\ndef constant(value, dtype=None, shape=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    if shape is None:\n        shape = ()\n    np_value = value * np.ones(shape)\n    const = C.constant(np_value,\n                       dtype=dtype,\n                       name=_prepare_name(name, 'constant'))\n    const._keras_shape = const.shape\n    const._uses_learning_phase = False\n    return const\n\n\ndef random_binomial(shape, p=0.0, dtype=None, seed=None):\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e7)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n    return C.random.bernoulli(shape=shape, dtype=dtype, mean=p, seed=seed)\n\n\ndef random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n    return C.random.uniform(shape=shape, dtype=dtype, low=minval, high=maxval, seed=seed)\n\n\ndef random_uniform_variable(shape, low, high,\n                            dtype=None, name=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    if name is None:\n        name = ''\n\n    scale = (high - low) / 2\n    p = C.parameter(\n        shape,\n        init=C.initializer.uniform(\n            scale,\n            seed=seed),\n        dtype=dtype,\n        name=name)\n    return variable(value=p.value + low + scale)\n\n\ndef random_normal_variable(\n        shape,\n        mean,\n        scale,\n        dtype=None,\n        name=None,\n        seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e7)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    if name is None:\n        name = ''\n\n    p = C.parameter(\n        shape=shape,\n        init=C.initializer.normal(\n            scale=scale,\n            seed=seed),\n        dtype=dtype,\n        name=name)\n    return variable(value=p.value + mean)\n\n\ndef random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n    return C.random.normal(shape=shape, mean=mean, scale=stddev, seed=seed, dtype=dtype)\n\n\ndef truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if seed is None:\n        seed = np.random.randint(1, 10e6)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    return C.parameter(\n        shape, init=C.initializer.truncated_normal(\n            stddev, seed=seed), dtype=dtype)\n\n\ndef dtype(x):\n    return _convert_dtype_string(x.dtype)\n\n\ndef zeros(shape, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    ctype = _convert_string_dtype(dtype)\n    return variable(value=np.zeros(shape, ctype), dtype=dtype, name=name)\n\n\ndef ones(shape, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    ctype = _convert_string_dtype(dtype)\n    return variable(value=np.ones(shape, ctype), dtype=dtype, name=name)\n\n\ndef eye(size, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    return variable(np.eye(size), dtype, name)\n\n\ndef zeros_like(x, dtype=None, name=None):\n    return x * 0\n\n\ndef ones_like(x, dtype=None, name=None):\n    return zeros_like(x) + 1\n\n\ndef count_params(x):\n    for _ in x.shape:\n        if _ == C.InferredDimension or _ == C.FreeDimension:\n            raise ValueError('CNTK backend: `count_params` with dynamic '\n                             'shape is not supported. Please provide '\n                             'fixed dimension instead of `None`.')\n\n    return np.prod(int_shape(x))\n\n\ndef cast(x, dtype):\n    # cntk calculate everything in float, so don't need case from bool / int\n    return x\n\n\ndef dot(x, y):\n    if len(x.shape) > 2 or len(y.shape) > 2:\n        y_shape = int_shape(y)\n        if len(y_shape) > 2:\n            permutation = [len(y_shape) - 2]\n            permutation += list(range(len(y_shape) - 2))\n            permutation += [len(y_shape) - 1]\n            y = C.transpose(y, perm=permutation)\n        return C.times(x, y, len(y_shape) - 1)\n    else:\n        return C.times(x, y)\n\n\ndef batch_dot(x, y, axes=None):\n    x_shape = int_shape(x)\n    y_shape = int_shape(y)\n\n    if isinstance(axes, int):\n        axes = (axes, axes)\n    if axes is None:\n        # behaves like tf.batch_matmul as default\n        axes = [len(x_shape) - 1, len(y_shape) - 2]\n    if b_any([isinstance(a, (list, tuple)) for a in axes]):\n        raise ValueError('Multiple target dimensions are not supported. ' +\n                         'Expected: None, int, (int, int), ' +\n                         'Provided: ' + str(axes))\n\n    if len(x_shape) == 2 and len(y_shape) == 2:\n        if axes[0] == axes[1]:\n            result = sum(x * y, axis=axes[0], keepdims=True)\n            return result if axes[0] == 1 else transpose(result)\n        else:\n            return sum(x * transpose(y), axis=axes[0], keepdims=True)\n    else:\n        if len(y_shape) == 2:\n            y = expand_dims(y)\n\n        normalized_axis = []\n        normalized_axis.append(_normalize_axis(axes[0], x)[0])\n        normalized_axis.append(_normalize_axis(axes[1], y)[0])\n        # transpose\n        i = normalized_axis[0]\n        while i < len(x.shape) - 1:\n            x = C.swapaxes(x, i, i + 1)\n            i += 1\n        i = normalized_axis[1]\n        while i > 0:\n            y = C.swapaxes(y, i, i - 1)\n            i -= 1\n        result = C.times(x, y, output_rank=(len(y.shape) - 1)\n                         if len(y.shape) > 1 else 1)\n        if len(y_shape) == 2:\n            result = squeeze(result, -1)\n        return result\n\n\ndef transpose(x):\n    return C.swapaxes(x, 0, 1)\n\n\ndef gather(reference, indices):\n    # There is a bug in cntk gather op which may cause crash.\n    # We have made a fix but not catched in CNTK 2.1 release.\n    # Will update with gather op in next release\n    if _get_cntk_version() >= 2.2:\n        return C.ops.gather(reference, indices)\n    else:\n        num_classes = reference.shape[0]\n        one_hot_matrix = C.ops.one_hot(indices, num_classes)\n        return C.times(one_hot_matrix, reference, output_rank=len(reference.shape) - 1)\n\n\ndef _remove_dims(x, axis, keepdims=False):\n    if keepdims is False and isinstance(axis, list):\n        # sequence axis is removed by default, so don't need reshape on it\n        reduce_axes = []\n        for a in axis:\n            if isinstance(a, C.Axis) is False:\n                reduce_axes.append(a)\n        return _reshape_dummy_dim(x, reduce_axes)\n    else:\n        if isinstance(axis, list):\n            has_seq = False\n            for a in axis:\n                if isinstance(a, C.Axis):\n                    has_seq = True\n                    break\n            if has_seq:\n                nones = _get_dynamic_axis_num(x)\n                x = expand_dims(x, nones)\n        return x\n\n\ndef max(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_max')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef min(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_min')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef sum(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_sum')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef prod(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_prod')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef logsumexp(x, axis=None, keepdims=False):\n    return log(sum(exp(x), axis=axis, keepdims=keepdims))\n\n\ndef var(x, axis=None, keepdims=False):\n    m = mean(x, axis, keepdims=True)\n    devs_squared = C.square(x - m)\n    return mean(devs_squared, axis=axis, keepdims=keepdims)\n\n\ndef std(x, axis=None, keepdims=False):\n    return C.sqrt(var(x, axis=axis, keepdims=keepdims))\n\n\ndef expand_dims(x, axis=-1):\n    shape = list(int_shape(x))\n    nones = _get_dynamic_axis_num(x)\n    index = axis if axis >= 0 else len(shape) + 1\n    shape.insert(index, 1)\n    new_shape = shape[nones:]\n    new_shape = tuple(\n        [C.InferredDimension if _ is None else _ for _ in new_shape])\n    result = C.reshape(x, new_shape)\n    if index < nones:\n        result._keras_shape = shape\n    return result\n\n\ndef squeeze(x, axis):\n    if isinstance(axis, tuple):\n        axis = list(axis)\n    if not isinstance(axis, list):\n        axis = [axis]\n\n    shape = list(int_shape(x))\n\n    _axis = []\n    for _ in axis:\n        if isinstance(_, int):\n            _axis.append(_ if _ >= 0 else _ + len(shape))\n\n    if len(_axis) == 0:\n        return x\n\n    nones = _get_dynamic_axis_num(x)\n    for _ in sorted(_axis, reverse=True):\n        del shape[_]\n\n    new_shape = shape[nones:]\n    new_shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in new_shape])\n    return C.reshape(x, new_shape)\n\n\ndef tile(x, n):\n    if isinstance(n, int):\n        n = (n,)\n    elif isinstance(n, list):\n        n = tuple(n)\n\n    shape = int_shape(x)\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    # Padding the axis\n    if len(n) < len(shape):\n        n = tuple([1 for _ in range(len(shape) - len(n))]) + n\n\n    if len(n) != len(shape):\n        raise NotImplementedError\n\n    i = num_dynamic_axis\n    for i, rep in enumerate(n):\n        if i >= num_dynamic_axis and shape[i] is not None:\n            tmp = [x] * rep\n            x = C.splice(*tmp, axis=i - num_dynamic_axis)\n        i += 1\n\n    return x\n\n\ndef _normalize_axis(axis, x):\n    shape = int_shape(x)\n    ndim = len(shape)\n\n    nones = _get_dynamic_axis_num(x)\n\n    if nones > ndim:\n        raise ValueError('CNTK Backend: tensor with keras shape: `%s` has '\n                         '%d cntk dynamic axis, this is not expected, please '\n                         'double check the keras shape history.' % (str(shape), nones))\n\n    # Current cntk does not support shape like (1, batch). so using the workaround\n    # here to mapping the correct axis. Will remove this tricky after we add support\n    # in native cntk op\n    cntk_axis = []\n    dynamic_axis_index = 0\n    for i in range(ndim):\n        if shape[i] is None and dynamic_axis_index < nones:\n            cntk_axis.append(x.dynamic_axes[dynamic_axis_index])\n            dynamic_axis_index += 1\n        else:\n            cntk_axis.append(i - dynamic_axis_index)\n\n    if dynamic_axis_index < nones:\n        i = 0\n        while dynamic_axis_index < nones:\n            cntk_axis[i] = x.dynamic_axes[dynamic_axis_index]\n            i += 1\n            dynamic_axis_index += 1\n\n        while i < len(cntk_axis):\n            cntk_axis[i] -= nones\n            i += 1\n\n    if isinstance(axis, tuple):\n        _axis = list(axis)\n    elif isinstance(axis, int):\n        _axis = [axis]\n    elif isinstance(axis, list):\n        _axis = list(axis)\n    else:\n        _axis = axis\n\n    if isinstance(_axis, list):\n        for i, a in enumerate(_axis):\n            if a is not None and a < 0:\n                _axis[i] = (a % ndim)\n            if _axis[i] is not None:\n                _axis[i] = cntk_axis[_axis[i]]\n    else:\n        if _axis is None:\n            _axis = C.Axis.all_axes()\n\n    return _axis\n\n\ndef _reshape_dummy_dim(x, axis):\n    shape = list(x.shape)\n\n    _axis = [_ + len(shape) if _ < 0 else _ for _ in axis]\n\n    if shape.count(C.InferredDimension) > 1 or shape.count(C.FreeDimension) > 1:\n        result = x\n        for index in sorted(_axis, reverse=True):\n            result = C.reshape(result,\n                               shape=(),\n                               begin_axis=index,\n                               end_axis=index + 1)\n        return result\n    else:\n        for index in sorted(_axis, reverse=True):\n            del shape[index]\n\n        shape = [C.InferredDimension if _ == C.FreeDimension else _ for _ in shape]\n        return C.reshape(x, shape)\n\n\ndef mean(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_mean')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef any(x, axis=None, keepdims=False):\n    reduce_result = sum(x, axis, keepdims=keepdims)\n    any_matrix = C.element_select(\n        reduce_result,\n        ones_like(reduce_result),\n        zeros_like(reduce_result))\n    if len(reduce_result.shape) == 0 and _get_dynamic_axis_num(x) == 0:\n        return C.reduce_sum(any_matrix)\n    else:\n        return any_matrix\n\n\ndef all(x, axis=None, keepdims=False):\n    reduce_result = prod(x, axis, keepdims=keepdims)\n    all_matrix = C.element_select(\n        reduce_result,\n        ones_like(reduce_result),\n        zeros_like(reduce_result))\n    if len(reduce_result.shape) == 0 and _get_dynamic_axis_num(x) == 0:\n        return C.reduce_sum(all_matrix)\n    else:\n        return all_matrix\n\n\ndef classification_error(target, output, axis=-1):\n    return C.ops.reduce_mean(\n        C.equal(\n            argmax(\n                output,\n                axis=-1),\n            argmax(\n                target,\n                axis=-1)),\n        axis=C.Axis.all_axes())\n\n\ndef argmax(x, axis=-1):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    output = C.ops.argmax(x, axis=axis[0])\n    return _reshape_dummy_dim(output, axis)\n\n\ndef argmin(x, axis=-1):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    output = C.ops.argmin(x, axis=axis[0])\n    return _reshape_dummy_dim(output, axis)\n\n\ndef square(x):\n    return C.square(x)\n\n\ndef abs(x):\n    return C.abs(x)\n\n\ndef sqrt(x):\n    return C.sqrt(x)\n\n\ndef exp(x):\n    return C.exp(x)\n\n\ndef log(x):\n    return C.log(x)\n\n\ndef round(x):\n    return C.round(x)\n\n\ndef sigmoid(x):\n    return C.sigmoid(x)\n\n\ndef sign(x):\n    return x / C.abs(x)\n\n\ndef pow(x, a):\n    return C.pow(x, a)\n\n\ndef clip(x, min_value, max_value):\n    if max_value is not None and max_value < min_value:\n        max_value = min_value\n    if max_value is None:\n        max_value = np.inf\n    if min_value is None:\n        min_value = -np.inf\n    return C.clip(x, min_value, max_value)\n\n\ndef binary_crossentropy(target, output, from_logits=False):\n    if from_logits:\n        output = C.sigmoid(output)\n    output = C.clip(output, epsilon(), 1.0 - epsilon())\n    output = -target * C.log(output) - (1.0 - target) * C.log(1.0 - output)\n    return output\n\n\ndef get_variable_shape(x):\n    return int_shape(x)\n\n\ndef update(x, new_x):\n    return C.assign(x, new_x)\n\n\ndef moving_average_update(variable, value, momentum):\n    return C.assign(variable, variable * momentum + value * (1. - momentum))\n\n\ndef update_add(x, increment):\n    result = x + increment\n    return C.assign(x, result)\n\n\ndef gradients(loss, variables):\n    # cntk does not support gradients as symbolic op,\n    # to hook up with keras model\n    # we will return a constant as place holder, the cntk learner will apply\n    # the gradient during training.\n    global grad_parameter_dict\n    if isinstance(variables, list) is False:\n        variables = [variables]\n    grads = []\n    for v in variables:\n        g = C.constant(0, shape=v.shape, name='keras_grad_placeholder')\n        grads.append(g)\n        grad_parameter_dict[g] = v\n    return grads\n\n\ndef equal(x, y):\n    return C.equal(x, y)\n\n\ndef not_equal(x, y):\n    return C.not_equal(x, y)\n\n\ndef greater(x, y):\n    return C.greater(x, y)\n\n\ndef greater_equal(x, y):\n    return C.greater_equal(x, y)\n\n\ndef less(x, y):\n    return C.less(x, y)\n\n\ndef less_equal(x, y):\n    return C.less_equal(x, y)\n\n\ndef maximum(x, y):\n    return C.element_max(x, y)\n\n\ndef minimum(x, y):\n    return C.element_min(x, y)\n\n\ndef sin(x):\n    return C.sin(x)\n\n\ndef cos(x):\n    return C.cos(x)\n\n\ndef normalize_batch_in_training(x, gamma, beta,\n                                reduction_axes, epsilon=1e-3):\n    if gamma is None:\n        if beta is None:\n            gamma = ones_like(x)\n        else:\n            gamma = ones_like(beta)\n    if beta is None:\n        if gamma is None:\n            beta = zeros_like(x)\n        else:\n            beta = zeros_like(gamma)\n\n    mean, variant = _moments(x, _normalize_axis(reduction_axes, x))\n\n    if sorted(reduction_axes) == list(range(ndim(x)))[:-1]:\n        normalized = batch_normalization(\n            x, mean, variant, beta, gamma, epsilon)\n    else:\n        # need broadcasting\n        target_shape = []\n        x_shape = int_shape(x)\n        # skip the batch axis\n        for axis in range(1, ndim(x)):\n            if axis in reduction_axes:\n                target_shape.append(1)\n                if ndim(gamma) > axis:\n                    gamma = C.reduce_mean(gamma, axis - 1)\n                    beta = C.reduce_mean(beta, axis - 1)\n            else:\n                target_shape.append(x_shape[axis])\n\n        broadcast_mean = C.reshape(mean, target_shape)\n        broadcast_var = C.reshape(variant, target_shape)\n        broadcast_gamma = C.reshape(gamma, target_shape)\n        broadcast_beta = C.reshape(beta, target_shape)\n        normalized = batch_normalization(\n            x,\n            broadcast_mean,\n            broadcast_var,\n            broadcast_beta,\n            broadcast_gamma,\n            epsilon)\n\n    return normalized, mean, variant\n\n\ndef _moments(x, axes=None, shift=None, keep_dims=False):\n    _axes = tuple(axes)\n    if shift is None:\n        shift = x\n        # Compute true mean while keeping the dims for proper broadcasting.\n        for axis in _axes:\n            shift = C.reduce_mean(shift, axis=axis)\n\n    shift = C.stop_gradient(shift)\n    shifted_mean = C.minus(x, shift)\n    for axis in _axes:\n        shifted_mean = C.reduce_mean(shifted_mean, axis=axis)\n\n    variance_mean = C.square(C.minus(x, shift))\n    for axis in _axes:\n        variance_mean = C.reduce_mean(variance_mean, axis=axis)\n\n    variance = C.minus(variance_mean, C.square(shifted_mean))\n    mean = C.plus(shifted_mean, shift)\n\n    if not keep_dims:\n        mean = squeeze(mean, _axes)\n        variance = squeeze(variance, _axes)\n\n    return mean, variance\n\n\ndef batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n    # The mean / var / beta / gamma may be processed by broadcast\n    # so it may have an extra batch axis with 1, it is not needed\n    # in cntk, need to remove those dummy axis.\n    if ndim(mean) == ndim(x) and shape(mean)[0] == 1:\n        mean = _reshape_dummy_dim(mean, [0])\n    if ndim(var) == ndim(x) and shape(var)[0] == 1:\n        var = _reshape_dummy_dim(var, [0])\n\n    if gamma is None:\n        gamma = ones_like(var)\n    elif ndim(gamma) == ndim(x) and shape(gamma)[0] == 1:\n        gamma = _reshape_dummy_dim(gamma, [0])\n\n    if beta is None:\n        beta = zeros_like(mean)\n    elif ndim(beta) == ndim(x) and shape(beta)[0] == 1:\n        beta = _reshape_dummy_dim(beta, [0])\n\n    return (x - mean) / C.sqrt(var + epsilon) * gamma + beta\n\n\ndef concatenate(tensors, axis=-1):\n    if len(tensors) == 0:\n        return None\n\n    axis = [axis]\n    axis = _normalize_axis(axis, tensors[0])\n    return C.splice(*tensors, axis=axis[0])\n\n\ndef flatten(x):\n    return reshape(x, (-1,))\n\n\ndef reshape(x, shape):\n    shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in shape])\n    if isinstance(x, C.variables.Parameter):\n        return C.reshape(x, shape)\n    else:\n        num_dynamic_axis = _get_dynamic_axis_num(x)\n\n        if num_dynamic_axis == 1 and len(shape) > 0 and shape[0] == -1:\n            # collapse axis with batch axis\n            if b_any(_ == C.InferredDimension for _ in x.shape) or b_any(\n                    _ == C.FreeDimension for _ in x.shape):\n                warnings.warn(\n                    'Warning: CNTK backend does not support '\n                    'collapse of batch axis with inferred dimension. '\n                    'The reshape did not take place.')\n                return x\n            return _reshape_batch(x, shape)\n        else:\n            # no collapse, then first need to padding the shape\n            if num_dynamic_axis >= len(shape):\n                i = 0\n                while i < len(shape):\n                    if shape[i] is None or shape[i] == -1:\n                        i += 1\n                    else:\n                        break\n                shape = tuple([-1 for _ in range(num_dynamic_axis - i)]) + shape\n\n            new_shape = list(shape)\n            new_shape = new_shape[num_dynamic_axis:]\n            new_shape = [C.InferredDimension if _ is None else _ for _ in new_shape]\n            return C.reshape(x, new_shape)\n\n\ndef permute_dimensions(x, pattern):\n    dims = len(int_shape(x))\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    if isinstance(pattern, list):\n        current_layout = [i for i in range(dims)]\n    else:\n        current_layout = tuple([i for i in range(dims)])\n\n    if num_dynamic_axis > 0 and pattern[:num_dynamic_axis] != current_layout[:num_dynamic_axis]:\n        raise ValueError('CNTK backend: the permute pattern %s '\n                         'requested permute on dynamic axis, '\n                         'which is not supported. Please do permute '\n                         'on static axis.' % pattern)\n\n    axis = list(pattern)\n    axis = axis[num_dynamic_axis:]\n    axis = _normalize_axis(axis, x)\n    return C.transpose(x, axis)\n\n\ndef resize_images(x, height_factor, width_factor, data_format, interpolation='nearest'):\n    if interpolation == 'nearest':\n        if data_format == 'channels_first':\n            output = repeat_elements(x, height_factor, axis=2)\n            output = repeat_elements(output, width_factor, axis=3)\n            return output\n        elif data_format == 'channels_last':\n            output = repeat_elements(x, height_factor, axis=1)\n            output = repeat_elements(output, width_factor, axis=2)\n            return output\n        else:\n            raise ValueError('CNTK Backend: Invalid data_format: %s' % data_format)\n    else:\n        raise NotImplementedError('CNTK only supports `nearest` interpolation.')\n\n\ndef resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n    if data_format == 'channels_first':\n        output = repeat_elements(x, depth_factor, axis=2)\n        output = repeat_elements(output, height_factor, axis=3)\n        output = repeat_elements(output, width_factor, axis=4)\n        return output\n    elif data_format == 'channels_last':\n        output = repeat_elements(x, depth_factor, axis=1)\n        output = repeat_elements(output, height_factor, axis=2)\n        output = repeat_elements(output, width_factor, axis=3)\n        return output\n    else:\n        raise ValueError('CNTK Backend: Invalid data_format: %s' % data_format)\n\n\ndef repeat_elements(x, rep, axis):\n    axis = _normalize_axis(axis, x)\n    axis = axis[0]\n    slices = []\n    shape = x.shape\n    i = 0\n    while i < shape[axis]:\n        tmp = C.ops.slice(x, axis, i, i + 1)\n        for _ in range(rep):\n            slices.append(tmp)\n        i += 1\n    return C.splice(*slices, axis=axis)\n\n\ndef repeat(x, n):\n    # this is a workaround for recurrent layer\n    # if n is inferred dimension,\n    # we can't figure out how to repeat it in cntk now\n    # return the same x to take cntk broadcast feature\n    # to make the recurrent layer work.\n    # need to be fixed in GA.\n    if n is C.InferredDimension or n is C.FreeDimension:\n        return x\n    index = 1 - _get_dynamic_axis_num(x)\n    if index < 0 or index > 1:\n        raise NotImplementedError\n\n    new_shape = list(x.shape)\n    new_shape.insert(index, 1)\n    new_shape = tuple(new_shape)\n    x = C.reshape(x, new_shape)\n    temp = [x] * n\n    return C.splice(*temp, axis=index)\n\n\ndef tanh(x):\n    return C.tanh(x)\n\n\ndef _static_rnn(step_function, inputs, initial_states,\n                go_backwards=False, mask=None, constants=None,\n                unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('Input should be at least 3D.')\n\n    # if the second axis is static axis, CNTK will do unroll by default\n    if shape[1] is None:\n        raise ValueError('CNTK Backend: the input of static rnn '\n                         'has shape `%s`, the second axis '\n                         'is not static. If you want to run '\n                         'rnn with non-static axis, please try '\n                         'dynamic rnn with sequence axis.' % shape)\n    if constants is None:\n        constants = []\n\n    if mask is not None:\n        mask_shape = int_shape(mask)\n        if len(mask_shape) == dims - 1:\n            mask = expand_dims(mask)\n\n    nones = _get_dynamic_axis_num(inputs)\n\n    states = tuple(initial_states)\n\n    outputs = []\n\n    time_axis = 1 - nones if nones > 0 else 1\n\n    if go_backwards:\n        i = shape[1] - 1\n        while i >= 0:\n            current = C.ops.slice(inputs, time_axis, i, i + 1)\n            # remove dummy dimension\n            current = squeeze(current, time_axis)\n\n            output, new_states = step_function(\n                current, tuple(states) + tuple(constants))\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n\n            if mask is not None:\n                mask_slice = C.ops.slice(mask, time_axis, i, i + 1)\n                mask_slice = squeeze(mask_slice, time_axis)\n                if len(outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = outputs[-1]\n                output = C.ops.element_select(mask_slice, output, prev_output)\n\n                return_states = []\n                for s, n_s in zip(states, new_states):\n                    return_states.append(\n                        C.ops.element_select(\n                            mask_slice, n_s, s))\n                new_states = return_states\n            outputs.append(output)\n            states = new_states\n            i -= 1\n    else:\n        i = 0\n        while i < shape[1]:\n            current = C.ops.slice(inputs, time_axis, i, i + 1)\n            # remove dummy dimension\n            current = squeeze(current, 1)\n\n            output, new_states = step_function(\n                current, tuple(states) + tuple(constants))\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n\n            if mask is not None:\n                mask_slice = C.ops.slice(mask, time_axis, i, i + 1)\n                mask_slice = squeeze(mask_slice, 1)\n                if len(outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = outputs[-1]\n                output = C.ops.element_select(mask_slice, output, prev_output)\n\n                return_states = []\n                for s, n_s in zip(states, new_states):\n                    return_states.append(\n                        C.ops.element_select(\n                            mask_slice, n_s, s))\n                new_states = return_states\n            outputs.append(output)\n            states = new_states[:len(states)]\n            i += 1\n\n    i = 1\n    # add the time_step axis back\n    final_output = expand_dims(outputs[0], 1)\n    last_output = outputs[0]\n    while i < len(outputs):\n        # add the time_step axis back\n        output_slice = expand_dims(outputs[i], 1)\n        final_output = C.splice(final_output, output_slice, axis=time_axis)\n        last_output = outputs[i]\n        i += 1\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, states\n\n\ndef rnn(step_function, inputs, initial_states,\n        go_backwards=False, mask=None, constants=None,\n        unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    global uses_learning_phase\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('CNTK Backend: the input of rnn has only rank %d '\n                         'Need at least rank 3 to run RNN.' % dims)\n\n    if _get_dynamic_axis_num(inputs) == 0 or unroll:\n        return _static_rnn(\n            step_function,\n            inputs,\n            initial_states,\n            go_backwards,\n            mask,\n            constants,\n            unroll,\n            input_length)\n\n    if constants is None:\n        constants = []\n\n    num_time_step = shape[1]\n    if num_time_step is None and not has_seq_axis(inputs):\n        num_time_step = inputs.shape[0]\n\n    initial = []\n    for s in initial_states:\n        if _get_dynamic_axis_num(s) == 0:\n            if hasattr(C, 'to_batch'):\n                initial.append(C.to_batch(s))\n            else:\n                initial.append(C.user_function(ConvertToBatch(s)))\n        else:\n            initial.append(s)\n\n    need_convert = not has_seq_axis(inputs)\n    if go_backwards and need_convert is False:\n        raise NotImplementedError('CNTK Backend: `go_backwards` is not supported with '\n                                  'variable-length sequences. Please specify a '\n                                  'static length for your sequences.')\n\n    rnn_inputs = inputs\n    if need_convert:\n        if go_backwards:\n            rnn_inputs = reverse(rnn_inputs, 1)\n\n        rnn_inputs = C.to_sequence(rnn_inputs)\n\n        rnn_constants = []\n        for constant in constants:\n            if isinstance(constant, list):\n                new_c = []\n                for c in constant:\n                    if _get_dynamic_axis_num(c) == 1:\n                        new_c.append(C.sequence.broadcast_as(c, rnn_inputs))\n                    else:\n                        new_c.append(c)\n                rnn_constants.append(new_c)\n            else:\n                if _get_dynamic_axis_num(constant) == 1:\n                    rnn_constants.append(C.sequence.broadcast_as(constant, rnn_inputs))\n                else:\n                    rnn_constants.append(constant)\n    else:\n        rnn_constants = constants\n\n    if mask is not None and not has_seq_axis(mask):\n        if go_backwards:\n            mask = reverse(mask, 1)\n        if len(int_shape(mask)) == 2:\n            mask = expand_dims(mask)\n        mask = C.to_sequence_like(mask, rnn_inputs)\n\n    states = tuple(initial)\n\n    with C.default_options(axis_offset=1):\n        def _recurrence(x, states, m):\n            # create place holder\n            place_holders = [C.placeholder(dynamic_axes=x.dynamic_axes) for _ in states]\n            past_values = []\n            for s, p in zip(states, place_holders):\n                past_values.append(C.sequence.past_value(p, s))\n            new_output, new_states = step_function(\n                x, tuple(past_values) + tuple(rnn_constants))\n\n            if getattr(new_output, '_uses_learning_phase', False):\n                global uses_learning_phase\n                uses_learning_phase = True\n\n            if m is not None:\n                new_states = [C.element_select(m, n, s) for n, s in zip(new_states, past_values)]\n            n_s = []\n            for o, p in zip(new_states, place_holders):\n                n_s.append(o.replace_placeholders({p: o.output}))\n            if len(n_s) > 0:\n                new_output = n_s[0]\n            return new_output, n_s\n\n        final_output, final_states = _recurrence(rnn_inputs, states, mask)\n        last_output = C.sequence.last(final_output)\n        last_states = [C.sequence.last(s) for s in final_states]\n\n    if need_convert:\n        final_output = C.sequence.unpack(final_output, 0, no_mask_output=True)\n        if num_time_step is not None and num_time_step is not C.FreeDimension:\n            final_output = _reshape_sequence(final_output, num_time_step)\n\n    f_stats = []\n    for l_s, i_s in zip(last_states, initial_states):\n        if _get_dynamic_axis_num(i_s) == 0 and _get_dynamic_axis_num(l_s) == 1:\n            if hasattr(C, 'unpack_batch'):\n                f_stats.append(C.unpack_batch(l_s))\n            else:\n                f_stats.append(C.user_function(ConvertToStatic(l_s, batch_size=i_s.shape[0])))\n        else:\n            f_stats.append(l_s)\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, f_stats\n\n\ndef has_seq_axis(x):\n    return hasattr(x, 'dynamic_axes') and len(x.dynamic_axes) > 1\n\n\ndef l2_normalize(x, axis=None):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    norm = C.sqrt(C.reduce_sum(C.square(x), axis=axis[0]))\n    return x / norm\n\n\ndef hard_sigmoid(x):\n    x = (0.2 * x) + 0.5\n    x = C.clip(x, 0.0, 1.0)\n    return x\n\n\ndef conv1d(x, kernel, strides=1, padding='valid',\n           data_format=None, dilation_rate=1):\n    data_format = normalize_data_format(data_format)\n\n    if padding == 'causal':\n        # causal (dilated) convolution:\n        left_pad = dilation_rate * (kernel.shape[0] - 1)\n        x = temporal_padding(x, (left_pad, 0))\n        padding = 'valid'\n\n    if data_format == 'channels_last':\n        x = C.swapaxes(x, 0, 1)\n        kernel = C.swapaxes(kernel, 0, 2)\n\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != 1:\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to 1. You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(\n        kernel,\n        x,\n        strides=strides,\n        auto_padding=[False, padding],\n        dilation=dilation_rate)\n\n    if data_format == 'channels_last':\n        x = C.swapaxes(x, 0, 1)\n    return x\n\n\ndef conv2d(x, kernel, strides=(1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != (1, 1):\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to (1, 1). '\n                         'You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(kernel,\n                      x,\n                      strides,\n                      auto_padding=[False, padding, padding],\n                      dilation=dilation_rate)\n\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n                     padding='valid', data_format=None, dilation_rate=1):\n    data_format = normalize_data_format(data_format)\n    if isinstance(strides, int):\n        strides = (strides,)\n    if isinstance(dilation_rate, int):\n        dilation_rate = (dilation_rate,)\n\n    if data_format == 'channels_last':\n        spatial_start_dim = 2\n    else:\n        spatial_start_dim = 3\n    x = expand_dims(x, spatial_start_dim)\n    depthwise_kernel = expand_dims(depthwise_kernel, 1)\n    pointwise_kernel = expand_dims(pointwise_kernel, 1)\n    strides = (1,) + strides + (1,)\n    dilation_rate = (1,) + dilation_rate\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dilation_rate == (1, 1):\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    x = _postprocess_conv2d_output(x, data_format)\n    return squeeze(x, spatial_start_dim)\n\n\ndef separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=dilation_rate[0],\n                          auto_padding=[False, padding, padding])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n                     data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    padding = _preprocess_border_mode(padding)\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=dilation_rate[0],\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != (1, 1, 1):\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to (1, 1, 1). '\n                         'You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(\n        kernel,\n        x,\n        strides,\n        auto_padding=[False, padding, padding, padding],\n        dilation=dilation_rate)\n\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n                     padding='valid', data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = (1,) + strides\n    # cntk output_shape does not include batch axis\n    output_shape = output_shape[1:]\n    # in keras2, need handle output shape in different format\n    if data_format == 'channels_last':\n        output_shape = transpose_shape(output_shape, 'channels_first',\n                                       spatial_axes=(0, 1, 2))\n\n    x = C.convolution_transpose(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding,\n            padding],\n        output_shape=output_shape)\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef pool2d(x, pool_size, strides=(1, 1),\n           padding='valid', data_format=None,\n           pool_mode='max'):\n    data_format = normalize_data_format(data_format)\n\n    padding = _preprocess_border_mode(padding)\n    strides = strides\n    pool_size = pool_size\n    x = _preprocess_conv2d_input(x, data_format)\n    if pool_mode == 'max':\n        x = C.pooling(\n            x,\n            C.MAX_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    elif pool_mode == 'avg':\n        x = C.pooling(\n            x,\n            C.AVG_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    else:\n        raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n           data_format=None, pool_mode='max'):\n    data_format = normalize_data_format(data_format)\n\n    padding = _preprocess_border_mode(padding)\n\n    x = _preprocess_conv3d_input(x, data_format)\n\n    if pool_mode == 'max':\n        x = C.pooling(\n            x,\n            C.MAX_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    elif pool_mode == 'avg':\n        x = C.pooling(\n            x,\n            C.AVG_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    else:\n        raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef relu(x, alpha=0., max_value=None):\n    if alpha != 0.:\n        negative_part = C.relu(-x)\n    x = C.relu(x)\n    if max_value is not None:\n        x = C.clip(x, 0.0, max_value)\n    if alpha != 0.:\n        x -= alpha * negative_part\n    return x\n\n\ndef dropout(x, level, noise_shape=None, seed=None):\n    if level < 0. or level >= 1:\n        raise ValueError('CNTK Backend: Invalid dropout level %s, '\n                         'must be in interval [0, 1].' % level)\n    return C.dropout(x, level)\n\n\ndef batch_flatten(x):\n    # cntk's batch axis is not in shape,\n    # so just flatten all the dim in x.shape\n    dim = np.prod(x.shape)\n    x = C.reshape(x, (-1,))\n    x._keras_shape = (None, dim)\n    return x\n\n\ndef softmax(x, axis=-1):\n    return C.softmax(x, axis=axis)\n\n\ndef softplus(x):\n    return C.softplus(x)\n\n\ndef softsign(x):\n    return x / (1 + C.abs(x))\n\n\ndef categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    # Here, unlike other backends, the tensors lack a batch dimension:\n    axis_without_batch = -1 if axis == -1 else axis - 1\n    output_dimensions = list(range(len(output.shape)))\n    if axis_without_batch != -1 and axis_without_batch not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis_without_batch),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(output.shape))))\n    # If the channels are not in the last axis, move them to be there:\n    if axis_without_batch != -1 and axis_without_batch != output_dimensions[-1]:\n        permutation = output_dimensions[:axis_without_batch]\n        permutation += output_dimensions[axis_without_batch + 1:]\n        permutation += [axis_without_batch]\n        output = C.transpose(output, permutation)\n        target = C.transpose(target, permutation)\n    if from_logits:\n        result = C.cross_entropy_with_softmax(output, target)\n        # cntk's result shape is (batch, 1), while keras expect (batch, )\n        return C.reshape(result, ())\n    else:\n        # scale preds so that the class probas of each sample sum to 1\n        output /= C.reduce_sum(output, axis=-1)\n        # avoid numerical instability with epsilon clipping\n        output = C.clip(output, epsilon(), 1.0 - epsilon())\n        return -sum(target * C.log(output), axis=-1)\n\n\ndef sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    # Here, unlike other backends, the tensors lack a batch dimension:\n    axis_without_batch = -1 if axis == -1 else axis - 1\n    output_dimensions = list(range(len(output.shape)))\n    if axis_without_batch != -1 and axis_without_batch not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis_without_batch),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(output.shape))))\n    target = C.one_hot(target, output.shape[axis_without_batch],\n                       axis=axis_without_batch)\n    target = C.reshape(target, output.shape)\n    return categorical_crossentropy(target, output, from_logits, axis=axis)\n\n\nclass Function(object):\n\n    def __init__(self, inputs, outputs, updates=[], **kwargs):\n        self.placeholders = inputs\n        self.trainer = None\n        self.unrelated_updates = None\n        self.updates = updates\n        if len(updates) > 0:\n            assert len(outputs) > 0\n            self.loss = outputs[0]\n            # need group update by gradient place holder\n            u_ops = []\n            unrelated_updates = []\n            for update in updates:\n                if isinstance(update, tuple):\n                    if len(update) != 2:\n                        raise NotImplementedError\n                    else:\n                        u = C.assign(update[0], update[1])\n                else:\n                    u = update\n\n                if len(u.arguments) == 0:\n                    u_ops.append(u)\n                else:\n                    unrelated_updates.append(u)\n\n            update_func = C.combine([u.output for u in u_ops])\n\n            grads = update_func.find_all_with_name('keras_grad_placeholder')\n\n            u_list = []\n            p_list = []\n            for g in grads:\n                if g in grad_parameter_dict:\n                    p_list.append(grad_parameter_dict[g])\n                    u_list.append(g)\n                else:\n                    raise ValueError(\n                        'CNTK backend: when constructing trainer, '\n                        'found gradient node `%s` which is not '\n                        'related to any parameters in the model. '\n                        'Please double check how the gradient node '\n                        'is constructed.' % g)\n\n            if len(u_list) > 0:\n                learner = C.cntk_py.universal_learner(p_list, u_list, update_func)\n\n                criterion = (\n                    outputs[0],\n                    outputs[1]) if len(outputs) > 1 else (\n                    outputs[0],\n                )\n                self.trainer = C.trainer.Trainer(\n                    outputs[0], criterion, [learner])\n                self.trainer_output = tuple([f.output for f in criterion])\n            elif len(u_ops) > 0:\n                unrelated_updates.extend(u_ops)\n\n            if len(unrelated_updates) > 0:\n                self.unrelated_updates = C.combine([_.output for _ in unrelated_updates])\n\n        if self.trainer is None:\n            self.metrics_outputs = [f.output for f in outputs]\n            self.metrics_func = C.combine(self.metrics_outputs)\n        # cntk only could handle loss and 1 metric in trainer, for metrics more\n        # than 2, need manual eval\n        elif len(outputs) > 2:\n            self.metrics_outputs = [f.output for f in outputs[2:]]\n            self.metrics_func = C.combine(self.metrics_outputs)\n        else:\n            self.metrics_func = None\n\n    @staticmethod\n    def _is_input_shape_compatible(input, placeholder):\n        if hasattr(input, 'shape') and hasattr(placeholder, 'shape'):\n            num_dynamic = get_num_dynamic_axis(placeholder)\n            input_shape = input.shape[num_dynamic:]\n            placeholder_shape = placeholder.shape\n            for i, p in zip(input_shape, placeholder_shape):\n                if i != p and p != C.InferredDimension and p != C.FreeDimension:\n                    return False\n        return True\n\n    def __call__(self, inputs):\n        global _LEARNING_PHASE_PLACEHOLDER\n        global _LEARNING_PHASE\n        assert isinstance(inputs, (list, tuple))\n        feed_dict = {}\n        for tensor, value in zip(self.placeholders, inputs):\n            # cntk only support calculate on float, do auto cast here\n            if (hasattr(value, 'dtype') and\n               value.dtype != np.float32 and\n               value.dtype != np.float64):\n                value = value.astype(np.float32)\n\n            if tensor == _LEARNING_PHASE_PLACEHOLDER:\n                _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(value)\n            else:\n                # in current version cntk can't support input with variable\n                # length. Will support it in next release.\n                if not self._is_input_shape_compatible(value, tensor):\n                    raise ValueError('CNTK backend: The placeholder has been resolved '\n                                     'to shape `%s`, but input shape is `%s`. Currently '\n                                     'CNTK can not take variable length inputs. Please '\n                                     'pass inputs that have a static shape.'\n                                     % (str(tensor.shape), str(value.shape)))\n            feed_dict[tensor] = value\n\n        updated = []\n        if self.trainer is not None:\n            input_dict = {}\n            for argument in self.loss.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError(\n                        'CNTK backend: argument %s is not found in inputs. '\n                        'Please double check the model and inputs in '\n                        '`train_function`.' % argument.name)\n\n            result = self.trainer.train_minibatch(\n                input_dict, self.trainer_output)\n\n            assert(len(result) == 2)\n            outputs = result[1]\n            for o in self.trainer_output:\n                updated.append(outputs[o])\n\n        if self.metrics_func is not None:\n            input_dict = {}\n            for argument in self.metrics_func.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError('CNTK backend: metrics argument %s '\n                                     'is not found in inputs. Please double '\n                                     'check the model and inputs.' % argument.name)\n            # Some ops (like dropout) won't be applied during \"eval\" in cntk.\n            # They only evaluated in training phase. To make it work, call\n            # \"forward\" method to let cntk know we want to evaluate them.from\n            # But the assign ops won't be executed under this mode, that's why\n            # we need this check.\n            if (self.unrelated_updates is None and\n                    (_LEARNING_PHASE_PLACEHOLDER.value == 1.0 or _LEARNING_PHASE == 1)):\n                _, output_values = self.metrics_func.forward(\n                    input_dict,\n                    self.metrics_func.outputs,\n                    (self.metrics_func.outputs[0],),\n                    as_numpy=False)\n            else:\n                output_values = self.metrics_func.eval(input_dict, as_numpy=False)\n            if isinstance(output_values, dict):\n                for o in self.metrics_outputs:\n                    value = output_values[o]\n                    v = value.asarray()\n                    updated.append(v)\n            else:\n                v = output_values.asarray()\n                for o in self.metrics_outputs:\n                    updated.append(v)\n\n        if self.unrelated_updates is not None:\n            input_dict = {}\n            for argument in self.unrelated_updates.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError(\n                        'CNTK backend: assign ops argument %s '\n                        'is not found in inputs. Please double '\n                        'check the model and inputs.' % argument.name)\n            self.unrelated_updates.eval(input_dict, as_numpy=False)\n        return updated\n\n\ndef function(inputs, outputs, updates=[], **kwargs):\n    return Function(inputs, outputs, updates=updates, **kwargs)\n\n\ndef temporal_padding(x, padding=(1, 1)):\n    assert len(padding) == 2\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 3 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, [padding], 'channels_last', num_dynamic_axis)\n\n\ndef _padding(x, pattern, axis):  # pragma: no cover\n    base_shape = x.shape\n    if b_any([dim < 0 for dim in base_shape]):\n        raise ValueError('CNTK Backend: padding input tensor with '\n                         'shape `%s` contains non-specified dimension, '\n                         'which is not supported. Please give fixed '\n                         'dimension to enable padding.' % base_shape)\n    if pattern[0] > 0:\n        prefix_shape = list(base_shape)\n        prefix_shape[axis] = pattern[0]\n        prefix_shape = tuple(prefix_shape)\n        x = C.splice(C.constant(value=0, shape=prefix_shape), x, axis=axis)\n        base_shape = x.shape\n    if pattern[1] > 0:\n        postfix_shape = list(base_shape)\n        postfix_shape[axis] = pattern[1]\n        postfix_shape = tuple(postfix_shape)\n        x = C.splice(x, C.constant(value=0, shape=postfix_shape), axis=axis)\n    return x\n\n\ndef pad(x, pad_info, data_format, num_dynamic_axis):\n    if hasattr(C, 'pad'):\n        pattern = [list(p) for p in pad_info]\n        if data_format == 'channels_first':\n            pattern = [[0, 0]] + pattern\n        else:\n            pattern = pattern + [[0, 0]]\n        if num_dynamic_axis == 0:\n            pattern = [[0, 0]] + pattern\n        return C.pad(x, pattern=pattern)\n    else:  # pragma: no cover\n        for (a, p) in enumerate(pad_info):\n            x = _padding(x, p,\n                         a + (1 if num_dynamic_axis == 0 else 0) +\n                         (1 if data_format == 'channels_first' else 0))\n        return x\n\n\ndef spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n    assert len(padding) == 2\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    data_format = normalize_data_format(data_format)\n\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 4 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, padding, data_format, num_dynamic_axis)\n\n\ndef spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n    assert len(padding) == 3\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    assert len(padding[2]) == 2\n    data_format = normalize_data_format(data_format)\n\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 5 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, padding, data_format, num_dynamic_axis)\n\n\ndef one_hot(indices, num_classes):\n    return C.one_hot(indices, num_classes)\n\n\ndef get_value(x):\n    if isinstance(\n            x,\n            C.variables.Parameter) or isinstance(\n            x,\n            C.variables.Constant):\n        return x.value\n    else:\n        return eval(x)\n\n\ndef batch_get_value(xs):\n    result = []\n    for x in xs:\n        if (isinstance(x, C.variables.Parameter) or\n           isinstance(x, C.variables.Constant)):\n            result.append(x.value)\n        else:\n            result.append(eval(x))\n    return result\n\n\ndef set_value(x, value):\n    if (isinstance(x, C.variables.Parameter) or\n       isinstance(x, C.variables.Constant)):\n        if isinstance(value, (float, int)):\n            value = np.full(x.shape, value, dtype=floatx())\n        x.value = value\n    else:\n        raise NotImplementedError\n\n\ndef print_tensor(x, message=''):\n    return C.user_function(\n        LambdaFunc(x,\n                   when=lambda x: True,\n                   execute=lambda x: print(message)))\n\n\ndef batch_set_value(tuples):\n    for t in tuples:\n        x = t[0]\n        value = t[1]\n        if isinstance(value, np.ndarray) is False:\n            value = np.asarray(value)\n        if isinstance(x, C.variables.Parameter):\n            x.value = value\n        else:\n            raise NotImplementedError\n\n\ndef stop_gradient(variables):\n    if isinstance(variables, (list, tuple)):\n        return map(C.stop_gradient, variables)\n    else:\n        return C.stop_gradient(variables)\n\n\ndef switch(condition, then_expression, else_expression):\n    ndim_cond = ndim(condition)\n    ndim_expr = ndim(then_expression)\n    if ndim_cond > ndim_expr:\n        raise ValueError('Rank of condition should be less'\n                         ' than or equal to rank of then and'\n                         ' else expressions. ndim(condition)=' +\n                         str(ndim_cond) + ', ndim(then_expression)'\n                         '=' + str(ndim_expr))\n    elif ndim_cond < ndim_expr:\n        shape_expr = int_shape(then_expression)\n        ndim_diff = ndim_expr - ndim_cond\n        for i in range(ndim_diff):\n            condition = expand_dims(condition)\n            condition = tile(condition, shape_expr[ndim_cond + i])\n    return C.element_select(condition,\n                            then_expression,\n                            else_expression)\n\n\ndef elu(x, alpha=1.):\n    res = C.elu(x)\n    if alpha == 1:\n        return res\n    else:\n        return C.element_select(C.greater(x, 0), res, alpha * res)\n\n\ndef in_top_k(predictions, targets, k):\n    _targets = C.one_hot(targets, predictions.shape[-1])\n    result = C.classification_error(predictions, _targets, topN=k)\n    return 1 - C.reshape(result, shape=())\n\n\ndef conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = (1,) + strides\n    # cntk output_shape does not include batch axis\n    output_shape = output_shape[1:]\n    # in keras2, need handle output shape in different format\n    if data_format == 'channels_last':\n        output_shape = transpose_shape(output_shape, 'channels_first',\n                                       spatial_axes=(0, 1))\n\n    x = C.convolution_transpose(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding],\n        output_shape=output_shape,\n        dilation=dilation_rate)\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef identity(x, name=None):\n    if name is None:\n        name = '%s_alias' % x.name\n    return C.alias(x, name=name)\n\n\ndef _preprocess_conv2d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, rows, cols)\n        # TF input shape: (samples, rows, cols, input_depth)\n        x = C.transpose(x, (2, 0, 1))\n    return x\n\n\ndef _preprocess_conv2d_kernel(kernel, data_format):\n    # As of Keras 2.0.0, all kernels are normalized\n    # on the format `(rows, cols, input_depth, depth)`,\n    # independently of `data_format`.\n    # CNTK expects `(depth, input_depth, rows, cols)`.\n    kernel = C.transpose(kernel, (3, 2, 0, 1))\n    return kernel\n\n\ndef _preprocess_border_mode(padding):\n    if padding == 'same':\n        padding = True\n    elif padding == 'valid':\n        padding = False\n    else:\n        raise ValueError('Invalid border mode: ' + str(padding))\n    return padding\n\n\ndef _postprocess_conv2d_output(x, data_format):\n    if data_format == 'channels_last':\n        x = C.transpose(x, (1, 2, 0))\n    return x\n\n\ndef _preprocess_conv3d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, conv_dim1, conv_dim2, conv_dim3)\n        # TF input shape: (samples, conv_dim1, conv_dim2, conv_dim3,\n        # input_depth)\n        x = C.transpose(x, (3, 0, 1, 2))\n    return x\n\n\ndef _preprocess_conv3d_kernel(kernel, dim_ordering):\n    kernel = C.transpose(kernel, (4, 3, 0, 1, 2))\n    return kernel\n\n\ndef _postprocess_conv3d_output(x, dim_ordering):\n    if dim_ordering == 'channels_last':\n        x = C.transpose(x, (1, 2, 3, 0))\n    return x\n\n\ndef _get_dynamic_axis_num(x):\n    if hasattr(x, 'dynamic_axes'):\n        return len(x.dynamic_axes)\n    else:\n        return 0\n\n\ndef _contain_seqence_axis(x):\n    if _get_dynamic_axis_num(x) > 1:\n        return x.dynamic_axes[1] == C.Axis.default_dynamic_axis()\n    else:\n        return False\n\n\ndef get_num_dynamic_axis(x):\n    return _get_dynamic_axis_num(x)\n\n\ndef _reduce_on_axis(x, axis, reduce_fun_name):\n    if isinstance(axis, list):\n        for a in axis:\n            if isinstance(a, C.Axis) \\\n                    and a != C.Axis.default_batch_axis() \\\n                    and hasattr(C.sequence, reduce_fun_name):\n                x = getattr(C.sequence, reduce_fun_name)(x, a)\n            else:\n                x = getattr(C, reduce_fun_name)(x, a)\n    else:\n        x = getattr(C, reduce_fun_name)(x, axis)\n    return x\n\n\ndef _reshape_sequence(x, time_step):\n    tmp_shape = list(int_shape(x))\n    tmp_shape[1] = time_step\n    return reshape(x, tmp_shape)\n\n\ndef local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    stride = strides[0]\n    kernel_shape = int_shape(kernel)\n    output_length, feature_dim, filters = kernel_shape\n\n    xs = []\n    for i in range(output_length):\n        slice_length = py_slice(i * stride,\n                                i * stride + kernel_size[0])\n        xs.append(reshape(inputs[:, slice_length, :],\n                          (-1, 1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=1)\n    # transpose kernel to output_filters first, to apply broadcast\n    weight = permute_dimensions(kernel, (2, 0, 1))\n    # Shape: (batch, filters, output_length, input_length * kernel_size)\n    output = x_aggregate * weight\n    # Shape: (batch, filters, output_length)\n    output = sum(output, axis=3)\n    # Shape: (batch, output_length, filters)\n    return permute_dimensions(output, (0, 2, 1))\n\n\ndef local_conv2d(inputs,\n                 kernel,\n                 kernel_size,\n                 strides,\n                 output_shape,\n                 data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    stride_row, stride_col = strides\n    output_row, output_col = output_shape\n    kernel_shape = int_shape(kernel)\n    _, feature_dim, filters = kernel_shape\n    xs = []\n\n    for i in range(output_row):\n        for j in range(output_col):\n            slice_row = py_slice(i * stride_row,\n                                 i * stride_row + kernel_size[0])\n            slice_col = py_slice(j * stride_col,\n                                 j * stride_col + kernel_size[1])\n            if data_format == 'channels_first':\n                xs.append(reshape(inputs[:, :, slice_row, slice_col],\n                                  (-1, 1, feature_dim)))\n            else:\n                xs.append(reshape(inputs[:, slice_row, slice_col, :],\n                                  (-1, 1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=1)\n    # transpose kernel to put filters first\n    weight = permute_dimensions(kernel, (2, 0, 1))\n    # shape: batch, filters, output_length, input_length * kernel_size\n    output = x_aggregate * weight\n    # shape: batch, filters, output_length\n    output = sum(output, axis=3)\n    # shape: batch, filters, row, col\n    output = reshape(output,\n                     (-1, filters, output_row, output_col))\n\n    if data_format == 'channels_last':\n        # shape: batch, row, col, filters\n        output = permute_dimensions(output, (0, 2, 3, 1))\n\n    return output\n\n\ndef reverse(x, axes):\n    if isinstance(axes, int):\n        axes = [axes]\n    cntk_axes = _normalize_axis(axes, x)\n    begin_index = [0 for _ in cntk_axes]\n    end_index = [0 for _ in cntk_axes]\n    strides = [-1 for _ in cntk_axes]\n    return C.slice(x, cntk_axes, begin_index, end_index, strides)\n\n\ndef slice(x, start, size):\n    raise NotImplementedError\n\n\ndef _reshape_batch(x, shape):\n    # there is a bug in cntk 2.1's unpack_batch implementation\n    if hasattr(C, 'unpack_batch') and _get_cntk_version() >= 2.2:\n        const_a = C.unpack_batch(x)\n        const_a = C.reshape(const_a, shape)\n        return C.to_batch(const_a)\n    else:\n        return C.user_function(ReshapeBatch(x, shape[1:]))\n\n\ndef _get_cntk_version():\n    version = C.__version__\n    if version.endswith('+'):\n        version = version[:-1]\n    # for hot fix, ignore all the . except the first one.\n    if len(version) > 2 and version[1] == '.':\n        version = version[:2] + version[2:].replace('.', '')\n    try:\n        return float(version)\n    except:\n        warnings.warn(\n            'CNTK backend warning: CNTK version not detected. '\n            'Will using CNTK 2.0 GA as default.')\n        return float(2.0)\n\n\nclass ReshapeBatch(C.ops.functions.UserFunction):\n    def __init__(self, input, shape, name='reshape_with_batch'):\n        super(ReshapeBatch, self).__init__([input], as_numpy=False, name=name)\n        self.from_shape = input.shape\n        self.target_shape = shape\n\n    def infer_outputs(self):\n        batch_axis = C.Axis.default_batch_axis()\n        return [\n            C.output_variable(\n                self.target_shape,\n                self.inputs[0].dtype,\n                [batch_axis])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        num_element = arguments.shape()[0] * np.prod(np.asarray(self.from_shape))\n        num_static_element = np.prod(np.asarray(self.target_shape))\n        num_batch = int(num_element / num_static_element)\n        result = arguments.data().as_shape((num_batch,) + self.target_shape)\n        return None, C.cntk_py.Value(result)\n\n    def backward(self, state, root_gradients):\n        grad_array_view = root_gradients.data()\n        num_element = root_gradients.shape()[0] * np.prod(np.asarray(self.target_shape))\n        num_static_element = np.prod(np.asarray(self.from_shape))\n        num_old_batch = int(num_element / num_static_element)\n        return C.cntk_py.Value(\n            grad_array_view.as_shape(\n                (num_old_batch,) + self.from_shape))\n\n\nclass ConvertToBatch(C.ops.functions.UserFunction):\n    \"\"\"Converts input first axis to CNTK batch axis.\n\n    We may introduce this operation in CNTK native\n    implementation later.\n\n    # Arguments\n        inputs: a cntk variable (parameter/constant)\n        name: name of this node\n    \"\"\"\n\n    def __init__(self, input, name='convert_to_batch'):\n        super(ConvertToBatch, self).__init__([input], as_numpy=False, name=name)\n\n    def infer_outputs(self):\n        batch_axis = C.Axis.default_batch_axis()\n        return [\n            C.output_variable(\n                self.inputs[0].shape[1:],\n                self.inputs[0].dtype,\n                [batch_axis])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        return None, C.cntk_py.Value(arguments.data())\n\n    def backward(self, state, root_gradients):\n        return C.cntk_py.Value(root_gradients.data())\n\n\nclass ConvertToStatic(C.ops.functions.UserFunction):\n    \"\"\"Converts input first axis to CNTK static axis.\n\n    We may introduce this operation in CNTK native\n    implementation later.\n\n    # Arguments\n        inputs: a cntk tensor which has batch axis\n        batch_size: size of batch axis.\n        name: name of this node.\n    \"\"\"\n\n    def __init__(self, input, batch_size, name='convert_to_static'):\n        super(ConvertToStatic, self).__init__([input], as_numpy=False, name=name)\n        self.target_shape = (batch_size,) + input.shape\n\n    def infer_outputs(self):\n        return [\n            C.output_variable(\n                self.target_shape,\n                self.inputs[0].dtype,\n                [])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        return None, C.cntk_py.Value(arguments.data())\n\n    def backward(self, state, root_gradients):\n        return C.cntk_py.Value(root_gradients.data())\n\n\nclass LambdaFunc(C.ops.functions.UserFunction):\n    def __init__(self,\n                 arg,\n                 when=lambda arg: True,\n                 execute=lambda arg: print(arg),\n                 name=''):\n        self.when = when\n        self.execute = execute\n\n        super(LambdaFunc, self).__init__([arg], name=name)\n\n    def infer_outputs(self):\n        return [\n            C.output_variable(\n                self.inputs[0].shape,\n                self.inputs[0].dtype,\n                self.inputs[0].dynamic_axes)]\n\n    def forward(self, argument, device=None, outputs_to_retain=None):\n        if self.when(argument):\n            self.execute(argument)\n\n        return None, argument\n\n    def backward(self, state, root_gradients):\n        return root_gradients\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cntk as C\nimport numpy as np\nfrom .common import floatx\nfrom .common import epsilon\nfrom .common import image_data_format\nfrom .common import normalize_data_format\nfrom ..utils.generic_utils import transpose_shape\nfrom collections import defaultdict\nfrom contextlib import contextmanager\nimport warnings\n\n\nC.set_global_option('align_axis', 1)\n\nb_any = any\npy_slice = slice\n\n\ndev = C.device.use_default_device()\nif dev.type() == 0:\n    warnings.warn(\n        'CNTK backend warning: GPU is not detected. '\n        'CNTK\\'s CPU version is not fully optimized,'\n        'please run with GPU to get better performance.')\n\n# A learning phase is a bool tensor used to run Keras models in\n# either train mode (learning_phase == 1) or test mode (learning_phase == 0).\n# LEARNING_PHASE_PLACEHOLDER is the placeholder for dynamic learning phase\n_LEARNING_PHASE_PLACEHOLDER = C.constant(shape=(), dtype=np.float32, value=1.0, name='_keras_learning_phase')\n# static learning phase flag, if it is not 0 or 1, we will go with dynamic learning phase tensor.\n_LEARNING_PHASE = -1\n_UID_PREFIXES = defaultdict(int)\n\n# cntk doesn't support gradient as symbolic op, to hook up with keras model,\n# we will create gradient as a constant placeholder, here use this global\n# map to keep the mapping from grad placeholder to parameter\ngrad_parameter_dict = {}\n\nNAME_SCOPE_STACK = []\n\n\n@contextmanager\ndef name_scope(name):\n    global NAME_SCOPE_STACK\n    NAME_SCOPE_STACK.append(name)\n    yield\n    NAME_SCOPE_STACK.pop()\n\n\ndef get_uid(prefix=''):\n    _UID_PREFIXES[prefix] += 1\n    return _UID_PREFIXES[prefix]\n\n\ndef learning_phase():\n    # If _LEARNING_PHASE is not 0 or 1, return dynamic learning phase tensor\n    return _LEARNING_PHASE if _LEARNING_PHASE in {0, 1} else _LEARNING_PHASE_PLACEHOLDER\n\n\ndef set_learning_phase(value):\n    global _LEARNING_PHASE\n    if value not in {0, 1}:\n        raise ValueError('CNTK Backend: Set learning phase '\n                         'with value %s is not supported, '\n                         'expected 0 or 1.' % value)\n    _LEARNING_PHASE = value\n\n\ndef clear_session():\n    \"\"\"Reset learning phase flag for cntk backend.\n    \"\"\"\n    global _LEARNING_PHASE\n    global _LEARNING_PHASE_PLACEHOLDER\n    _LEARNING_PHASE = -1\n    _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(1.0)\n\n\ndef in_train_phase(x, alt, training=None):\n    global _LEARNING_PHASE\n    if training is None:\n        training = learning_phase()\n        uses_learning_phase = True\n    else:\n        uses_learning_phase = False\n\n    # CNTK currently don't support cond op, so here we use\n    # element_select approach as workaround. It may have\n    # perf issue, will resolve it later with cntk cond op.\n    if callable(x) and isinstance(x, C.cntk_py.Function) is False:\n        x = x()\n    if callable(alt) and isinstance(alt, C.cntk_py.Function) is False:\n        alt = alt()\n\n    if training is True:\n        x._uses_learning_phase = uses_learning_phase\n        return x\n    else:\n        # if _LEARNING_PHASE is static\n        if isinstance(training, int) or isinstance(training, bool):\n            result = x if training == 1 or training is True else alt\n        else:\n            result = C.element_select(training, x, alt)\n        result._uses_learning_phase = uses_learning_phase\n        return result\n\n\ndef in_test_phase(x, alt, training=None):\n    return in_train_phase(alt, x, training=training)\n\n\ndef _convert_string_dtype(dtype):\n    if dtype == 'float32':\n        return np.float32\n    elif dtype == 'float64':\n        return np.float64\n    elif dtype == 'float16':\n        return np.float16\n    else:\n        # cntk only running with float,\n        # try to cast to float to run the model\n        return np.float32\n\n\ndef _convert_dtype_string(dtype):\n    if dtype == np.float32:\n        return 'float32'\n    elif dtype == np.float64:\n        return 'float64'\n    elif dtype == np.float16:\n        return 'float16'\n    else:\n        raise ValueError('CNTK Backend: Unsupported dtype: %s. '\n                         'CNTK only supports float32, float64, and '\n                         'float16.' % dtype)\n\n\ndef variable(value, dtype=None, name=None, constraint=None):\n    \"\"\"Instantiates a variable and returns it.\n\n    # Arguments\n        value: Numpy array, initial value of the tensor.\n        dtype: Tensor type.\n        name: Optional name string for the tensor.\n        constraint: Optional projection function to be\n            applied to the variable after an optimizer update.\n\n    # Returns\n        A variable instance (with Keras metadata included).\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n\n    if name is None:\n        name = ''\n\n    if isinstance(\n            value,\n            C.variables.Constant) or isinstance(\n            value,\n            C.variables.Parameter):\n        value = value.value\n\n    # we don't support init parameter with symbolic op, so eval it first as\n    # workaround\n    if isinstance(value, C.cntk_py.Function):\n        value = eval(value)\n\n    shape = value.shape if hasattr(value, 'shape') else ()\n    if hasattr(value, 'dtype') and value.dtype != dtype and len(shape) > 0:\n        value = value.astype(dtype)\n\n    # TODO: remove the conversion when cntk supports int32, int64\n    # https://docs.microsoft.com/en-us/python/api/cntk.variables.parameter\n    dtype = 'float32' if 'int' in str(dtype) else dtype\n\n    v = C.parameter(shape=shape,\n                    init=value,\n                    dtype=dtype,\n                    name=_prepare_name(name, 'variable'))\n    v._keras_shape = v.shape\n    v._uses_learning_phase = False\n    v.constraint = constraint\n    return v\n\n\ndef bias_add(x, bias, data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    dims = len(x.shape)\n    if dims > 0 and x.shape[0] == C.InferredDimension:\n        dims -= 1\n\n    bias_dims = len(bias.shape)\n    if bias_dims != 1 and bias_dims != dims:\n        raise ValueError('Unexpected bias dimensions %d, '\n                         'expected 1 or %d dimensions' % (bias_dims, dims))\n\n    if dims == 4:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1, 1, 1)\n            else:\n                shape = (bias.shape[3],) + bias.shape[:3]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, 1, 1, bias.shape[0])\n            else:\n                shape = bias.shape\n    elif dims == 3:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1, 1)\n            else:\n                shape = (bias.shape[2],) + bias.shape[:2]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, 1, bias.shape[0])\n            else:\n                shape = bias.shape\n    elif dims == 2:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1)\n            else:\n                shape = (bias.shape[1],) + bias.shape[:1]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, bias.shape[0])\n            else:\n                shape = bias.shape\n    else:\n        shape = bias.shape\n    return x + reshape(bias, shape)\n\n\ndef eval(x):\n    if isinstance(x, C.cntk_py.Function):\n        return x.eval()\n    elif isinstance(x, C.variables.Constant) or isinstance(x, C.variables.Parameter):\n        return x.value\n    else:\n        raise ValueError('CNTK Backend: `eval` method on '\n                         '`%s` type is not supported. '\n                         'CNTK only supports `eval` with '\n                         '`Function`, `Constant` or '\n                         '`Parameter`.' % type(x))\n\n\ndef placeholder(\n        shape=None,\n        ndim=None,\n        dtype=None,\n        sparse=False,\n        name=None,\n        dynamic_axis_num=1):\n    if dtype is None:\n        dtype = floatx()\n    if not shape:\n        if ndim:\n            shape = tuple([None for _ in range(ndim)])\n\n    dynamic_dimension = C.FreeDimension if _get_cntk_version() >= 2.2 else C.InferredDimension\n    cntk_shape = [dynamic_dimension if s is None else s for s in shape]\n    cntk_shape = tuple(cntk_shape)\n\n    if dynamic_axis_num > len(cntk_shape):\n        raise ValueError('CNTK backend: creating placeholder with '\n                         '%d dimension is not supported, at least '\n                         '%d dimensions are needed.'\n                         % (len(cntk_shape), dynamic_axis_num))\n\n    if name is None:\n        name = ''\n\n    cntk_shape = cntk_shape[dynamic_axis_num:]\n\n    x = C.input(\n        shape=cntk_shape,\n        dtype=_convert_string_dtype(dtype),\n        is_sparse=sparse,\n        name=name)\n    x._keras_shape = shape\n    x._uses_learning_phase = False\n    x._cntk_placeholder = True\n    return x\n\n\ndef is_placeholder(x):\n    \"\"\"Returns whether `x` is a placeholder.\n\n    # Arguments\n        x: A candidate placeholder.\n\n    # Returns\n        Boolean.\n    \"\"\"\n    return hasattr(x, '_cntk_placeholder') and x._cntk_placeholder\n\n\ndef is_keras_tensor(x):\n    if not is_tensor(x):\n        raise ValueError('Unexpectedly found an instance of type `' +\n                         str(type(x)) + '`. '\n                         'Expected a symbolic tensor instance.')\n    return hasattr(x, '_keras_history')\n\n\ndef is_tensor(x):\n    return isinstance(x, (C.variables.Constant,\n                          C.variables.Variable,\n                          C.variables.Parameter,\n                          C.ops.functions.Function))\n\n\ndef shape(x):\n    shape = list(int_shape(x))\n    num_dynamic = _get_dynamic_axis_num(x)\n    non_dyn_shape = []\n    for i in range(len(x.shape)):\n        if shape[i + num_dynamic] is None:\n            non_dyn_shape.append(x.shape[i])\n        else:\n            non_dyn_shape.append(shape[i + num_dynamic])\n    return shape[:num_dynamic] + non_dyn_shape\n\n\ndef is_sparse(tensor):\n    return tensor.is_sparse\n\n\ndef int_shape(x):\n    if hasattr(x, '_keras_shape'):\n        return x._keras_shape\n\n    shape = x.shape\n    if hasattr(x, 'dynamic_axes'):\n        dynamic_shape = [None for a in x.dynamic_axes]\n        shape = tuple(dynamic_shape) + shape\n    return shape\n\n\ndef ndim(x):\n    shape = int_shape(x)\n    return len(shape)\n\n\ndef _prepare_name(name, default):\n    prefix = '_'.join(NAME_SCOPE_STACK)\n    if name is None or name == '':\n        return prefix + '/' + default\n    return prefix + '/' + name\n\n\ndef constant(value, dtype=None, shape=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    if shape is None:\n        shape = ()\n    np_value = value * np.ones(shape)\n    const = C.constant(np_value,\n                       dtype=dtype,\n                       name=_prepare_name(name, 'constant'))\n    const._keras_shape = const.shape\n    const._uses_learning_phase = False\n    return const\n\n\ndef random_binomial(shape, p=0.0, dtype=None, seed=None):\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e7)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n    return C.random.bernoulli(shape=shape, dtype=dtype, mean=p, seed=seed)\n\n\ndef random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n    return C.random.uniform(shape=shape, dtype=dtype, low=minval, high=maxval, seed=seed)\n\n\ndef random_uniform_variable(shape, low, high,\n                            dtype=None, name=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    if name is None:\n        name = ''\n\n    scale = (high - low) / 2\n    p = C.parameter(\n        shape,\n        init=C.initializer.uniform(\n            scale,\n            seed=seed),\n        dtype=dtype,\n        name=name)\n    return variable(value=p.value + low + scale)\n\n\ndef random_normal_variable(\n        shape,\n        mean,\n        scale,\n        dtype=None,\n        name=None,\n        seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e7)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    if name is None:\n        name = ''\n\n    p = C.parameter(\n        shape=shape,\n        init=C.initializer.normal(\n            scale=scale,\n            seed=seed),\n        dtype=dtype,\n        name=name)\n    return variable(value=p.value + mean)\n\n\ndef random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n    return C.random.normal(shape=shape, mean=mean, scale=stddev, seed=seed, dtype=dtype)\n\n\ndef truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if seed is None:\n        seed = np.random.randint(1, 10e6)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    return C.parameter(\n        shape, init=C.initializer.truncated_normal(\n            stddev, seed=seed), dtype=dtype)\n\n\ndef dtype(x):\n    return _convert_dtype_string(x.dtype)\n\n\ndef zeros(shape, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    ctype = _convert_string_dtype(dtype)\n    return variable(value=np.zeros(shape, ctype), dtype=dtype, name=name)\n\n\ndef ones(shape, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    ctype = _convert_string_dtype(dtype)\n    return variable(value=np.ones(shape, ctype), dtype=dtype, name=name)\n\n\ndef eye(size, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    return variable(np.eye(size), dtype, name)\n\n\ndef zeros_like(x, dtype=None, name=None):\n    return x * 0\n\n\ndef ones_like(x, dtype=None, name=None):\n    return zeros_like(x) + 1\n\n\ndef count_params(x):\n    for _ in x.shape:\n        if _ == C.InferredDimension or _ == C.FreeDimension:\n            raise ValueError('CNTK backend: `count_params` with dynamic '\n                             'shape is not supported. Please provide '\n                             'fixed dimension instead of `None`.')\n\n    return np.prod(int_shape(x))\n\n\ndef cast(x, dtype):\n    # cntk calculate everything in float, so don't need case from bool / int\n    return x\n\n\ndef dot(x, y):\n    if len(x.shape) > 2 or len(y.shape) > 2:\n        y_shape = int_shape(y)\n        if len(y_shape) > 2:\n            permutation = [len(y_shape) - 2]\n            permutation += list(range(len(y_shape) - 2))\n            permutation += [len(y_shape) - 1]\n            y = C.transpose(y, perm=permutation)\n        return C.times(x, y, len(y_shape) - 1)\n    else:\n        return C.times(x, y)\n\n\ndef batch_dot(x, y, axes=None):\n    x_shape = int_shape(x)\n    y_shape = int_shape(y)\n\n    if isinstance(axes, int):\n        axes = (axes, axes)\n    if axes is None:\n        # behaves like tf.batch_matmul as default\n        axes = [len(x_shape) - 1, len(y_shape) - 2]\n    if b_any([isinstance(a, (list, tuple)) for a in axes]):\n        raise ValueError('Multiple target dimensions are not supported. ' +\n                         'Expected: None, int, (int, int), ' +\n                         'Provided: ' + str(axes))\n\n    if len(x_shape) == 2 and len(y_shape) == 2:\n        if axes[0] == axes[1]:\n            result = sum(x * y, axis=axes[0], keepdims=True)\n            return result if axes[0] == 1 else transpose(result)\n        else:\n            return sum(x * transpose(y), axis=axes[0], keepdims=True)\n    else:\n        if len(y_shape) == 2:\n            y = expand_dims(y)\n\n        normalized_axis = []\n        normalized_axis.append(_normalize_axis(axes[0], x)[0])\n        normalized_axis.append(_normalize_axis(axes[1], y)[0])\n        # transpose\n        i = normalized_axis[0]\n        while i < len(x.shape) - 1:\n            x = C.swapaxes(x, i, i + 1)\n            i += 1\n        i = normalized_axis[1]\n        while i > 0:\n            y = C.swapaxes(y, i, i - 1)\n            i -= 1\n        result = C.times(x, y, output_rank=(len(y.shape) - 1)\n                         if len(y.shape) > 1 else 1)\n        if len(y_shape) == 2:\n            result = squeeze(result, -1)\n        return result\n\n\ndef transpose(x):\n    return C.swapaxes(x, 0, 1)\n\n\ndef gather(reference, indices):\n    # There is a bug in cntk gather op which may cause crash.\n    # We have made a fix but not catched in CNTK 2.1 release.\n    # Will update with gather op in next release\n    if _get_cntk_version() >= 2.2:\n        return C.ops.gather(reference, indices)\n    else:\n        num_classes = reference.shape[0]\n        one_hot_matrix = C.ops.one_hot(indices, num_classes)\n        return C.times(one_hot_matrix, reference, output_rank=len(reference.shape) - 1)\n\n\ndef _remove_dims(x, axis, keepdims=False):\n    if keepdims is False and isinstance(axis, list):\n        # sequence axis is removed by default, so don't need reshape on it\n        reduce_axes = []\n        for a in axis:\n            if isinstance(a, C.Axis) is False:\n                reduce_axes.append(a)\n        return _reshape_dummy_dim(x, reduce_axes)\n    else:\n        if isinstance(axis, list):\n            has_seq = False\n            for a in axis:\n                if isinstance(a, C.Axis):\n                    has_seq = True\n                    break\n            if has_seq:\n                nones = _get_dynamic_axis_num(x)\n                x = expand_dims(x, nones)\n        return x\n\n\ndef max(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_max')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef min(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_min')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef sum(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_sum')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef prod(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_prod')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef logsumexp(x, axis=None, keepdims=False):\n    return log(sum(exp(x), axis=axis, keepdims=keepdims))\n\n\ndef var(x, axis=None, keepdims=False):\n    m = mean(x, axis, keepdims=True)\n    devs_squared = C.square(x - m)\n    return mean(devs_squared, axis=axis, keepdims=keepdims)\n\n\ndef std(x, axis=None, keepdims=False):\n    return C.sqrt(var(x, axis=axis, keepdims=keepdims))\n\n\ndef expand_dims(x, axis=-1):\n    shape = list(int_shape(x))\n    nones = _get_dynamic_axis_num(x)\n    index = axis if axis >= 0 else len(shape) + 1\n    shape.insert(index, 1)\n    new_shape = shape[nones:]\n    new_shape = tuple(\n        [C.InferredDimension if _ is None else _ for _ in new_shape])\n    result = C.reshape(x, new_shape)\n    if index < nones:\n        result._keras_shape = shape\n    return result\n\n\ndef squeeze(x, axis):\n    if isinstance(axis, tuple):\n        axis = list(axis)\n    if not isinstance(axis, list):\n        axis = [axis]\n\n    shape = list(int_shape(x))\n\n    _axis = []\n    for _ in axis:\n        if isinstance(_, int):\n            _axis.append(_ if _ >= 0 else _ + len(shape))\n\n    if len(_axis) == 0:\n        return x\n\n    nones = _get_dynamic_axis_num(x)\n    for _ in sorted(_axis, reverse=True):\n        del shape[_]\n\n    new_shape = shape[nones:]\n    new_shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in new_shape])\n    return C.reshape(x, new_shape)\n\n\ndef tile(x, n):\n    if isinstance(n, int):\n        n = (n,)\n    elif isinstance(n, list):\n        n = tuple(n)\n\n    shape = int_shape(x)\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    # Padding the axis\n    if len(n) < len(shape):\n        n = tuple([1 for _ in range(len(shape) - len(n))]) + n\n\n    if len(n) != len(shape):\n        raise NotImplementedError\n\n    i = num_dynamic_axis\n    for i, rep in enumerate(n):\n        if i >= num_dynamic_axis and shape[i] is not None:\n            tmp = [x] * rep\n            x = C.splice(*tmp, axis=i - num_dynamic_axis)\n        i += 1\n\n    return x\n\n\ndef _normalize_axis(axis, x):\n    shape = int_shape(x)\n    ndim = len(shape)\n\n    nones = _get_dynamic_axis_num(x)\n\n    if nones > ndim:\n        raise ValueError('CNTK Backend: tensor with keras shape: `%s` has '\n                         '%d cntk dynamic axis, this is not expected, please '\n                         'double check the keras shape history.' % (str(shape), nones))\n\n    # Current cntk does not support shape like (1, batch). so using the workaround\n    # here to mapping the correct axis. Will remove this tricky after we add support\n    # in native cntk op\n    cntk_axis = []\n    dynamic_axis_index = 0\n    for i in range(ndim):\n        if shape[i] is None and dynamic_axis_index < nones:\n            cntk_axis.append(x.dynamic_axes[dynamic_axis_index])\n            dynamic_axis_index += 1\n        else:\n            cntk_axis.append(i - dynamic_axis_index)\n\n    if dynamic_axis_index < nones:\n        i = 0\n        while dynamic_axis_index < nones:\n            cntk_axis[i] = x.dynamic_axes[dynamic_axis_index]\n            i += 1\n            dynamic_axis_index += 1\n\n        while i < len(cntk_axis):\n            cntk_axis[i] -= nones\n            i += 1\n\n    if isinstance(axis, tuple):\n        _axis = list(axis)\n    elif isinstance(axis, int):\n        _axis = [axis]\n    elif isinstance(axis, list):\n        _axis = list(axis)\n    else:\n        _axis = axis\n\n    if isinstance(_axis, list):\n        for i, a in enumerate(_axis):\n            if a is not None and a < 0:\n                _axis[i] = (a % ndim)\n            if _axis[i] is not None:\n                _axis[i] = cntk_axis[_axis[i]]\n    else:\n        if _axis is None:\n            _axis = C.Axis.all_axes()\n\n    return _axis\n\n\ndef _reshape_dummy_dim(x, axis):\n    shape = list(x.shape)\n\n    _axis = [_ + len(shape) if _ < 0 else _ for _ in axis]\n\n    if shape.count(C.InferredDimension) > 1 or shape.count(C.FreeDimension) > 1:\n        result = x\n        for index in sorted(_axis, reverse=True):\n            result = C.reshape(result,\n                               shape=(),\n                               begin_axis=index,\n                               end_axis=index + 1)\n        return result\n    else:\n        for index in sorted(_axis, reverse=True):\n            del shape[index]\n\n        shape = [C.InferredDimension if _ == C.FreeDimension else _ for _ in shape]\n        return C.reshape(x, shape)\n\n\ndef mean(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_mean')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef any(x, axis=None, keepdims=False):\n    reduce_result = sum(x, axis, keepdims=keepdims)\n    any_matrix = C.element_select(\n        reduce_result,\n        ones_like(reduce_result),\n        zeros_like(reduce_result))\n    if len(reduce_result.shape) == 0 and _get_dynamic_axis_num(x) == 0:\n        return C.reduce_sum(any_matrix)\n    else:\n        return any_matrix\n\n\ndef all(x, axis=None, keepdims=False):\n    reduce_result = prod(x, axis, keepdims=keepdims)\n    all_matrix = C.element_select(\n        reduce_result,\n        ones_like(reduce_result),\n        zeros_like(reduce_result))\n    if len(reduce_result.shape) == 0 and _get_dynamic_axis_num(x) == 0:\n        return C.reduce_sum(all_matrix)\n    else:\n        return all_matrix\n\n\ndef classification_error(target, output, axis=-1):\n    return C.ops.reduce_mean(\n        C.equal(\n            argmax(\n                output,\n                axis=-1),\n            argmax(\n                target,\n                axis=-1)),\n        axis=C.Axis.all_axes())\n\n\ndef argmax(x, axis=-1):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    output = C.ops.argmax(x, axis=axis[0])\n    return _reshape_dummy_dim(output, axis)\n\n\ndef argmin(x, axis=-1):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    output = C.ops.argmin(x, axis=axis[0])\n    return _reshape_dummy_dim(output, axis)\n\n\ndef square(x):\n    return C.square(x)\n\n\ndef abs(x):\n    return C.abs(x)\n\n\ndef sqrt(x):\n    return C.sqrt(x)\n\n\ndef exp(x):\n    return C.exp(x)\n\n\ndef log(x):\n    return C.log(x)\n\n\ndef round(x):\n    return C.round(x)\n\n\ndef sigmoid(x):\n    return C.sigmoid(x)\n\n\ndef sign(x):\n    return x / C.abs(x)\n\n\ndef pow(x, a):\n    return C.pow(x, a)\n\n\ndef clip(x, min_value, max_value):\n    if max_value is not None and max_value < min_value:\n        max_value = min_value\n    if max_value is None:\n        max_value = np.inf\n    if min_value is None:\n        min_value = -np.inf\n    return C.clip(x, min_value, max_value)\n\n\ndef binary_crossentropy(target, output, from_logits=False):\n    if from_logits:\n        output = C.sigmoid(output)\n    output = C.clip(output, epsilon(), 1.0 - epsilon())\n    output = -target * C.log(output) - (1.0 - target) * C.log(1.0 - output)\n    return output\n\n\ndef get_variable_shape(x):\n    return int_shape(x)\n\n\ndef update(x, new_x):\n    return C.assign(x, new_x)\n\n\ndef moving_average_update(variable, value, momentum):\n    return C.assign(variable, variable * momentum + value * (1. - momentum))\n\n\ndef update_add(x, increment):\n    result = x + increment\n    return C.assign(x, result)\n\n\ndef gradients(loss, variables):\n    # cntk does not support gradients as symbolic op,\n    # to hook up with keras model\n    # we will return a constant as place holder, the cntk learner will apply\n    # the gradient during training.\n    global grad_parameter_dict\n    if isinstance(variables, list) is False:\n        variables = [variables]\n    grads = []\n    for v in variables:\n        g = C.constant(0, shape=v.shape, name='keras_grad_placeholder')\n        grads.append(g)\n        grad_parameter_dict[g] = v\n    return grads\n\n\ndef equal(x, y):\n    return C.equal(x, y)\n\n\ndef not_equal(x, y):\n    return C.not_equal(x, y)\n\n\ndef greater(x, y):\n    return C.greater(x, y)\n\n\ndef greater_equal(x, y):\n    return C.greater_equal(x, y)\n\n\ndef less(x, y):\n    return C.less(x, y)\n\n\ndef less_equal(x, y):\n    return C.less_equal(x, y)\n\n\ndef maximum(x, y):\n    return C.element_max(x, y)\n\n\ndef minimum(x, y):\n    return C.element_min(x, y)\n\n\ndef sin(x):\n    return C.sin(x)\n\n\ndef cos(x):\n    return C.cos(x)\n\n\ndef normalize_batch_in_training(x, gamma, beta,\n                                reduction_axes, epsilon=1e-3):\n    if gamma is None:\n        if beta is None:\n            gamma = ones_like(x)\n        else:\n            gamma = ones_like(beta)\n    if beta is None:\n        if gamma is None:\n            beta = zeros_like(x)\n        else:\n            beta = zeros_like(gamma)\n\n    mean, variant = _moments(x, _normalize_axis(reduction_axes, x))\n\n    if sorted(reduction_axes) == list(range(ndim(x)))[:-1]:\n        normalized = batch_normalization(\n            x, mean, variant, beta, gamma, epsilon)\n    else:\n        # need broadcasting\n        target_shape = []\n        x_shape = int_shape(x)\n        # skip the batch axis\n        for axis in range(1, ndim(x)):\n            if axis in reduction_axes:\n                target_shape.append(1)\n                if ndim(gamma) > axis:\n                    gamma = C.reduce_mean(gamma, axis - 1)\n                    beta = C.reduce_mean(beta, axis - 1)\n            else:\n                target_shape.append(x_shape[axis])\n\n        broadcast_mean = C.reshape(mean, target_shape)\n        broadcast_var = C.reshape(variant, target_shape)\n        broadcast_gamma = C.reshape(gamma, target_shape)\n        broadcast_beta = C.reshape(beta, target_shape)\n        normalized = batch_normalization(\n            x,\n            broadcast_mean,\n            broadcast_var,\n            broadcast_beta,\n            broadcast_gamma,\n            epsilon)\n\n    return normalized, mean, variant\n\n\ndef _moments(x, axes=None, shift=None, keep_dims=False):\n    _axes = tuple(axes)\n    if shift is None:\n        shift = x\n        # Compute true mean while keeping the dims for proper broadcasting.\n        for axis in _axes:\n            shift = C.reduce_mean(shift, axis=axis)\n\n    shift = C.stop_gradient(shift)\n    shifted_mean = C.minus(x, shift)\n    for axis in _axes:\n        shifted_mean = C.reduce_mean(shifted_mean, axis=axis)\n\n    variance_mean = C.square(C.minus(x, shift))\n    for axis in _axes:\n        variance_mean = C.reduce_mean(variance_mean, axis=axis)\n\n    variance = C.minus(variance_mean, C.square(shifted_mean))\n    mean = C.plus(shifted_mean, shift)\n\n    if not keep_dims:\n        mean = squeeze(mean, _axes)\n        variance = squeeze(variance, _axes)\n\n    return mean, variance\n\n\ndef batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n    # The mean / var / beta / gamma may be processed by broadcast\n    # so it may have an extra batch axis with 1, it is not needed\n    # in cntk, need to remove those dummy axis.\n    if ndim(mean) == ndim(x) and shape(mean)[0] == 1:\n        mean = _reshape_dummy_dim(mean, [0])\n    if ndim(var) == ndim(x) and shape(var)[0] == 1:\n        var = _reshape_dummy_dim(var, [0])\n\n    if gamma is None:\n        gamma = ones_like(var)\n    elif ndim(gamma) == ndim(x) and shape(gamma)[0] == 1:\n        gamma = _reshape_dummy_dim(gamma, [0])\n\n    if beta is None:\n        beta = zeros_like(mean)\n    elif ndim(beta) == ndim(x) and shape(beta)[0] == 1:\n        beta = _reshape_dummy_dim(beta, [0])\n\n    return (x - mean) / C.sqrt(var + epsilon) * gamma + beta\n\n\ndef concatenate(tensors, axis=-1):\n    if len(tensors) == 0:\n        return None\n\n    axis = [axis]\n    axis = _normalize_axis(axis, tensors[0])\n    return C.splice(*tensors, axis=axis[0])\n\n\ndef flatten(x):\n    return reshape(x, (-1,))\n\n\ndef reshape(x, shape):\n    shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in shape])\n    if isinstance(x, C.variables.Parameter):\n        return C.reshape(x, shape)\n    else:\n        num_dynamic_axis = _get_dynamic_axis_num(x)\n\n        if num_dynamic_axis == 1 and len(shape) > 0 and shape[0] == -1:\n            # collapse axis with batch axis\n            if b_any(_ == C.InferredDimension for _ in x.shape) or b_any(\n                    _ == C.FreeDimension for _ in x.shape):\n                warnings.warn(\n                    'Warning: CNTK backend does not support '\n                    'collapse of batch axis with inferred dimension. '\n                    'The reshape did not take place.')\n                return x\n            return _reshape_batch(x, shape)\n        else:\n            # no collapse, then first need to padding the shape\n            if num_dynamic_axis >= len(shape):\n                i = 0\n                while i < len(shape):\n                    if shape[i] is None or shape[i] == -1:\n                        i += 1\n                    else:\n                        break\n                shape = tuple([-1 for _ in range(num_dynamic_axis - i)]) + shape\n\n            new_shape = list(shape)\n            new_shape = new_shape[num_dynamic_axis:]\n            new_shape = [C.InferredDimension if _ is None else _ for _ in new_shape]\n            return C.reshape(x, new_shape)\n\n\ndef permute_dimensions(x, pattern):\n    dims = len(int_shape(x))\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    if isinstance(pattern, list):\n        current_layout = [i for i in range(dims)]\n    else:\n        current_layout = tuple([i for i in range(dims)])\n\n    if num_dynamic_axis > 0 and pattern[:num_dynamic_axis] != current_layout[:num_dynamic_axis]:\n        raise ValueError('CNTK backend: the permute pattern %s '\n                         'requested permute on dynamic axis, '\n                         'which is not supported. Please do permute '\n                         'on static axis.' % pattern)\n\n    axis = list(pattern)\n    axis = axis[num_dynamic_axis:]\n    axis = _normalize_axis(axis, x)\n    return C.transpose(x, axis)\n\n\ndef resize_images(x, height_factor, width_factor, data_format, interpolation='nearest'):\n    if interpolation == 'nearest':\n        if data_format == 'channels_first':\n            output = repeat_elements(x, height_factor, axis=2)\n            output = repeat_elements(output, width_factor, axis=3)\n            return output\n        elif data_format == 'channels_last':\n            output = repeat_elements(x, height_factor, axis=1)\n            output = repeat_elements(output, width_factor, axis=2)\n            return output\n        else:\n            raise ValueError('CNTK Backend: Invalid data_format: %s' % data_format)\n    else:\n        raise NotImplementedError('CNTK only supports `nearest` interpolation.')\n\n\ndef resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n    if data_format == 'channels_first':\n        output = repeat_elements(x, depth_factor, axis=2)\n        output = repeat_elements(output, height_factor, axis=3)\n        output = repeat_elements(output, width_factor, axis=4)\n        return output\n    elif data_format == 'channels_last':\n        output = repeat_elements(x, depth_factor, axis=1)\n        output = repeat_elements(output, height_factor, axis=2)\n        output = repeat_elements(output, width_factor, axis=3)\n        return output\n    else:\n        raise ValueError('CNTK Backend: Invalid data_format: %s' % data_format)\n\n\ndef repeat_elements(x, rep, axis):\n    axis = _normalize_axis(axis, x)\n    axis = axis[0]\n    slices = []\n    shape = x.shape\n    i = 0\n    while i < shape[axis]:\n        tmp = C.ops.slice(x, axis, i, i + 1)\n        for _ in range(rep):\n            slices.append(tmp)\n        i += 1\n    return C.splice(*slices, axis=axis)\n\n\ndef repeat(x, n):\n    # this is a workaround for recurrent layer\n    # if n is inferred dimension,\n    # we can't figure out how to repeat it in cntk now\n    # return the same x to take cntk broadcast feature\n    # to make the recurrent layer work.\n    # need to be fixed in GA.\n    if n is C.InferredDimension or n is C.FreeDimension:\n        return x\n    index = 1 - _get_dynamic_axis_num(x)\n    if index < 0 or index > 1:\n        raise NotImplementedError\n\n    new_shape = list(x.shape)\n    new_shape.insert(index, 1)\n    new_shape = tuple(new_shape)\n    x = C.reshape(x, new_shape)\n    temp = [x] * n\n    return C.splice(*temp, axis=index)\n\n\ndef tanh(x):\n    return C.tanh(x)\n\n\ndef _static_rnn(step_function, inputs, initial_states,\n                go_backwards=False, mask=None, constants=None,\n                unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('Input should be at least 3D.')\n\n    # if the second axis is static axis, CNTK will do unroll by default\n    if shape[1] is None:\n        raise ValueError('CNTK Backend: the input of static rnn '\n                         'has shape `%s`, the second axis '\n                         'is not static. If you want to run '\n                         'rnn with non-static axis, please try '\n                         'dynamic rnn with sequence axis.' % shape)\n    if constants is None:\n        constants = []\n\n    if mask is not None:\n        mask_shape = int_shape(mask)\n        if len(mask_shape) == dims - 1:\n            mask = expand_dims(mask)\n\n    nones = _get_dynamic_axis_num(inputs)\n\n    states = tuple(initial_states)\n\n    outputs = []\n\n    time_axis = 1 - nones if nones > 0 else 1\n\n    if go_backwards:\n        i = shape[1] - 1\n        while i >= 0:\n            current = C.ops.slice(inputs, time_axis, i, i + 1)\n            # remove dummy dimension\n            current = squeeze(current, time_axis)\n\n            output, new_states = step_function(\n                current, tuple(states) + tuple(constants))\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n\n            if mask is not None:\n                mask_slice = C.ops.slice(mask, time_axis, i, i + 1)\n                mask_slice = squeeze(mask_slice, time_axis)\n                if len(outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = outputs[-1]\n                output = C.ops.element_select(mask_slice, output, prev_output)\n\n                return_states = []\n                for s, n_s in zip(states, new_states):\n                    return_states.append(\n                        C.ops.element_select(\n                            mask_slice, n_s, s))\n                new_states = return_states\n            outputs.append(output)\n            states = new_states\n            i -= 1\n    else:\n        i = 0\n        while i < shape[1]:\n            current = C.ops.slice(inputs, time_axis, i, i + 1)\n            # remove dummy dimension\n            current = squeeze(current, 1)\n\n            output, new_states = step_function(\n                current, tuple(states) + tuple(constants))\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n\n            if mask is not None:\n                mask_slice = C.ops.slice(mask, time_axis, i, i + 1)\n                mask_slice = squeeze(mask_slice, 1)\n                if len(outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = outputs[-1]\n                output = C.ops.element_select(mask_slice, output, prev_output)\n\n                return_states = []\n                for s, n_s in zip(states, new_states):\n                    return_states.append(\n                        C.ops.element_select(\n                            mask_slice, n_s, s))\n                new_states = return_states\n            outputs.append(output)\n            states = new_states[:len(states)]\n            i += 1\n\n    i = 1\n    # add the time_step axis back\n    final_output = expand_dims(outputs[0], 1)\n    last_output = outputs[0]\n    while i < len(outputs):\n        # add the time_step axis back\n        output_slice = expand_dims(outputs[i], 1)\n        final_output = C.splice(final_output, output_slice, axis=time_axis)\n        last_output = outputs[i]\n        i += 1\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, states\n\n\ndef rnn(step_function, inputs, initial_states,\n        go_backwards=False, mask=None, constants=None,\n        unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    global uses_learning_phase\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('CNTK Backend: the input of rnn has only rank %d '\n                         'Need at least rank 3 to run RNN.' % dims)\n\n    if _get_dynamic_axis_num(inputs) == 0 or unroll:\n        return _static_rnn(\n            step_function,\n            inputs,\n            initial_states,\n            go_backwards,\n            mask,\n            constants,\n            unroll,\n            input_length)\n\n    if constants is None:\n        constants = []\n\n    num_time_step = shape[1]\n    if num_time_step is None and not has_seq_axis(inputs):\n        num_time_step = inputs.shape[0]\n\n    initial = []\n    for s in initial_states:\n        if _get_dynamic_axis_num(s) == 0:\n            if hasattr(C, 'to_batch'):\n                initial.append(C.to_batch(s))\n            else:\n                initial.append(C.user_function(ConvertToBatch(s)))\n        else:\n            initial.append(s)\n\n    need_convert = not has_seq_axis(inputs)\n    if go_backwards and need_convert is False:\n        raise NotImplementedError('CNTK Backend: `go_backwards` is not supported with '\n                                  'variable-length sequences. Please specify a '\n                                  'static length for your sequences.')\n\n    rnn_inputs = inputs\n    if need_convert:\n        if go_backwards:\n            rnn_inputs = reverse(rnn_inputs, 1)\n\n        rnn_inputs = C.to_sequence(rnn_inputs)\n\n        rnn_constants = []\n        for constant in constants:\n            if isinstance(constant, list):\n                new_c = []\n                for c in constant:\n                    if _get_dynamic_axis_num(c) == 1:\n                        new_c.append(C.sequence.broadcast_as(c, rnn_inputs))\n                    else:\n                        new_c.append(c)\n                rnn_constants.append(new_c)\n            else:\n                if _get_dynamic_axis_num(constant) == 1:\n                    rnn_constants.append(C.sequence.broadcast_as(constant, rnn_inputs))\n                else:\n                    rnn_constants.append(constant)\n    else:\n        rnn_constants = constants\n\n    if mask is not None and not has_seq_axis(mask):\n        if go_backwards:\n            mask = reverse(mask, 1)\n        if len(int_shape(mask)) == 2:\n            mask = expand_dims(mask)\n        mask = C.to_sequence_like(mask, rnn_inputs)\n\n    states = tuple(initial)\n\n    with C.default_options(axis_offset=1):\n        def _recurrence(x, states, m):\n            # create place holder\n            place_holders = [C.placeholder(dynamic_axes=x.dynamic_axes) for _ in states]\n            past_values = []\n            for s, p in zip(states, place_holders):\n                past_values.append(C.sequence.past_value(p, s))\n            new_output, new_states = step_function(\n                x, tuple(past_values) + tuple(rnn_constants))\n\n            if getattr(new_output, '_uses_learning_phase', False):\n                global uses_learning_phase\n                uses_learning_phase = True\n\n            if m is not None:\n                new_states = [C.element_select(m, n, s) for n, s in zip(new_states, past_values)]\n            n_s = []\n            for o, p in zip(new_states, place_holders):\n                n_s.append(o.replace_placeholders({p: o.output}))\n            if len(n_s) > 0:\n                new_output = n_s[-1]\n            return new_output, n_s\n\n        final_output, final_states = _recurrence(rnn_inputs, states, mask)\n        last_output = C.sequence.last(final_output)\n        last_states = [C.sequence.last(s) for s in final_states]\n\n    if need_convert:\n        final_output = C.sequence.unpack(final_output, 0, no_mask_output=True)\n        if num_time_step is not None and num_time_step is not C.FreeDimension:\n            final_output = _reshape_sequence(final_output, num_time_step)\n\n    f_stats = []\n    for l_s, i_s in zip(last_states, initial_states):\n        if _get_dynamic_axis_num(i_s) == 0 and _get_dynamic_axis_num(l_s) == 1:\n            if hasattr(C, 'unpack_batch'):\n                f_stats.append(C.unpack_batch(l_s))\n            else:\n                f_stats.append(C.user_function(ConvertToStatic(l_s, batch_size=i_s.shape[0])))\n        else:\n            f_stats.append(l_s)\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, f_stats\n\n\ndef has_seq_axis(x):\n    return hasattr(x, 'dynamic_axes') and len(x.dynamic_axes) > 1\n\n\ndef l2_normalize(x, axis=None):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    norm = C.sqrt(C.reduce_sum(C.square(x), axis=axis[0]))\n    return x / norm\n\n\ndef hard_sigmoid(x):\n    x = (0.2 * x) + 0.5\n    x = C.clip(x, 0.0, 1.0)\n    return x\n\n\ndef conv1d(x, kernel, strides=1, padding='valid',\n           data_format=None, dilation_rate=1):\n    data_format = normalize_data_format(data_format)\n\n    if padding == 'causal':\n        # causal (dilated) convolution:\n        left_pad = dilation_rate * (kernel.shape[0] - 1)\n        x = temporal_padding(x, (left_pad, 0))\n        padding = 'valid'\n\n    if data_format == 'channels_last':\n        x = C.swapaxes(x, 0, 1)\n        kernel = C.swapaxes(kernel, 0, 2)\n\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != 1:\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to 1. You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(\n        kernel,\n        x,\n        strides=strides,\n        auto_padding=[False, padding],\n        dilation=dilation_rate)\n\n    if data_format == 'channels_last':\n        x = C.swapaxes(x, 0, 1)\n    return x\n\n\ndef conv2d(x, kernel, strides=(1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != (1, 1):\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to (1, 1). '\n                         'You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(kernel,\n                      x,\n                      strides,\n                      auto_padding=[False, padding, padding],\n                      dilation=dilation_rate)\n\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n                     padding='valid', data_format=None, dilation_rate=1):\n    data_format = normalize_data_format(data_format)\n    if isinstance(strides, int):\n        strides = (strides,)\n    if isinstance(dilation_rate, int):\n        dilation_rate = (dilation_rate,)\n\n    if data_format == 'channels_last':\n        spatial_start_dim = 2\n    else:\n        spatial_start_dim = 3\n    x = expand_dims(x, spatial_start_dim)\n    depthwise_kernel = expand_dims(depthwise_kernel, 1)\n    pointwise_kernel = expand_dims(pointwise_kernel, 1)\n    strides = (1,) + strides + (1,)\n    dilation_rate = (1,) + dilation_rate\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dilation_rate == (1, 1):\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    x = _postprocess_conv2d_output(x, data_format)\n    return squeeze(x, spatial_start_dim)\n\n\ndef separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=dilation_rate[0],\n                          auto_padding=[False, padding, padding])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n                     data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    padding = _preprocess_border_mode(padding)\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=dilation_rate[0],\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != (1, 1, 1):\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to (1, 1, 1). '\n                         'You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(\n        kernel,\n        x,\n        strides,\n        auto_padding=[False, padding, padding, padding],\n        dilation=dilation_rate)\n\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n                     padding='valid', data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = (1,) + strides\n    # cntk output_shape does not include batch axis\n    output_shape = output_shape[1:]\n    # in keras2, need handle output shape in different format\n    if data_format == 'channels_last':\n        output_shape = transpose_shape(output_shape, 'channels_first',\n                                       spatial_axes=(0, 1, 2))\n\n    x = C.convolution_transpose(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding,\n            padding],\n        output_shape=output_shape)\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef pool2d(x, pool_size, strides=(1, 1),\n           padding='valid', data_format=None,\n           pool_mode='max'):\n    data_format = normalize_data_format(data_format)\n\n    padding = _preprocess_border_mode(padding)\n    strides = strides\n    pool_size = pool_size\n    x = _preprocess_conv2d_input(x, data_format)\n    if pool_mode == 'max':\n        x = C.pooling(\n            x,\n            C.MAX_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    elif pool_mode == 'avg':\n        x = C.pooling(\n            x,\n            C.AVG_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    else:\n        raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n           data_format=None, pool_mode='max'):\n    data_format = normalize_data_format(data_format)\n\n    padding = _preprocess_border_mode(padding)\n\n    x = _preprocess_conv3d_input(x, data_format)\n\n    if pool_mode == 'max':\n        x = C.pooling(\n            x,\n            C.MAX_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    elif pool_mode == 'avg':\n        x = C.pooling(\n            x,\n            C.AVG_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    else:\n        raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef relu(x, alpha=0., max_value=None):\n    if alpha != 0.:\n        negative_part = C.relu(-x)\n    x = C.relu(x)\n    if max_value is not None:\n        x = C.clip(x, 0.0, max_value)\n    if alpha != 0.:\n        x -= alpha * negative_part\n    return x\n\n\ndef dropout(x, level, noise_shape=None, seed=None):\n    if level < 0. or level >= 1:\n        raise ValueError('CNTK Backend: Invalid dropout level %s, '\n                         'must be in interval [0, 1].' % level)\n    return C.dropout(x, level)\n\n\ndef batch_flatten(x):\n    # cntk's batch axis is not in shape,\n    # so just flatten all the dim in x.shape\n    dim = np.prod(x.shape)\n    x = C.reshape(x, (-1,))\n    x._keras_shape = (None, dim)\n    return x\n\n\ndef softmax(x, axis=-1):\n    return C.softmax(x, axis=axis)\n\n\ndef softplus(x):\n    return C.softplus(x)\n\n\ndef softsign(x):\n    return x / (1 + C.abs(x))\n\n\ndef categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    # Here, unlike other backends, the tensors lack a batch dimension:\n    axis_without_batch = -1 if axis == -1 else axis - 1\n    output_dimensions = list(range(len(output.shape)))\n    if axis_without_batch != -1 and axis_without_batch not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis_without_batch),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(output.shape))))\n    # If the channels are not in the last axis, move them to be there:\n    if axis_without_batch != -1 and axis_without_batch != output_dimensions[-1]:\n        permutation = output_dimensions[:axis_without_batch]\n        permutation += output_dimensions[axis_without_batch + 1:]\n        permutation += [axis_without_batch]\n        output = C.transpose(output, permutation)\n        target = C.transpose(target, permutation)\n    if from_logits:\n        result = C.cross_entropy_with_softmax(output, target)\n        # cntk's result shape is (batch, 1), while keras expect (batch, )\n        return C.reshape(result, ())\n    else:\n        # scale preds so that the class probas of each sample sum to 1\n        output /= C.reduce_sum(output, axis=-1)\n        # avoid numerical instability with epsilon clipping\n        output = C.clip(output, epsilon(), 1.0 - epsilon())\n        return -sum(target * C.log(output), axis=-1)\n\n\ndef sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    # Here, unlike other backends, the tensors lack a batch dimension:\n    axis_without_batch = -1 if axis == -1 else axis - 1\n    output_dimensions = list(range(len(output.shape)))\n    if axis_without_batch != -1 and axis_without_batch not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis_without_batch),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(output.shape))))\n    target = C.one_hot(target, output.shape[axis_without_batch],\n                       axis=axis_without_batch)\n    target = C.reshape(target, output.shape)\n    return categorical_crossentropy(target, output, from_logits, axis=axis)\n\n\nclass Function(object):\n\n    def __init__(self, inputs, outputs, updates=[], **kwargs):\n        self.placeholders = inputs\n        self.trainer = None\n        self.unrelated_updates = None\n        self.updates = updates\n        if len(updates) > 0:\n            assert len(outputs) > 0\n            self.loss = outputs[0]\n            # need group update by gradient place holder\n            u_ops = []\n            unrelated_updates = []\n            for update in updates:\n                if isinstance(update, tuple):\n                    if len(update) != 2:\n                        raise NotImplementedError\n                    else:\n                        u = C.assign(update[0], update[1])\n                else:\n                    u = update\n\n                if len(u.arguments) == 0:\n                    u_ops.append(u)\n                else:\n                    unrelated_updates.append(u)\n\n            update_func = C.combine([u.output for u in u_ops])\n\n            grads = update_func.find_all_with_name('keras_grad_placeholder')\n\n            u_list = []\n            p_list = []\n            for g in grads:\n                if g in grad_parameter_dict:\n                    p_list.append(grad_parameter_dict[g])\n                    u_list.append(g)\n                else:\n                    raise ValueError(\n                        'CNTK backend: when constructing trainer, '\n                        'found gradient node `%s` which is not '\n                        'related to any parameters in the model. '\n                        'Please double check how the gradient node '\n                        'is constructed.' % g)\n\n            if len(u_list) > 0:\n                learner = C.cntk_py.universal_learner(p_list, u_list, update_func)\n\n                criterion = (\n                    outputs[0],\n                    outputs[1]) if len(outputs) > 1 else (\n                    outputs[0],\n                )\n                self.trainer = C.trainer.Trainer(\n                    outputs[0], criterion, [learner])\n                self.trainer_output = tuple([f.output for f in criterion])\n            elif len(u_ops) > 0:\n                unrelated_updates.extend(u_ops)\n\n            if len(unrelated_updates) > 0:\n                self.unrelated_updates = C.combine([_.output for _ in unrelated_updates])\n\n        if self.trainer is None:\n            self.metrics_outputs = [f.output for f in outputs]\n            self.metrics_func = C.combine(self.metrics_outputs)\n        # cntk only could handle loss and 1 metric in trainer, for metrics more\n        # than 2, need manual eval\n        elif len(outputs) > 2:\n            self.metrics_outputs = [f.output for f in outputs[2:]]\n            self.metrics_func = C.combine(self.metrics_outputs)\n        else:\n            self.metrics_func = None\n\n    @staticmethod\n    def _is_input_shape_compatible(input, placeholder):\n        if hasattr(input, 'shape') and hasattr(placeholder, 'shape'):\n            num_dynamic = get_num_dynamic_axis(placeholder)\n            input_shape = input.shape[num_dynamic:]\n            placeholder_shape = placeholder.shape\n            for i, p in zip(input_shape, placeholder_shape):\n                if i != p and p != C.InferredDimension and p != C.FreeDimension:\n                    return False\n        return True\n\n    def __call__(self, inputs):\n        global _LEARNING_PHASE_PLACEHOLDER\n        global _LEARNING_PHASE\n        assert isinstance(inputs, (list, tuple))\n        feed_dict = {}\n        for tensor, value in zip(self.placeholders, inputs):\n            # cntk only support calculate on float, do auto cast here\n            if (hasattr(value, 'dtype') and\n               value.dtype != np.float32 and\n               value.dtype != np.float64):\n                value = value.astype(np.float32)\n\n            if tensor == _LEARNING_PHASE_PLACEHOLDER:\n                _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(value)\n            else:\n                # in current version cntk can't support input with variable\n                # length. Will support it in next release.\n                if not self._is_input_shape_compatible(value, tensor):\n                    raise ValueError('CNTK backend: The placeholder has been resolved '\n                                     'to shape `%s`, but input shape is `%s`. Currently '\n                                     'CNTK can not take variable length inputs. Please '\n                                     'pass inputs that have a static shape.'\n                                     % (str(tensor.shape), str(value.shape)))\n            feed_dict[tensor] = value\n\n        updated = []\n        if self.trainer is not None:\n            input_dict = {}\n            for argument in self.loss.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError(\n                        'CNTK backend: argument %s is not found in inputs. '\n                        'Please double check the model and inputs in '\n                        '`train_function`.' % argument.name)\n\n            result = self.trainer.train_minibatch(\n                input_dict, self.trainer_output)\n\n            assert(len(result) == 2)\n            outputs = result[1]\n            for o in self.trainer_output:\n                updated.append(outputs[o])\n\n        if self.metrics_func is not None:\n            input_dict = {}\n            for argument in self.metrics_func.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError('CNTK backend: metrics argument %s '\n                                     'is not found in inputs. Please double '\n                                     'check the model and inputs.' % argument.name)\n            # Some ops (like dropout) won't be applied during \"eval\" in cntk.\n            # They only evaluated in training phase. To make it work, call\n            # \"forward\" method to let cntk know we want to evaluate them.from\n            # But the assign ops won't be executed under this mode, that's why\n            # we need this check.\n            if (self.unrelated_updates is None and\n                    (_LEARNING_PHASE_PLACEHOLDER.value == 1.0 or _LEARNING_PHASE == 1)):\n                _, output_values = self.metrics_func.forward(\n                    input_dict,\n                    self.metrics_func.outputs,\n                    (self.metrics_func.outputs[0],),\n                    as_numpy=False)\n            else:\n                output_values = self.metrics_func.eval(input_dict, as_numpy=False)\n            if isinstance(output_values, dict):\n                for o in self.metrics_outputs:\n                    value = output_values[o]\n                    v = value.asarray()\n                    updated.append(v)\n            else:\n                v = output_values.asarray()\n                for o in self.metrics_outputs:\n                    updated.append(v)\n\n        if self.unrelated_updates is not None:\n            input_dict = {}\n            for argument in self.unrelated_updates.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError(\n                        'CNTK backend: assign ops argument %s '\n                        'is not found in inputs. Please double '\n                        'check the model and inputs.' % argument.name)\n            self.unrelated_updates.eval(input_dict, as_numpy=False)\n        return updated\n\n\ndef function(inputs, outputs, updates=[], **kwargs):\n    return Function(inputs, outputs, updates=updates, **kwargs)\n\n\ndef temporal_padding(x, padding=(1, 1)):\n    assert len(padding) == 2\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 3 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, [padding], 'channels_last', num_dynamic_axis)\n\n\ndef _padding(x, pattern, axis):  # pragma: no cover\n    base_shape = x.shape\n    if b_any([dim < 0 for dim in base_shape]):\n        raise ValueError('CNTK Backend: padding input tensor with '\n                         'shape `%s` contains non-specified dimension, '\n                         'which is not supported. Please give fixed '\n                         'dimension to enable padding.' % base_shape)\n    if pattern[0] > 0:\n        prefix_shape = list(base_shape)\n        prefix_shape[axis] = pattern[0]\n        prefix_shape = tuple(prefix_shape)\n        x = C.splice(C.constant(value=0, shape=prefix_shape), x, axis=axis)\n        base_shape = x.shape\n    if pattern[1] > 0:\n        postfix_shape = list(base_shape)\n        postfix_shape[axis] = pattern[1]\n        postfix_shape = tuple(postfix_shape)\n        x = C.splice(x, C.constant(value=0, shape=postfix_shape), axis=axis)\n    return x\n\n\ndef pad(x, pad_info, data_format, num_dynamic_axis):\n    if hasattr(C, 'pad'):\n        pattern = [list(p) for p in pad_info]\n        if data_format == 'channels_first':\n            pattern = [[0, 0]] + pattern\n        else:\n            pattern = pattern + [[0, 0]]\n        if num_dynamic_axis == 0:\n            pattern = [[0, 0]] + pattern\n        return C.pad(x, pattern=pattern)\n    else:  # pragma: no cover\n        for (a, p) in enumerate(pad_info):\n            x = _padding(x, p,\n                         a + (1 if num_dynamic_axis == 0 else 0) +\n                         (1 if data_format == 'channels_first' else 0))\n        return x\n\n\ndef spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n    assert len(padding) == 2\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    data_format = normalize_data_format(data_format)\n\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 4 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, padding, data_format, num_dynamic_axis)\n\n\ndef spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n    assert len(padding) == 3\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    assert len(padding[2]) == 2\n    data_format = normalize_data_format(data_format)\n\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 5 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, padding, data_format, num_dynamic_axis)\n\n\ndef one_hot(indices, num_classes):\n    return C.one_hot(indices, num_classes)\n\n\ndef get_value(x):\n    if isinstance(\n            x,\n            C.variables.Parameter) or isinstance(\n            x,\n            C.variables.Constant):\n        return x.value\n    else:\n        return eval(x)\n\n\ndef batch_get_value(xs):\n    result = []\n    for x in xs:\n        if (isinstance(x, C.variables.Parameter) or\n           isinstance(x, C.variables.Constant)):\n            result.append(x.value)\n        else:\n            result.append(eval(x))\n    return result\n\n\ndef set_value(x, value):\n    if (isinstance(x, C.variables.Parameter) or\n       isinstance(x, C.variables.Constant)):\n        if isinstance(value, (float, int)):\n            value = np.full(x.shape, value, dtype=floatx())\n        x.value = value\n    else:\n        raise NotImplementedError\n\n\ndef print_tensor(x, message=''):\n    return C.user_function(\n        LambdaFunc(x,\n                   when=lambda x: True,\n                   execute=lambda x: print(message)))\n\n\ndef batch_set_value(tuples):\n    for t in tuples:\n        x = t[0]\n        value = t[1]\n        if isinstance(value, np.ndarray) is False:\n            value = np.asarray(value)\n        if isinstance(x, C.variables.Parameter):\n            x.value = value\n        else:\n            raise NotImplementedError\n\n\ndef stop_gradient(variables):\n    if isinstance(variables, (list, tuple)):\n        return map(C.stop_gradient, variables)\n    else:\n        return C.stop_gradient(variables)\n\n\ndef switch(condition, then_expression, else_expression):\n    ndim_cond = ndim(condition)\n    ndim_expr = ndim(then_expression)\n    if ndim_cond > ndim_expr:\n        raise ValueError('Rank of condition should be less'\n                         ' than or equal to rank of then and'\n                         ' else expressions. ndim(condition)=' +\n                         str(ndim_cond) + ', ndim(then_expression)'\n                         '=' + str(ndim_expr))\n    elif ndim_cond < ndim_expr:\n        shape_expr = int_shape(then_expression)\n        ndim_diff = ndim_expr - ndim_cond\n        for i in range(ndim_diff):\n            condition = expand_dims(condition)\n            condition = tile(condition, shape_expr[ndim_cond + i])\n    return C.element_select(condition,\n                            then_expression,\n                            else_expression)\n\n\ndef elu(x, alpha=1.):\n    res = C.elu(x)\n    if alpha == 1:\n        return res\n    else:\n        return C.element_select(C.greater(x, 0), res, alpha * res)\n\n\ndef in_top_k(predictions, targets, k):\n    _targets = C.one_hot(targets, predictions.shape[-1])\n    result = C.classification_error(predictions, _targets, topN=k)\n    return 1 - C.reshape(result, shape=())\n\n\ndef conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = (1,) + strides\n    # cntk output_shape does not include batch axis\n    output_shape = output_shape[1:]\n    # in keras2, need handle output shape in different format\n    if data_format == 'channels_last':\n        output_shape = transpose_shape(output_shape, 'channels_first',\n                                       spatial_axes=(0, 1))\n\n    x = C.convolution_transpose(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding],\n        output_shape=output_shape,\n        dilation=dilation_rate)\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef identity(x, name=None):\n    if name is None:\n        name = '%s_alias' % x.name\n    return C.alias(x, name=name)\n\n\ndef _preprocess_conv2d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, rows, cols)\n        # TF input shape: (samples, rows, cols, input_depth)\n        x = C.transpose(x, (2, 0, 1))\n    return x\n\n\ndef _preprocess_conv2d_kernel(kernel, data_format):\n    # As of Keras 2.0.0, all kernels are normalized\n    # on the format `(rows, cols, input_depth, depth)`,\n    # independently of `data_format`.\n    # CNTK expects `(depth, input_depth, rows, cols)`.\n    kernel = C.transpose(kernel, (3, 2, 0, 1))\n    return kernel\n\n\ndef _preprocess_border_mode(padding):\n    if padding == 'same':\n        padding = True\n    elif padding == 'valid':\n        padding = False\n    else:\n        raise ValueError('Invalid border mode: ' + str(padding))\n    return padding\n\n\ndef _postprocess_conv2d_output(x, data_format):\n    if data_format == 'channels_last':\n        x = C.transpose(x, (1, 2, 0))\n    return x\n\n\ndef _preprocess_conv3d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, conv_dim1, conv_dim2, conv_dim3)\n        # TF input shape: (samples, conv_dim1, conv_dim2, conv_dim3,\n        # input_depth)\n        x = C.transpose(x, (3, 0, 1, 2))\n    return x\n\n\ndef _preprocess_conv3d_kernel(kernel, dim_ordering):\n    kernel = C.transpose(kernel, (4, 3, 0, 1, 2))\n    return kernel\n\n\ndef _postprocess_conv3d_output(x, dim_ordering):\n    if dim_ordering == 'channels_last':\n        x = C.transpose(x, (1, 2, 3, 0))\n    return x\n\n\ndef _get_dynamic_axis_num(x):\n    if hasattr(x, 'dynamic_axes'):\n        return len(x.dynamic_axes)\n    else:\n        return 0\n\n\ndef _contain_seqence_axis(x):\n    if _get_dynamic_axis_num(x) > 1:\n        return x.dynamic_axes[1] == C.Axis.default_dynamic_axis()\n    else:\n        return False\n\n\ndef get_num_dynamic_axis(x):\n    return _get_dynamic_axis_num(x)\n\n\ndef _reduce_on_axis(x, axis, reduce_fun_name):\n    if isinstance(axis, list):\n        for a in axis:\n            if isinstance(a, C.Axis) \\\n                    and a != C.Axis.default_batch_axis() \\\n                    and hasattr(C.sequence, reduce_fun_name):\n                x = getattr(C.sequence, reduce_fun_name)(x, a)\n            else:\n                x = getattr(C, reduce_fun_name)(x, a)\n    else:\n        x = getattr(C, reduce_fun_name)(x, axis)\n    return x\n\n\ndef _reshape_sequence(x, time_step):\n    tmp_shape = list(int_shape(x))\n    tmp_shape[1] = time_step\n    return reshape(x, tmp_shape)\n\n\ndef local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    stride = strides[0]\n    kernel_shape = int_shape(kernel)\n    output_length, feature_dim, filters = kernel_shape\n\n    xs = []\n    for i in range(output_length):\n        slice_length = py_slice(i * stride,\n                                i * stride + kernel_size[0])\n        xs.append(reshape(inputs[:, slice_length, :],\n                          (-1, 1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=1)\n    # transpose kernel to output_filters first, to apply broadcast\n    weight = permute_dimensions(kernel, (2, 0, 1))\n    # Shape: (batch, filters, output_length, input_length * kernel_size)\n    output = x_aggregate * weight\n    # Shape: (batch, filters, output_length)\n    output = sum(output, axis=3)\n    # Shape: (batch, output_length, filters)\n    return permute_dimensions(output, (0, 2, 1))\n\n\ndef local_conv2d(inputs,\n                 kernel,\n                 kernel_size,\n                 strides,\n                 output_shape,\n                 data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    stride_row, stride_col = strides\n    output_row, output_col = output_shape\n    kernel_shape = int_shape(kernel)\n    _, feature_dim, filters = kernel_shape\n    xs = []\n\n    for i in range(output_row):\n        for j in range(output_col):\n            slice_row = py_slice(i * stride_row,\n                                 i * stride_row + kernel_size[0])\n            slice_col = py_slice(j * stride_col,\n                                 j * stride_col + kernel_size[1])\n            if data_format == 'channels_first':\n                xs.append(reshape(inputs[:, :, slice_row, slice_col],\n                                  (-1, 1, feature_dim)))\n            else:\n                xs.append(reshape(inputs[:, slice_row, slice_col, :],\n                                  (-1, 1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=1)\n    # transpose kernel to put filters first\n    weight = permute_dimensions(kernel, (2, 0, 1))\n    # shape: batch, filters, output_length, input_length * kernel_size\n    output = x_aggregate * weight\n    # shape: batch, filters, output_length\n    output = sum(output, axis=3)\n    # shape: batch, filters, row, col\n    output = reshape(output,\n                     (-1, filters, output_row, output_col))\n\n    if data_format == 'channels_last':\n        # shape: batch, row, col, filters\n        output = permute_dimensions(output, (0, 2, 3, 1))\n\n    return output\n\n\ndef reverse(x, axes):\n    if isinstance(axes, int):\n        axes = [axes]\n    cntk_axes = _normalize_axis(axes, x)\n    begin_index = [0 for _ in cntk_axes]\n    end_index = [0 for _ in cntk_axes]\n    strides = [-1 for _ in cntk_axes]\n    return C.slice(x, cntk_axes, begin_index, end_index, strides)\n\n\ndef slice(x, start, size):\n    raise NotImplementedError\n\n\ndef _reshape_batch(x, shape):\n    # there is a bug in cntk 2.1's unpack_batch implementation\n    if hasattr(C, 'unpack_batch') and _get_cntk_version() >= 2.2:\n        const_a = C.unpack_batch(x)\n        const_a = C.reshape(const_a, shape)\n        return C.to_batch(const_a)\n    else:\n        return C.user_function(ReshapeBatch(x, shape[1:]))\n\n\ndef _get_cntk_version():\n    version = C.__version__\n    if version.endswith('+'):\n        version = version[:-1]\n    # for hot fix, ignore all the . except the first one.\n    if len(version) > 2 and version[1] == '.':\n        version = version[:2] + version[2:].replace('.', '')\n    try:\n        return float(version)\n    except:\n        warnings.warn(\n            'CNTK backend warning: CNTK version not detected. '\n            'Will using CNTK 2.0 GA as default.')\n        return float(2.0)\n\n\nclass ReshapeBatch(C.ops.functions.UserFunction):\n    def __init__(self, input, shape, name='reshape_with_batch'):\n        super(ReshapeBatch, self).__init__([input], as_numpy=False, name=name)\n        self.from_shape = input.shape\n        self.target_shape = shape\n\n    def infer_outputs(self):\n        batch_axis = C.Axis.default_batch_axis()\n        return [\n            C.output_variable(\n                self.target_shape,\n                self.inputs[0].dtype,\n                [batch_axis])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        num_element = arguments.shape()[0] * np.prod(np.asarray(self.from_shape))\n        num_static_element = np.prod(np.asarray(self.target_shape))\n        num_batch = int(num_element / num_static_element)\n        result = arguments.data().as_shape((num_batch,) + self.target_shape)\n        return None, C.cntk_py.Value(result)\n\n    def backward(self, state, root_gradients):\n        grad_array_view = root_gradients.data()\n        num_element = root_gradients.shape()[0] * np.prod(np.asarray(self.target_shape))\n        num_static_element = np.prod(np.asarray(self.from_shape))\n        num_old_batch = int(num_element / num_static_element)\n        return C.cntk_py.Value(\n            grad_array_view.as_shape(\n                (num_old_batch,) + self.from_shape))\n\n\nclass ConvertToBatch(C.ops.functions.UserFunction):\n    \"\"\"Converts input first axis to CNTK batch axis.\n\n    We may introduce this operation in CNTK native\n    implementation later.\n\n    # Arguments\n        inputs: a cntk variable (parameter/constant)\n        name: name of this node\n    \"\"\"\n\n    def __init__(self, input, name='convert_to_batch'):\n        super(ConvertToBatch, self).__init__([input], as_numpy=False, name=name)\n\n    def infer_outputs(self):\n        batch_axis = C.Axis.default_batch_axis()\n        return [\n            C.output_variable(\n                self.inputs[0].shape[1:],\n                self.inputs[0].dtype,\n                [batch_axis])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        return None, C.cntk_py.Value(arguments.data())\n\n    def backward(self, state, root_gradients):\n        return C.cntk_py.Value(root_gradients.data())\n\n\nclass ConvertToStatic(C.ops.functions.UserFunction):\n    \"\"\"Converts input first axis to CNTK static axis.\n\n    We may introduce this operation in CNTK native\n    implementation later.\n\n    # Arguments\n        inputs: a cntk tensor which has batch axis\n        batch_size: size of batch axis.\n        name: name of this node.\n    \"\"\"\n\n    def __init__(self, input, batch_size, name='convert_to_static'):\n        super(ConvertToStatic, self).__init__([input], as_numpy=False, name=name)\n        self.target_shape = (batch_size,) + input.shape\n\n    def infer_outputs(self):\n        return [\n            C.output_variable(\n                self.target_shape,\n                self.inputs[0].dtype,\n                [])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        return None, C.cntk_py.Value(arguments.data())\n\n    def backward(self, state, root_gradients):\n        return C.cntk_py.Value(root_gradients.data())\n\n\nclass LambdaFunc(C.ops.functions.UserFunction):\n    def __init__(self,\n                 arg,\n                 when=lambda arg: True,\n                 execute=lambda arg: print(arg),\n                 name=''):\n        self.when = when\n        self.execute = execute\n\n        super(LambdaFunc, self).__init__([arg], name=name)\n\n    def infer_outputs(self):\n        return [\n            C.output_variable(\n                self.inputs[0].shape,\n                self.inputs[0].dtype,\n                self.inputs[0].dynamic_axes)]\n\n    def forward(self, argument, device=None, outputs_to_retain=None):\n        if self.when(argument):\n            self.execute(argument)\n\n        return None, argument\n\n    def backward(self, state, root_gradients):\n        return root_gradients\n",
                "max_stars_repo_path": "BugsInPy/BugsInPy/temp/projects/keras/bug-19-fixed/keras/keras/backend/cntk_backend.py,BugsInPy/BugsInPy/temp/projects/keras/bug-19-buggy/keras/keras/backend/cntk_backend.py",
                "max_stars_repo_name": "NA",
                "max_stars_count": 0,
                "__cluster__": 33
            },
            {
                "id": "pretrain_python_data_1529",
                "content": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cntk as C\nimport numpy as np\nfrom .common import floatx, epsilon, image_dim_ordering, image_data_format\nfrom collections import defaultdict\nfrom contextlib import contextmanager\nimport warnings\n\n\nC.set_global_option('align_axis', 1)\n\nb_any = any\n\n\ndev = C.device.use_default_device()\nif dev.type() == 0:\n    warnings.warn(\n        'CNTK backend warning: GPU is not detected. '\n        'CNTK\\'s CPU version is not fully optimized,'\n        'please run with GPU to get better performance.')\n\n# A learning phase is a bool tensor used to run Keras models in\n# either train mode (learning_phase == 1) or test mode (learning_phase == 0).\n# LEARNING_PHASE_PLACEHOLDER is the placeholder for dynamic learning phase\n_LEARNING_PHASE_PLACEHOLDER = C.constant(shape=(), dtype=np.float32, value=1.0, name='_keras_learning_phase')\n# static learning phase flag, if it is not 0 or 1, we will go with dynamic learning phase tensor.\n_LEARNING_PHASE = -1\n_UID_PREFIXES = defaultdict(int)\n\n# cntk doesn't support gradient as symbolic op, to hook up with keras model,\n# we will create gradient as a constant placeholder, here use this global\n# map to keep the mapping from grad placeholder to parameter\ngrad_parameter_dict = {}\n\nNAME_SCOPE_STACK = []\n\n\n@contextmanager\ndef name_scope(name):\n    global NAME_SCOPE_STACK\n    NAME_SCOPE_STACK.append(name)\n    yield\n    NAME_SCOPE_STACK.pop()\n\n\ndef get_uid(prefix=''):\n    _UID_PREFIXES[prefix] += 1\n    return _UID_PREFIXES[prefix]\n\n\ndef learning_phase():\n    # If _LEARNING_PHASE is not 0 or 1, return dynamic learning phase tensor\n    return _LEARNING_PHASE if _LEARNING_PHASE in {0, 1} else _LEARNING_PHASE_PLACEHOLDER\n\n\ndef set_learning_phase(value):\n    global _LEARNING_PHASE\n    if value not in {0, 1}:\n        raise ValueError('CNTK Backend: Set learning phase '\n                         'with value %s is not supported, '\n                         'expected 0 or 1.' % value)\n    _LEARNING_PHASE = value\n\n\ndef clear_session():\n    \"\"\"Reset learning phase flag for cntk backend.\n    \"\"\"\n    global _LEARNING_PHASE\n    global _LEARNING_PHASE_PLACEHOLDER\n    _LEARNING_PHASE = -1\n    _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(1.0)\n\n\ndef in_train_phase(x, alt, training=None):\n    global _LEARNING_PHASE\n    if training is None:\n        training = learning_phase()\n        uses_learning_phase = True\n    else:\n        uses_learning_phase = False\n\n    # CNTK currently don't support cond op, so here we use\n    # element_select approach as workaround. It may have\n    # perf issue, will resolve it later with cntk cond op.\n    if callable(x) and isinstance(x, C.cntk_py.Function) is False:\n        x = x()\n    if callable(alt) and isinstance(alt, C.cntk_py.Function) is False:\n        alt = alt()\n\n    if training is True:\n        x._uses_learning_phase = uses_learning_phase\n        return x\n    else:\n        # if _LEARNING_PHASE is static\n        if isinstance(training, int) or isinstance(training, bool):\n            result = x if training == 1 or training is True else alt\n        else:\n            result = C.element_select(training, x, alt)\n        result._uses_learning_phase = uses_learning_phase\n        return result\n\n\ndef in_test_phase(x, alt, training=None):\n    return in_train_phase(alt, x, training=training)\n\n\ndef _convert_string_dtype(dtype):\n    # cntk only support float32 and float64\n    if dtype == 'float32':\n        return np.float32\n    elif dtype == 'float64':\n        return np.float64\n    else:\n        # cntk only running with float,\n        # try to cast to float to run the model\n        return np.float32\n\n\ndef _convert_dtype_string(dtype):\n    if dtype == np.float32:\n        return 'float32'\n    elif dtype == np.float64:\n        return 'float64'\n    else:\n        raise ValueError('CNTK Backend: Unsupported dtype: %s. '\n                         'CNTK only supports float32 and '\n                         'float64.' % dtype)\n\n\ndef variable(value, dtype=None, name=None, constraint=None):\n    \"\"\"Instantiates a variable and returns it.\n\n    # Arguments\n        value: Numpy array, initial value of the tensor.\n        dtype: Tensor type.\n        name: Optional name string for the tensor.\n        constraint: Optional projection function to be\n            applied to the variable after an optimizer update.\n\n    # Returns\n        A variable instance (with Keras metadata included).\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n\n    if name is None:\n        name = ''\n\n    if isinstance(\n            value,\n            C.variables.Constant) or isinstance(\n            value,\n            C.variables.Parameter):\n        value = value.value\n\n    # we don't support init parameter with symbolic op, so eval it first as\n    # workaround\n    if isinstance(value, C.cntk_py.Function):\n        value = eval(value)\n\n    shape = value.shape if hasattr(value, 'shape') else ()\n    if hasattr(value, 'dtype') and value.dtype != dtype and len(shape) > 0:\n        value = value.astype(dtype)\n\n    # TODO: remove the conversion when cntk supports int32, int64\n    # https://docs.microsoft.com/en-us/python/api/cntk.variables.parameter\n    dtype = 'float32' if 'int' in str(dtype) else dtype\n\n    v = C.parameter(shape=shape,\n                    init=value,\n                    dtype=dtype,\n                    name=_prepare_name(name, 'variable'))\n    v._keras_shape = v.shape\n    v._uses_learning_phase = False\n    v.constraint = constraint\n    return v\n\n\ndef bias_add(x, bias, data_format=None):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    dims = len(x.shape)\n    if dims > 0 and x.shape[0] == C.InferredDimension:\n        dims -= 1\n\n    bias_dims = len(bias.shape)\n    if bias_dims != 1 and bias_dims != dims:\n        raise ValueError('Unexpected bias dimensions %d, '\n                         'expected 1 or %d dimensions' % (bias_dims, dims))\n\n    if dims == 4:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1, 1, 1)\n            else:\n                shape = (bias.shape[3],) + bias.shape[:3]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, 1, 1, bias.shape[0])\n            else:\n                shape = bias.shape\n    elif dims == 3:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1, 1)\n            else:\n                shape = (bias.shape[2],) + bias.shape[:2]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, 1, bias.shape[0])\n            else:\n                shape = bias.shape\n    elif dims == 2:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1)\n            else:\n                shape = (bias.shape[1],) + bias.shape[:1]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, bias.shape[0])\n            else:\n                shape = bias.shape\n    else:\n        shape = bias.shape\n    return x + reshape(bias, shape)\n\n\ndef eval(x):\n    if isinstance(x, C.cntk_py.Function):\n        return x.eval()\n    elif isinstance(x, C.variables.Constant) or isinstance(x, C.variables.Parameter):\n        return x.value\n    else:\n        raise ValueError('CNTK Backend: `eval` method on '\n                         '`%s` type is not supported. '\n                         'CNTK only supports `eval` with '\n                         '`Function`, `Constant` or '\n                         '`Parameter`.' % type(x))\n\n\ndef placeholder(\n        shape=None,\n        ndim=None,\n        dtype=None,\n        sparse=False,\n        name=None,\n        dynamic_axis_num=1):\n    if dtype is None:\n        dtype = floatx()\n    if not shape:\n        if ndim:\n            shape = tuple([None for _ in range(ndim)])\n\n    dynamic_dimension = C.FreeDimension if _get_cntk_version() >= 2.2 else C.InferredDimension\n    cntk_shape = [dynamic_dimension if s is None else s for s in shape]\n    cntk_shape = tuple(cntk_shape)\n\n    if dynamic_axis_num > len(cntk_shape):\n        raise ValueError('CNTK backend: creating placeholder with '\n                         '%d dimension is not supported, at least '\n                         '%d dimensions are needed.'\n                         % (len(cntk_shape, dynamic_axis_num)))\n\n    if name is None:\n        name = ''\n\n    cntk_shape = cntk_shape[dynamic_axis_num:]\n\n    x = C.input(\n        shape=cntk_shape,\n        dtype=_convert_string_dtype(dtype),\n        is_sparse=sparse,\n        name=name)\n    x._keras_shape = shape\n    x._uses_learning_phase = False\n    x._cntk_placeholder = True\n    return x\n\n\ndef is_placeholder(x):\n    \"\"\"Returns whether `x` is a placeholder.\n\n    # Arguments\n        x: A candidate placeholder.\n\n    # Returns\n        Boolean.\n    \"\"\"\n    return hasattr(x, '_cntk_placeholder') and x._cntk_placeholder\n\n\ndef is_keras_tensor(x):\n    if not is_tensor(x):\n        raise ValueError('Unexpectedly found an instance of type `' +\n                         str(type(x)) + '`. '\n                         'Expected a symbolic tensor instance.')\n    return hasattr(x, '_keras_history')\n\n\ndef is_tensor(x):\n    return isinstance(x, (C.variables.Constant,\n                          C.variables.Variable,\n                          C.variables.Parameter,\n                          C.ops.functions.Function))\n\n\ndef shape(x):\n    shape = list(int_shape(x))\n    num_dynamic = _get_dynamic_axis_num(x)\n    non_dyn_shape = []\n    for i in range(len(x.shape)):\n        if shape[i + num_dynamic] is None:\n            non_dyn_shape.append(x.shape[i])\n        else:\n            non_dyn_shape.append(shape[i + num_dynamic])\n    return shape[:num_dynamic] + non_dyn_shape\n\n\ndef is_sparse(tensor):\n    return tensor.is_sparse\n\n\ndef int_shape(x):\n    if hasattr(x, '_keras_shape'):\n        return x._keras_shape\n\n    shape = x.shape\n    if hasattr(x, 'dynamic_axes'):\n        dynamic_shape = [None for a in x.dynamic_axes]\n        shape = tuple(dynamic_shape) + shape\n    return shape\n\n\ndef ndim(x):\n    shape = int_shape(x)\n    return len(shape)\n\n\ndef _prepare_name(name, default):\n    prefix = '_'.join(NAME_SCOPE_STACK)\n    if name is None or name == '':\n        return prefix + '/' + default\n    return prefix + '/' + name\n\n\ndef constant(value, dtype=None, shape=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    if shape is None:\n        shape = ()\n    np_value = value * np.ones(shape)\n    const = C.constant(np_value,\n                       dtype=dtype,\n                       name=_prepare_name(name, 'constant'))\n    const._keras_shape = const.shape\n    const._uses_learning_phase = False\n    return const\n\n\ndef random_binomial(shape, p=0.0, dtype=None, seed=None):\n    # use numpy workaround now\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e7)\n        np.random.seed(seed)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    size = 1\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n        size *= _\n\n    binomial = np.random.binomial(1, p, size).astype(dtype).reshape(shape)\n    return variable(value=binomial, dtype=dtype)\n\n\ndef random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n\n    return random_uniform_variable(shape, minval, maxval, dtype, seed)\n\n\ndef random_uniform_variable(shape, low, high,\n                            dtype=None, name=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    if name is None:\n        name = ''\n\n    scale = (high - low) / 2\n    p = C.parameter(\n        shape,\n        init=C.initializer.uniform(\n            scale,\n            seed=seed),\n        dtype=dtype,\n        name=name)\n    return variable(value=p.value + low + scale)\n\n\ndef random_normal_variable(\n        shape,\n        mean,\n        scale,\n        dtype=None,\n        name=None,\n        seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e7)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    if name is None:\n        name = ''\n\n    return C.parameter(\n        shape=shape,\n        init=C.initializer.normal(\n            scale=scale,\n            seed=seed),\n        dtype=dtype,\n        name=name)\n\n\ndef random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n    # how to apply mean and stddev\n    return random_normal_variable(shape=shape, mean=mean, scale=1.0, seed=seed)\n\n\ndef truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if seed is None:\n        seed = np.random.randint(1, 10e6)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    return C.parameter(\n        shape, init=C.initializer.truncated_normal(\n            stddev, seed=seed), dtype=dtype)\n\n\ndef dtype(x):\n    return _convert_dtype_string(x.dtype)\n\n\ndef zeros(shape, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    ctype = _convert_string_dtype(dtype)\n    return variable(value=np.zeros(shape, ctype), dtype=dtype, name=name)\n\n\ndef ones(shape, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    ctype = _convert_string_dtype(dtype)\n    return variable(value=np.ones(shape, ctype), dtype=dtype, name=name)\n\n\ndef eye(size, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    return variable(np.eye(size), dtype, name)\n\n\ndef zeros_like(x, dtype=None, name=None):\n    return x * 0\n\n\ndef ones_like(x, dtype=None, name=None):\n    return zeros_like(x) + 1\n\n\ndef count_params(x):\n    for _ in x.shape:\n        if _ == C.InferredDimension or _ == C.FreeDimension:\n            raise ValueError('CNTK backend: `count_params` with dynamic '\n                             'shape is not supported. Please provide '\n                             'fixed dimension instead of `None`.')\n\n    return np.prod(int_shape(x))\n\n\ndef cast(x, dtype):\n    # cntk calculate everything in float, so don't need case from bool / int\n    return x\n\n\ndef dot(x, y):\n    if len(x.shape) > 2 or len(y.shape) > 2:\n        y_shape = int_shape(y)\n        if len(y_shape) > 2:\n            permutation = [len(y_shape) - 2]\n            permutation += list(range(len(y_shape) - 2))\n            permutation += [len(y_shape) - 1]\n            y = C.transpose(y, perm=permutation)\n        return C.times(x, y, len(y_shape) - 1)\n    else:\n        return C.times(x, y)\n\n\ndef batch_dot(x, y, axes=None):\n    x_shape = int_shape(x)\n    y_shape = int_shape(y)\n\n    if isinstance(axes, int):\n        axes = (axes, axes)\n    if axes is None:\n        # behaves like tf.batch_matmul as default\n        axes = [len(x_shape) - 1, len(y_shape) - 2]\n    if b_any([isinstance(a, (list, tuple)) for a in axes]):\n        raise ValueError('Multiple target dimensions are not supported. ' +\n                         'Expected: None, int, (int, int), ' +\n                         'Provided: ' + str(axes))\n\n    if len(x_shape) == 2 and len(y_shape) == 2:\n        if axes[0] == axes[1]:\n            result = sum(x * y, axis=axes[0], keepdims=True)\n            return result if axes[0] == 1 else transpose(result)\n        else:\n            return sum(x * transpose(y), axis=axes[0], keepdims=True)\n    else:\n        if len(y_shape) == 2:\n            y = expand_dims(y)\n\n        normalized_axis = []\n        normalized_axis.append(_normalize_axis(axes[0], x)[0])\n        normalized_axis.append(_normalize_axis(axes[1], y)[0])\n        # transpose\n        i = normalized_axis[0]\n        while i < len(x.shape) - 1:\n            x = C.swapaxes(x, i, i + 1)\n            i += 1\n        i = normalized_axis[1]\n        while i > 0:\n            y = C.swapaxes(y, i, i - 1)\n            i -= 1\n        result = C.times(x, y, output_rank=(len(y.shape) - 1)\n                         if len(y.shape) > 1 else 1)\n        if len(y_shape) == 2:\n            result = squeeze(result, -1)\n        return result\n\n\ndef transpose(x):\n    return C.swapaxes(x, 0, 1)\n\n\ndef gather(reference, indices):\n    # There is a bug in cntk gather op which may cause crash.\n    # We have made a fix but not catched in CNTK 2.1 release.\n    # Will update with gather op in next release\n    if _get_cntk_version() >= 2.2:\n        return C.ops.gather(reference, indices)\n    else:\n        num_classes = reference.shape[0]\n        one_hot_matrix = C.ops.one_hot(indices, num_classes)\n        return C.times(one_hot_matrix, reference, output_rank=len(reference.shape) - 1)\n\n\ndef _remove_dims(x, axis, keepdims=False):\n    if keepdims is False and isinstance(axis, list):\n        # sequence axis is removed by default, so don't need reshape on it\n        reduce_axes = []\n        for a in axis:\n            if isinstance(a, C.Axis) is False:\n                reduce_axes.append(a)\n        return _reshape_dummy_dim(x, reduce_axes)\n    else:\n        if isinstance(axis, list):\n            has_seq = False\n            for a in axis:\n                if isinstance(a, C.Axis):\n                    has_seq = True\n                    break\n            if has_seq:\n                nones = _get_dynamic_axis_num(x)\n                x = expand_dims(x, nones)\n        return x\n\n\ndef max(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_max')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef min(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_min')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef sum(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_sum')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef prod(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_prod')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef logsumexp(x, axis=None, keepdims=False):\n    return log(sum(exp(x), axis=axis, keepdims=keepdims))\n\n\ndef var(x, axis=None, keepdims=False):\n    m = mean(x, axis, keepdims=True)\n    devs_squared = C.square(x - m)\n    return mean(devs_squared, axis=axis, keepdims=keepdims)\n\n\ndef std(x, axis=None, keepdims=False):\n    return C.sqrt(var(x, axis=axis, keepdims=keepdims))\n\n\ndef expand_dims(x, axis=-1):\n    shape = list(int_shape(x))\n    nones = _get_dynamic_axis_num(x)\n    index = axis if axis >= 0 else len(shape) + 1\n    shape.insert(index, 1)\n    new_shape = shape[nones:]\n    new_shape = tuple(\n        [C.InferredDimension if _ is None else _ for _ in new_shape])\n    result = C.reshape(x, new_shape)\n    if index < nones:\n        result._keras_shape = shape\n    return result\n\n\ndef squeeze(x, axis):\n    if isinstance(axis, tuple):\n        axis = list(axis)\n    if not isinstance(axis, list):\n        axis = [axis]\n\n    shape = list(int_shape(x))\n\n    _axis = []\n    for _ in axis:\n        if isinstance(_, int):\n            _axis.append(_ if _ >= 0 else _ + len(shape))\n\n    if len(_axis) == 0:\n        return x\n\n    nones = _get_dynamic_axis_num(x)\n    for _ in sorted(_axis, reverse=True):\n        del shape[_]\n\n    new_shape = shape[nones:]\n    new_shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in new_shape])\n    return C.reshape(x, new_shape)\n\n\ndef tile(x, n):\n    if isinstance(n, int):\n        n = (n,)\n    elif isinstance(n, list):\n        n = tuple(n)\n\n    shape = int_shape(x)\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    # Padding the axis\n    if len(n) < len(shape):\n        n = tuple([1 for _ in range(len(shape) - len(n))]) + n\n\n    if len(n) != len(shape):\n        raise NotImplementedError\n\n    i = num_dynamic_axis\n    for i, rep in enumerate(n):\n        if i >= num_dynamic_axis and shape[i] is not None:\n            tmp = [x] * rep\n            x = C.splice(*tmp, axis=i - num_dynamic_axis)\n        i += 1\n\n    return x\n\n\ndef _normalize_axis(axis, x):\n    shape = int_shape(x)\n    ndim = len(shape)\n\n    nones = _get_dynamic_axis_num(x)\n\n    if nones > ndim:\n        raise ValueError('CNTK Backend: tensor with keras shape: `%s` has '\n                         '%d cntk dynamic axis, this is not expected, please '\n                         'double check the keras shape history.' % (str(shape), nones))\n\n    # Current cntk does not support shape like (1, batch). so using the workaround\n    # here to mapping the correct axis. Will remove this tricky after we add support\n    # in native cntk op\n    cntk_axis = []\n    dynamic_axis_index = 0\n    for i in range(ndim):\n        if shape[i] is None and dynamic_axis_index < nones:\n            cntk_axis.append(x.dynamic_axes[dynamic_axis_index])\n            dynamic_axis_index += 1\n        else:\n            cntk_axis.append(i - dynamic_axis_index)\n\n    if dynamic_axis_index < nones:\n        i = 0\n        while dynamic_axis_index < nones:\n            cntk_axis[i] = x.dynamic_axes[dynamic_axis_index]\n            i += 1\n            dynamic_axis_index += 1\n\n        while i < len(cntk_axis):\n            cntk_axis[i] -= nones\n            i += 1\n\n    if isinstance(axis, tuple):\n        _axis = list(axis)\n    elif isinstance(axis, int):\n        _axis = [axis]\n    elif isinstance(axis, list):\n        _axis = list(axis)\n    else:\n        _axis = axis\n\n    if isinstance(_axis, list):\n        for i, a in enumerate(_axis):\n            if a is not None and a < 0:\n                _axis[i] = (a % ndim)\n            if _axis[i] is not None:\n                _axis[i] = cntk_axis[_axis[i]]\n    else:\n        if _axis is None:\n            _axis = C.Axis.all_axes()\n\n    return _axis\n\n\ndef _reshape_dummy_dim(x, axis):\n    shape = list(x.shape)\n\n    _axis = [_ + len(shape) if _ < 0 else _ for _ in axis]\n\n    if shape.count(C.InferredDimension) > 1 or shape.count(C.FreeDimension) > 1:\n        result = x\n        for index in sorted(_axis, reverse=True):\n            result = C.reshape(result,\n                               shape=(),\n                               begin_axis=index,\n                               end_axis=index + 1)\n        return result\n    else:\n        for index in sorted(_axis, reverse=True):\n            del shape[index]\n\n        shape = [C.InferredDimension if _ == C.FreeDimension else _ for _ in shape]\n        return C.reshape(x, shape)\n\n\ndef mean(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_mean')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef any(x, axis=None, keepdims=False):\n    reduce_result = sum(x, axis, keepdims=keepdims)\n    any_matrix = C.element_select(\n        reduce_result,\n        ones_like(reduce_result),\n        zeros_like(reduce_result))\n    if len(reduce_result.shape) == 0 and _get_dynamic_axis_num(x) == 0:\n        return C.reduce_sum(any_matrix)\n    else:\n        return any_matrix\n\n\ndef all(x, axis=None, keepdims=False):\n    reduce_result = prod(x, axis, keepdims=keepdims)\n    all_matrix = C.element_select(\n        reduce_result,\n        ones_like(reduce_result),\n        zeros_like(reduce_result))\n    if len(reduce_result.shape) == 0 and _get_dynamic_axis_num(x) == 0:\n        return C.reduce_sum(all_matrix)\n    else:\n        return all_matrix\n\n\ndef classification_error(target, output, axis=-1):\n    return C.ops.reduce_mean(\n        C.equal(\n            argmax(\n                output,\n                axis=-1),\n            argmax(\n                target,\n                axis=-1)),\n        axis=C.Axis.all_axes())\n\n\ndef argmax(x, axis=-1):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    output = C.ops.argmax(x, axis=axis[0])\n    return _reshape_dummy_dim(output, axis)\n\n\ndef argmin(x, axis=-1):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    output = C.ops.argmin(x, axis=axis[0])\n    return _reshape_dummy_dim(output, axis)\n\n\ndef square(x):\n    return C.square(x)\n\n\ndef abs(x):\n    return C.abs(x)\n\n\ndef sqrt(x):\n    return C.sqrt(x)\n\n\ndef exp(x):\n    return C.exp(x)\n\n\ndef log(x):\n    return C.log(x)\n\n\ndef round(x):\n    return C.round(x)\n\n\ndef sigmoid(x):\n    return C.sigmoid(x)\n\n\ndef sign(x):\n    return x / C.abs(x)\n\n\ndef pow(x, a):\n    return C.pow(x, a)\n\n\ndef clip(x, min_value, max_value):\n    if max_value is not None and max_value < min_value:\n        max_value = min_value\n    if max_value is None:\n        max_value = np.inf\n    if min_value is None:\n        min_value = -np.inf\n    return C.clip(x, min_value, max_value)\n\n\ndef binary_crossentropy(target, output, from_logits=False):\n    if from_logits:\n        output = C.sigmoid(output)\n    output = C.clip(output, epsilon(), 1.0 - epsilon())\n    output = -target * C.log(output) - (1.0 - target) * C.log(1.0 - output)\n    return output\n\n\ndef get_variable_shape(x):\n    return int_shape(x)\n\n\ndef update(x, new_x):\n    return C.assign(x, new_x)\n\n\ndef moving_average_update(variable, value, momentum):\n    return C.assign(variable, variable * momentum + value * (1. - momentum))\n\n\ndef update_add(x, increment):\n    result = x + increment\n    return C.assign(x, result)\n\n\ndef gradients(loss, variables):\n    # cntk does not support gradients as symbolic op,\n    # to hook up with keras model\n    # we will return a constant as place holder, the cntk learner will apply\n    # the gradient during training.\n    global grad_parameter_dict\n    if isinstance(variables, list) is False:\n        variables = [variables]\n    grads = []\n    for v in variables:\n        g = C.constant(0, shape=v.shape, name='keras_grad_placeholder')\n        grads.append(g)\n        grad_parameter_dict[g] = v\n    return grads\n\n\ndef equal(x, y):\n    return C.equal(x, y)\n\n\ndef not_equal(x, y):\n    return C.not_equal(x, y)\n\n\ndef greater(x, y):\n    return C.greater(x, y)\n\n\ndef greater_equal(x, y):\n    return C.greater_equal(x, y)\n\n\ndef less(x, y):\n    return C.less(x, y)\n\n\ndef less_equal(x, y):\n    return C.less_equal(x, y)\n\n\ndef maximum(x, y):\n    return C.element_max(x, y)\n\n\ndef minimum(x, y):\n    return C.element_min(x, y)\n\n\ndef sin(x):\n    return C.sin(x)\n\n\ndef cos(x):\n    return C.cos(x)\n\n\ndef normalize_batch_in_training(x, gamma, beta,\n                                reduction_axes, epsilon=1e-3):\n    if gamma is None:\n        if beta is None:\n            gamma = ones_like(x)\n        else:\n            gamma = ones_like(beta)\n    if beta is None:\n        if gamma is None:\n            beta = zeros_like(x)\n        else:\n            beta = zeros_like(gamma)\n\n    mean, variant = _moments(x, _normalize_axis(reduction_axes, x))\n\n    if sorted(reduction_axes) == list(range(ndim(x)))[:-1]:\n        normalized = batch_normalization(\n            x, mean, variant, beta, gamma, epsilon)\n    else:\n        # need broadcasting\n        target_shape = []\n        x_shape = int_shape(x)\n        # skip the batch axis\n        for axis in range(1, ndim(x)):\n            if axis in reduction_axes:\n                target_shape.append(1)\n                if ndim(gamma) > axis:\n                    gamma = C.reduce_mean(gamma, axis - 1)\n                    beta = C.reduce_mean(beta, axis - 1)\n            else:\n                target_shape.append(x_shape[axis])\n\n        broadcast_mean = C.reshape(mean, target_shape)\n        broadcast_var = C.reshape(variant, target_shape)\n        broadcast_gamma = C.reshape(gamma, target_shape)\n        broadcast_beta = C.reshape(beta, target_shape)\n        normalized = batch_normalization(\n            x,\n            broadcast_mean,\n            broadcast_var,\n            broadcast_beta,\n            broadcast_gamma,\n            epsilon)\n\n    return normalized, mean, variant\n\n\ndef _moments(x, axes=None, shift=None, keep_dims=False):\n    _axes = tuple(axes)\n    if shift is None:\n        shift = x\n        # Compute true mean while keeping the dims for proper broadcasting.\n        for axis in _axes:\n            shift = C.reduce_mean(shift, axis=axis)\n\n    shift = C.stop_gradient(shift)\n    shifted_mean = C.minus(x, shift)\n    for axis in _axes:\n        shifted_mean = C.reduce_mean(shifted_mean, axis=axis)\n\n    variance_mean = C.square(C.minus(x, shift))\n    for axis in _axes:\n        variance_mean = C.reduce_mean(variance_mean, axis=axis)\n\n    variance = C.minus(variance_mean, C.square(shifted_mean))\n    mean = C.plus(shifted_mean, shift)\n\n    if not keep_dims:\n        mean = squeeze(mean, _axes)\n        variance = squeeze(variance, _axes)\n\n    return mean, variance\n\n\ndef batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n    # The mean / var / beta / gamma may be processed by broadcast\n    # so it may have an extra batch axis with 1, it is not needed\n    # in cntk, need to remove those dummy axis.\n    if ndim(mean) == ndim(x) and shape(mean)[0] == 1:\n        mean = _reshape_dummy_dim(mean, [0])\n    if ndim(var) == ndim(x) and shape(var)[0] == 1:\n        var = _reshape_dummy_dim(var, [0])\n\n    if gamma is None:\n        gamma = ones_like(var)\n    elif ndim(gamma) == ndim(x) and shape(gamma)[0] == 1:\n        gamma = _reshape_dummy_dim(gamma, [0])\n\n    if beta is None:\n        beta = zeros_like(mean)\n    elif ndim(beta) == ndim(x) and shape(beta)[0] == 1:\n        beta = _reshape_dummy_dim(beta, [0])\n\n    return (x - mean) / (C.sqrt(var) + epsilon) * gamma + beta\n\n\ndef concatenate(tensors, axis=-1):\n    if len(tensors) == 0:\n        return None\n\n    axis = [axis]\n    axis = _normalize_axis(axis, tensors[0])\n    return C.splice(*tensors, axis=axis[0])\n\n\ndef flatten(x):\n    return reshape(x, (-1,))\n\n\ndef reshape(x, shape):\n    shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in shape])\n    if isinstance(x, C.variables.Parameter):\n        return C.reshape(x, shape)\n    else:\n        num_dynamic_axis = _get_dynamic_axis_num(x)\n\n        if num_dynamic_axis == 1 and len(shape) > 0 and shape[0] == -1:\n            # collapse axis with batch axis\n            if b_any(_ == C.InferredDimension for _ in x.shape) or b_any(\n                    _ == C.FreeDimension for _ in x.shape):\n                warnings.warn(\n                    'Warning: CNTK backend does not support '\n                    'collapse of batch axis with inferred dimension. '\n                    'The reshape did not take place.')\n                return x\n            return _reshape_batch(x, shape)\n        else:\n            # no collapse, then first need to padding the shape\n            if num_dynamic_axis >= len(shape):\n                i = 0\n                while i < len(shape):\n                    if shape[i] is None or shape[i] == -1:\n                        i += 1\n                    else:\n                        break\n                shape = tuple([-1 for _ in range(num_dynamic_axis - i)]) + shape\n\n            new_shape = list(shape)\n            new_shape = new_shape[num_dynamic_axis:]\n            new_shape = [C.InferredDimension if _ is None else _ for _ in new_shape]\n            return C.reshape(x, new_shape)\n\n\ndef permute_dimensions(x, pattern):\n    dims = len(int_shape(x))\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    if isinstance(pattern, list):\n        current_layout = [i for i in range(dims)]\n    else:\n        current_layout = tuple([i for i in range(dims)])\n\n    if num_dynamic_axis > 0 and pattern[:num_dynamic_axis] != current_layout[:num_dynamic_axis]:\n        raise ValueError('CNTK backend: the permute pattern %s '\n                         'requested permute on dynamic axis, '\n                         'which is not supported. Please do permute '\n                         'on static axis.' % pattern)\n\n    axis = list(pattern)\n    axis = axis[num_dynamic_axis:]\n    axis = _normalize_axis(axis, x)\n    return C.transpose(x, axis)\n\n\ndef resize_images(x, height_factor, width_factor, data_format):\n    if data_format == 'channels_first':\n        output = repeat_elements(x, height_factor, axis=2)\n        output = repeat_elements(output, width_factor, axis=3)\n        return output\n    elif data_format == 'channels_last':\n        output = repeat_elements(x, height_factor, axis=1)\n        output = repeat_elements(output, width_factor, axis=2)\n        return output\n    else:\n        raise ValueError('CNTK Backend: Invalid data_format:', data_format)\n\n\ndef resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n    if data_format == 'channels_first':\n        output = repeat_elements(x, depth_factor, axis=2)\n        output = repeat_elements(output, height_factor, axis=3)\n        output = repeat_elements(output, width_factor, axis=4)\n        return output\n    elif data_format == 'channels_last':\n        output = repeat_elements(x, depth_factor, axis=1)\n        output = repeat_elements(output, height_factor, axis=2)\n        output = repeat_elements(output, width_factor, axis=3)\n        return output\n    else:\n        raise ValueError('CNTK Backend: Invalid data_format:', data_format)\n\n\ndef repeat_elements(x, rep, axis):\n    axis = _normalize_axis(axis, x)\n    axis = axis[0]\n    slices = []\n    shape = x.shape\n    i = 0\n    while i < shape[axis]:\n        tmp = C.ops.slice(x, axis, i, i + 1)\n        for _ in range(rep):\n            slices.append(tmp)\n        i += 1\n    return C.splice(*slices, axis=axis)\n\n\ndef repeat(x, n):\n    # this is a workaround for recurrent layer\n    # if n is inferred dimension,\n    # we can't figure out how to repeat it in cntk now\n    # return the same x to take cntk broadcast feature\n    # to make the recurrent layer work.\n    # need to be fixed in GA.\n    if n is C.InferredDimension or n is C.FreeDimension:\n        return x\n    index = 1 - _get_dynamic_axis_num(x)\n    if index < 0 or index > 1:\n        raise NotImplementedError\n\n    new_shape = list(x.shape)\n    new_shape.insert(index, 1)\n    new_shape = tuple(new_shape)\n    x = C.reshape(x, new_shape)\n    temp = [x] * n\n    return C.splice(*temp, axis=index)\n\n\ndef tanh(x):\n    return C.tanh(x)\n\n\ndef _static_rnn(step_function, inputs, initial_states,\n                go_backwards=False, mask=None, constants=None,\n                unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('Input should be at least 3D.')\n\n    # if the second axis is static axis, CNTK will do unroll by default\n    if shape[1] is None:\n        raise ValueError('CNTK Backend: the input of static rnn '\n                         'has shape `%s`, the second axis '\n                         'is not static. If you want to run '\n                         'rnn with non-static axis, please try '\n                         'dynamic rnn with sequence axis.' % shape)\n    if constants is None:\n        constants = []\n\n    if mask is not None:\n        mask_shape = int_shape(mask)\n        if len(mask_shape) == dims - 1:\n            mask = expand_dims(mask)\n\n    nones = _get_dynamic_axis_num(inputs)\n\n    states = tuple(initial_states)\n\n    outputs = []\n\n    time_axis = 1 - nones if nones > 0 else 1\n\n    if go_backwards:\n        i = shape[1] - 1\n        while i >= 0:\n            current = C.ops.slice(inputs, time_axis, i, i + 1)\n            # remove dummy dimension\n            current = squeeze(current, time_axis)\n\n            output, new_states = step_function(\n                current, tuple(states) + tuple(constants))\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n\n            if mask is not None:\n                mask_slice = C.ops.slice(mask, time_axis, i, i + 1)\n                mask_slice = squeeze(mask_slice, time_axis)\n                if len(outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = outputs[-1]\n                output = C.ops.element_select(mask_slice, output, prev_output)\n\n                return_states = []\n                for s, n_s in zip(states, new_states):\n                    return_states.append(\n                        C.ops.element_select(\n                            mask_slice, n_s, s))\n                new_states = return_states\n            outputs.append(output)\n            states = new_states\n            i -= 1\n    else:\n        i = 0\n        while i < shape[1]:\n            current = C.ops.slice(inputs, time_axis, i, i + 1)\n            # remove dummy dimension\n            current = squeeze(current, 1)\n\n            output, new_states = step_function(\n                current, tuple(states) + tuple(constants))\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n\n            if mask is not None:\n                mask_slice = C.ops.slice(mask, time_axis, i, i + 1)\n                mask_slice = squeeze(mask_slice, 1)\n                if len(outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = outputs[-1]\n                output = C.ops.element_select(mask_slice, output, prev_output)\n\n                return_states = []\n                for s, n_s in zip(states, new_states):\n                    return_states.append(\n                        C.ops.element_select(\n                            mask_slice, n_s, s))\n                new_states = return_states\n            outputs.append(output)\n            states = new_states[:len(states)]\n            i += 1\n\n    i = 1\n    # add the time_step axis back\n    final_output = expand_dims(outputs[0], 1)\n    last_output = outputs[0]\n    while i < len(outputs):\n        # add the time_step axis back\n        output_slice = expand_dims(outputs[i], 1)\n        final_output = C.splice(final_output, output_slice, axis=time_axis)\n        last_output = outputs[i]\n        i += 1\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, states\n\n\ndef rnn(step_function, inputs, initial_states,\n        go_backwards=False, mask=None, constants=None,\n        unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    global uses_learning_phase\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('CNTK Backend: the input of rnn has only rank %d '\n                         'Need at least rank 3 to run RNN.' % dims)\n\n    if _get_dynamic_axis_num(inputs) == 0 or unroll:\n        return _static_rnn(\n            step_function,\n            inputs,\n            initial_states,\n            go_backwards,\n            mask,\n            constants,\n            unroll,\n            input_length)\n\n    if constants is None:\n        constants = []\n\n    num_time_step = shape[1]\n    if num_time_step is None and not has_seq_axis(inputs):\n        num_time_step = inputs.shape[0]\n\n    initial = []\n    for s in initial_states:\n        if _get_dynamic_axis_num(s) == 0:\n            if hasattr(C, 'to_batch'):\n                initial.append(C.to_batch(s))\n            else:\n                initial.append(C.user_function(ConvertToBatch(s)))\n        else:\n            initial.append(s)\n\n    need_convert = not has_seq_axis(inputs)\n    if go_backwards and need_convert is False:\n        raise NotImplementedError('CNTK Backend: `go_backwards` is not supported with '\n                                  'variable-length sequences. Please specify a '\n                                  'static length for your sequences.')\n\n    rnn_inputs = inputs\n    if need_convert:\n        if go_backwards:\n            rnn_inputs = reverse(rnn_inputs, 1)\n\n        rnn_inputs = C.to_sequence(rnn_inputs)\n\n        rnn_constants = []\n        for constant in constants:\n            if isinstance(constant, list):\n                new_c = []\n                for c in constant:\n                    if _get_dynamic_axis_num(c) == 1:\n                        new_c.append(C.sequence.broadcast_as(c, rnn_inputs))\n                    else:\n                        new_c.append(c)\n                rnn_constants.append(new_c)\n            else:\n                if _get_dynamic_axis_num(constant) == 1:\n                    rnn_constants.append(C.sequence.broadcast_as(constant, rnn_inputs))\n                else:\n                    rnn_constants.append(constant)\n    else:\n        rnn_constants = constants\n\n    if mask is not None and not has_seq_axis(mask):\n        if go_backwards:\n            mask = reverse(mask, 1)\n        if len(int_shape(mask)) == 2:\n            mask = expand_dims(mask)\n        mask = C.to_sequence_like(mask, rnn_inputs)\n\n    states = tuple(initial)\n\n    with C.default_options(axis_offset=1):\n        def _recurrence(x, states, m):\n            # create place holder\n            place_holders = [C.placeholder(dynamic_axes=x.dynamic_axes) for _ in states]\n            past_values = []\n            for s, p in zip(states, place_holders):\n                past_values.append(C.sequence.past_value(p, s))\n            new_output, new_states = step_function(\n                x, tuple(past_values) + tuple(rnn_constants))\n\n            if getattr(new_output, '_uses_learning_phase', False):\n                global uses_learning_phase\n                uses_learning_phase = True\n\n            if m is not None:\n                new_states = [C.element_select(m, n, s) for n, s in zip(new_states, past_values)]\n            n_s = []\n            for o, p in zip(new_states, place_holders):\n                n_s.append(o.replace_placeholders({p: o.output}))\n            if len(n_s) > 0:\n                new_output = n_s[0]\n            return new_output, n_s\n\n        final_output, final_states = _recurrence(rnn_inputs, states, mask)\n        last_output = C.sequence.last(final_output)\n        last_states = [C.sequence.last(s) for s in final_states]\n\n    if need_convert:\n        final_output = C.sequence.unpack(final_output, 0, no_mask_output=True)\n        if num_time_step is not None and num_time_step is not C.FreeDimension:\n            final_output = _reshape_sequence(final_output, num_time_step)\n\n    f_stats = []\n    for l_s, i_s in zip(last_states, initial_states):\n        if _get_dynamic_axis_num(i_s) == 0 and _get_dynamic_axis_num(l_s) == 1:\n            if hasattr(C, 'unpack_batch'):\n                f_stats.append(C.unpack_batch(l_s))\n            else:\n                f_stats.append(C.user_function(ConvertToStatic(l_s, batch_size=i_s.shape[0])))\n        else:\n            f_stats.append(l_s)\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, f_stats\n\n\ndef has_seq_axis(x):\n    return hasattr(x, 'dynamic_axes') and len(x.dynamic_axes) > 1\n\n\ndef l2_normalize(x, axis=None):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    norm = C.sqrt(C.reduce_sum(C.square(x), axis=axis[0]))\n    return x / norm\n\n\ndef hard_sigmoid(x):\n    x = (0.2 * x) + 0.5\n    x = C.clip(x, 0.0, 1.0)\n    return x\n\n\ndef conv1d(x, kernel, strides=1, padding='valid',\n           data_format=None, dilation_rate=1):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    if padding == 'causal':\n        # causal (dilated) convolution:\n        left_pad = dilation_rate * (kernel.shape[0] - 1)\n        x = temporal_padding(x, (left_pad, 0))\n        padding = 'valid'\n\n    if data_format == 'channels_last':\n        x = C.swapaxes(x, 0, 1)\n        kernel = C.swapaxes(kernel, 0, 2)\n\n    padding = _preprocess_border_mode(padding)\n    strides = [strides]\n    x = C.convolution(\n        kernel,\n        x,\n        strides=tuple(strides),\n        auto_padding=[\n            False,\n            padding])\n\n    if data_format == 'channels_last':\n        x = C.swapaxes(x, 0, 1)\n    return x\n\n\ndef conv2d(x, kernel, strides=(1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1)):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(\n            kernel,\n            x,\n            strides,\n            auto_padding=[\n                False,\n                padding,\n                padding])\n    else:\n        assert dilation_rate[0] == dilation_rate[1]\n        assert strides == (1, 1), 'Invalid strides for dilated convolution'\n        x = C.convolution(\n            kernel,\n            x,\n            strides=dilation_rate[0],\n            auto_padding=[\n                False,\n                padding,\n                padding])\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n                     padding='valid', data_format=None, dilation_rate=1):\n    raise NotImplementedError\n\n\ndef separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=dilation_rate[0],\n                          auto_padding=[False, padding, padding])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n                     data_format=None, dilation_rate=(1, 1)):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    padding = _preprocess_border_mode(padding)\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=dilation_rate[0],\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1, 1)):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = strides + (strides[0],)\n\n    x = C.convolution(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding,\n            padding])\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n                     padding='valid', data_format=None):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = (1,) + strides\n    # cntk output_shape does not include batch axis\n    output_shape = output_shape[1:]\n    # in keras2, need handle output shape in different format\n    if data_format == 'channels_last':\n        shape = list(output_shape)\n        shape[0] = output_shape[3]\n        shape[1] = output_shape[0]\n        shape[2] = output_shape[1]\n        shape[3] = output_shape[2]\n        output_shape = tuple(shape)\n\n    x = C.convolution_transpose(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding,\n            padding],\n        output_shape=output_shape)\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef pool2d(x, pool_size, strides=(1, 1),\n           padding='valid', data_format=None,\n           pool_mode='max'):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    padding = _preprocess_border_mode(padding)\n    strides = strides\n    pool_size = pool_size\n    x = _preprocess_conv2d_input(x, data_format)\n    if pool_mode == 'max':\n        x = C.pooling(\n            x,\n            C.MAX_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    elif pool_mode == 'avg':\n        x = C.pooling(\n            x,\n            C.AVG_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    else:\n        raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n           data_format=None, pool_mode='max'):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    padding = _preprocess_border_mode(padding)\n\n    x = _preprocess_conv3d_input(x, data_format)\n\n    if pool_mode == 'max':\n        x = C.pooling(\n            x,\n            C.MAX_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    elif pool_mode == 'avg':\n        x = C.pooling(\n            x,\n            C.AVG_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    else:\n        raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef relu(x, alpha=0., max_value=None):\n    if alpha != 0.:\n        negative_part = C.relu(-x)\n    x = C.relu(x)\n    if max_value is not None:\n        x = C.clip(x, 0.0, max_value)\n    if alpha != 0.:\n        x -= alpha * negative_part\n    return x\n\n\ndef dropout(x, level, noise_shape=None, seed=None):\n    if level < 0. or level >= 1:\n        raise ValueError('CNTK Backend: Invalid dropout level %s, '\n                         'must be in interval [0, 1].' % level)\n    return C.dropout(x, level)\n\n\ndef batch_flatten(x):\n    # cntk's batch axis is not in shape,\n    # so just flatten all the dim in x.shape\n    dim = np.prod(x.shape)\n    x = C.reshape(x, (-1,))\n    x._keras_shape = (None, dim)\n    return x\n\n\ndef softmax(x, axis=-1):\n    return C.softmax(x, axis=axis)\n\n\ndef softplus(x):\n    return C.softplus(x)\n\n\ndef softsign(x):\n    return x / (1 + C.abs(x))\n\n\ndef categorical_crossentropy(target, output, from_logits=False):\n    if from_logits:\n        result = C.cross_entropy_with_softmax(output, target)\n        # cntk's result shape is (batch, 1), while keras expect (batch, )\n        return C.reshape(result, ())\n    else:\n        # scale preds so that the class probas of each sample sum to 1\n        output /= C.reduce_sum(output, axis=-1)\n        # avoid numerical instability with epsilon clipping\n        output = C.clip(output, epsilon(), 1.0 - epsilon())\n        return -sum(target * C.log(output), axis=-1)\n\n\ndef sparse_categorical_crossentropy(target, output, from_logits=False):\n    target = C.one_hot(target, output.shape[-1])\n    target = C.reshape(target, output.shape)\n    return categorical_crossentropy(target, output, from_logits)\n\n\nclass Function(object):\n\n    def __init__(self, inputs, outputs, updates=[], **kwargs):\n        self.placeholders = inputs\n        self.trainer = None\n        self.unrelated_updates = None\n        self.updates = updates\n        if len(updates) > 0:\n            assert len(outputs) > 0\n            self.loss = outputs[0]\n            # need group update by gradient place holder\n            u_ops = []\n            unrelated_updates = []\n            for update in updates:\n                if isinstance(update, tuple):\n                    if len(update) != 2:\n                        raise NotImplementedError\n                    else:\n                        u = C.assign(update[0], update[1])\n                else:\n                    u = update\n\n                if len(u.arguments) == 0:\n                    u_ops.append(u)\n                else:\n                    unrelated_updates.append(u)\n\n            update_func = C.combine([u.output for u in u_ops])\n\n            grads = update_func.find_all_with_name('keras_grad_placeholder')\n\n            u_list = []\n            p_list = []\n            for g in grads:\n                if g in grad_parameter_dict:\n                    p_list.append(grad_parameter_dict[g])\n                    u_list.append(g)\n                else:\n                    raise ValueError(\n                        'CNTK backend: when constructing trainer, '\n                        'found gradient node `%s` which is not '\n                        'related to any parameters in the model. '\n                        'Please double check how the gradient node '\n                        'is constructed.' % g)\n\n            if len(u_list) > 0:\n                learner = C.cntk_py.universal_learner(p_list, u_list, update_func)\n\n                criterion = (\n                    outputs[0],\n                    outputs[1]) if len(outputs) > 1 else (\n                    outputs[0],\n                )\n                self.trainer = C.trainer.Trainer(\n                    outputs[0], criterion, [learner])\n                self.trainer_output = tuple([f.output for f in criterion])\n            elif len(u_ops) > 0:\n                unrelated_updates.extend(u_ops)\n\n            if len(unrelated_updates) > 0:\n                self.unrelated_updates = C.combine([_.output for _ in unrelated_updates])\n\n        if self.trainer is None:\n            self.metrics_outputs = [f.output for f in outputs]\n            self.metrics_func = C.combine(self.metrics_outputs)\n        # cntk only could handle loss and 1 metric in trainer, for metrics more\n        # than 2, need manual eval\n        elif len(outputs) > 2:\n            self.metrics_outputs = [f.output for f in outputs[2:]]\n            self.metrics_func = C.combine(self.metrics_outputs)\n        else:\n            self.metrics_func = None\n\n    @staticmethod\n    def _is_input_shape_compatible(input, placeholder):\n        if hasattr(input, 'shape') and hasattr(placeholder, 'shape'):\n            num_dynamic = get_num_dynamic_axis(placeholder)\n            input_shape = input.shape[num_dynamic:]\n            placeholder_shape = placeholder.shape\n            for i, p in zip(input_shape, placeholder_shape):\n                if i != p and p != C.InferredDimension and p != C.FreeDimension:\n                    return False\n        return True\n\n    def __call__(self, inputs):\n        global _LEARNING_PHASE_PLACEHOLDER\n        global _LEARNING_PHASE\n        assert isinstance(inputs, (list, tuple))\n        feed_dict = {}\n        for tensor, value in zip(self.placeholders, inputs):\n            # cntk only support calculate on float, do auto cast here\n            if (hasattr(value, 'dtype') and\n               value.dtype != np.float32 and\n               value.dtype != np.float64):\n                value = value.astype(np.float32)\n\n            if tensor == _LEARNING_PHASE_PLACEHOLDER:\n                _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(value)\n            else:\n                # in current version cntk can't support input with variable\n                # length. Will support it in next release.\n                if not self._is_input_shape_compatible(value, tensor):\n                    raise ValueError('CNTK backend: The placeholder has been resolved '\n                                     'to shape `%s`, but input shape is `%s`. Currently '\n                                     'CNTK can not take variable length inputs. Please '\n                                     'pass inputs that have a static shape.'\n                                     % (str(tensor.shape), str(value.shape)))\n            feed_dict[tensor] = value\n\n        updated = []\n        if self.trainer is not None:\n            input_dict = {}\n            for argument in self.loss.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError(\n                        'CNTK backend: argument %s is not found in inputs. '\n                        'Please double check the model and inputs in '\n                        '`train_function`.' % argument.name)\n\n            result = self.trainer.train_minibatch(\n                input_dict, self.trainer_output)\n\n            assert(len(result) == 2)\n            outputs = result[1]\n            for o in self.trainer_output:\n                updated.append(outputs[o])\n\n        if self.metrics_func is not None:\n            input_dict = {}\n            for argument in self.metrics_func.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError('CNTK backend: metrics argument %s '\n                                     'is not found in inputs. Please double '\n                                     'check the model and inputs.' % argument.name)\n            # Some ops (like dropout) won't be applied during \"eval\" in cntk.\n            # They only evaluated in training phase. To make it work, call\n            # \"forward\" method to let cntk know we want to evaluate them.from\n            # But the assign ops won't be executed under this mode, that's why\n            # we need this check.\n            if (self.unrelated_updates is None and\n                    (_LEARNING_PHASE_PLACEHOLDER.value == 1.0 or _LEARNING_PHASE == 1)):\n                _, output_values = self.metrics_func.forward(\n                    input_dict,\n                    self.metrics_func.outputs,\n                    (self.metrics_func.outputs[0],),\n                    as_numpy=False)\n            else:\n                output_values = self.metrics_func.eval(input_dict, as_numpy=False)\n            if isinstance(output_values, dict):\n                for o in self.metrics_outputs:\n                    value = output_values[o]\n                    v = value.asarray()\n                    updated.append(v)\n            else:\n                v = output_values.asarray()\n                for o in self.metrics_outputs:\n                    updated.append(v)\n\n        if self.unrelated_updates is not None:\n            input_dict = {}\n            for argument in self.unrelated_updates.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError(\n                        'CNTK backend: assign ops argument %s '\n                        'is not found in inputs. Please double '\n                        'check the model and inputs.' % argument.name)\n            self.unrelated_updates.eval(input_dict, as_numpy=False)\n        return updated\n\n\ndef function(inputs, outputs, updates=[], **kwargs):\n    return Function(inputs, outputs, updates=updates, **kwargs)\n\n\ndef temporal_padding(x, padding=(1, 1)):\n    assert len(padding) == 2\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    base_shape = x.shape\n    if num_dynamic_axis > 0:\n        assert len(base_shape) == 2\n        if hasattr(C, 'pad'):\n            x = C.pad(x, pattern=[padding, (0, 0)])\n        else:\n            x = _padding(x, padding, 0)\n    else:\n        assert len(base_shape) == 3\n        if hasattr(C, 'pad'):\n            x = C.pad(x, pattern=[(0, 0), padding, (0, 0)])\n        else:\n            x = _padding(x, padding, 1)\n    return x\n\n\ndef _padding(x, pattern, axis):\n    base_shape = x.shape\n    if b_any([dim < 0 for dim in base_shape]):\n        raise ValueError('CNTK Backend: padding input tensor with '\n                         'shape `%s` contains non-specified dimension, '\n                         'which is not supported. Please give fixed '\n                         'dimension to enable padding.' % base_shape)\n    if pattern[0] > 0:\n        prefix_shape = list(base_shape)\n        prefix_shape[axis] = pattern[0]\n        prefix_shape = tuple(prefix_shape)\n        x = C.splice(C.constant(value=0, shape=prefix_shape), x, axis=axis)\n        base_shape = x.shape\n    if pattern[1] > 0:\n        postfix_shape = list(base_shape)\n        postfix_shape[axis] = pattern[1]\n        postfix_shape = tuple(postfix_shape)\n        x = C.splice(x, C.constant(value=0, shape=postfix_shape), axis=axis)\n    return x\n\n\ndef spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n    assert len(padding) == 2\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    base_shape = x.shape\n    if data_format == 'channels_first':\n        if num_dynamic_axis > 0:\n            assert len(base_shape) == 3\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[[0, 0], list(padding[0]), list(padding[1])])\n            else:\n                x = _padding(x, padding[0], 1)\n                x = _padding(x, padding[1], 2)\n        else:\n            assert len(base_shape) == 4\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[[0, 0], [0, 0], list(padding[0]), list(padding[1])])\n            else:\n                x = _padding(x, padding[0], 2)\n                x = _padding(x, padding[1], 3)\n    else:\n        if num_dynamic_axis > 0:\n            assert len(base_shape) == 3\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[list(padding[0]), list(padding[1]), [0, 0]])\n            else:\n                x = _padding(x, padding[0], 0)\n                x = _padding(x, padding[1], 1)\n        else:\n            assert len(base_shape) == 4\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[[0, 0], list(padding[0]), list(padding[1]), [0, 0]])\n            else:\n                x = _padding(x, padding[0], 1)\n                x = _padding(x, padding[1], 2)\n    return x\n\n\ndef spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n    assert len(padding) == 3\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    assert len(padding[2]) == 2\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    base_shape = x.shape\n    if data_format == 'channels_first':\n        if num_dynamic_axis > 0:\n            assert len(base_shape) == 4\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[[0, 0], list(padding[0]), list(padding[1]), list(padding[2])])\n            else:\n                x = _padding(x, padding[0], 1)\n                x = _padding(x, padding[1], 2)\n                x = _padding(x, padding[2], 3)\n        else:\n            assert len(base_shape) == 5\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[[0, 0], [0, 0], list(padding[0]), list(padding[1]), list(padding[2])])\n            else:\n                x = _padding(x, padding[0], 2)\n                x = _padding(x, padding[1], 3)\n                x = _padding(x, padding[2], 4)\n    else:\n        if num_dynamic_axis > 0:\n            assert len(base_shape) == 4\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[list(padding[0]), list(padding[1]), list(padding[2]), [0, 0]])\n            else:\n                x = _padding(x, padding[0], 0)\n                x = _padding(x, padding[1], 1)\n                x = _padding(x, padding[2], 2)\n        else:\n            assert len(base_shape) == 5\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[[0, 0], list(padding[0]), list(padding[1]), list(padding[2]), [0, 0]])\n            else:\n                x = _padding(x, padding[0], 1)\n                x = _padding(x, padding[1], 2)\n                x = _padding(x, padding[2], 3)\n    return x\n\n\ndef one_hot(indices, num_classes):\n    return C.one_hot(indices, num_classes)\n\n\ndef get_value(x):\n    if isinstance(\n            x,\n            C.variables.Parameter) or isinstance(\n            x,\n            C.variables.Constant):\n        return x.value\n    else:\n        return eval(x)\n\n\ndef batch_get_value(xs):\n    result = []\n    for x in xs:\n        if (isinstance(x, C.variables.Parameter) or\n           isinstance(x, C.variables.Constant)):\n            result.append(x.value)\n        else:\n            result.append(eval(x))\n    return result\n\n\ndef set_value(x, value):\n    if (isinstance(x, C.variables.Parameter) or\n       isinstance(x, C.variables.Constant)):\n        if isinstance(value, (float, int)):\n            value = np.full(x.shape, value, dtype=floatx())\n        x.value = value\n    else:\n        raise NotImplementedError\n\n\ndef print_tensor(x, message=''):\n    return C.user_function(\n        LambdaFunc(x,\n                   when=lambda x: True,\n                   execute=lambda x: print(message)))\n\n\ndef batch_set_value(tuples):\n    for t in tuples:\n        x = t[0]\n        value = t[1]\n        if isinstance(value, np.ndarray) is False:\n            value = np.asarray(value)\n        if isinstance(x, C.variables.Parameter):\n            x.value = value\n        else:\n            raise NotImplementedError\n\n\ndef stop_gradient(variables):\n    if isinstance(variables, (list, tuple)):\n        return map(C.stop_gradient, variables)\n    else:\n        return C.stop_gradient(variables)\n\n\ndef switch(condition, then_expression, else_expression):\n    ndim_cond = ndim(condition)\n    ndim_expr = ndim(then_expression)\n    if ndim_cond > ndim_expr:\n        raise ValueError('Rank of condition should be less'\n                         ' than or equal to rank of then and'\n                         ' else expressions. ndim(condition)=' +\n                         str(ndim_cond) + ', ndim(then_expression)'\n                         '=' + str(ndim_expr))\n    elif ndim_cond < ndim_expr:\n        shape_expr = int_shape(then_expression)\n        ndim_diff = ndim_expr - ndim_cond\n        for i in range(ndim_diff):\n            condition = expand_dims(condition)\n            condition = tile(condition, shape_expr[ndim_cond + i])\n    return C.element_select(condition,\n                            then_expression,\n                            else_expression)\n\n\ndef elu(x, alpha=1.):\n    res = C.elu(x)\n    if alpha == 1:\n        return res\n    else:\n        return C.element_select(C.greater(x, 0), res, alpha * res)\n\n\ndef in_top_k(predictions, targets, k):\n    _targets = C.one_hot(targets, predictions.shape[-1])\n    result = C.classification_error(predictions, _targets, topN=k)\n    return 1 - C.reshape(result, shape=())\n\n\ndef conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = (1,) + strides\n    # cntk output_shape does not include batch axis\n    output_shape = output_shape[1:]\n    # in keras2, need handle output shape in different format\n    if data_format == 'channels_last':\n        shape = list(output_shape)\n        shape[0] = output_shape[2]\n        shape[1] = output_shape[0]\n        shape[2] = output_shape[1]\n        output_shape = tuple(shape)\n\n    x = C.convolution_transpose(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding],\n        output_shape=output_shape)\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef identity(x, name=None):\n    if name is None:\n        name = '%s_alias' % x.name\n    return C.alias(x, name=name)\n\n\ndef _preprocess_conv2d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, rows, cols)\n        # TF input shape: (samples, rows, cols, input_depth)\n        x = C.transpose(x, (2, 0, 1))\n    return x\n\n\ndef _preprocess_conv2d_kernel(kernel, data_format):\n    # As of Keras 2.0.0, all kernels are normalized\n    # on the format `(rows, cols, input_depth, depth)`,\n    # independently of `data_format`.\n    # CNTK expects `(depth, input_depth, rows, cols)`.\n    kernel = C.transpose(kernel, (3, 2, 0, 1))\n    return kernel\n\n\ndef _preprocess_border_mode(padding):\n    if padding == 'same':\n        padding = True\n    elif padding == 'valid':\n        padding = False\n    else:\n        raise ValueError('Invalid border mode: ' + str(padding))\n    return padding\n\n\ndef _postprocess_conv2d_output(x, data_format):\n    if data_format == 'channels_last':\n        x = C.transpose(x, (1, 2, 0))\n    return x\n\n\ndef _preprocess_conv3d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, conv_dim1, conv_dim2, conv_dim3)\n        # TF input shape: (samples, conv_dim1, conv_dim2, conv_dim3,\n        # input_depth)\n        x = C.transpose(x, (3, 0, 1, 2))\n    return x\n\n\ndef _preprocess_conv3d_kernel(kernel, dim_ordering):\n    kernel = C.transpose(kernel, (4, 3, 0, 1, 2))\n    return kernel\n\n\ndef _postprocess_conv3d_output(x, dim_ordering):\n    if dim_ordering == 'channels_last':\n        x = C.transpose(x, (1, 2, 3, 0))\n    return x\n\n\ndef _get_dynamic_axis_num(x):\n    if hasattr(x, 'dynamic_axes'):\n        return len(x.dynamic_axes)\n    else:\n        return 0\n\n\ndef _contain_seqence_axis(x):\n    if _get_dynamic_axis_num(x) > 1:\n        return x.dynamic_axes[1] == C.Axis.default_dynamic_axis()\n    else:\n        return False\n\n\ndef get_num_dynamic_axis(x):\n    return _get_dynamic_axis_num(x)\n\n\ndef _reduce_on_axis(x, axis, reduce_fun_name):\n    if isinstance(axis, list):\n        for a in axis:\n            if isinstance(a, C.Axis) \\\n                    and a != C.Axis.default_batch_axis() \\\n                    and hasattr(C.sequence, reduce_fun_name):\n                x = getattr(C.sequence, reduce_fun_name)(x, a)\n            else:\n                x = getattr(C, reduce_fun_name)(x, a)\n    else:\n        x = getattr(C, reduce_fun_name)(x, axis)\n    return x\n\n\ndef _reshape_sequence(x, time_step):\n    tmp_shape = list(int_shape(x))\n    tmp_shape[1] = time_step\n    return reshape(x, tmp_shape)\n\n\ndef local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    stride = strides[0]\n    kernel_shape = int_shape(kernel)\n    output_length, feature_dim, filters = kernel_shape\n\n    xs = []\n    for i in range(output_length):\n        slice_length = slice(i * stride,\n                             i * stride + kernel_size[0])\n        xs.append(reshape(inputs[:, slice_length, :],\n                          (-1, 1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=1)\n    # transpose kernel to output_filters first, to apply broadcast\n    weight = permute_dimensions(kernel, (2, 0, 1))\n    # Shape: (batch, filters, output_length, input_length * kernel_size)\n    output = x_aggregate * weight\n    # Shape: (batch, filters, output_length)\n    output = sum(output, axis=3)\n    # Shape: (batch, output_length, filters)\n    return permute_dimensions(output, (0, 2, 1))\n\n\ndef local_conv2d(inputs,\n                 kernel,\n                 kernel_size,\n                 strides,\n                 output_shape,\n                 data_format=None):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    stride_row, stride_col = strides\n    output_row, output_col = output_shape\n    kernel_shape = int_shape(kernel)\n    _, feature_dim, filters = kernel_shape\n    xs = []\n\n    for i in range(output_row):\n        for j in range(output_col):\n            slice_row = slice(i * stride_row,\n                              i * stride_row + kernel_size[0])\n            slice_col = slice(j * stride_col,\n                              j * stride_col + kernel_size[1])\n            if data_format == 'channels_first':\n                xs.append(reshape(inputs[:, :, slice_row, slice_col],\n                                  (-1, 1, feature_dim)))\n            else:\n                xs.append(reshape(inputs[:, slice_row, slice_col, :],\n                                  (-1, 1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=1)\n    # transpose kernel to put filters first\n    weight = permute_dimensions(kernel, (2, 0, 1))\n    # shape: batch, filters, output_length, input_length * kernel_size\n    output = x_aggregate * weight\n    # shape: batch, filters, output_length\n    output = sum(output, axis=3)\n    # shape: batch, filters, row, col\n    output = reshape(output,\n                     (-1, filters, output_row, output_col))\n\n    if data_format == 'channels_last':\n        # shape: batch, row, col, filters\n        output = permute_dimensions(output, (0, 2, 3, 1))\n\n    return output\n\n\ndef reverse(x, axes):\n    if isinstance(axes, int):\n        axes = [axes]\n    cntk_axes = _normalize_axis(axes, x)\n    begin_index = [0 for _ in cntk_axes]\n    end_index = [0 for _ in cntk_axes]\n    strides = [-1 for _ in cntk_axes]\n    return C.slice(x, cntk_axes, begin_index, end_index, strides)\n\n\ndef _reshape_batch(x, shape):\n    # there is a bug in cntk 2.1's unpack_batch implementation\n    if hasattr(C, 'unpack_batch') and _get_cntk_version() >= 2.2:\n        const_a = C.unpack_batch(x)\n        const_a = C.reshape(const_a, shape)\n        return C.to_batch(const_a)\n    else:\n        return C.user_function(ReshapeBatch(x, shape[1:]))\n\n\ndef _get_cntk_version():\n    version = C.__version__\n    if version.endswith('+'):\n        version = version[:-1]\n    # for hot fix, ignore all the . except the first one.\n    if len(version) > 2 and version[1] == '.':\n        version = version[:2] + version[2:].replace('.', '')\n    try:\n        return float(version)\n    except:\n        warnings.warn(\n            'CNTK backend warning: CNTK version not detected. '\n            'Will using CNTK 2.0 GA as default.')\n        return float(2.0)\n\n\nclass ReshapeBatch(C.ops.functions.UserFunction):\n    def __init__(self, input, shape, name='reshape_with_batch'):\n        super(ReshapeBatch, self).__init__([input], as_numpy=False, name=name)\n        self.from_shape = input.shape\n        self.target_shape = shape\n\n    def infer_outputs(self):\n        batch_axis = C.Axis.default_batch_axis()\n        return [\n            C.output_variable(\n                self.target_shape,\n                self.inputs[0].dtype,\n                [batch_axis])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        num_element = arguments.shape()[0] * np.prod(np.asarray(self.from_shape))\n        num_static_element = np.prod(np.asarray(self.target_shape))\n        num_batch = int(num_element / num_static_element)\n        result = arguments.data().as_shape((num_batch,) + self.target_shape)\n        return None, C.cntk_py.Value(result)\n\n    def backward(self, state, root_gradients):\n        grad_array_view = root_gradients.data()\n        num_element = root_gradients.shape()[0] * np.prod(np.asarray(self.target_shape))\n        num_static_element = np.prod(np.asarray(self.from_shape))\n        num_old_batch = int(num_element / num_static_element)\n        return C.cntk_py.Value(\n            grad_array_view.as_shape(\n                (num_old_batch,) + self.from_shape))\n\n\nclass ConvertToBatch(C.ops.functions.UserFunction):\n    \"\"\"Converts input first axis to CNTK batch axis.\n\n    We may introduce this operation in CNTK native\n    implementation later.\n\n    # Arguments\n        inputs: a cntk variable (parameter/constant)\n        name: name of this node\n    \"\"\"\n\n    def __init__(self, input, name='convert_to_batch'):\n        super(ConvertToBatch, self).__init__([input], as_numpy=False, name=name)\n\n    def infer_outputs(self):\n        batch_axis = C.Axis.default_batch_axis()\n        return [\n            C.output_variable(\n                self.inputs[0].shape[1:],\n                self.inputs[0].dtype,\n                [batch_axis])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        return None, C.cntk_py.Value(arguments.data())\n\n    def backward(self, state, root_gradients):\n        return C.cntk_py.Value(root_gradients.data())\n\n\nclass ConvertToStatic(C.ops.functions.UserFunction):\n    \"\"\"Converts input first axis to CNTK static axis.\n\n    We may introduce this operation in CNTK native\n    implementation later.\n\n    # Arguments\n        inputs: a cntk tensor which has batch axis\n        batch_size: size of batch axis.\n        name: name of this node.\n    \"\"\"\n\n    def __init__(self, input, batch_size, name='convert_to_static'):\n        super(ConvertToStatic, self).__init__([input], as_numpy=False, name=name)\n        self.target_shape = (batch_size,) + input.shape\n\n    def infer_outputs(self):\n        return [\n            C.output_variable(\n                self.target_shape,\n                self.inputs[0].dtype,\n                [])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        return None, C.cntk_py.Value(arguments.data())\n\n    def backward(self, state, root_gradients):\n        return C.cntk_py.Value(root_gradients.data())\n\n\nclass LambdaFunc(C.ops.functions.UserFunction):\n    def __init__(self,\n                 arg,\n                 when=lambda arg: True,\n                 execute=lambda arg: print(arg),\n                 name=''):\n        self.when = when\n        self.execute = execute\n\n        super(LambdaFunc, self).__init__([arg], name=name)\n\n    def infer_outputs(self):\n        return [\n            C.output_variable(\n                self.inputs[0].shape,\n                self.inputs[0].dtype,\n                self.inputs[0].dynamic_axes)]\n\n    def forward(self, argument, device=None, outputs_to_retain=None):\n        if self.when(argument):\n            self.execute(argument)\n\n        return None, argument\n\n    def backward(self, state, root_gradients):\n        return root_gradients\n",
                "max_stars_repo_path": "deep_learning/keras/keras/backend/cntk_backend.py",
                "max_stars_repo_name": "xpennec/applications",
                "max_stars_count": 21,
                "__cluster__": 33
            }
        ],
        [
            {
                "id": "test_keras-bug-20",
                "content": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cntk as C\nimport numpy as np\nfrom .common import floatx\nfrom .common import epsilon\nfrom .common import image_data_format\nfrom .common import normalize_data_format\nfrom ..utils.generic_utils import transpose_shape\nfrom collections import defaultdict\nfrom contextlib import contextmanager\nimport warnings\n\n\nC.set_global_option('align_axis', 1)\n\nb_any = any\npy_slice = slice\n\n\ndev = C.device.use_default_device()\nif dev.type() == 0:\n    warnings.warn(\n        'CNTK backend warning: GPU is not detected. '\n        'CNTK\\'s CPU version is not fully optimized,'\n        'please run with GPU to get better performance.')\n\n# A learning phase is a bool tensor used to run Keras models in\n# either train mode (learning_phase == 1) or test mode (learning_phase == 0).\n# LEARNING_PHASE_PLACEHOLDER is the placeholder for dynamic learning phase\n_LEARNING_PHASE_PLACEHOLDER = C.constant(shape=(), dtype=np.float32, value=1.0, name='_keras_learning_phase')\n# static learning phase flag, if it is not 0 or 1, we will go with dynamic learning phase tensor.\n_LEARNING_PHASE = -1\n_UID_PREFIXES = defaultdict(int)\n\n# cntk doesn't support gradient as symbolic op, to hook up with keras model,\n# we will create gradient as a constant placeholder, here use this global\n# map to keep the mapping from grad placeholder to parameter\ngrad_parameter_dict = {}\n\nNAME_SCOPE_STACK = []\n\n\n@contextmanager\ndef name_scope(name):\n    global NAME_SCOPE_STACK\n    NAME_SCOPE_STACK.append(name)\n    yield\n    NAME_SCOPE_STACK.pop()\n\n\ndef get_uid(prefix=''):\n    _UID_PREFIXES[prefix] += 1\n    return _UID_PREFIXES[prefix]\n\n\ndef learning_phase():\n    # If _LEARNING_PHASE is not 0 or 1, return dynamic learning phase tensor\n    return _LEARNING_PHASE if _LEARNING_PHASE in {0, 1} else _LEARNING_PHASE_PLACEHOLDER\n\n\ndef set_learning_phase(value):\n    global _LEARNING_PHASE\n    if value not in {0, 1}:\n        raise ValueError('CNTK Backend: Set learning phase '\n                         'with value %s is not supported, '\n                         'expected 0 or 1.' % value)\n    _LEARNING_PHASE = value\n\n\ndef clear_session():\n    \"\"\"Reset learning phase flag for cntk backend.\n    \"\"\"\n    global _LEARNING_PHASE\n    global _LEARNING_PHASE_PLACEHOLDER\n    _LEARNING_PHASE = -1\n    _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(1.0)\n\n\ndef in_train_phase(x, alt, training=None):\n    global _LEARNING_PHASE\n    if training is None:\n        training = learning_phase()\n        uses_learning_phase = True\n    else:\n        uses_learning_phase = False\n\n    # CNTK currently don't support cond op, so here we use\n    # element_select approach as workaround. It may have\n    # perf issue, will resolve it later with cntk cond op.\n    if callable(x) and isinstance(x, C.cntk_py.Function) is False:\n        x = x()\n    if callable(alt) and isinstance(alt, C.cntk_py.Function) is False:\n        alt = alt()\n\n    if training is True:\n        x._uses_learning_phase = uses_learning_phase\n        return x\n    else:\n        # if _LEARNING_PHASE is static\n        if isinstance(training, int) or isinstance(training, bool):\n            result = x if training == 1 or training is True else alt\n        else:\n            result = C.element_select(training, x, alt)\n        result._uses_learning_phase = uses_learning_phase\n        return result\n\n\ndef in_test_phase(x, alt, training=None):\n    return in_train_phase(alt, x, training=training)\n\n\ndef _convert_string_dtype(dtype):\n    if dtype == 'float32':\n        return np.float32\n    elif dtype == 'float64':\n        return np.float64\n    elif dtype == 'float16':\n        return np.float16\n    else:\n        # cntk only running with float,\n        # try to cast to float to run the model\n        return np.float32\n\n\ndef _convert_dtype_string(dtype):\n    if dtype == np.float32:\n        return 'float32'\n    elif dtype == np.float64:\n        return 'float64'\n    elif dtype == np.float16:\n        return 'float16'\n    else:\n        raise ValueError('CNTK Backend: Unsupported dtype: %s. '\n                         'CNTK only supports float32, float64, and '\n                         'float16.' % dtype)\n\n\ndef variable(value, dtype=None, name=None, constraint=None):\n    \"\"\"Instantiates a variable and returns it.\n\n    # Arguments\n        value: Numpy array, initial value of the tensor.\n        dtype: Tensor type.\n        name: Optional name string for the tensor.\n        constraint: Optional projection function to be\n            applied to the variable after an optimizer update.\n\n    # Returns\n        A variable instance (with Keras metadata included).\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n\n    if name is None:\n        name = ''\n\n    if isinstance(\n            value,\n            C.variables.Constant) or isinstance(\n            value,\n            C.variables.Parameter):\n        value = value.value\n\n    # we don't support init parameter with symbolic op, so eval it first as\n    # workaround\n    if isinstance(value, C.cntk_py.Function):\n        value = eval(value)\n\n    shape = value.shape if hasattr(value, 'shape') else ()\n    if hasattr(value, 'dtype') and value.dtype != dtype and len(shape) > 0:\n        value = value.astype(dtype)\n\n    # TODO: remove the conversion when cntk supports int32, int64\n    # https://docs.microsoft.com/en-us/python/api/cntk.variables.parameter\n    dtype = 'float32' if 'int' in str(dtype) else dtype\n\n    v = C.parameter(shape=shape,\n                    init=value,\n                    dtype=dtype,\n                    name=_prepare_name(name, 'variable'))\n    v._keras_shape = v.shape\n    v._uses_learning_phase = False\n    v.constraint = constraint\n    return v\n\n\ndef bias_add(x, bias, data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    dims = len(x.shape)\n    if dims > 0 and x.shape[0] == C.InferredDimension:\n        dims -= 1\n\n    bias_dims = len(bias.shape)\n    if bias_dims != 1 and bias_dims != dims:\n        raise ValueError('Unexpected bias dimensions %d, '\n                         'expected 1 or %d dimensions' % (bias_dims, dims))\n\n    if dims == 4:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1, 1, 1)\n            else:\n                shape = (bias.shape[3],) + bias.shape[:3]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, 1, 1, bias.shape[0])\n            else:\n                shape = bias.shape\n    elif dims == 3:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1, 1)\n            else:\n                shape = (bias.shape[2],) + bias.shape[:2]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, 1, bias.shape[0])\n            else:\n                shape = bias.shape\n    elif dims == 2:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1)\n            else:\n                shape = (bias.shape[1],) + bias.shape[:1]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, bias.shape[0])\n            else:\n                shape = bias.shape\n    else:\n        shape = bias.shape\n    return x + reshape(bias, shape)\n\n\ndef eval(x):\n    if isinstance(x, C.cntk_py.Function):\n        return x.eval()\n    elif isinstance(x, C.variables.Constant) or isinstance(x, C.variables.Parameter):\n        return x.value\n    else:\n        raise ValueError('CNTK Backend: `eval` method on '\n                         '`%s` type is not supported. '\n                         'CNTK only supports `eval` with '\n                         '`Function`, `Constant` or '\n                         '`Parameter`.' % type(x))\n\n\ndef placeholder(\n        shape=None,\n        ndim=None,\n        dtype=None,\n        sparse=False,\n        name=None,\n        dynamic_axis_num=1):\n    if dtype is None:\n        dtype = floatx()\n    if not shape:\n        if ndim:\n            shape = tuple([None for _ in range(ndim)])\n\n    dynamic_dimension = C.FreeDimension if _get_cntk_version() >= 2.2 else C.InferredDimension\n    cntk_shape = [dynamic_dimension if s is None else s for s in shape]\n    cntk_shape = tuple(cntk_shape)\n\n    if dynamic_axis_num > len(cntk_shape):\n        raise ValueError('CNTK backend: creating placeholder with '\n                         '%d dimension is not supported, at least '\n                         '%d dimensions are needed.'\n                         % (len(cntk_shape), dynamic_axis_num))\n\n    if name is None:\n        name = ''\n\n    cntk_shape = cntk_shape[dynamic_axis_num:]\n\n    x = C.input(\n        shape=cntk_shape,\n        dtype=_convert_string_dtype(dtype),\n        is_sparse=sparse,\n        name=name)\n    x._keras_shape = shape\n    x._uses_learning_phase = False\n    x._cntk_placeholder = True\n    return x\n\n\ndef is_placeholder(x):\n    \"\"\"Returns whether `x` is a placeholder.\n\n    # Arguments\n        x: A candidate placeholder.\n\n    # Returns\n        Boolean.\n    \"\"\"\n    return hasattr(x, '_cntk_placeholder') and x._cntk_placeholder\n\n\ndef is_keras_tensor(x):\n    if not is_tensor(x):\n        raise ValueError('Unexpectedly found an instance of type `' +\n                         str(type(x)) + '`. '\n                         'Expected a symbolic tensor instance.')\n    return hasattr(x, '_keras_history')\n\n\ndef is_tensor(x):\n    return isinstance(x, (C.variables.Constant,\n                          C.variables.Variable,\n                          C.variables.Parameter,\n                          C.ops.functions.Function))\n\n\ndef shape(x):\n    shape = list(int_shape(x))\n    num_dynamic = _get_dynamic_axis_num(x)\n    non_dyn_shape = []\n    for i in range(len(x.shape)):\n        if shape[i + num_dynamic] is None:\n            non_dyn_shape.append(x.shape[i])\n        else:\n            non_dyn_shape.append(shape[i + num_dynamic])\n    return shape[:num_dynamic] + non_dyn_shape\n\n\ndef is_sparse(tensor):\n    return tensor.is_sparse\n\n\ndef int_shape(x):\n    if hasattr(x, '_keras_shape'):\n        return x._keras_shape\n\n    shape = x.shape\n    if hasattr(x, 'dynamic_axes'):\n        dynamic_shape = [None for a in x.dynamic_axes]\n        shape = tuple(dynamic_shape) + shape\n    return shape\n\n\ndef ndim(x):\n    shape = int_shape(x)\n    return len(shape)\n\n\ndef _prepare_name(name, default):\n    prefix = '_'.join(NAME_SCOPE_STACK)\n    if name is None or name == '':\n        return prefix + '/' + default\n    return prefix + '/' + name\n\n\ndef constant(value, dtype=None, shape=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    if shape is None:\n        shape = ()\n    np_value = value * np.ones(shape)\n    const = C.constant(np_value,\n                       dtype=dtype,\n                       name=_prepare_name(name, 'constant'))\n    const._keras_shape = const.shape\n    const._uses_learning_phase = False\n    return const\n\n\ndef random_binomial(shape, p=0.0, dtype=None, seed=None):\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e7)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n    return C.random.bernoulli(shape=shape, dtype=dtype, mean=p, seed=seed)\n\n\ndef random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n    return C.random.uniform(shape=shape, dtype=dtype, low=minval, high=maxval, seed=seed)\n\n\ndef random_uniform_variable(shape, low, high,\n                            dtype=None, name=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    if name is None:\n        name = ''\n\n    scale = (high - low) / 2\n    p = C.parameter(\n        shape,\n        init=C.initializer.uniform(\n            scale,\n            seed=seed),\n        dtype=dtype,\n        name=name)\n    return variable(value=p.value + low + scale)\n\n\ndef random_normal_variable(\n        shape,\n        mean,\n        scale,\n        dtype=None,\n        name=None,\n        seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e7)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    if name is None:\n        name = ''\n\n    p = C.parameter(\n        shape=shape,\n        init=C.initializer.normal(\n            scale=scale,\n            seed=seed),\n        dtype=dtype,\n        name=name)\n    return variable(value=p.value + mean)\n\n\ndef random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n    return C.random.normal(shape=shape, mean=mean, scale=stddev, seed=seed, dtype=dtype)\n\n\ndef truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if seed is None:\n        seed = np.random.randint(1, 10e6)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    return C.parameter(\n        shape, init=C.initializer.truncated_normal(\n            stddev, seed=seed), dtype=dtype)\n\n\ndef dtype(x):\n    return _convert_dtype_string(x.dtype)\n\n\ndef zeros(shape, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    ctype = _convert_string_dtype(dtype)\n    return variable(value=np.zeros(shape, ctype), dtype=dtype, name=name)\n\n\ndef ones(shape, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    ctype = _convert_string_dtype(dtype)\n    return variable(value=np.ones(shape, ctype), dtype=dtype, name=name)\n\n\ndef eye(size, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    return variable(np.eye(size), dtype, name)\n\n\ndef zeros_like(x, dtype=None, name=None):\n    return x * 0\n\n\ndef ones_like(x, dtype=None, name=None):\n    return zeros_like(x) + 1\n\n\ndef count_params(x):\n    for _ in x.shape:\n        if _ == C.InferredDimension or _ == C.FreeDimension:\n            raise ValueError('CNTK backend: `count_params` with dynamic '\n                             'shape is not supported. Please provide '\n                             'fixed dimension instead of `None`.')\n\n    return np.prod(int_shape(x))\n\n\ndef cast(x, dtype):\n    # cntk calculate everything in float, so don't need case from bool / int\n    return x\n\n\ndef dot(x, y):\n    if len(x.shape) > 2 or len(y.shape) > 2:\n        y_shape = int_shape(y)\n        if len(y_shape) > 2:\n            permutation = [len(y_shape) - 2]\n            permutation += list(range(len(y_shape) - 2))\n            permutation += [len(y_shape) - 1]\n            y = C.transpose(y, perm=permutation)\n        return C.times(x, y, len(y_shape) - 1)\n    else:\n        return C.times(x, y)\n\n\ndef batch_dot(x, y, axes=None):\n    x_shape = int_shape(x)\n    y_shape = int_shape(y)\n\n    if isinstance(axes, int):\n        axes = (axes, axes)\n    if axes is None:\n        # behaves like tf.batch_matmul as default\n        axes = [len(x_shape) - 1, len(y_shape) - 2]\n    if b_any([isinstance(a, (list, tuple)) for a in axes]):\n        raise ValueError('Multiple target dimensions are not supported. ' +\n                         'Expected: None, int, (int, int), ' +\n                         'Provided: ' + str(axes))\n\n    if len(x_shape) == 2 and len(y_shape) == 2:\n        if axes[0] == axes[1]:\n            result = sum(x * y, axis=axes[0], keepdims=True)\n            return result if axes[0] == 1 else transpose(result)\n        else:\n            return sum(x * transpose(y), axis=axes[0], keepdims=True)\n    else:\n        if len(y_shape) == 2:\n            y = expand_dims(y)\n\n        normalized_axis = []\n        normalized_axis.append(_normalize_axis(axes[0], x)[0])\n        normalized_axis.append(_normalize_axis(axes[1], y)[0])\n        # transpose\n        i = normalized_axis[0]\n        while i < len(x.shape) - 1:\n            x = C.swapaxes(x, i, i + 1)\n            i += 1\n        i = normalized_axis[1]\n        while i > 0:\n            y = C.swapaxes(y, i, i - 1)\n            i -= 1\n        result = C.times(x, y, output_rank=(len(y.shape) - 1)\n                         if len(y.shape) > 1 else 1)\n        if len(y_shape) == 2:\n            result = squeeze(result, -1)\n        return result\n\n\ndef transpose(x):\n    return C.swapaxes(x, 0, 1)\n\n\ndef gather(reference, indices):\n    # There is a bug in cntk gather op which may cause crash.\n    # We have made a fix but not catched in CNTK 2.1 release.\n    # Will update with gather op in next release\n    if _get_cntk_version() >= 2.2:\n        return C.ops.gather(reference, indices)\n    else:\n        num_classes = reference.shape[0]\n        one_hot_matrix = C.ops.one_hot(indices, num_classes)\n        return C.times(one_hot_matrix, reference, output_rank=len(reference.shape) - 1)\n\n\ndef _remove_dims(x, axis, keepdims=False):\n    if keepdims is False and isinstance(axis, list):\n        # sequence axis is removed by default, so don't need reshape on it\n        reduce_axes = []\n        for a in axis:\n            if isinstance(a, C.Axis) is False:\n                reduce_axes.append(a)\n        return _reshape_dummy_dim(x, reduce_axes)\n    else:\n        if isinstance(axis, list):\n            has_seq = False\n            for a in axis:\n                if isinstance(a, C.Axis):\n                    has_seq = True\n                    break\n            if has_seq:\n                nones = _get_dynamic_axis_num(x)\n                x = expand_dims(x, nones)\n        return x\n\n\ndef max(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_max')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef min(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_min')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef sum(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_sum')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef prod(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_prod')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef logsumexp(x, axis=None, keepdims=False):\n    return log(sum(exp(x), axis=axis, keepdims=keepdims))\n\n\ndef var(x, axis=None, keepdims=False):\n    m = mean(x, axis, keepdims=True)\n    devs_squared = C.square(x - m)\n    return mean(devs_squared, axis=axis, keepdims=keepdims)\n\n\ndef std(x, axis=None, keepdims=False):\n    return C.sqrt(var(x, axis=axis, keepdims=keepdims))\n\n\ndef expand_dims(x, axis=-1):\n    shape = list(int_shape(x))\n    nones = _get_dynamic_axis_num(x)\n    index = axis if axis >= 0 else len(shape) + 1\n    shape.insert(index, 1)\n    new_shape = shape[nones:]\n    new_shape = tuple(\n        [C.InferredDimension if _ is None else _ for _ in new_shape])\n    result = C.reshape(x, new_shape)\n    if index < nones:\n        result._keras_shape = shape\n    return result\n\n\ndef squeeze(x, axis):\n    if isinstance(axis, tuple):\n        axis = list(axis)\n    if not isinstance(axis, list):\n        axis = [axis]\n\n    shape = list(int_shape(x))\n\n    _axis = []\n    for _ in axis:\n        if isinstance(_, int):\n            _axis.append(_ if _ >= 0 else _ + len(shape))\n\n    if len(_axis) == 0:\n        return x\n\n    nones = _get_dynamic_axis_num(x)\n    for _ in sorted(_axis, reverse=True):\n        del shape[_]\n\n    new_shape = shape[nones:]\n    new_shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in new_shape])\n    return C.reshape(x, new_shape)\n\n\ndef tile(x, n):\n    if isinstance(n, int):\n        n = (n,)\n    elif isinstance(n, list):\n        n = tuple(n)\n\n    shape = int_shape(x)\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    # Padding the axis\n    if len(n) < len(shape):\n        n = tuple([1 for _ in range(len(shape) - len(n))]) + n\n\n    if len(n) != len(shape):\n        raise NotImplementedError\n\n    i = num_dynamic_axis\n    for i, rep in enumerate(n):\n        if i >= num_dynamic_axis and shape[i] is not None:\n            tmp = [x] * rep\n            x = C.splice(*tmp, axis=i - num_dynamic_axis)\n        i += 1\n\n    return x\n\n\ndef _normalize_axis(axis, x):\n    shape = int_shape(x)\n    ndim = len(shape)\n\n    nones = _get_dynamic_axis_num(x)\n\n    if nones > ndim:\n        raise ValueError('CNTK Backend: tensor with keras shape: `%s` has '\n                         '%d cntk dynamic axis, this is not expected, please '\n                         'double check the keras shape history.' % (str(shape), nones))\n\n    # Current cntk does not support shape like (1, batch). so using the workaround\n    # here to mapping the correct axis. Will remove this tricky after we add support\n    # in native cntk op\n    cntk_axis = []\n    dynamic_axis_index = 0\n    for i in range(ndim):\n        if shape[i] is None and dynamic_axis_index < nones:\n            cntk_axis.append(x.dynamic_axes[dynamic_axis_index])\n            dynamic_axis_index += 1\n        else:\n            cntk_axis.append(i - dynamic_axis_index)\n\n    if dynamic_axis_index < nones:\n        i = 0\n        while dynamic_axis_index < nones:\n            cntk_axis[i] = x.dynamic_axes[dynamic_axis_index]\n            i += 1\n            dynamic_axis_index += 1\n\n        while i < len(cntk_axis):\n            cntk_axis[i] -= nones\n            i += 1\n\n    if isinstance(axis, tuple):\n        _axis = list(axis)\n    elif isinstance(axis, int):\n        _axis = [axis]\n    elif isinstance(axis, list):\n        _axis = list(axis)\n    else:\n        _axis = axis\n\n    if isinstance(_axis, list):\n        for i, a in enumerate(_axis):\n            if a is not None and a < 0:\n                _axis[i] = (a % ndim)\n            if _axis[i] is not None:\n                _axis[i] = cntk_axis[_axis[i]]\n    else:\n        if _axis is None:\n            _axis = C.Axis.all_axes()\n\n    return _axis\n\n\ndef _reshape_dummy_dim(x, axis):\n    shape = list(x.shape)\n\n    _axis = [_ + len(shape) if _ < 0 else _ for _ in axis]\n\n    if shape.count(C.InferredDimension) > 1 or shape.count(C.FreeDimension) > 1:\n        result = x\n        for index in sorted(_axis, reverse=True):\n            result = C.reshape(result,\n                               shape=(),\n                               begin_axis=index,\n                               end_axis=index + 1)\n        return result\n    else:\n        for index in sorted(_axis, reverse=True):\n            del shape[index]\n\n        shape = [C.InferredDimension if _ == C.FreeDimension else _ for _ in shape]\n        return C.reshape(x, shape)\n\n\ndef mean(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_mean')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef any(x, axis=None, keepdims=False):\n    reduce_result = sum(x, axis, keepdims=keepdims)\n    any_matrix = C.element_select(\n        reduce_result,\n        ones_like(reduce_result),\n        zeros_like(reduce_result))\n    if len(reduce_result.shape) == 0 and _get_dynamic_axis_num(x) == 0:\n        return C.reduce_sum(any_matrix)\n    else:\n        return any_matrix\n\n\ndef all(x, axis=None, keepdims=False):\n    reduce_result = prod(x, axis, keepdims=keepdims)\n    all_matrix = C.element_select(\n        reduce_result,\n        ones_like(reduce_result),\n        zeros_like(reduce_result))\n    if len(reduce_result.shape) == 0 and _get_dynamic_axis_num(x) == 0:\n        return C.reduce_sum(all_matrix)\n    else:\n        return all_matrix\n\n\ndef classification_error(target, output, axis=-1):\n    return C.ops.reduce_mean(\n        C.equal(\n            argmax(\n                output,\n                axis=-1),\n            argmax(\n                target,\n                axis=-1)),\n        axis=C.Axis.all_axes())\n\n\ndef argmax(x, axis=-1):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    output = C.ops.argmax(x, axis=axis[0])\n    return _reshape_dummy_dim(output, axis)\n\n\ndef argmin(x, axis=-1):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    output = C.ops.argmin(x, axis=axis[0])\n    return _reshape_dummy_dim(output, axis)\n\n\ndef square(x):\n    return C.square(x)\n\n\ndef abs(x):\n    return C.abs(x)\n\n\ndef sqrt(x):\n    return C.sqrt(x)\n\n\ndef exp(x):\n    return C.exp(x)\n\n\ndef log(x):\n    return C.log(x)\n\n\ndef round(x):\n    return C.round(x)\n\n\ndef sigmoid(x):\n    return C.sigmoid(x)\n\n\ndef sign(x):\n    return x / C.abs(x)\n\n\ndef pow(x, a):\n    return C.pow(x, a)\n\n\ndef clip(x, min_value, max_value):\n    if max_value is not None and max_value < min_value:\n        max_value = min_value\n    if max_value is None:\n        max_value = np.inf\n    if min_value is None:\n        min_value = -np.inf\n    return C.clip(x, min_value, max_value)\n\n\ndef binary_crossentropy(target, output, from_logits=False):\n    if from_logits:\n        output = C.sigmoid(output)\n    output = C.clip(output, epsilon(), 1.0 - epsilon())\n    output = -target * C.log(output) - (1.0 - target) * C.log(1.0 - output)\n    return output\n\n\ndef get_variable_shape(x):\n    return int_shape(x)\n\n\ndef update(x, new_x):\n    return C.assign(x, new_x)\n\n\ndef moving_average_update(variable, value, momentum):\n    return C.assign(variable, variable * momentum + value * (1. - momentum))\n\n\ndef update_add(x, increment):\n    result = x + increment\n    return C.assign(x, result)\n\n\ndef gradients(loss, variables):\n    # cntk does not support gradients as symbolic op,\n    # to hook up with keras model\n    # we will return a constant as place holder, the cntk learner will apply\n    # the gradient during training.\n    global grad_parameter_dict\n    if isinstance(variables, list) is False:\n        variables = [variables]\n    grads = []\n    for v in variables:\n        g = C.constant(0, shape=v.shape, name='keras_grad_placeholder')\n        grads.append(g)\n        grad_parameter_dict[g] = v\n    return grads\n\n\ndef equal(x, y):\n    return C.equal(x, y)\n\n\ndef not_equal(x, y):\n    return C.not_equal(x, y)\n\n\ndef greater(x, y):\n    return C.greater(x, y)\n\n\ndef greater_equal(x, y):\n    return C.greater_equal(x, y)\n\n\ndef less(x, y):\n    return C.less(x, y)\n\n\ndef less_equal(x, y):\n    return C.less_equal(x, y)\n\n\ndef maximum(x, y):\n    return C.element_max(x, y)\n\n\ndef minimum(x, y):\n    return C.element_min(x, y)\n\n\ndef sin(x):\n    return C.sin(x)\n\n\ndef cos(x):\n    return C.cos(x)\n\n\ndef normalize_batch_in_training(x, gamma, beta,\n                                reduction_axes, epsilon=1e-3):\n    if gamma is None:\n        if beta is None:\n            gamma = ones_like(x)\n        else:\n            gamma = ones_like(beta)\n    if beta is None:\n        if gamma is None:\n            beta = zeros_like(x)\n        else:\n            beta = zeros_like(gamma)\n\n    mean, variant = _moments(x, _normalize_axis(reduction_axes, x))\n\n    if sorted(reduction_axes) == list(range(ndim(x)))[:-1]:\n        normalized = batch_normalization(\n            x, mean, variant, beta, gamma, epsilon)\n    else:\n        # need broadcasting\n        target_shape = []\n        x_shape = int_shape(x)\n        # skip the batch axis\n        for axis in range(1, ndim(x)):\n            if axis in reduction_axes:\n                target_shape.append(1)\n                if ndim(gamma) > axis:\n                    gamma = C.reduce_mean(gamma, axis - 1)\n                    beta = C.reduce_mean(beta, axis - 1)\n            else:\n                target_shape.append(x_shape[axis])\n\n        broadcast_mean = C.reshape(mean, target_shape)\n        broadcast_var = C.reshape(variant, target_shape)\n        broadcast_gamma = C.reshape(gamma, target_shape)\n        broadcast_beta = C.reshape(beta, target_shape)\n        normalized = batch_normalization(\n            x,\n            broadcast_mean,\n            broadcast_var,\n            broadcast_beta,\n            broadcast_gamma,\n            epsilon)\n\n    return normalized, mean, variant\n\n\ndef _moments(x, axes=None, shift=None, keep_dims=False):\n    _axes = tuple(axes)\n    if shift is None:\n        shift = x\n        # Compute true mean while keeping the dims for proper broadcasting.\n        for axis in _axes:\n            shift = C.reduce_mean(shift, axis=axis)\n\n    shift = C.stop_gradient(shift)\n    shifted_mean = C.minus(x, shift)\n    for axis in _axes:\n        shifted_mean = C.reduce_mean(shifted_mean, axis=axis)\n\n    variance_mean = C.square(C.minus(x, shift))\n    for axis in _axes:\n        variance_mean = C.reduce_mean(variance_mean, axis=axis)\n\n    variance = C.minus(variance_mean, C.square(shifted_mean))\n    mean = C.plus(shifted_mean, shift)\n\n    if not keep_dims:\n        mean = squeeze(mean, _axes)\n        variance = squeeze(variance, _axes)\n\n    return mean, variance\n\n\ndef batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n    # The mean / var / beta / gamma may be processed by broadcast\n    # so it may have an extra batch axis with 1, it is not needed\n    # in cntk, need to remove those dummy axis.\n    if ndim(mean) == ndim(x) and shape(mean)[0] == 1:\n        mean = _reshape_dummy_dim(mean, [0])\n    if ndim(var) == ndim(x) and shape(var)[0] == 1:\n        var = _reshape_dummy_dim(var, [0])\n\n    if gamma is None:\n        gamma = ones_like(var)\n    elif ndim(gamma) == ndim(x) and shape(gamma)[0] == 1:\n        gamma = _reshape_dummy_dim(gamma, [0])\n\n    if beta is None:\n        beta = zeros_like(mean)\n    elif ndim(beta) == ndim(x) and shape(beta)[0] == 1:\n        beta = _reshape_dummy_dim(beta, [0])\n\n    return (x - mean) / C.sqrt(var + epsilon) * gamma + beta\n\n\ndef concatenate(tensors, axis=-1):\n    if len(tensors) == 0:\n        return None\n\n    axis = [axis]\n    axis = _normalize_axis(axis, tensors[0])\n    return C.splice(*tensors, axis=axis[0])\n\n\ndef flatten(x):\n    return reshape(x, (-1,))\n\n\ndef reshape(x, shape):\n    shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in shape])\n    if isinstance(x, C.variables.Parameter):\n        return C.reshape(x, shape)\n    else:\n        num_dynamic_axis = _get_dynamic_axis_num(x)\n\n        if num_dynamic_axis == 1 and len(shape) > 0 and shape[0] == -1:\n            # collapse axis with batch axis\n            if b_any(_ == C.InferredDimension for _ in x.shape) or b_any(\n                    _ == C.FreeDimension for _ in x.shape):\n                warnings.warn(\n                    'Warning: CNTK backend does not support '\n                    'collapse of batch axis with inferred dimension. '\n                    'The reshape did not take place.')\n                return x\n            return _reshape_batch(x, shape)\n        else:\n            # no collapse, then first need to padding the shape\n            if num_dynamic_axis >= len(shape):\n                i = 0\n                while i < len(shape):\n                    if shape[i] is None or shape[i] == -1:\n                        i += 1\n                    else:\n                        break\n                shape = tuple([-1 for _ in range(num_dynamic_axis - i)]) + shape\n\n            new_shape = list(shape)\n            new_shape = new_shape[num_dynamic_axis:]\n            new_shape = [C.InferredDimension if _ is None else _ for _ in new_shape]\n            return C.reshape(x, new_shape)\n\n\ndef permute_dimensions(x, pattern):\n    dims = len(int_shape(x))\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    if isinstance(pattern, list):\n        current_layout = [i for i in range(dims)]\n    else:\n        current_layout = tuple([i for i in range(dims)])\n\n    if num_dynamic_axis > 0 and pattern[:num_dynamic_axis] != current_layout[:num_dynamic_axis]:\n        raise ValueError('CNTK backend: the permute pattern %s '\n                         'requested permute on dynamic axis, '\n                         'which is not supported. Please do permute '\n                         'on static axis.' % pattern)\n\n    axis = list(pattern)\n    axis = axis[num_dynamic_axis:]\n    axis = _normalize_axis(axis, x)\n    return C.transpose(x, axis)\n\n\ndef resize_images(x, height_factor, width_factor, data_format, interpolation='nearest'):\n    if interpolation == 'nearest':\n        if data_format == 'channels_first':\n            output = repeat_elements(x, height_factor, axis=2)\n            output = repeat_elements(output, width_factor, axis=3)\n            return output\n        elif data_format == 'channels_last':\n            output = repeat_elements(x, height_factor, axis=1)\n            output = repeat_elements(output, width_factor, axis=2)\n            return output\n        else:\n            raise ValueError('CNTK Backend: Invalid data_format: %s' % data_format)\n    else:\n        raise NotImplementedError('CNTK only supports `nearest` interpolation.')\n\n\ndef resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n    if data_format == 'channels_first':\n        output = repeat_elements(x, depth_factor, axis=2)\n        output = repeat_elements(output, height_factor, axis=3)\n        output = repeat_elements(output, width_factor, axis=4)\n        return output\n    elif data_format == 'channels_last':\n        output = repeat_elements(x, depth_factor, axis=1)\n        output = repeat_elements(output, height_factor, axis=2)\n        output = repeat_elements(output, width_factor, axis=3)\n        return output\n    else:\n        raise ValueError('CNTK Backend: Invalid data_format: %s' % data_format)\n\n\ndef repeat_elements(x, rep, axis):\n    axis = _normalize_axis(axis, x)\n    axis = axis[0]\n    slices = []\n    shape = x.shape\n    i = 0\n    while i < shape[axis]:\n        tmp = C.ops.slice(x, axis, i, i + 1)\n        for _ in range(rep):\n            slices.append(tmp)\n        i += 1\n    return C.splice(*slices, axis=axis)\n\n\ndef repeat(x, n):\n    # this is a workaround for recurrent layer\n    # if n is inferred dimension,\n    # we can't figure out how to repeat it in cntk now\n    # return the same x to take cntk broadcast feature\n    # to make the recurrent layer work.\n    # need to be fixed in GA.\n    if n is C.InferredDimension or n is C.FreeDimension:\n        return x\n    index = 1 - _get_dynamic_axis_num(x)\n    if index < 0 or index > 1:\n        raise NotImplementedError\n\n    new_shape = list(x.shape)\n    new_shape.insert(index, 1)\n    new_shape = tuple(new_shape)\n    x = C.reshape(x, new_shape)\n    temp = [x] * n\n    return C.splice(*temp, axis=index)\n\n\ndef tanh(x):\n    return C.tanh(x)\n\n\ndef _static_rnn(step_function, inputs, initial_states,\n                go_backwards=False, mask=None, constants=None,\n                unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('Input should be at least 3D.')\n\n    # if the second axis is static axis, CNTK will do unroll by default\n    if shape[1] is None:\n        raise ValueError('CNTK Backend: the input of static rnn '\n                         'has shape `%s`, the second axis '\n                         'is not static. If you want to run '\n                         'rnn with non-static axis, please try '\n                         'dynamic rnn with sequence axis.' % shape)\n    if constants is None:\n        constants = []\n\n    if mask is not None:\n        mask_shape = int_shape(mask)\n        if len(mask_shape) == dims - 1:\n            mask = expand_dims(mask)\n\n    nones = _get_dynamic_axis_num(inputs)\n\n    states = tuple(initial_states)\n\n    outputs = []\n\n    time_axis = 1 - nones if nones > 0 else 1\n\n    if go_backwards:\n        i = shape[1] - 1\n        while i >= 0:\n            current = C.ops.slice(inputs, time_axis, i, i + 1)\n            # remove dummy dimension\n            current = squeeze(current, time_axis)\n\n            output, new_states = step_function(\n                current, tuple(states) + tuple(constants))\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n\n            if mask is not None:\n                mask_slice = C.ops.slice(mask, time_axis, i, i + 1)\n                mask_slice = squeeze(mask_slice, time_axis)\n                if len(outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = outputs[-1]\n                output = C.ops.element_select(mask_slice, output, prev_output)\n\n                return_states = []\n                for s, n_s in zip(states, new_states):\n                    return_states.append(\n                        C.ops.element_select(\n                            mask_slice, n_s, s))\n                new_states = return_states\n            outputs.append(output)\n            states = new_states\n            i -= 1\n    else:\n        i = 0\n        while i < shape[1]:\n            current = C.ops.slice(inputs, time_axis, i, i + 1)\n            # remove dummy dimension\n            current = squeeze(current, 1)\n\n            output, new_states = step_function(\n                current, tuple(states) + tuple(constants))\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n\n            if mask is not None:\n                mask_slice = C.ops.slice(mask, time_axis, i, i + 1)\n                mask_slice = squeeze(mask_slice, 1)\n                if len(outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = outputs[-1]\n                output = C.ops.element_select(mask_slice, output, prev_output)\n\n                return_states = []\n                for s, n_s in zip(states, new_states):\n                    return_states.append(\n                        C.ops.element_select(\n                            mask_slice, n_s, s))\n                new_states = return_states\n            outputs.append(output)\n            states = new_states[:len(states)]\n            i += 1\n\n    i = 1\n    # add the time_step axis back\n    final_output = expand_dims(outputs[0], 1)\n    last_output = outputs[0]\n    while i < len(outputs):\n        # add the time_step axis back\n        output_slice = expand_dims(outputs[i], 1)\n        final_output = C.splice(final_output, output_slice, axis=time_axis)\n        last_output = outputs[i]\n        i += 1\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, states\n\n\ndef rnn(step_function, inputs, initial_states,\n        go_backwards=False, mask=None, constants=None,\n        unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    global uses_learning_phase\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('CNTK Backend: the input of rnn has only rank %d '\n                         'Need at least rank 3 to run RNN.' % dims)\n\n    if _get_dynamic_axis_num(inputs) == 0 or unroll:\n        return _static_rnn(\n            step_function,\n            inputs,\n            initial_states,\n            go_backwards,\n            mask,\n            constants,\n            unroll,\n            input_length)\n\n    if constants is None:\n        constants = []\n\n    num_time_step = shape[1]\n    if num_time_step is None and not has_seq_axis(inputs):\n        num_time_step = inputs.shape[0]\n\n    initial = []\n    for s in initial_states:\n        if _get_dynamic_axis_num(s) == 0:\n            if hasattr(C, 'to_batch'):\n                initial.append(C.to_batch(s))\n            else:\n                initial.append(C.user_function(ConvertToBatch(s)))\n        else:\n            initial.append(s)\n\n    need_convert = not has_seq_axis(inputs)\n    if go_backwards and need_convert is False:\n        raise NotImplementedError('CNTK Backend: `go_backwards` is not supported with '\n                                  'variable-length sequences. Please specify a '\n                                  'static length for your sequences.')\n\n    rnn_inputs = inputs\n    if need_convert:\n        if go_backwards:\n            rnn_inputs = reverse(rnn_inputs, 1)\n\n        rnn_inputs = C.to_sequence(rnn_inputs)\n\n        rnn_constants = []\n        for constant in constants:\n            if isinstance(constant, list):\n                new_c = []\n                for c in constant:\n                    if _get_dynamic_axis_num(c) == 1:\n                        new_c.append(C.sequence.broadcast_as(c, rnn_inputs))\n                    else:\n                        new_c.append(c)\n                rnn_constants.append(new_c)\n            else:\n                if _get_dynamic_axis_num(constant) == 1:\n                    rnn_constants.append(C.sequence.broadcast_as(constant, rnn_inputs))\n                else:\n                    rnn_constants.append(constant)\n    else:\n        rnn_constants = constants\n\n    if mask is not None and not has_seq_axis(mask):\n        if go_backwards:\n            mask = reverse(mask, 1)\n        if len(int_shape(mask)) == 2:\n            mask = expand_dims(mask)\n        mask = C.to_sequence_like(mask, rnn_inputs)\n\n    states = tuple(initial)\n\n    with C.default_options(axis_offset=1):\n        def _recurrence(x, states, m):\n            # create place holder\n            place_holders = [C.placeholder(dynamic_axes=x.dynamic_axes) for _ in states]\n            past_values = []\n            for s, p in zip(states, place_holders):\n                past_values.append(C.sequence.past_value(p, s))\n            new_output, new_states = step_function(\n                x, tuple(past_values) + tuple(rnn_constants))\n\n            if getattr(new_output, '_uses_learning_phase', False):\n                global uses_learning_phase\n                uses_learning_phase = True\n\n            if m is not None:\n                new_states = [C.element_select(m, n, s) for n, s in zip(new_states, past_values)]\n            n_s = []\n            for o, p in zip(new_states, place_holders):\n                n_s.append(o.replace_placeholders({p: o.output}))\n            if len(n_s) > 0:\n                new_output = n_s[0]\n            return new_output, n_s\n\n        final_output, final_states = _recurrence(rnn_inputs, states, mask)\n        last_output = C.sequence.last(final_output)\n        last_states = [C.sequence.last(s) for s in final_states]\n\n    if need_convert:\n        final_output = C.sequence.unpack(final_output, 0, no_mask_output=True)\n        if num_time_step is not None and num_time_step is not C.FreeDimension:\n            final_output = _reshape_sequence(final_output, num_time_step)\n\n    f_stats = []\n    for l_s, i_s in zip(last_states, initial_states):\n        if _get_dynamic_axis_num(i_s) == 0 and _get_dynamic_axis_num(l_s) == 1:\n            if hasattr(C, 'unpack_batch'):\n                f_stats.append(C.unpack_batch(l_s))\n            else:\n                f_stats.append(C.user_function(ConvertToStatic(l_s, batch_size=i_s.shape[0])))\n        else:\n            f_stats.append(l_s)\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, f_stats\n\n\ndef has_seq_axis(x):\n    return hasattr(x, 'dynamic_axes') and len(x.dynamic_axes) > 1\n\n\ndef l2_normalize(x, axis=None):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    norm = C.sqrt(C.reduce_sum(C.square(x), axis=axis[0]))\n    return x / norm\n\n\ndef hard_sigmoid(x):\n    x = (0.2 * x) + 0.5\n    x = C.clip(x, 0.0, 1.0)\n    return x\n\n\ndef conv1d(x, kernel, strides=1, padding='valid',\n           data_format=None, dilation_rate=1):\n    data_format = normalize_data_format(data_format)\n\n    if padding == 'causal':\n        # causal (dilated) convolution:\n        left_pad = dilation_rate * (kernel.shape[0] - 1)\n        x = temporal_padding(x, (left_pad, 0))\n        padding = 'valid'\n\n    if data_format == 'channels_last':\n        x = C.swapaxes(x, 0, 1)\n        kernel = C.swapaxes(kernel, 0, 2)\n\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != 1:\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to 1. You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(\n        kernel,\n        x,\n        strides=strides,\n        auto_padding=[False, padding],\n        dilation=dilation_rate)\n\n    if data_format == 'channels_last':\n        x = C.swapaxes(x, 0, 1)\n    return x\n\n\ndef conv2d(x, kernel, strides=(1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != (1, 1):\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to (1, 1). '\n                         'You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(kernel,\n                      x,\n                      strides,\n                      auto_padding=[False, padding, padding],\n                      dilation=dilation_rate)\n\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n                     padding='valid', data_format=None, dilation_rate=1):\n    data_format = normalize_data_format(data_format)\n    if isinstance(strides, int):\n        strides = (strides,)\n    if isinstance(dilation_rate, int):\n        dilation_rate = (dilation_rate,)\n\n    if data_format == 'channels_last':\n        spatial_start_dim = 2\n    else:\n        spatial_start_dim = 3\n    x = expand_dims(x, spatial_start_dim)\n    depthwise_kernel = expand_dims(depthwise_kernel, 1)\n    pointwise_kernel = expand_dims(pointwise_kernel, 1)\n    strides = (1,) + strides + (1,)\n    dilation_rate = (1,) + dilation_rate\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dilation_rate == (1, 1):\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    x = _postprocess_conv2d_output(x, data_format)\n    return squeeze(x, spatial_start_dim)\n\n\ndef separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=dilation_rate[0],\n                          auto_padding=[False, padding, padding])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n                     data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    padding = _preprocess_border_mode(padding)\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=dilation_rate[0],\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != (1, 1, 1):\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to (1, 1, 1). '\n                         'You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(\n        kernel,\n        x,\n        strides,\n        auto_padding=[False, padding, padding, padding],\n        dilation=dilation_rate)\n\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n                     padding='valid', data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = (1,) + strides\n    # cntk output_shape does not include batch axis\n    output_shape = output_shape[1:]\n    # in keras2, need handle output shape in different format\n    if data_format == 'channels_last':\n        output_shape = transpose_shape(output_shape, 'channels_first',\n                                       spatial_axes=(0, 1, 2))\n\n    x = C.convolution_transpose(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding,\n            padding],\n        output_shape=output_shape)\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef pool2d(x, pool_size, strides=(1, 1),\n           padding='valid', data_format=None,\n           pool_mode='max'):\n    data_format = normalize_data_format(data_format)\n\n    padding = _preprocess_border_mode(padding)\n    strides = strides\n    pool_size = pool_size\n    x = _preprocess_conv2d_input(x, data_format)\n    if pool_mode == 'max':\n        x = C.pooling(\n            x,\n            C.MAX_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    elif pool_mode == 'avg':\n        x = C.pooling(\n            x,\n            C.AVG_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    else:\n        raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n           data_format=None, pool_mode='max'):\n    data_format = normalize_data_format(data_format)\n\n    padding = _preprocess_border_mode(padding)\n\n    x = _preprocess_conv3d_input(x, data_format)\n\n    if pool_mode == 'max':\n        x = C.pooling(\n            x,\n            C.MAX_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    elif pool_mode == 'avg':\n        x = C.pooling(\n            x,\n            C.AVG_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    else:\n        raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef relu(x, alpha=0., max_value=None):\n    if alpha != 0.:\n        negative_part = C.relu(-x)\n    x = C.relu(x)\n    if max_value is not None:\n        x = C.clip(x, 0.0, max_value)\n    if alpha != 0.:\n        x -= alpha * negative_part\n    return x\n\n\ndef dropout(x, level, noise_shape=None, seed=None):\n    if level < 0. or level >= 1:\n        raise ValueError('CNTK Backend: Invalid dropout level %s, '\n                         'must be in interval [0, 1].' % level)\n    return C.dropout(x, level)\n\n\ndef batch_flatten(x):\n    # cntk's batch axis is not in shape,\n    # so just flatten all the dim in x.shape\n    dim = np.prod(x.shape)\n    x = C.reshape(x, (-1,))\n    x._keras_shape = (None, dim)\n    return x\n\n\ndef softmax(x, axis=-1):\n    return C.softmax(x, axis=axis)\n\n\ndef softplus(x):\n    return C.softplus(x)\n\n\ndef softsign(x):\n    return x / (1 + C.abs(x))\n\n\ndef categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    # Here, unlike other backends, the tensors lack a batch dimension:\n    axis_without_batch = -1 if axis == -1 else axis - 1\n    output_dimensions = list(range(len(output.shape)))\n    if axis_without_batch != -1 and axis_without_batch not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis_without_batch),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(output.shape))))\n    # If the channels are not in the last axis, move them to be there:\n    if axis_without_batch != -1 and axis_without_batch != output_dimensions[-1]:\n        permutation = output_dimensions[:axis_without_batch]\n        permutation += output_dimensions[axis_without_batch + 1:]\n        permutation += [axis_without_batch]\n        output = C.transpose(output, permutation)\n        target = C.transpose(target, permutation)\n    if from_logits:\n        result = C.cross_entropy_with_softmax(output, target)\n        # cntk's result shape is (batch, 1), while keras expect (batch, )\n        return C.reshape(result, ())\n    else:\n        # scale preds so that the class probas of each sample sum to 1\n        output /= C.reduce_sum(output, axis=-1)\n        # avoid numerical instability with epsilon clipping\n        output = C.clip(output, epsilon(), 1.0 - epsilon())\n        return -sum(target * C.log(output), axis=-1)\n\n\ndef sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    # Here, unlike other backends, the tensors lack a batch dimension:\n    axis_without_batch = -1 if axis == -1 else axis - 1\n    output_dimensions = list(range(len(output.shape)))\n    if axis_without_batch != -1 and axis_without_batch not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis_without_batch),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(output.shape))))\n    target = C.one_hot(target, output.shape[axis_without_batch],\n                       axis=axis_without_batch)\n    target = C.reshape(target, output.shape)\n    return categorical_crossentropy(target, output, from_logits, axis=axis)\n\n\nclass Function(object):\n\n    def __init__(self, inputs, outputs, updates=[], **kwargs):\n        self.placeholders = inputs\n        self.trainer = None\n        self.unrelated_updates = None\n        self.updates = updates\n        if len(updates) > 0:\n            assert len(outputs) > 0\n            self.loss = outputs[0]\n            # need group update by gradient place holder\n            u_ops = []\n            unrelated_updates = []\n            for update in updates:\n                if isinstance(update, tuple):\n                    if len(update) != 2:\n                        raise NotImplementedError\n                    else:\n                        u = C.assign(update[0], update[1])\n                else:\n                    u = update\n\n                if len(u.arguments) == 0:\n                    u_ops.append(u)\n                else:\n                    unrelated_updates.append(u)\n\n            update_func = C.combine([u.output for u in u_ops])\n\n            grads = update_func.find_all_with_name('keras_grad_placeholder')\n\n            u_list = []\n            p_list = []\n            for g in grads:\n                if g in grad_parameter_dict:\n                    p_list.append(grad_parameter_dict[g])\n                    u_list.append(g)\n                else:\n                    raise ValueError(\n                        'CNTK backend: when constructing trainer, '\n                        'found gradient node `%s` which is not '\n                        'related to any parameters in the model. '\n                        'Please double check how the gradient node '\n                        'is constructed.' % g)\n\n            if len(u_list) > 0:\n                learner = C.cntk_py.universal_learner(p_list, u_list, update_func)\n\n                criterion = (\n                    outputs[0],\n                    outputs[1]) if len(outputs) > 1 else (\n                    outputs[0],\n                )\n                self.trainer = C.trainer.Trainer(\n                    outputs[0], criterion, [learner])\n                self.trainer_output = tuple([f.output for f in criterion])\n            elif len(u_ops) > 0:\n                unrelated_updates.extend(u_ops)\n\n            if len(unrelated_updates) > 0:\n                self.unrelated_updates = C.combine([_.output for _ in unrelated_updates])\n\n        if self.trainer is None:\n            self.metrics_outputs = [f.output for f in outputs]\n            self.metrics_func = C.combine(self.metrics_outputs)\n        # cntk only could handle loss and 1 metric in trainer, for metrics more\n        # than 2, need manual eval\n        elif len(outputs) > 2:\n            self.metrics_outputs = [f.output for f in outputs[2:]]\n            self.metrics_func = C.combine(self.metrics_outputs)\n        else:\n            self.metrics_func = None\n\n    @staticmethod\n    def _is_input_shape_compatible(input, placeholder):\n        if hasattr(input, 'shape') and hasattr(placeholder, 'shape'):\n            num_dynamic = get_num_dynamic_axis(placeholder)\n            input_shape = input.shape[num_dynamic:]\n            placeholder_shape = placeholder.shape\n            for i, p in zip(input_shape, placeholder_shape):\n                if i != p and p != C.InferredDimension and p != C.FreeDimension:\n                    return False\n        return True\n\n    def __call__(self, inputs):\n        global _LEARNING_PHASE_PLACEHOLDER\n        global _LEARNING_PHASE\n        assert isinstance(inputs, (list, tuple))\n        feed_dict = {}\n        for tensor, value in zip(self.placeholders, inputs):\n            # cntk only support calculate on float, do auto cast here\n            if (hasattr(value, 'dtype') and\n               value.dtype != np.float32 and\n               value.dtype != np.float64):\n                value = value.astype(np.float32)\n\n            if tensor == _LEARNING_PHASE_PLACEHOLDER:\n                _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(value)\n            else:\n                # in current version cntk can't support input with variable\n                # length. Will support it in next release.\n                if not self._is_input_shape_compatible(value, tensor):\n                    raise ValueError('CNTK backend: The placeholder has been resolved '\n                                     'to shape `%s`, but input shape is `%s`. Currently '\n                                     'CNTK can not take variable length inputs. Please '\n                                     'pass inputs that have a static shape.'\n                                     % (str(tensor.shape), str(value.shape)))\n            feed_dict[tensor] = value\n\n        updated = []\n        if self.trainer is not None:\n            input_dict = {}\n            for argument in self.loss.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError(\n                        'CNTK backend: argument %s is not found in inputs. '\n                        'Please double check the model and inputs in '\n                        '`train_function`.' % argument.name)\n\n            result = self.trainer.train_minibatch(\n                input_dict, self.trainer_output)\n\n            assert(len(result) == 2)\n            outputs = result[1]\n            for o in self.trainer_output:\n                updated.append(outputs[o])\n\n        if self.metrics_func is not None:\n            input_dict = {}\n            for argument in self.metrics_func.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError('CNTK backend: metrics argument %s '\n                                     'is not found in inputs. Please double '\n                                     'check the model and inputs.' % argument.name)\n            # Some ops (like dropout) won't be applied during \"eval\" in cntk.\n            # They only evaluated in training phase. To make it work, call\n            # \"forward\" method to let cntk know we want to evaluate them.from\n            # But the assign ops won't be executed under this mode, that's why\n            # we need this check.\n            if (self.unrelated_updates is None and\n                    (_LEARNING_PHASE_PLACEHOLDER.value == 1.0 or _LEARNING_PHASE == 1)):\n                _, output_values = self.metrics_func.forward(\n                    input_dict,\n                    self.metrics_func.outputs,\n                    (self.metrics_func.outputs[0],),\n                    as_numpy=False)\n            else:\n                output_values = self.metrics_func.eval(input_dict, as_numpy=False)\n            if isinstance(output_values, dict):\n                for o in self.metrics_outputs:\n                    value = output_values[o]\n                    v = value.asarray()\n                    updated.append(v)\n            else:\n                v = output_values.asarray()\n                for o in self.metrics_outputs:\n                    updated.append(v)\n\n        if self.unrelated_updates is not None:\n            input_dict = {}\n            for argument in self.unrelated_updates.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError(\n                        'CNTK backend: assign ops argument %s '\n                        'is not found in inputs. Please double '\n                        'check the model and inputs.' % argument.name)\n            self.unrelated_updates.eval(input_dict, as_numpy=False)\n        return updated\n\n\ndef function(inputs, outputs, updates=[], **kwargs):\n    return Function(inputs, outputs, updates=updates, **kwargs)\n\n\ndef temporal_padding(x, padding=(1, 1)):\n    assert len(padding) == 2\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 3 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, [padding], 'channels_last', num_dynamic_axis)\n\n\ndef _padding(x, pattern, axis):  # pragma: no cover\n    base_shape = x.shape\n    if b_any([dim < 0 for dim in base_shape]):\n        raise ValueError('CNTK Backend: padding input tensor with '\n                         'shape `%s` contains non-specified dimension, '\n                         'which is not supported. Please give fixed '\n                         'dimension to enable padding.' % base_shape)\n    if pattern[0] > 0:\n        prefix_shape = list(base_shape)\n        prefix_shape[axis] = pattern[0]\n        prefix_shape = tuple(prefix_shape)\n        x = C.splice(C.constant(value=0, shape=prefix_shape), x, axis=axis)\n        base_shape = x.shape\n    if pattern[1] > 0:\n        postfix_shape = list(base_shape)\n        postfix_shape[axis] = pattern[1]\n        postfix_shape = tuple(postfix_shape)\n        x = C.splice(x, C.constant(value=0, shape=postfix_shape), axis=axis)\n    return x\n\n\ndef pad(x, pad_info, data_format, num_dynamic_axis):\n    if hasattr(C, 'pad'):\n        pattern = [list(p) for p in pad_info]\n        if data_format == 'channels_first':\n            pattern = [[0, 0]] + pattern\n        else:\n            pattern = pattern + [[0, 0]]\n        if num_dynamic_axis == 0:\n            pattern = [[0, 0]] + pattern\n        return C.pad(x, pattern=pattern)\n    else:  # pragma: no cover\n        for (a, p) in enumerate(pad_info):\n            x = _padding(x, p,\n                         a + (1 if num_dynamic_axis == 0 else 0) +\n                         (1 if data_format == 'channels_first' else 0))\n        return x\n\n\ndef spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n    assert len(padding) == 2\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    data_format = normalize_data_format(data_format)\n\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 4 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, padding, data_format, num_dynamic_axis)\n\n\ndef spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n    assert len(padding) == 3\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    assert len(padding[2]) == 2\n    data_format = normalize_data_format(data_format)\n\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 5 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, padding, data_format, num_dynamic_axis)\n\n\ndef one_hot(indices, num_classes):\n    return C.one_hot(indices, num_classes)\n\n\ndef get_value(x):\n    if isinstance(\n            x,\n            C.variables.Parameter) or isinstance(\n            x,\n            C.variables.Constant):\n        return x.value\n    else:\n        return eval(x)\n\n\ndef batch_get_value(xs):\n    result = []\n    for x in xs:\n        if (isinstance(x, C.variables.Parameter) or\n           isinstance(x, C.variables.Constant)):\n            result.append(x.value)\n        else:\n            result.append(eval(x))\n    return result\n\n\ndef set_value(x, value):\n    if (isinstance(x, C.variables.Parameter) or\n       isinstance(x, C.variables.Constant)):\n        if isinstance(value, (float, int)):\n            value = np.full(x.shape, value, dtype=floatx())\n        x.value = value\n    else:\n        raise NotImplementedError\n\n\ndef print_tensor(x, message=''):\n    return C.user_function(\n        LambdaFunc(x,\n                   when=lambda x: True,\n                   execute=lambda x: print(message)))\n\n\ndef batch_set_value(tuples):\n    for t in tuples:\n        x = t[0]\n        value = t[1]\n        if isinstance(value, np.ndarray) is False:\n            value = np.asarray(value)\n        if isinstance(x, C.variables.Parameter):\n            x.value = value\n        else:\n            raise NotImplementedError\n\n\ndef stop_gradient(variables):\n    if isinstance(variables, (list, tuple)):\n        return map(C.stop_gradient, variables)\n    else:\n        return C.stop_gradient(variables)\n\n\ndef switch(condition, then_expression, else_expression):\n    ndim_cond = ndim(condition)\n    ndim_expr = ndim(then_expression)\n    if ndim_cond > ndim_expr:\n        raise ValueError('Rank of condition should be less'\n                         ' than or equal to rank of then and'\n                         ' else expressions. ndim(condition)=' +\n                         str(ndim_cond) + ', ndim(then_expression)'\n                         '=' + str(ndim_expr))\n    elif ndim_cond < ndim_expr:\n        shape_expr = int_shape(then_expression)\n        ndim_diff = ndim_expr - ndim_cond\n        for i in range(ndim_diff):\n            condition = expand_dims(condition)\n            condition = tile(condition, shape_expr[ndim_cond + i])\n    return C.element_select(condition,\n                            then_expression,\n                            else_expression)\n\n\ndef elu(x, alpha=1.):\n    res = C.elu(x)\n    if alpha == 1:\n        return res\n    else:\n        return C.element_select(C.greater(x, 0), res, alpha * res)\n\n\ndef in_top_k(predictions, targets, k):\n    _targets = C.one_hot(targets, predictions.shape[-1])\n    result = C.classification_error(predictions, _targets, topN=k)\n    return 1 - C.reshape(result, shape=())\n\n\ndef conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = (1,) + strides\n    # cntk output_shape does not include batch axis\n    output_shape = output_shape[1:]\n    # in keras2, need handle output shape in different format\n    if data_format == 'channels_last':\n        output_shape = transpose_shape(output_shape, 'channels_first',\n                                       spatial_axes=(0, 1))\n\n    x = C.convolution_transpose(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding],\n        output_shape=output_shape)\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef identity(x, name=None):\n    if name is None:\n        name = '%s_alias' % x.name\n    return C.alias(x, name=name)\n\n\ndef _preprocess_conv2d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, rows, cols)\n        # TF input shape: (samples, rows, cols, input_depth)\n        x = C.transpose(x, (2, 0, 1))\n    return x\n\n\ndef _preprocess_conv2d_kernel(kernel, data_format):\n    # As of Keras 2.0.0, all kernels are normalized\n    # on the format `(rows, cols, input_depth, depth)`,\n    # independently of `data_format`.\n    # CNTK expects `(depth, input_depth, rows, cols)`.\n    kernel = C.transpose(kernel, (3, 2, 0, 1))\n    return kernel\n\n\ndef _preprocess_border_mode(padding):\n    if padding == 'same':\n        padding = True\n    elif padding == 'valid':\n        padding = False\n    else:\n        raise ValueError('Invalid border mode: ' + str(padding))\n    return padding\n\n\ndef _postprocess_conv2d_output(x, data_format):\n    if data_format == 'channels_last':\n        x = C.transpose(x, (1, 2, 0))\n    return x\n\n\ndef _preprocess_conv3d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, conv_dim1, conv_dim2, conv_dim3)\n        # TF input shape: (samples, conv_dim1, conv_dim2, conv_dim3,\n        # input_depth)\n        x = C.transpose(x, (3, 0, 1, 2))\n    return x\n\n\ndef _preprocess_conv3d_kernel(kernel, dim_ordering):\n    kernel = C.transpose(kernel, (4, 3, 0, 1, 2))\n    return kernel\n\n\ndef _postprocess_conv3d_output(x, dim_ordering):\n    if dim_ordering == 'channels_last':\n        x = C.transpose(x, (1, 2, 3, 0))\n    return x\n\n\ndef _get_dynamic_axis_num(x):\n    if hasattr(x, 'dynamic_axes'):\n        return len(x.dynamic_axes)\n    else:\n        return 0\n\n\ndef _contain_seqence_axis(x):\n    if _get_dynamic_axis_num(x) > 1:\n        return x.dynamic_axes[1] == C.Axis.default_dynamic_axis()\n    else:\n        return False\n\n\ndef get_num_dynamic_axis(x):\n    return _get_dynamic_axis_num(x)\n\n\ndef _reduce_on_axis(x, axis, reduce_fun_name):\n    if isinstance(axis, list):\n        for a in axis:\n            if isinstance(a, C.Axis) \\\n                    and a != C.Axis.default_batch_axis() \\\n                    and hasattr(C.sequence, reduce_fun_name):\n                x = getattr(C.sequence, reduce_fun_name)(x, a)\n            else:\n                x = getattr(C, reduce_fun_name)(x, a)\n    else:\n        x = getattr(C, reduce_fun_name)(x, axis)\n    return x\n\n\ndef _reshape_sequence(x, time_step):\n    tmp_shape = list(int_shape(x))\n    tmp_shape[1] = time_step\n    return reshape(x, tmp_shape)\n\n\ndef local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    stride = strides[0]\n    kernel_shape = int_shape(kernel)\n    output_length, feature_dim, filters = kernel_shape\n\n    xs = []\n    for i in range(output_length):\n        slice_length = py_slice(i * stride,\n                                i * stride + kernel_size[0])\n        xs.append(reshape(inputs[:, slice_length, :],\n                          (-1, 1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=1)\n    # transpose kernel to output_filters first, to apply broadcast\n    weight = permute_dimensions(kernel, (2, 0, 1))\n    # Shape: (batch, filters, output_length, input_length * kernel_size)\n    output = x_aggregate * weight\n    # Shape: (batch, filters, output_length)\n    output = sum(output, axis=3)\n    # Shape: (batch, output_length, filters)\n    return permute_dimensions(output, (0, 2, 1))\n\n\ndef local_conv2d(inputs,\n                 kernel,\n                 kernel_size,\n                 strides,\n                 output_shape,\n                 data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    stride_row, stride_col = strides\n    output_row, output_col = output_shape\n    kernel_shape = int_shape(kernel)\n    _, feature_dim, filters = kernel_shape\n    xs = []\n\n    for i in range(output_row):\n        for j in range(output_col):\n            slice_row = py_slice(i * stride_row,\n                                 i * stride_row + kernel_size[0])\n            slice_col = py_slice(j * stride_col,\n                                 j * stride_col + kernel_size[1])\n            if data_format == 'channels_first':\n                xs.append(reshape(inputs[:, :, slice_row, slice_col],\n                                  (-1, 1, feature_dim)))\n            else:\n                xs.append(reshape(inputs[:, slice_row, slice_col, :],\n                                  (-1, 1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=1)\n    # transpose kernel to put filters first\n    weight = permute_dimensions(kernel, (2, 0, 1))\n    # shape: batch, filters, output_length, input_length * kernel_size\n    output = x_aggregate * weight\n    # shape: batch, filters, output_length\n    output = sum(output, axis=3)\n    # shape: batch, filters, row, col\n    output = reshape(output,\n                     (-1, filters, output_row, output_col))\n\n    if data_format == 'channels_last':\n        # shape: batch, row, col, filters\n        output = permute_dimensions(output, (0, 2, 3, 1))\n\n    return output\n\n\ndef reverse(x, axes):\n    if isinstance(axes, int):\n        axes = [axes]\n    cntk_axes = _normalize_axis(axes, x)\n    begin_index = [0 for _ in cntk_axes]\n    end_index = [0 for _ in cntk_axes]\n    strides = [-1 for _ in cntk_axes]\n    return C.slice(x, cntk_axes, begin_index, end_index, strides)\n\n\ndef slice(x, start, size):\n    raise NotImplementedError\n\n\ndef _reshape_batch(x, shape):\n    # there is a bug in cntk 2.1's unpack_batch implementation\n    if hasattr(C, 'unpack_batch') and _get_cntk_version() >= 2.2:\n        const_a = C.unpack_batch(x)\n        const_a = C.reshape(const_a, shape)\n        return C.to_batch(const_a)\n    else:\n        return C.user_function(ReshapeBatch(x, shape[1:]))\n\n\ndef _get_cntk_version():\n    version = C.__version__\n    if version.endswith('+'):\n        version = version[:-1]\n    # for hot fix, ignore all the . except the first one.\n    if len(version) > 2 and version[1] == '.':\n        version = version[:2] + version[2:].replace('.', '')\n    try:\n        return float(version)\n    except:\n        warnings.warn(\n            'CNTK backend warning: CNTK version not detected. '\n            'Will using CNTK 2.0 GA as default.')\n        return float(2.0)\n\n\nclass ReshapeBatch(C.ops.functions.UserFunction):\n    def __init__(self, input, shape, name='reshape_with_batch'):\n        super(ReshapeBatch, self).__init__([input], as_numpy=False, name=name)\n        self.from_shape = input.shape\n        self.target_shape = shape\n\n    def infer_outputs(self):\n        batch_axis = C.Axis.default_batch_axis()\n        return [\n            C.output_variable(\n                self.target_shape,\n                self.inputs[0].dtype,\n                [batch_axis])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        num_element = arguments.shape()[0] * np.prod(np.asarray(self.from_shape))\n        num_static_element = np.prod(np.asarray(self.target_shape))\n        num_batch = int(num_element / num_static_element)\n        result = arguments.data().as_shape((num_batch,) + self.target_shape)\n        return None, C.cntk_py.Value(result)\n\n    def backward(self, state, root_gradients):\n        grad_array_view = root_gradients.data()\n        num_element = root_gradients.shape()[0] * np.prod(np.asarray(self.target_shape))\n        num_static_element = np.prod(np.asarray(self.from_shape))\n        num_old_batch = int(num_element / num_static_element)\n        return C.cntk_py.Value(\n            grad_array_view.as_shape(\n                (num_old_batch,) + self.from_shape))\n\n\nclass ConvertToBatch(C.ops.functions.UserFunction):\n    \"\"\"Converts input first axis to CNTK batch axis.\n\n    We may introduce this operation in CNTK native\n    implementation later.\n\n    # Arguments\n        inputs: a cntk variable (parameter/constant)\n        name: name of this node\n    \"\"\"\n\n    def __init__(self, input, name='convert_to_batch'):\n        super(ConvertToBatch, self).__init__([input], as_numpy=False, name=name)\n\n    def infer_outputs(self):\n        batch_axis = C.Axis.default_batch_axis()\n        return [\n            C.output_variable(\n                self.inputs[0].shape[1:],\n                self.inputs[0].dtype,\n                [batch_axis])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        return None, C.cntk_py.Value(arguments.data())\n\n    def backward(self, state, root_gradients):\n        return C.cntk_py.Value(root_gradients.data())\n\n\nclass ConvertToStatic(C.ops.functions.UserFunction):\n    \"\"\"Converts input first axis to CNTK static axis.\n\n    We may introduce this operation in CNTK native\n    implementation later.\n\n    # Arguments\n        inputs: a cntk tensor which has batch axis\n        batch_size: size of batch axis.\n        name: name of this node.\n    \"\"\"\n\n    def __init__(self, input, batch_size, name='convert_to_static'):\n        super(ConvertToStatic, self).__init__([input], as_numpy=False, name=name)\n        self.target_shape = (batch_size,) + input.shape\n\n    def infer_outputs(self):\n        return [\n            C.output_variable(\n                self.target_shape,\n                self.inputs[0].dtype,\n                [])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        return None, C.cntk_py.Value(arguments.data())\n\n    def backward(self, state, root_gradients):\n        return C.cntk_py.Value(root_gradients.data())\n\n\nclass LambdaFunc(C.ops.functions.UserFunction):\n    def __init__(self,\n                 arg,\n                 when=lambda arg: True,\n                 execute=lambda arg: print(arg),\n                 name=''):\n        self.when = when\n        self.execute = execute\n\n        super(LambdaFunc, self).__init__([arg], name=name)\n\n    def infer_outputs(self):\n        return [\n            C.output_variable(\n                self.inputs[0].shape,\n                self.inputs[0].dtype,\n                self.inputs[0].dynamic_axes)]\n\n    def forward(self, argument, device=None, outputs_to_retain=None):\n        if self.when(argument):\n            self.execute(argument)\n\n        return None, argument\n\n    def backward(self, state, root_gradients):\n        return root_gradients\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cntk as C\nimport numpy as np\nfrom .common import floatx\nfrom .common import epsilon\nfrom .common import image_data_format\nfrom .common import normalize_data_format\nfrom ..utils.generic_utils import transpose_shape\nfrom collections import defaultdict\nfrom contextlib import contextmanager\nimport warnings\n\n\nC.set_global_option('align_axis', 1)\n\nb_any = any\npy_slice = slice\n\n\ndev = C.device.use_default_device()\nif dev.type() == 0:\n    warnings.warn(\n        'CNTK backend warning: GPU is not detected. '\n        'CNTK\\'s CPU version is not fully optimized,'\n        'please run with GPU to get better performance.')\n\n# A learning phase is a bool tensor used to run Keras models in\n# either train mode (learning_phase == 1) or test mode (learning_phase == 0).\n# LEARNING_PHASE_PLACEHOLDER is the placeholder for dynamic learning phase\n_LEARNING_PHASE_PLACEHOLDER = C.constant(shape=(), dtype=np.float32, value=1.0, name='_keras_learning_phase')\n# static learning phase flag, if it is not 0 or 1, we will go with dynamic learning phase tensor.\n_LEARNING_PHASE = -1\n_UID_PREFIXES = defaultdict(int)\n\n# cntk doesn't support gradient as symbolic op, to hook up with keras model,\n# we will create gradient as a constant placeholder, here use this global\n# map to keep the mapping from grad placeholder to parameter\ngrad_parameter_dict = {}\n\nNAME_SCOPE_STACK = []\n\n\n@contextmanager\ndef name_scope(name):\n    global NAME_SCOPE_STACK\n    NAME_SCOPE_STACK.append(name)\n    yield\n    NAME_SCOPE_STACK.pop()\n\n\ndef get_uid(prefix=''):\n    _UID_PREFIXES[prefix] += 1\n    return _UID_PREFIXES[prefix]\n\n\ndef learning_phase():\n    # If _LEARNING_PHASE is not 0 or 1, return dynamic learning phase tensor\n    return _LEARNING_PHASE if _LEARNING_PHASE in {0, 1} else _LEARNING_PHASE_PLACEHOLDER\n\n\ndef set_learning_phase(value):\n    global _LEARNING_PHASE\n    if value not in {0, 1}:\n        raise ValueError('CNTK Backend: Set learning phase '\n                         'with value %s is not supported, '\n                         'expected 0 or 1.' % value)\n    _LEARNING_PHASE = value\n\n\ndef clear_session():\n    \"\"\"Reset learning phase flag for cntk backend.\n    \"\"\"\n    global _LEARNING_PHASE\n    global _LEARNING_PHASE_PLACEHOLDER\n    _LEARNING_PHASE = -1\n    _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(1.0)\n\n\ndef in_train_phase(x, alt, training=None):\n    global _LEARNING_PHASE\n    if training is None:\n        training = learning_phase()\n        uses_learning_phase = True\n    else:\n        uses_learning_phase = False\n\n    # CNTK currently don't support cond op, so here we use\n    # element_select approach as workaround. It may have\n    # perf issue, will resolve it later with cntk cond op.\n    if callable(x) and isinstance(x, C.cntk_py.Function) is False:\n        x = x()\n    if callable(alt) and isinstance(alt, C.cntk_py.Function) is False:\n        alt = alt()\n\n    if training is True:\n        x._uses_learning_phase = uses_learning_phase\n        return x\n    else:\n        # if _LEARNING_PHASE is static\n        if isinstance(training, int) or isinstance(training, bool):\n            result = x if training == 1 or training is True else alt\n        else:\n            result = C.element_select(training, x, alt)\n        result._uses_learning_phase = uses_learning_phase\n        return result\n\n\ndef in_test_phase(x, alt, training=None):\n    return in_train_phase(alt, x, training=training)\n\n\ndef _convert_string_dtype(dtype):\n    if dtype == 'float32':\n        return np.float32\n    elif dtype == 'float64':\n        return np.float64\n    elif dtype == 'float16':\n        return np.float16\n    else:\n        # cntk only running with float,\n        # try to cast to float to run the model\n        return np.float32\n\n\ndef _convert_dtype_string(dtype):\n    if dtype == np.float32:\n        return 'float32'\n    elif dtype == np.float64:\n        return 'float64'\n    elif dtype == np.float16:\n        return 'float16'\n    else:\n        raise ValueError('CNTK Backend: Unsupported dtype: %s. '\n                         'CNTK only supports float32, float64, and '\n                         'float16.' % dtype)\n\n\ndef variable(value, dtype=None, name=None, constraint=None):\n    \"\"\"Instantiates a variable and returns it.\n\n    # Arguments\n        value: Numpy array, initial value of the tensor.\n        dtype: Tensor type.\n        name: Optional name string for the tensor.\n        constraint: Optional projection function to be\n            applied to the variable after an optimizer update.\n\n    # Returns\n        A variable instance (with Keras metadata included).\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n\n    if name is None:\n        name = ''\n\n    if isinstance(\n            value,\n            C.variables.Constant) or isinstance(\n            value,\n            C.variables.Parameter):\n        value = value.value\n\n    # we don't support init parameter with symbolic op, so eval it first as\n    # workaround\n    if isinstance(value, C.cntk_py.Function):\n        value = eval(value)\n\n    shape = value.shape if hasattr(value, 'shape') else ()\n    if hasattr(value, 'dtype') and value.dtype != dtype and len(shape) > 0:\n        value = value.astype(dtype)\n\n    # TODO: remove the conversion when cntk supports int32, int64\n    # https://docs.microsoft.com/en-us/python/api/cntk.variables.parameter\n    dtype = 'float32' if 'int' in str(dtype) else dtype\n\n    v = C.parameter(shape=shape,\n                    init=value,\n                    dtype=dtype,\n                    name=_prepare_name(name, 'variable'))\n    v._keras_shape = v.shape\n    v._uses_learning_phase = False\n    v.constraint = constraint\n    return v\n\n\ndef bias_add(x, bias, data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    dims = len(x.shape)\n    if dims > 0 and x.shape[0] == C.InferredDimension:\n        dims -= 1\n\n    bias_dims = len(bias.shape)\n    if bias_dims != 1 and bias_dims != dims:\n        raise ValueError('Unexpected bias dimensions %d, '\n                         'expected 1 or %d dimensions' % (bias_dims, dims))\n\n    if dims == 4:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1, 1, 1)\n            else:\n                shape = (bias.shape[3],) + bias.shape[:3]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, 1, 1, bias.shape[0])\n            else:\n                shape = bias.shape\n    elif dims == 3:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1, 1)\n            else:\n                shape = (bias.shape[2],) + bias.shape[:2]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, 1, bias.shape[0])\n            else:\n                shape = bias.shape\n    elif dims == 2:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1)\n            else:\n                shape = (bias.shape[1],) + bias.shape[:1]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, bias.shape[0])\n            else:\n                shape = bias.shape\n    else:\n        shape = bias.shape\n    return x + reshape(bias, shape)\n\n\ndef eval(x):\n    if isinstance(x, C.cntk_py.Function):\n        return x.eval()\n    elif isinstance(x, C.variables.Constant) or isinstance(x, C.variables.Parameter):\n        return x.value\n    else:\n        raise ValueError('CNTK Backend: `eval` method on '\n                         '`%s` type is not supported. '\n                         'CNTK only supports `eval` with '\n                         '`Function`, `Constant` or '\n                         '`Parameter`.' % type(x))\n\n\ndef placeholder(\n        shape=None,\n        ndim=None,\n        dtype=None,\n        sparse=False,\n        name=None,\n        dynamic_axis_num=1):\n    if dtype is None:\n        dtype = floatx()\n    if not shape:\n        if ndim:\n            shape = tuple([None for _ in range(ndim)])\n\n    dynamic_dimension = C.FreeDimension if _get_cntk_version() >= 2.2 else C.InferredDimension\n    cntk_shape = [dynamic_dimension if s is None else s for s in shape]\n    cntk_shape = tuple(cntk_shape)\n\n    if dynamic_axis_num > len(cntk_shape):\n        raise ValueError('CNTK backend: creating placeholder with '\n                         '%d dimension is not supported, at least '\n                         '%d dimensions are needed.'\n                         % (len(cntk_shape), dynamic_axis_num))\n\n    if name is None:\n        name = ''\n\n    cntk_shape = cntk_shape[dynamic_axis_num:]\n\n    x = C.input(\n        shape=cntk_shape,\n        dtype=_convert_string_dtype(dtype),\n        is_sparse=sparse,\n        name=name)\n    x._keras_shape = shape\n    x._uses_learning_phase = False\n    x._cntk_placeholder = True\n    return x\n\n\ndef is_placeholder(x):\n    \"\"\"Returns whether `x` is a placeholder.\n\n    # Arguments\n        x: A candidate placeholder.\n\n    # Returns\n        Boolean.\n    \"\"\"\n    return hasattr(x, '_cntk_placeholder') and x._cntk_placeholder\n\n\ndef is_keras_tensor(x):\n    if not is_tensor(x):\n        raise ValueError('Unexpectedly found an instance of type `' +\n                         str(type(x)) + '`. '\n                         'Expected a symbolic tensor instance.')\n    return hasattr(x, '_keras_history')\n\n\ndef is_tensor(x):\n    return isinstance(x, (C.variables.Constant,\n                          C.variables.Variable,\n                          C.variables.Parameter,\n                          C.ops.functions.Function))\n\n\ndef shape(x):\n    shape = list(int_shape(x))\n    num_dynamic = _get_dynamic_axis_num(x)\n    non_dyn_shape = []\n    for i in range(len(x.shape)):\n        if shape[i + num_dynamic] is None:\n            non_dyn_shape.append(x.shape[i])\n        else:\n            non_dyn_shape.append(shape[i + num_dynamic])\n    return shape[:num_dynamic] + non_dyn_shape\n\n\ndef is_sparse(tensor):\n    return tensor.is_sparse\n\n\ndef int_shape(x):\n    if hasattr(x, '_keras_shape'):\n        return x._keras_shape\n\n    shape = x.shape\n    if hasattr(x, 'dynamic_axes'):\n        dynamic_shape = [None for a in x.dynamic_axes]\n        shape = tuple(dynamic_shape) + shape\n    return shape\n\n\ndef ndim(x):\n    shape = int_shape(x)\n    return len(shape)\n\n\ndef _prepare_name(name, default):\n    prefix = '_'.join(NAME_SCOPE_STACK)\n    if name is None or name == '':\n        return prefix + '/' + default\n    return prefix + '/' + name\n\n\ndef constant(value, dtype=None, shape=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    if shape is None:\n        shape = ()\n    np_value = value * np.ones(shape)\n    const = C.constant(np_value,\n                       dtype=dtype,\n                       name=_prepare_name(name, 'constant'))\n    const._keras_shape = const.shape\n    const._uses_learning_phase = False\n    return const\n\n\ndef random_binomial(shape, p=0.0, dtype=None, seed=None):\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e7)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n    return C.random.bernoulli(shape=shape, dtype=dtype, mean=p, seed=seed)\n\n\ndef random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n    return C.random.uniform(shape=shape, dtype=dtype, low=minval, high=maxval, seed=seed)\n\n\ndef random_uniform_variable(shape, low, high,\n                            dtype=None, name=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    if name is None:\n        name = ''\n\n    scale = (high - low) / 2\n    p = C.parameter(\n        shape,\n        init=C.initializer.uniform(\n            scale,\n            seed=seed),\n        dtype=dtype,\n        name=name)\n    return variable(value=p.value + low + scale)\n\n\ndef random_normal_variable(\n        shape,\n        mean,\n        scale,\n        dtype=None,\n        name=None,\n        seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e7)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    if name is None:\n        name = ''\n\n    p = C.parameter(\n        shape=shape,\n        init=C.initializer.normal(\n            scale=scale,\n            seed=seed),\n        dtype=dtype,\n        name=name)\n    return variable(value=p.value + mean)\n\n\ndef random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n    return C.random.normal(shape=shape, mean=mean, scale=stddev, seed=seed, dtype=dtype)\n\n\ndef truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if seed is None:\n        seed = np.random.randint(1, 10e6)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    return C.parameter(\n        shape, init=C.initializer.truncated_normal(\n            stddev, seed=seed), dtype=dtype)\n\n\ndef dtype(x):\n    return _convert_dtype_string(x.dtype)\n\n\ndef zeros(shape, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    ctype = _convert_string_dtype(dtype)\n    return variable(value=np.zeros(shape, ctype), dtype=dtype, name=name)\n\n\ndef ones(shape, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    ctype = _convert_string_dtype(dtype)\n    return variable(value=np.ones(shape, ctype), dtype=dtype, name=name)\n\n\ndef eye(size, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    return variable(np.eye(size), dtype, name)\n\n\ndef zeros_like(x, dtype=None, name=None):\n    return x * 0\n\n\ndef ones_like(x, dtype=None, name=None):\n    return zeros_like(x) + 1\n\n\ndef count_params(x):\n    for _ in x.shape:\n        if _ == C.InferredDimension or _ == C.FreeDimension:\n            raise ValueError('CNTK backend: `count_params` with dynamic '\n                             'shape is not supported. Please provide '\n                             'fixed dimension instead of `None`.')\n\n    return np.prod(int_shape(x))\n\n\ndef cast(x, dtype):\n    # cntk calculate everything in float, so don't need case from bool / int\n    return x\n\n\ndef dot(x, y):\n    if len(x.shape) > 2 or len(y.shape) > 2:\n        y_shape = int_shape(y)\n        if len(y_shape) > 2:\n            permutation = [len(y_shape) - 2]\n            permutation += list(range(len(y_shape) - 2))\n            permutation += [len(y_shape) - 1]\n            y = C.transpose(y, perm=permutation)\n        return C.times(x, y, len(y_shape) - 1)\n    else:\n        return C.times(x, y)\n\n\ndef batch_dot(x, y, axes=None):\n    x_shape = int_shape(x)\n    y_shape = int_shape(y)\n\n    if isinstance(axes, int):\n        axes = (axes, axes)\n    if axes is None:\n        # behaves like tf.batch_matmul as default\n        axes = [len(x_shape) - 1, len(y_shape) - 2]\n    if b_any([isinstance(a, (list, tuple)) for a in axes]):\n        raise ValueError('Multiple target dimensions are not supported. ' +\n                         'Expected: None, int, (int, int), ' +\n                         'Provided: ' + str(axes))\n\n    if len(x_shape) == 2 and len(y_shape) == 2:\n        if axes[0] == axes[1]:\n            result = sum(x * y, axis=axes[0], keepdims=True)\n            return result if axes[0] == 1 else transpose(result)\n        else:\n            return sum(x * transpose(y), axis=axes[0], keepdims=True)\n    else:\n        if len(y_shape) == 2:\n            y = expand_dims(y)\n\n        normalized_axis = []\n        normalized_axis.append(_normalize_axis(axes[0], x)[0])\n        normalized_axis.append(_normalize_axis(axes[1], y)[0])\n        # transpose\n        i = normalized_axis[0]\n        while i < len(x.shape) - 1:\n            x = C.swapaxes(x, i, i + 1)\n            i += 1\n        i = normalized_axis[1]\n        while i > 0:\n            y = C.swapaxes(y, i, i - 1)\n            i -= 1\n        result = C.times(x, y, output_rank=(len(y.shape) - 1)\n                         if len(y.shape) > 1 else 1)\n        if len(y_shape) == 2:\n            result = squeeze(result, -1)\n        return result\n\n\ndef transpose(x):\n    return C.swapaxes(x, 0, 1)\n\n\ndef gather(reference, indices):\n    # There is a bug in cntk gather op which may cause crash.\n    # We have made a fix but not catched in CNTK 2.1 release.\n    # Will update with gather op in next release\n    if _get_cntk_version() >= 2.2:\n        return C.ops.gather(reference, indices)\n    else:\n        num_classes = reference.shape[0]\n        one_hot_matrix = C.ops.one_hot(indices, num_classes)\n        return C.times(one_hot_matrix, reference, output_rank=len(reference.shape) - 1)\n\n\ndef _remove_dims(x, axis, keepdims=False):\n    if keepdims is False and isinstance(axis, list):\n        # sequence axis is removed by default, so don't need reshape on it\n        reduce_axes = []\n        for a in axis:\n            if isinstance(a, C.Axis) is False:\n                reduce_axes.append(a)\n        return _reshape_dummy_dim(x, reduce_axes)\n    else:\n        if isinstance(axis, list):\n            has_seq = False\n            for a in axis:\n                if isinstance(a, C.Axis):\n                    has_seq = True\n                    break\n            if has_seq:\n                nones = _get_dynamic_axis_num(x)\n                x = expand_dims(x, nones)\n        return x\n\n\ndef max(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_max')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef min(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_min')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef sum(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_sum')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef prod(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_prod')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef logsumexp(x, axis=None, keepdims=False):\n    return log(sum(exp(x), axis=axis, keepdims=keepdims))\n\n\ndef var(x, axis=None, keepdims=False):\n    m = mean(x, axis, keepdims=True)\n    devs_squared = C.square(x - m)\n    return mean(devs_squared, axis=axis, keepdims=keepdims)\n\n\ndef std(x, axis=None, keepdims=False):\n    return C.sqrt(var(x, axis=axis, keepdims=keepdims))\n\n\ndef expand_dims(x, axis=-1):\n    shape = list(int_shape(x))\n    nones = _get_dynamic_axis_num(x)\n    index = axis if axis >= 0 else len(shape) + 1\n    shape.insert(index, 1)\n    new_shape = shape[nones:]\n    new_shape = tuple(\n        [C.InferredDimension if _ is None else _ for _ in new_shape])\n    result = C.reshape(x, new_shape)\n    if index < nones:\n        result._keras_shape = shape\n    return result\n\n\ndef squeeze(x, axis):\n    if isinstance(axis, tuple):\n        axis = list(axis)\n    if not isinstance(axis, list):\n        axis = [axis]\n\n    shape = list(int_shape(x))\n\n    _axis = []\n    for _ in axis:\n        if isinstance(_, int):\n            _axis.append(_ if _ >= 0 else _ + len(shape))\n\n    if len(_axis) == 0:\n        return x\n\n    nones = _get_dynamic_axis_num(x)\n    for _ in sorted(_axis, reverse=True):\n        del shape[_]\n\n    new_shape = shape[nones:]\n    new_shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in new_shape])\n    return C.reshape(x, new_shape)\n\n\ndef tile(x, n):\n    if isinstance(n, int):\n        n = (n,)\n    elif isinstance(n, list):\n        n = tuple(n)\n\n    shape = int_shape(x)\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    # Padding the axis\n    if len(n) < len(shape):\n        n = tuple([1 for _ in range(len(shape) - len(n))]) + n\n\n    if len(n) != len(shape):\n        raise NotImplementedError\n\n    i = num_dynamic_axis\n    for i, rep in enumerate(n):\n        if i >= num_dynamic_axis and shape[i] is not None:\n            tmp = [x] * rep\n            x = C.splice(*tmp, axis=i - num_dynamic_axis)\n        i += 1\n\n    return x\n\n\ndef _normalize_axis(axis, x):\n    shape = int_shape(x)\n    ndim = len(shape)\n\n    nones = _get_dynamic_axis_num(x)\n\n    if nones > ndim:\n        raise ValueError('CNTK Backend: tensor with keras shape: `%s` has '\n                         '%d cntk dynamic axis, this is not expected, please '\n                         'double check the keras shape history.' % (str(shape), nones))\n\n    # Current cntk does not support shape like (1, batch). so using the workaround\n    # here to mapping the correct axis. Will remove this tricky after we add support\n    # in native cntk op\n    cntk_axis = []\n    dynamic_axis_index = 0\n    for i in range(ndim):\n        if shape[i] is None and dynamic_axis_index < nones:\n            cntk_axis.append(x.dynamic_axes[dynamic_axis_index])\n            dynamic_axis_index += 1\n        else:\n            cntk_axis.append(i - dynamic_axis_index)\n\n    if dynamic_axis_index < nones:\n        i = 0\n        while dynamic_axis_index < nones:\n            cntk_axis[i] = x.dynamic_axes[dynamic_axis_index]\n            i += 1\n            dynamic_axis_index += 1\n\n        while i < len(cntk_axis):\n            cntk_axis[i] -= nones\n            i += 1\n\n    if isinstance(axis, tuple):\n        _axis = list(axis)\n    elif isinstance(axis, int):\n        _axis = [axis]\n    elif isinstance(axis, list):\n        _axis = list(axis)\n    else:\n        _axis = axis\n\n    if isinstance(_axis, list):\n        for i, a in enumerate(_axis):\n            if a is not None and a < 0:\n                _axis[i] = (a % ndim)\n            if _axis[i] is not None:\n                _axis[i] = cntk_axis[_axis[i]]\n    else:\n        if _axis is None:\n            _axis = C.Axis.all_axes()\n\n    return _axis\n\n\ndef _reshape_dummy_dim(x, axis):\n    shape = list(x.shape)\n\n    _axis = [_ + len(shape) if _ < 0 else _ for _ in axis]\n\n    if shape.count(C.InferredDimension) > 1 or shape.count(C.FreeDimension) > 1:\n        result = x\n        for index in sorted(_axis, reverse=True):\n            result = C.reshape(result,\n                               shape=(),\n                               begin_axis=index,\n                               end_axis=index + 1)\n        return result\n    else:\n        for index in sorted(_axis, reverse=True):\n            del shape[index]\n\n        shape = [C.InferredDimension if _ == C.FreeDimension else _ for _ in shape]\n        return C.reshape(x, shape)\n\n\ndef mean(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_mean')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef any(x, axis=None, keepdims=False):\n    reduce_result = sum(x, axis, keepdims=keepdims)\n    any_matrix = C.element_select(\n        reduce_result,\n        ones_like(reduce_result),\n        zeros_like(reduce_result))\n    if len(reduce_result.shape) == 0 and _get_dynamic_axis_num(x) == 0:\n        return C.reduce_sum(any_matrix)\n    else:\n        return any_matrix\n\n\ndef all(x, axis=None, keepdims=False):\n    reduce_result = prod(x, axis, keepdims=keepdims)\n    all_matrix = C.element_select(\n        reduce_result,\n        ones_like(reduce_result),\n        zeros_like(reduce_result))\n    if len(reduce_result.shape) == 0 and _get_dynamic_axis_num(x) == 0:\n        return C.reduce_sum(all_matrix)\n    else:\n        return all_matrix\n\n\ndef classification_error(target, output, axis=-1):\n    return C.ops.reduce_mean(\n        C.equal(\n            argmax(\n                output,\n                axis=-1),\n            argmax(\n                target,\n                axis=-1)),\n        axis=C.Axis.all_axes())\n\n\ndef argmax(x, axis=-1):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    output = C.ops.argmax(x, axis=axis[0])\n    return _reshape_dummy_dim(output, axis)\n\n\ndef argmin(x, axis=-1):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    output = C.ops.argmin(x, axis=axis[0])\n    return _reshape_dummy_dim(output, axis)\n\n\ndef square(x):\n    return C.square(x)\n\n\ndef abs(x):\n    return C.abs(x)\n\n\ndef sqrt(x):\n    return C.sqrt(x)\n\n\ndef exp(x):\n    return C.exp(x)\n\n\ndef log(x):\n    return C.log(x)\n\n\ndef round(x):\n    return C.round(x)\n\n\ndef sigmoid(x):\n    return C.sigmoid(x)\n\n\ndef sign(x):\n    return x / C.abs(x)\n\n\ndef pow(x, a):\n    return C.pow(x, a)\n\n\ndef clip(x, min_value, max_value):\n    if max_value is not None and max_value < min_value:\n        max_value = min_value\n    if max_value is None:\n        max_value = np.inf\n    if min_value is None:\n        min_value = -np.inf\n    return C.clip(x, min_value, max_value)\n\n\ndef binary_crossentropy(target, output, from_logits=False):\n    if from_logits:\n        output = C.sigmoid(output)\n    output = C.clip(output, epsilon(), 1.0 - epsilon())\n    output = -target * C.log(output) - (1.0 - target) * C.log(1.0 - output)\n    return output\n\n\ndef get_variable_shape(x):\n    return int_shape(x)\n\n\ndef update(x, new_x):\n    return C.assign(x, new_x)\n\n\ndef moving_average_update(variable, value, momentum):\n    return C.assign(variable, variable * momentum + value * (1. - momentum))\n\n\ndef update_add(x, increment):\n    result = x + increment\n    return C.assign(x, result)\n\n\ndef gradients(loss, variables):\n    # cntk does not support gradients as symbolic op,\n    # to hook up with keras model\n    # we will return a constant as place holder, the cntk learner will apply\n    # the gradient during training.\n    global grad_parameter_dict\n    if isinstance(variables, list) is False:\n        variables = [variables]\n    grads = []\n    for v in variables:\n        g = C.constant(0, shape=v.shape, name='keras_grad_placeholder')\n        grads.append(g)\n        grad_parameter_dict[g] = v\n    return grads\n\n\ndef equal(x, y):\n    return C.equal(x, y)\n\n\ndef not_equal(x, y):\n    return C.not_equal(x, y)\n\n\ndef greater(x, y):\n    return C.greater(x, y)\n\n\ndef greater_equal(x, y):\n    return C.greater_equal(x, y)\n\n\ndef less(x, y):\n    return C.less(x, y)\n\n\ndef less_equal(x, y):\n    return C.less_equal(x, y)\n\n\ndef maximum(x, y):\n    return C.element_max(x, y)\n\n\ndef minimum(x, y):\n    return C.element_min(x, y)\n\n\ndef sin(x):\n    return C.sin(x)\n\n\ndef cos(x):\n    return C.cos(x)\n\n\ndef normalize_batch_in_training(x, gamma, beta,\n                                reduction_axes, epsilon=1e-3):\n    if gamma is None:\n        if beta is None:\n            gamma = ones_like(x)\n        else:\n            gamma = ones_like(beta)\n    if beta is None:\n        if gamma is None:\n            beta = zeros_like(x)\n        else:\n            beta = zeros_like(gamma)\n\n    mean, variant = _moments(x, _normalize_axis(reduction_axes, x))\n\n    if sorted(reduction_axes) == list(range(ndim(x)))[:-1]:\n        normalized = batch_normalization(\n            x, mean, variant, beta, gamma, epsilon)\n    else:\n        # need broadcasting\n        target_shape = []\n        x_shape = int_shape(x)\n        # skip the batch axis\n        for axis in range(1, ndim(x)):\n            if axis in reduction_axes:\n                target_shape.append(1)\n                if ndim(gamma) > axis:\n                    gamma = C.reduce_mean(gamma, axis - 1)\n                    beta = C.reduce_mean(beta, axis - 1)\n            else:\n                target_shape.append(x_shape[axis])\n\n        broadcast_mean = C.reshape(mean, target_shape)\n        broadcast_var = C.reshape(variant, target_shape)\n        broadcast_gamma = C.reshape(gamma, target_shape)\n        broadcast_beta = C.reshape(beta, target_shape)\n        normalized = batch_normalization(\n            x,\n            broadcast_mean,\n            broadcast_var,\n            broadcast_beta,\n            broadcast_gamma,\n            epsilon)\n\n    return normalized, mean, variant\n\n\ndef _moments(x, axes=None, shift=None, keep_dims=False):\n    _axes = tuple(axes)\n    if shift is None:\n        shift = x\n        # Compute true mean while keeping the dims for proper broadcasting.\n        for axis in _axes:\n            shift = C.reduce_mean(shift, axis=axis)\n\n    shift = C.stop_gradient(shift)\n    shifted_mean = C.minus(x, shift)\n    for axis in _axes:\n        shifted_mean = C.reduce_mean(shifted_mean, axis=axis)\n\n    variance_mean = C.square(C.minus(x, shift))\n    for axis in _axes:\n        variance_mean = C.reduce_mean(variance_mean, axis=axis)\n\n    variance = C.minus(variance_mean, C.square(shifted_mean))\n    mean = C.plus(shifted_mean, shift)\n\n    if not keep_dims:\n        mean = squeeze(mean, _axes)\n        variance = squeeze(variance, _axes)\n\n    return mean, variance\n\n\ndef batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n    # The mean / var / beta / gamma may be processed by broadcast\n    # so it may have an extra batch axis with 1, it is not needed\n    # in cntk, need to remove those dummy axis.\n    if ndim(mean) == ndim(x) and shape(mean)[0] == 1:\n        mean = _reshape_dummy_dim(mean, [0])\n    if ndim(var) == ndim(x) and shape(var)[0] == 1:\n        var = _reshape_dummy_dim(var, [0])\n\n    if gamma is None:\n        gamma = ones_like(var)\n    elif ndim(gamma) == ndim(x) and shape(gamma)[0] == 1:\n        gamma = _reshape_dummy_dim(gamma, [0])\n\n    if beta is None:\n        beta = zeros_like(mean)\n    elif ndim(beta) == ndim(x) and shape(beta)[0] == 1:\n        beta = _reshape_dummy_dim(beta, [0])\n\n    return (x - mean) / C.sqrt(var + epsilon) * gamma + beta\n\n\ndef concatenate(tensors, axis=-1):\n    if len(tensors) == 0:\n        return None\n\n    axis = [axis]\n    axis = _normalize_axis(axis, tensors[0])\n    return C.splice(*tensors, axis=axis[0])\n\n\ndef flatten(x):\n    return reshape(x, (-1,))\n\n\ndef reshape(x, shape):\n    shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in shape])\n    if isinstance(x, C.variables.Parameter):\n        return C.reshape(x, shape)\n    else:\n        num_dynamic_axis = _get_dynamic_axis_num(x)\n\n        if num_dynamic_axis == 1 and len(shape) > 0 and shape[0] == -1:\n            # collapse axis with batch axis\n            if b_any(_ == C.InferredDimension for _ in x.shape) or b_any(\n                    _ == C.FreeDimension for _ in x.shape):\n                warnings.warn(\n                    'Warning: CNTK backend does not support '\n                    'collapse of batch axis with inferred dimension. '\n                    'The reshape did not take place.')\n                return x\n            return _reshape_batch(x, shape)\n        else:\n            # no collapse, then first need to padding the shape\n            if num_dynamic_axis >= len(shape):\n                i = 0\n                while i < len(shape):\n                    if shape[i] is None or shape[i] == -1:\n                        i += 1\n                    else:\n                        break\n                shape = tuple([-1 for _ in range(num_dynamic_axis - i)]) + shape\n\n            new_shape = list(shape)\n            new_shape = new_shape[num_dynamic_axis:]\n            new_shape = [C.InferredDimension if _ is None else _ for _ in new_shape]\n            return C.reshape(x, new_shape)\n\n\ndef permute_dimensions(x, pattern):\n    dims = len(int_shape(x))\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    if isinstance(pattern, list):\n        current_layout = [i for i in range(dims)]\n    else:\n        current_layout = tuple([i for i in range(dims)])\n\n    if num_dynamic_axis > 0 and pattern[:num_dynamic_axis] != current_layout[:num_dynamic_axis]:\n        raise ValueError('CNTK backend: the permute pattern %s '\n                         'requested permute on dynamic axis, '\n                         'which is not supported. Please do permute '\n                         'on static axis.' % pattern)\n\n    axis = list(pattern)\n    axis = axis[num_dynamic_axis:]\n    axis = _normalize_axis(axis, x)\n    return C.transpose(x, axis)\n\n\ndef resize_images(x, height_factor, width_factor, data_format, interpolation='nearest'):\n    if interpolation == 'nearest':\n        if data_format == 'channels_first':\n            output = repeat_elements(x, height_factor, axis=2)\n            output = repeat_elements(output, width_factor, axis=3)\n            return output\n        elif data_format == 'channels_last':\n            output = repeat_elements(x, height_factor, axis=1)\n            output = repeat_elements(output, width_factor, axis=2)\n            return output\n        else:\n            raise ValueError('CNTK Backend: Invalid data_format: %s' % data_format)\n    else:\n        raise NotImplementedError('CNTK only supports `nearest` interpolation.')\n\n\ndef resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n    if data_format == 'channels_first':\n        output = repeat_elements(x, depth_factor, axis=2)\n        output = repeat_elements(output, height_factor, axis=3)\n        output = repeat_elements(output, width_factor, axis=4)\n        return output\n    elif data_format == 'channels_last':\n        output = repeat_elements(x, depth_factor, axis=1)\n        output = repeat_elements(output, height_factor, axis=2)\n        output = repeat_elements(output, width_factor, axis=3)\n        return output\n    else:\n        raise ValueError('CNTK Backend: Invalid data_format: %s' % data_format)\n\n\ndef repeat_elements(x, rep, axis):\n    axis = _normalize_axis(axis, x)\n    axis = axis[0]\n    slices = []\n    shape = x.shape\n    i = 0\n    while i < shape[axis]:\n        tmp = C.ops.slice(x, axis, i, i + 1)\n        for _ in range(rep):\n            slices.append(tmp)\n        i += 1\n    return C.splice(*slices, axis=axis)\n\n\ndef repeat(x, n):\n    # this is a workaround for recurrent layer\n    # if n is inferred dimension,\n    # we can't figure out how to repeat it in cntk now\n    # return the same x to take cntk broadcast feature\n    # to make the recurrent layer work.\n    # need to be fixed in GA.\n    if n is C.InferredDimension or n is C.FreeDimension:\n        return x\n    index = 1 - _get_dynamic_axis_num(x)\n    if index < 0 or index > 1:\n        raise NotImplementedError\n\n    new_shape = list(x.shape)\n    new_shape.insert(index, 1)\n    new_shape = tuple(new_shape)\n    x = C.reshape(x, new_shape)\n    temp = [x] * n\n    return C.splice(*temp, axis=index)\n\n\ndef tanh(x):\n    return C.tanh(x)\n\n\ndef _static_rnn(step_function, inputs, initial_states,\n                go_backwards=False, mask=None, constants=None,\n                unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('Input should be at least 3D.')\n\n    # if the second axis is static axis, CNTK will do unroll by default\n    if shape[1] is None:\n        raise ValueError('CNTK Backend: the input of static rnn '\n                         'has shape `%s`, the second axis '\n                         'is not static. If you want to run '\n                         'rnn with non-static axis, please try '\n                         'dynamic rnn with sequence axis.' % shape)\n    if constants is None:\n        constants = []\n\n    if mask is not None:\n        mask_shape = int_shape(mask)\n        if len(mask_shape) == dims - 1:\n            mask = expand_dims(mask)\n\n    nones = _get_dynamic_axis_num(inputs)\n\n    states = tuple(initial_states)\n\n    outputs = []\n\n    time_axis = 1 - nones if nones > 0 else 1\n\n    if go_backwards:\n        i = shape[1] - 1\n        while i >= 0:\n            current = C.ops.slice(inputs, time_axis, i, i + 1)\n            # remove dummy dimension\n            current = squeeze(current, time_axis)\n\n            output, new_states = step_function(\n                current, tuple(states) + tuple(constants))\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n\n            if mask is not None:\n                mask_slice = C.ops.slice(mask, time_axis, i, i + 1)\n                mask_slice = squeeze(mask_slice, time_axis)\n                if len(outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = outputs[-1]\n                output = C.ops.element_select(mask_slice, output, prev_output)\n\n                return_states = []\n                for s, n_s in zip(states, new_states):\n                    return_states.append(\n                        C.ops.element_select(\n                            mask_slice, n_s, s))\n                new_states = return_states\n            outputs.append(output)\n            states = new_states\n            i -= 1\n    else:\n        i = 0\n        while i < shape[1]:\n            current = C.ops.slice(inputs, time_axis, i, i + 1)\n            # remove dummy dimension\n            current = squeeze(current, 1)\n\n            output, new_states = step_function(\n                current, tuple(states) + tuple(constants))\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n\n            if mask is not None:\n                mask_slice = C.ops.slice(mask, time_axis, i, i + 1)\n                mask_slice = squeeze(mask_slice, 1)\n                if len(outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = outputs[-1]\n                output = C.ops.element_select(mask_slice, output, prev_output)\n\n                return_states = []\n                for s, n_s in zip(states, new_states):\n                    return_states.append(\n                        C.ops.element_select(\n                            mask_slice, n_s, s))\n                new_states = return_states\n            outputs.append(output)\n            states = new_states[:len(states)]\n            i += 1\n\n    i = 1\n    # add the time_step axis back\n    final_output = expand_dims(outputs[0], 1)\n    last_output = outputs[0]\n    while i < len(outputs):\n        # add the time_step axis back\n        output_slice = expand_dims(outputs[i], 1)\n        final_output = C.splice(final_output, output_slice, axis=time_axis)\n        last_output = outputs[i]\n        i += 1\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, states\n\n\ndef rnn(step_function, inputs, initial_states,\n        go_backwards=False, mask=None, constants=None,\n        unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    global uses_learning_phase\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('CNTK Backend: the input of rnn has only rank %d '\n                         'Need at least rank 3 to run RNN.' % dims)\n\n    if _get_dynamic_axis_num(inputs) == 0 or unroll:\n        return _static_rnn(\n            step_function,\n            inputs,\n            initial_states,\n            go_backwards,\n            mask,\n            constants,\n            unroll,\n            input_length)\n\n    if constants is None:\n        constants = []\n\n    num_time_step = shape[1]\n    if num_time_step is None and not has_seq_axis(inputs):\n        num_time_step = inputs.shape[0]\n\n    initial = []\n    for s in initial_states:\n        if _get_dynamic_axis_num(s) == 0:\n            if hasattr(C, 'to_batch'):\n                initial.append(C.to_batch(s))\n            else:\n                initial.append(C.user_function(ConvertToBatch(s)))\n        else:\n            initial.append(s)\n\n    need_convert = not has_seq_axis(inputs)\n    if go_backwards and need_convert is False:\n        raise NotImplementedError('CNTK Backend: `go_backwards` is not supported with '\n                                  'variable-length sequences. Please specify a '\n                                  'static length for your sequences.')\n\n    rnn_inputs = inputs\n    if need_convert:\n        if go_backwards:\n            rnn_inputs = reverse(rnn_inputs, 1)\n\n        rnn_inputs = C.to_sequence(rnn_inputs)\n\n        rnn_constants = []\n        for constant in constants:\n            if isinstance(constant, list):\n                new_c = []\n                for c in constant:\n                    if _get_dynamic_axis_num(c) == 1:\n                        new_c.append(C.sequence.broadcast_as(c, rnn_inputs))\n                    else:\n                        new_c.append(c)\n                rnn_constants.append(new_c)\n            else:\n                if _get_dynamic_axis_num(constant) == 1:\n                    rnn_constants.append(C.sequence.broadcast_as(constant, rnn_inputs))\n                else:\n                    rnn_constants.append(constant)\n    else:\n        rnn_constants = constants\n\n    if mask is not None and not has_seq_axis(mask):\n        if go_backwards:\n            mask = reverse(mask, 1)\n        if len(int_shape(mask)) == 2:\n            mask = expand_dims(mask)\n        mask = C.to_sequence_like(mask, rnn_inputs)\n\n    states = tuple(initial)\n\n    with C.default_options(axis_offset=1):\n        def _recurrence(x, states, m):\n            # create place holder\n            place_holders = [C.placeholder(dynamic_axes=x.dynamic_axes) for _ in states]\n            past_values = []\n            for s, p in zip(states, place_holders):\n                past_values.append(C.sequence.past_value(p, s))\n            new_output, new_states = step_function(\n                x, tuple(past_values) + tuple(rnn_constants))\n\n            if getattr(new_output, '_uses_learning_phase', False):\n                global uses_learning_phase\n                uses_learning_phase = True\n\n            if m is not None:\n                new_states = [C.element_select(m, n, s) for n, s in zip(new_states, past_values)]\n            n_s = []\n            for o, p in zip(new_states, place_holders):\n                n_s.append(o.replace_placeholders({p: o.output}))\n            if len(n_s) > 0:\n                new_output = n_s[0]\n            return new_output, n_s\n\n        final_output, final_states = _recurrence(rnn_inputs, states, mask)\n        last_output = C.sequence.last(final_output)\n        last_states = [C.sequence.last(s) for s in final_states]\n\n    if need_convert:\n        final_output = C.sequence.unpack(final_output, 0, no_mask_output=True)\n        if num_time_step is not None and num_time_step is not C.FreeDimension:\n            final_output = _reshape_sequence(final_output, num_time_step)\n\n    f_stats = []\n    for l_s, i_s in zip(last_states, initial_states):\n        if _get_dynamic_axis_num(i_s) == 0 and _get_dynamic_axis_num(l_s) == 1:\n            if hasattr(C, 'unpack_batch'):\n                f_stats.append(C.unpack_batch(l_s))\n            else:\n                f_stats.append(C.user_function(ConvertToStatic(l_s, batch_size=i_s.shape[0])))\n        else:\n            f_stats.append(l_s)\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, f_stats\n\n\ndef has_seq_axis(x):\n    return hasattr(x, 'dynamic_axes') and len(x.dynamic_axes) > 1\n\n\ndef l2_normalize(x, axis=None):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    norm = C.sqrt(C.reduce_sum(C.square(x), axis=axis[0]))\n    return x / norm\n\n\ndef hard_sigmoid(x):\n    x = (0.2 * x) + 0.5\n    x = C.clip(x, 0.0, 1.0)\n    return x\n\n\ndef conv1d(x, kernel, strides=1, padding='valid',\n           data_format=None, dilation_rate=1):\n    data_format = normalize_data_format(data_format)\n\n    if padding == 'causal':\n        # causal (dilated) convolution:\n        left_pad = dilation_rate * (kernel.shape[0] - 1)\n        x = temporal_padding(x, (left_pad, 0))\n        padding = 'valid'\n\n    if data_format == 'channels_last':\n        x = C.swapaxes(x, 0, 1)\n        kernel = C.swapaxes(kernel, 0, 2)\n\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != 1:\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to 1. You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(\n        kernel,\n        x,\n        strides=strides,\n        auto_padding=[False, padding],\n        dilation=dilation_rate)\n\n    if data_format == 'channels_last':\n        x = C.swapaxes(x, 0, 1)\n    return x\n\n\ndef conv2d(x, kernel, strides=(1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != (1, 1):\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to (1, 1). '\n                         'You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(kernel,\n                      x,\n                      strides,\n                      auto_padding=[False, padding, padding],\n                      dilation=dilation_rate)\n\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n                     padding='valid', data_format=None, dilation_rate=1):\n    data_format = normalize_data_format(data_format)\n    if isinstance(strides, int):\n        strides = (strides,)\n    if isinstance(dilation_rate, int):\n        dilation_rate = (dilation_rate,)\n\n    if data_format == 'channels_last':\n        spatial_start_dim = 2\n    else:\n        spatial_start_dim = 3\n    x = expand_dims(x, spatial_start_dim)\n    depthwise_kernel = expand_dims(depthwise_kernel, 1)\n    pointwise_kernel = expand_dims(pointwise_kernel, 1)\n    strides = (1,) + strides + (1,)\n    dilation_rate = (1,) + dilation_rate\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dilation_rate == (1, 1):\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    x = _postprocess_conv2d_output(x, data_format)\n    return squeeze(x, spatial_start_dim)\n\n\ndef separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=dilation_rate[0],\n                          auto_padding=[False, padding, padding])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n                     data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    padding = _preprocess_border_mode(padding)\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=dilation_rate[0],\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dev.type() == 0 and dilation_rate != (1, 1, 1):\n        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                         'Please set `dilation_rate` to (1, 1, 1). '\n                         'You passed: %s' % (dilation_rate,))\n\n    x = C.convolution(\n        kernel,\n        x,\n        strides,\n        auto_padding=[False, padding, padding, padding],\n        dilation=dilation_rate)\n\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n                     padding='valid', data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = (1,) + strides\n    # cntk output_shape does not include batch axis\n    output_shape = output_shape[1:]\n    # in keras2, need handle output shape in different format\n    if data_format == 'channels_last':\n        output_shape = transpose_shape(output_shape, 'channels_first',\n                                       spatial_axes=(0, 1, 2))\n\n    x = C.convolution_transpose(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding,\n            padding],\n        output_shape=output_shape)\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef pool2d(x, pool_size, strides=(1, 1),\n           padding='valid', data_format=None,\n           pool_mode='max'):\n    data_format = normalize_data_format(data_format)\n\n    padding = _preprocess_border_mode(padding)\n    strides = strides\n    pool_size = pool_size\n    x = _preprocess_conv2d_input(x, data_format)\n    if pool_mode == 'max':\n        x = C.pooling(\n            x,\n            C.MAX_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    elif pool_mode == 'avg':\n        x = C.pooling(\n            x,\n            C.AVG_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    else:\n        raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n           data_format=None, pool_mode='max'):\n    data_format = normalize_data_format(data_format)\n\n    padding = _preprocess_border_mode(padding)\n\n    x = _preprocess_conv3d_input(x, data_format)\n\n    if pool_mode == 'max':\n        x = C.pooling(\n            x,\n            C.MAX_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    elif pool_mode == 'avg':\n        x = C.pooling(\n            x,\n            C.AVG_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    else:\n        raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef relu(x, alpha=0., max_value=None):\n    if alpha != 0.:\n        negative_part = C.relu(-x)\n    x = C.relu(x)\n    if max_value is not None:\n        x = C.clip(x, 0.0, max_value)\n    if alpha != 0.:\n        x -= alpha * negative_part\n    return x\n\n\ndef dropout(x, level, noise_shape=None, seed=None):\n    if level < 0. or level >= 1:\n        raise ValueError('CNTK Backend: Invalid dropout level %s, '\n                         'must be in interval [0, 1].' % level)\n    return C.dropout(x, level)\n\n\ndef batch_flatten(x):\n    # cntk's batch axis is not in shape,\n    # so just flatten all the dim in x.shape\n    dim = np.prod(x.shape)\n    x = C.reshape(x, (-1,))\n    x._keras_shape = (None, dim)\n    return x\n\n\ndef softmax(x, axis=-1):\n    return C.softmax(x, axis=axis)\n\n\ndef softplus(x):\n    return C.softplus(x)\n\n\ndef softsign(x):\n    return x / (1 + C.abs(x))\n\n\ndef categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    # Here, unlike other backends, the tensors lack a batch dimension:\n    axis_without_batch = -1 if axis == -1 else axis - 1\n    output_dimensions = list(range(len(output.shape)))\n    if axis_without_batch != -1 and axis_without_batch not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis_without_batch),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(output.shape))))\n    # If the channels are not in the last axis, move them to be there:\n    if axis_without_batch != -1 and axis_without_batch != output_dimensions[-1]:\n        permutation = output_dimensions[:axis_without_batch]\n        permutation += output_dimensions[axis_without_batch + 1:]\n        permutation += [axis_without_batch]\n        output = C.transpose(output, permutation)\n        target = C.transpose(target, permutation)\n    if from_logits:\n        result = C.cross_entropy_with_softmax(output, target)\n        # cntk's result shape is (batch, 1), while keras expect (batch, )\n        return C.reshape(result, ())\n    else:\n        # scale preds so that the class probas of each sample sum to 1\n        output /= C.reduce_sum(output, axis=-1)\n        # avoid numerical instability with epsilon clipping\n        output = C.clip(output, epsilon(), 1.0 - epsilon())\n        return -sum(target * C.log(output), axis=-1)\n\n\ndef sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    # Here, unlike other backends, the tensors lack a batch dimension:\n    axis_without_batch = -1 if axis == -1 else axis - 1\n    output_dimensions = list(range(len(output.shape)))\n    if axis_without_batch != -1 and axis_without_batch not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis_without_batch),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(output.shape))))\n    target = C.one_hot(target, output.shape[axis_without_batch],\n                       axis=axis_without_batch)\n    target = C.reshape(target, output.shape)\n    return categorical_crossentropy(target, output, from_logits, axis=axis)\n\n\nclass Function(object):\n\n    def __init__(self, inputs, outputs, updates=[], **kwargs):\n        self.placeholders = inputs\n        self.trainer = None\n        self.unrelated_updates = None\n        self.updates = updates\n        if len(updates) > 0:\n            assert len(outputs) > 0\n            self.loss = outputs[0]\n            # need group update by gradient place holder\n            u_ops = []\n            unrelated_updates = []\n            for update in updates:\n                if isinstance(update, tuple):\n                    if len(update) != 2:\n                        raise NotImplementedError\n                    else:\n                        u = C.assign(update[0], update[1])\n                else:\n                    u = update\n\n                if len(u.arguments) == 0:\n                    u_ops.append(u)\n                else:\n                    unrelated_updates.append(u)\n\n            update_func = C.combine([u.output for u in u_ops])\n\n            grads = update_func.find_all_with_name('keras_grad_placeholder')\n\n            u_list = []\n            p_list = []\n            for g in grads:\n                if g in grad_parameter_dict:\n                    p_list.append(grad_parameter_dict[g])\n                    u_list.append(g)\n                else:\n                    raise ValueError(\n                        'CNTK backend: when constructing trainer, '\n                        'found gradient node `%s` which is not '\n                        'related to any parameters in the model. '\n                        'Please double check how the gradient node '\n                        'is constructed.' % g)\n\n            if len(u_list) > 0:\n                learner = C.cntk_py.universal_learner(p_list, u_list, update_func)\n\n                criterion = (\n                    outputs[0],\n                    outputs[1]) if len(outputs) > 1 else (\n                    outputs[0],\n                )\n                self.trainer = C.trainer.Trainer(\n                    outputs[0], criterion, [learner])\n                self.trainer_output = tuple([f.output for f in criterion])\n            elif len(u_ops) > 0:\n                unrelated_updates.extend(u_ops)\n\n            if len(unrelated_updates) > 0:\n                self.unrelated_updates = C.combine([_.output for _ in unrelated_updates])\n\n        if self.trainer is None:\n            self.metrics_outputs = [f.output for f in outputs]\n            self.metrics_func = C.combine(self.metrics_outputs)\n        # cntk only could handle loss and 1 metric in trainer, for metrics more\n        # than 2, need manual eval\n        elif len(outputs) > 2:\n            self.metrics_outputs = [f.output for f in outputs[2:]]\n            self.metrics_func = C.combine(self.metrics_outputs)\n        else:\n            self.metrics_func = None\n\n    @staticmethod\n    def _is_input_shape_compatible(input, placeholder):\n        if hasattr(input, 'shape') and hasattr(placeholder, 'shape'):\n            num_dynamic = get_num_dynamic_axis(placeholder)\n            input_shape = input.shape[num_dynamic:]\n            placeholder_shape = placeholder.shape\n            for i, p in zip(input_shape, placeholder_shape):\n                if i != p and p != C.InferredDimension and p != C.FreeDimension:\n                    return False\n        return True\n\n    def __call__(self, inputs):\n        global _LEARNING_PHASE_PLACEHOLDER\n        global _LEARNING_PHASE\n        assert isinstance(inputs, (list, tuple))\n        feed_dict = {}\n        for tensor, value in zip(self.placeholders, inputs):\n            # cntk only support calculate on float, do auto cast here\n            if (hasattr(value, 'dtype') and\n               value.dtype != np.float32 and\n               value.dtype != np.float64):\n                value = value.astype(np.float32)\n\n            if tensor == _LEARNING_PHASE_PLACEHOLDER:\n                _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(value)\n            else:\n                # in current version cntk can't support input with variable\n                # length. Will support it in next release.\n                if not self._is_input_shape_compatible(value, tensor):\n                    raise ValueError('CNTK backend: The placeholder has been resolved '\n                                     'to shape `%s`, but input shape is `%s`. Currently '\n                                     'CNTK can not take variable length inputs. Please '\n                                     'pass inputs that have a static shape.'\n                                     % (str(tensor.shape), str(value.shape)))\n            feed_dict[tensor] = value\n\n        updated = []\n        if self.trainer is not None:\n            input_dict = {}\n            for argument in self.loss.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError(\n                        'CNTK backend: argument %s is not found in inputs. '\n                        'Please double check the model and inputs in '\n                        '`train_function`.' % argument.name)\n\n            result = self.trainer.train_minibatch(\n                input_dict, self.trainer_output)\n\n            assert(len(result) == 2)\n            outputs = result[1]\n            for o in self.trainer_output:\n                updated.append(outputs[o])\n\n        if self.metrics_func is not None:\n            input_dict = {}\n            for argument in self.metrics_func.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError('CNTK backend: metrics argument %s '\n                                     'is not found in inputs. Please double '\n                                     'check the model and inputs.' % argument.name)\n            # Some ops (like dropout) won't be applied during \"eval\" in cntk.\n            # They only evaluated in training phase. To make it work, call\n            # \"forward\" method to let cntk know we want to evaluate them.from\n            # But the assign ops won't be executed under this mode, that's why\n            # we need this check.\n            if (self.unrelated_updates is None and\n                    (_LEARNING_PHASE_PLACEHOLDER.value == 1.0 or _LEARNING_PHASE == 1)):\n                _, output_values = self.metrics_func.forward(\n                    input_dict,\n                    self.metrics_func.outputs,\n                    (self.metrics_func.outputs[0],),\n                    as_numpy=False)\n            else:\n                output_values = self.metrics_func.eval(input_dict, as_numpy=False)\n            if isinstance(output_values, dict):\n                for o in self.metrics_outputs:\n                    value = output_values[o]\n                    v = value.asarray()\n                    updated.append(v)\n            else:\n                v = output_values.asarray()\n                for o in self.metrics_outputs:\n                    updated.append(v)\n\n        if self.unrelated_updates is not None:\n            input_dict = {}\n            for argument in self.unrelated_updates.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError(\n                        'CNTK backend: assign ops argument %s '\n                        'is not found in inputs. Please double '\n                        'check the model and inputs.' % argument.name)\n            self.unrelated_updates.eval(input_dict, as_numpy=False)\n        return updated\n\n\ndef function(inputs, outputs, updates=[], **kwargs):\n    return Function(inputs, outputs, updates=updates, **kwargs)\n\n\ndef temporal_padding(x, padding=(1, 1)):\n    assert len(padding) == 2\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 3 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, [padding], 'channels_last', num_dynamic_axis)\n\n\ndef _padding(x, pattern, axis):  # pragma: no cover\n    base_shape = x.shape\n    if b_any([dim < 0 for dim in base_shape]):\n        raise ValueError('CNTK Backend: padding input tensor with '\n                         'shape `%s` contains non-specified dimension, '\n                         'which is not supported. Please give fixed '\n                         'dimension to enable padding.' % base_shape)\n    if pattern[0] > 0:\n        prefix_shape = list(base_shape)\n        prefix_shape[axis] = pattern[0]\n        prefix_shape = tuple(prefix_shape)\n        x = C.splice(C.constant(value=0, shape=prefix_shape), x, axis=axis)\n        base_shape = x.shape\n    if pattern[1] > 0:\n        postfix_shape = list(base_shape)\n        postfix_shape[axis] = pattern[1]\n        postfix_shape = tuple(postfix_shape)\n        x = C.splice(x, C.constant(value=0, shape=postfix_shape), axis=axis)\n    return x\n\n\ndef pad(x, pad_info, data_format, num_dynamic_axis):\n    if hasattr(C, 'pad'):\n        pattern = [list(p) for p in pad_info]\n        if data_format == 'channels_first':\n            pattern = [[0, 0]] + pattern\n        else:\n            pattern = pattern + [[0, 0]]\n        if num_dynamic_axis == 0:\n            pattern = [[0, 0]] + pattern\n        return C.pad(x, pattern=pattern)\n    else:  # pragma: no cover\n        for (a, p) in enumerate(pad_info):\n            x = _padding(x, p,\n                         a + (1 if num_dynamic_axis == 0 else 0) +\n                         (1 if data_format == 'channels_first' else 0))\n        return x\n\n\ndef spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n    assert len(padding) == 2\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    data_format = normalize_data_format(data_format)\n\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 4 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, padding, data_format, num_dynamic_axis)\n\n\ndef spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n    assert len(padding) == 3\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    assert len(padding[2]) == 2\n    data_format = normalize_data_format(data_format)\n\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    assert len(x.shape) == 5 - (1 if num_dynamic_axis > 0 else 0)\n    return pad(x, padding, data_format, num_dynamic_axis)\n\n\ndef one_hot(indices, num_classes):\n    return C.one_hot(indices, num_classes)\n\n\ndef get_value(x):\n    if isinstance(\n            x,\n            C.variables.Parameter) or isinstance(\n            x,\n            C.variables.Constant):\n        return x.value\n    else:\n        return eval(x)\n\n\ndef batch_get_value(xs):\n    result = []\n    for x in xs:\n        if (isinstance(x, C.variables.Parameter) or\n           isinstance(x, C.variables.Constant)):\n            result.append(x.value)\n        else:\n            result.append(eval(x))\n    return result\n\n\ndef set_value(x, value):\n    if (isinstance(x, C.variables.Parameter) or\n       isinstance(x, C.variables.Constant)):\n        if isinstance(value, (float, int)):\n            value = np.full(x.shape, value, dtype=floatx())\n        x.value = value\n    else:\n        raise NotImplementedError\n\n\ndef print_tensor(x, message=''):\n    return C.user_function(\n        LambdaFunc(x,\n                   when=lambda x: True,\n                   execute=lambda x: print(message)))\n\n\ndef batch_set_value(tuples):\n    for t in tuples:\n        x = t[0]\n        value = t[1]\n        if isinstance(value, np.ndarray) is False:\n            value = np.asarray(value)\n        if isinstance(x, C.variables.Parameter):\n            x.value = value\n        else:\n            raise NotImplementedError\n\n\ndef stop_gradient(variables):\n    if isinstance(variables, (list, tuple)):\n        return map(C.stop_gradient, variables)\n    else:\n        return C.stop_gradient(variables)\n\n\ndef switch(condition, then_expression, else_expression):\n    ndim_cond = ndim(condition)\n    ndim_expr = ndim(then_expression)\n    if ndim_cond > ndim_expr:\n        raise ValueError('Rank of condition should be less'\n                         ' than or equal to rank of then and'\n                         ' else expressions. ndim(condition)=' +\n                         str(ndim_cond) + ', ndim(then_expression)'\n                         '=' + str(ndim_expr))\n    elif ndim_cond < ndim_expr:\n        shape_expr = int_shape(then_expression)\n        ndim_diff = ndim_expr - ndim_cond\n        for i in range(ndim_diff):\n            condition = expand_dims(condition)\n            condition = tile(condition, shape_expr[ndim_cond + i])\n    return C.element_select(condition,\n                            then_expression,\n                            else_expression)\n\n\ndef elu(x, alpha=1.):\n    res = C.elu(x)\n    if alpha == 1:\n        return res\n    else:\n        return C.element_select(C.greater(x, 0), res, alpha * res)\n\n\ndef in_top_k(predictions, targets, k):\n    _targets = C.one_hot(targets, predictions.shape[-1])\n    result = C.classification_error(predictions, _targets, topN=k)\n    return 1 - C.reshape(result, shape=())\n\n\ndef conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    data_format = normalize_data_format(data_format)\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = (1,) + strides\n    # cntk output_shape does not include batch axis\n    output_shape = output_shape[1:]\n    # in keras2, need handle output shape in different format\n    if data_format == 'channels_last':\n        output_shape = transpose_shape(output_shape, 'channels_first',\n                                       spatial_axes=(0, 1))\n\n    x = C.convolution_transpose(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding],\n        output_shape=output_shape,\n        dilation=dilation_rate)\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef identity(x, name=None):\n    if name is None:\n        name = '%s_alias' % x.name\n    return C.alias(x, name=name)\n\n\ndef _preprocess_conv2d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, rows, cols)\n        # TF input shape: (samples, rows, cols, input_depth)\n        x = C.transpose(x, (2, 0, 1))\n    return x\n\n\ndef _preprocess_conv2d_kernel(kernel, data_format):\n    # As of Keras 2.0.0, all kernels are normalized\n    # on the format `(rows, cols, input_depth, depth)`,\n    # independently of `data_format`.\n    # CNTK expects `(depth, input_depth, rows, cols)`.\n    kernel = C.transpose(kernel, (3, 2, 0, 1))\n    return kernel\n\n\ndef _preprocess_border_mode(padding):\n    if padding == 'same':\n        padding = True\n    elif padding == 'valid':\n        padding = False\n    else:\n        raise ValueError('Invalid border mode: ' + str(padding))\n    return padding\n\n\ndef _postprocess_conv2d_output(x, data_format):\n    if data_format == 'channels_last':\n        x = C.transpose(x, (1, 2, 0))\n    return x\n\n\ndef _preprocess_conv3d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, conv_dim1, conv_dim2, conv_dim3)\n        # TF input shape: (samples, conv_dim1, conv_dim2, conv_dim3,\n        # input_depth)\n        x = C.transpose(x, (3, 0, 1, 2))\n    return x\n\n\ndef _preprocess_conv3d_kernel(kernel, dim_ordering):\n    kernel = C.transpose(kernel, (4, 3, 0, 1, 2))\n    return kernel\n\n\ndef _postprocess_conv3d_output(x, dim_ordering):\n    if dim_ordering == 'channels_last':\n        x = C.transpose(x, (1, 2, 3, 0))\n    return x\n\n\ndef _get_dynamic_axis_num(x):\n    if hasattr(x, 'dynamic_axes'):\n        return len(x.dynamic_axes)\n    else:\n        return 0\n\n\ndef _contain_seqence_axis(x):\n    if _get_dynamic_axis_num(x) > 1:\n        return x.dynamic_axes[1] == C.Axis.default_dynamic_axis()\n    else:\n        return False\n\n\ndef get_num_dynamic_axis(x):\n    return _get_dynamic_axis_num(x)\n\n\ndef _reduce_on_axis(x, axis, reduce_fun_name):\n    if isinstance(axis, list):\n        for a in axis:\n            if isinstance(a, C.Axis) \\\n                    and a != C.Axis.default_batch_axis() \\\n                    and hasattr(C.sequence, reduce_fun_name):\n                x = getattr(C.sequence, reduce_fun_name)(x, a)\n            else:\n                x = getattr(C, reduce_fun_name)(x, a)\n    else:\n        x = getattr(C, reduce_fun_name)(x, axis)\n    return x\n\n\ndef _reshape_sequence(x, time_step):\n    tmp_shape = list(int_shape(x))\n    tmp_shape[1] = time_step\n    return reshape(x, tmp_shape)\n\n\ndef local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    stride = strides[0]\n    kernel_shape = int_shape(kernel)\n    output_length, feature_dim, filters = kernel_shape\n\n    xs = []\n    for i in range(output_length):\n        slice_length = py_slice(i * stride,\n                                i * stride + kernel_size[0])\n        xs.append(reshape(inputs[:, slice_length, :],\n                          (-1, 1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=1)\n    # transpose kernel to output_filters first, to apply broadcast\n    weight = permute_dimensions(kernel, (2, 0, 1))\n    # Shape: (batch, filters, output_length, input_length * kernel_size)\n    output = x_aggregate * weight\n    # Shape: (batch, filters, output_length)\n    output = sum(output, axis=3)\n    # Shape: (batch, output_length, filters)\n    return permute_dimensions(output, (0, 2, 1))\n\n\ndef local_conv2d(inputs,\n                 kernel,\n                 kernel_size,\n                 strides,\n                 output_shape,\n                 data_format=None):\n    data_format = normalize_data_format(data_format)\n\n    stride_row, stride_col = strides\n    output_row, output_col = output_shape\n    kernel_shape = int_shape(kernel)\n    _, feature_dim, filters = kernel_shape\n    xs = []\n\n    for i in range(output_row):\n        for j in range(output_col):\n            slice_row = py_slice(i * stride_row,\n                                 i * stride_row + kernel_size[0])\n            slice_col = py_slice(j * stride_col,\n                                 j * stride_col + kernel_size[1])\n            if data_format == 'channels_first':\n                xs.append(reshape(inputs[:, :, slice_row, slice_col],\n                                  (-1, 1, feature_dim)))\n            else:\n                xs.append(reshape(inputs[:, slice_row, slice_col, :],\n                                  (-1, 1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=1)\n    # transpose kernel to put filters first\n    weight = permute_dimensions(kernel, (2, 0, 1))\n    # shape: batch, filters, output_length, input_length * kernel_size\n    output = x_aggregate * weight\n    # shape: batch, filters, output_length\n    output = sum(output, axis=3)\n    # shape: batch, filters, row, col\n    output = reshape(output,\n                     (-1, filters, output_row, output_col))\n\n    if data_format == 'channels_last':\n        # shape: batch, row, col, filters\n        output = permute_dimensions(output, (0, 2, 3, 1))\n\n    return output\n\n\ndef reverse(x, axes):\n    if isinstance(axes, int):\n        axes = [axes]\n    cntk_axes = _normalize_axis(axes, x)\n    begin_index = [0 for _ in cntk_axes]\n    end_index = [0 for _ in cntk_axes]\n    strides = [-1 for _ in cntk_axes]\n    return C.slice(x, cntk_axes, begin_index, end_index, strides)\n\n\ndef slice(x, start, size):\n    raise NotImplementedError\n\n\ndef _reshape_batch(x, shape):\n    # there is a bug in cntk 2.1's unpack_batch implementation\n    if hasattr(C, 'unpack_batch') and _get_cntk_version() >= 2.2:\n        const_a = C.unpack_batch(x)\n        const_a = C.reshape(const_a, shape)\n        return C.to_batch(const_a)\n    else:\n        return C.user_function(ReshapeBatch(x, shape[1:]))\n\n\ndef _get_cntk_version():\n    version = C.__version__\n    if version.endswith('+'):\n        version = version[:-1]\n    # for hot fix, ignore all the . except the first one.\n    if len(version) > 2 and version[1] == '.':\n        version = version[:2] + version[2:].replace('.', '')\n    try:\n        return float(version)\n    except:\n        warnings.warn(\n            'CNTK backend warning: CNTK version not detected. '\n            'Will using CNTK 2.0 GA as default.')\n        return float(2.0)\n\n\nclass ReshapeBatch(C.ops.functions.UserFunction):\n    def __init__(self, input, shape, name='reshape_with_batch'):\n        super(ReshapeBatch, self).__init__([input], as_numpy=False, name=name)\n        self.from_shape = input.shape\n        self.target_shape = shape\n\n    def infer_outputs(self):\n        batch_axis = C.Axis.default_batch_axis()\n        return [\n            C.output_variable(\n                self.target_shape,\n                self.inputs[0].dtype,\n                [batch_axis])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        num_element = arguments.shape()[0] * np.prod(np.asarray(self.from_shape))\n        num_static_element = np.prod(np.asarray(self.target_shape))\n        num_batch = int(num_element / num_static_element)\n        result = arguments.data().as_shape((num_batch,) + self.target_shape)\n        return None, C.cntk_py.Value(result)\n\n    def backward(self, state, root_gradients):\n        grad_array_view = root_gradients.data()\n        num_element = root_gradients.shape()[0] * np.prod(np.asarray(self.target_shape))\n        num_static_element = np.prod(np.asarray(self.from_shape))\n        num_old_batch = int(num_element / num_static_element)\n        return C.cntk_py.Value(\n            grad_array_view.as_shape(\n                (num_old_batch,) + self.from_shape))\n\n\nclass ConvertToBatch(C.ops.functions.UserFunction):\n    \"\"\"Converts input first axis to CNTK batch axis.\n\n    We may introduce this operation in CNTK native\n    implementation later.\n\n    # Arguments\n        inputs: a cntk variable (parameter/constant)\n        name: name of this node\n    \"\"\"\n\n    def __init__(self, input, name='convert_to_batch'):\n        super(ConvertToBatch, self).__init__([input], as_numpy=False, name=name)\n\n    def infer_outputs(self):\n        batch_axis = C.Axis.default_batch_axis()\n        return [\n            C.output_variable(\n                self.inputs[0].shape[1:],\n                self.inputs[0].dtype,\n                [batch_axis])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        return None, C.cntk_py.Value(arguments.data())\n\n    def backward(self, state, root_gradients):\n        return C.cntk_py.Value(root_gradients.data())\n\n\nclass ConvertToStatic(C.ops.functions.UserFunction):\n    \"\"\"Converts input first axis to CNTK static axis.\n\n    We may introduce this operation in CNTK native\n    implementation later.\n\n    # Arguments\n        inputs: a cntk tensor which has batch axis\n        batch_size: size of batch axis.\n        name: name of this node.\n    \"\"\"\n\n    def __init__(self, input, batch_size, name='convert_to_static'):\n        super(ConvertToStatic, self).__init__([input], as_numpy=False, name=name)\n        self.target_shape = (batch_size,) + input.shape\n\n    def infer_outputs(self):\n        return [\n            C.output_variable(\n                self.target_shape,\n                self.inputs[0].dtype,\n                [])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        return None, C.cntk_py.Value(arguments.data())\n\n    def backward(self, state, root_gradients):\n        return C.cntk_py.Value(root_gradients.data())\n\n\nclass LambdaFunc(C.ops.functions.UserFunction):\n    def __init__(self,\n                 arg,\n                 when=lambda arg: True,\n                 execute=lambda arg: print(arg),\n                 name=''):\n        self.when = when\n        self.execute = execute\n\n        super(LambdaFunc, self).__init__([arg], name=name)\n\n    def infer_outputs(self):\n        return [\n            C.output_variable(\n                self.inputs[0].shape,\n                self.inputs[0].dtype,\n                self.inputs[0].dynamic_axes)]\n\n    def forward(self, argument, device=None, outputs_to_retain=None):\n        if self.when(argument):\n            self.execute(argument)\n\n        return None, argument\n\n    def backward(self, state, root_gradients):\n        return root_gradients\n",
                "max_stars_repo_path": "BugsInPy/BugsInPy/temp/projects/keras/bug-20-fixed/keras/keras/backend/cntk_backend.py,BugsInPy/BugsInPy/temp/projects/keras/bug-20-buggy/keras/keras/backend/cntk_backend.py",
                "max_stars_repo_name": "NA",
                "max_stars_count": 0,
                "__cluster__": 33
            },
            {
                "id": "pretrain_python_data_1529",
                "content": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cntk as C\nimport numpy as np\nfrom .common import floatx, epsilon, image_dim_ordering, image_data_format\nfrom collections import defaultdict\nfrom contextlib import contextmanager\nimport warnings\n\n\nC.set_global_option('align_axis', 1)\n\nb_any = any\n\n\ndev = C.device.use_default_device()\nif dev.type() == 0:\n    warnings.warn(\n        'CNTK backend warning: GPU is not detected. '\n        'CNTK\\'s CPU version is not fully optimized,'\n        'please run with GPU to get better performance.')\n\n# A learning phase is a bool tensor used to run Keras models in\n# either train mode (learning_phase == 1) or test mode (learning_phase == 0).\n# LEARNING_PHASE_PLACEHOLDER is the placeholder for dynamic learning phase\n_LEARNING_PHASE_PLACEHOLDER = C.constant(shape=(), dtype=np.float32, value=1.0, name='_keras_learning_phase')\n# static learning phase flag, if it is not 0 or 1, we will go with dynamic learning phase tensor.\n_LEARNING_PHASE = -1\n_UID_PREFIXES = defaultdict(int)\n\n# cntk doesn't support gradient as symbolic op, to hook up with keras model,\n# we will create gradient as a constant placeholder, here use this global\n# map to keep the mapping from grad placeholder to parameter\ngrad_parameter_dict = {}\n\nNAME_SCOPE_STACK = []\n\n\n@contextmanager\ndef name_scope(name):\n    global NAME_SCOPE_STACK\n    NAME_SCOPE_STACK.append(name)\n    yield\n    NAME_SCOPE_STACK.pop()\n\n\ndef get_uid(prefix=''):\n    _UID_PREFIXES[prefix] += 1\n    return _UID_PREFIXES[prefix]\n\n\ndef learning_phase():\n    # If _LEARNING_PHASE is not 0 or 1, return dynamic learning phase tensor\n    return _LEARNING_PHASE if _LEARNING_PHASE in {0, 1} else _LEARNING_PHASE_PLACEHOLDER\n\n\ndef set_learning_phase(value):\n    global _LEARNING_PHASE\n    if value not in {0, 1}:\n        raise ValueError('CNTK Backend: Set learning phase '\n                         'with value %s is not supported, '\n                         'expected 0 or 1.' % value)\n    _LEARNING_PHASE = value\n\n\ndef clear_session():\n    \"\"\"Reset learning phase flag for cntk backend.\n    \"\"\"\n    global _LEARNING_PHASE\n    global _LEARNING_PHASE_PLACEHOLDER\n    _LEARNING_PHASE = -1\n    _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(1.0)\n\n\ndef in_train_phase(x, alt, training=None):\n    global _LEARNING_PHASE\n    if training is None:\n        training = learning_phase()\n        uses_learning_phase = True\n    else:\n        uses_learning_phase = False\n\n    # CNTK currently don't support cond op, so here we use\n    # element_select approach as workaround. It may have\n    # perf issue, will resolve it later with cntk cond op.\n    if callable(x) and isinstance(x, C.cntk_py.Function) is False:\n        x = x()\n    if callable(alt) and isinstance(alt, C.cntk_py.Function) is False:\n        alt = alt()\n\n    if training is True:\n        x._uses_learning_phase = uses_learning_phase\n        return x\n    else:\n        # if _LEARNING_PHASE is static\n        if isinstance(training, int) or isinstance(training, bool):\n            result = x if training == 1 or training is True else alt\n        else:\n            result = C.element_select(training, x, alt)\n        result._uses_learning_phase = uses_learning_phase\n        return result\n\n\ndef in_test_phase(x, alt, training=None):\n    return in_train_phase(alt, x, training=training)\n\n\ndef _convert_string_dtype(dtype):\n    # cntk only support float32 and float64\n    if dtype == 'float32':\n        return np.float32\n    elif dtype == 'float64':\n        return np.float64\n    else:\n        # cntk only running with float,\n        # try to cast to float to run the model\n        return np.float32\n\n\ndef _convert_dtype_string(dtype):\n    if dtype == np.float32:\n        return 'float32'\n    elif dtype == np.float64:\n        return 'float64'\n    else:\n        raise ValueError('CNTK Backend: Unsupported dtype: %s. '\n                         'CNTK only supports float32 and '\n                         'float64.' % dtype)\n\n\ndef variable(value, dtype=None, name=None, constraint=None):\n    \"\"\"Instantiates a variable and returns it.\n\n    # Arguments\n        value: Numpy array, initial value of the tensor.\n        dtype: Tensor type.\n        name: Optional name string for the tensor.\n        constraint: Optional projection function to be\n            applied to the variable after an optimizer update.\n\n    # Returns\n        A variable instance (with Keras metadata included).\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n\n    if name is None:\n        name = ''\n\n    if isinstance(\n            value,\n            C.variables.Constant) or isinstance(\n            value,\n            C.variables.Parameter):\n        value = value.value\n\n    # we don't support init parameter with symbolic op, so eval it first as\n    # workaround\n    if isinstance(value, C.cntk_py.Function):\n        value = eval(value)\n\n    shape = value.shape if hasattr(value, 'shape') else ()\n    if hasattr(value, 'dtype') and value.dtype != dtype and len(shape) > 0:\n        value = value.astype(dtype)\n\n    # TODO: remove the conversion when cntk supports int32, int64\n    # https://docs.microsoft.com/en-us/python/api/cntk.variables.parameter\n    dtype = 'float32' if 'int' in str(dtype) else dtype\n\n    v = C.parameter(shape=shape,\n                    init=value,\n                    dtype=dtype,\n                    name=_prepare_name(name, 'variable'))\n    v._keras_shape = v.shape\n    v._uses_learning_phase = False\n    v.constraint = constraint\n    return v\n\n\ndef bias_add(x, bias, data_format=None):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    dims = len(x.shape)\n    if dims > 0 and x.shape[0] == C.InferredDimension:\n        dims -= 1\n\n    bias_dims = len(bias.shape)\n    if bias_dims != 1 and bias_dims != dims:\n        raise ValueError('Unexpected bias dimensions %d, '\n                         'expected 1 or %d dimensions' % (bias_dims, dims))\n\n    if dims == 4:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1, 1, 1)\n            else:\n                shape = (bias.shape[3],) + bias.shape[:3]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, 1, 1, bias.shape[0])\n            else:\n                shape = bias.shape\n    elif dims == 3:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1, 1)\n            else:\n                shape = (bias.shape[2],) + bias.shape[:2]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, 1, bias.shape[0])\n            else:\n                shape = bias.shape\n    elif dims == 2:\n        if data_format == 'channels_first':\n            if bias_dims == 1:\n                shape = (bias.shape[0], 1)\n            else:\n                shape = (bias.shape[1],) + bias.shape[:1]\n        elif data_format == 'channels_last':\n            if bias_dims == 1:\n                shape = (1, bias.shape[0])\n            else:\n                shape = bias.shape\n    else:\n        shape = bias.shape\n    return x + reshape(bias, shape)\n\n\ndef eval(x):\n    if isinstance(x, C.cntk_py.Function):\n        return x.eval()\n    elif isinstance(x, C.variables.Constant) or isinstance(x, C.variables.Parameter):\n        return x.value\n    else:\n        raise ValueError('CNTK Backend: `eval` method on '\n                         '`%s` type is not supported. '\n                         'CNTK only supports `eval` with '\n                         '`Function`, `Constant` or '\n                         '`Parameter`.' % type(x))\n\n\ndef placeholder(\n        shape=None,\n        ndim=None,\n        dtype=None,\n        sparse=False,\n        name=None,\n        dynamic_axis_num=1):\n    if dtype is None:\n        dtype = floatx()\n    if not shape:\n        if ndim:\n            shape = tuple([None for _ in range(ndim)])\n\n    dynamic_dimension = C.FreeDimension if _get_cntk_version() >= 2.2 else C.InferredDimension\n    cntk_shape = [dynamic_dimension if s is None else s for s in shape]\n    cntk_shape = tuple(cntk_shape)\n\n    if dynamic_axis_num > len(cntk_shape):\n        raise ValueError('CNTK backend: creating placeholder with '\n                         '%d dimension is not supported, at least '\n                         '%d dimensions are needed.'\n                         % (len(cntk_shape, dynamic_axis_num)))\n\n    if name is None:\n        name = ''\n\n    cntk_shape = cntk_shape[dynamic_axis_num:]\n\n    x = C.input(\n        shape=cntk_shape,\n        dtype=_convert_string_dtype(dtype),\n        is_sparse=sparse,\n        name=name)\n    x._keras_shape = shape\n    x._uses_learning_phase = False\n    x._cntk_placeholder = True\n    return x\n\n\ndef is_placeholder(x):\n    \"\"\"Returns whether `x` is a placeholder.\n\n    # Arguments\n        x: A candidate placeholder.\n\n    # Returns\n        Boolean.\n    \"\"\"\n    return hasattr(x, '_cntk_placeholder') and x._cntk_placeholder\n\n\ndef is_keras_tensor(x):\n    if not is_tensor(x):\n        raise ValueError('Unexpectedly found an instance of type `' +\n                         str(type(x)) + '`. '\n                         'Expected a symbolic tensor instance.')\n    return hasattr(x, '_keras_history')\n\n\ndef is_tensor(x):\n    return isinstance(x, (C.variables.Constant,\n                          C.variables.Variable,\n                          C.variables.Parameter,\n                          C.ops.functions.Function))\n\n\ndef shape(x):\n    shape = list(int_shape(x))\n    num_dynamic = _get_dynamic_axis_num(x)\n    non_dyn_shape = []\n    for i in range(len(x.shape)):\n        if shape[i + num_dynamic] is None:\n            non_dyn_shape.append(x.shape[i])\n        else:\n            non_dyn_shape.append(shape[i + num_dynamic])\n    return shape[:num_dynamic] + non_dyn_shape\n\n\ndef is_sparse(tensor):\n    return tensor.is_sparse\n\n\ndef int_shape(x):\n    if hasattr(x, '_keras_shape'):\n        return x._keras_shape\n\n    shape = x.shape\n    if hasattr(x, 'dynamic_axes'):\n        dynamic_shape = [None for a in x.dynamic_axes]\n        shape = tuple(dynamic_shape) + shape\n    return shape\n\n\ndef ndim(x):\n    shape = int_shape(x)\n    return len(shape)\n\n\ndef _prepare_name(name, default):\n    prefix = '_'.join(NAME_SCOPE_STACK)\n    if name is None or name == '':\n        return prefix + '/' + default\n    return prefix + '/' + name\n\n\ndef constant(value, dtype=None, shape=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    if shape is None:\n        shape = ()\n    np_value = value * np.ones(shape)\n    const = C.constant(np_value,\n                       dtype=dtype,\n                       name=_prepare_name(name, 'constant'))\n    const._keras_shape = const.shape\n    const._uses_learning_phase = False\n    return const\n\n\ndef random_binomial(shape, p=0.0, dtype=None, seed=None):\n    # use numpy workaround now\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e7)\n        np.random.seed(seed)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    size = 1\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n        size *= _\n\n    binomial = np.random.binomial(1, p, size).astype(dtype).reshape(shape)\n    return variable(value=binomial, dtype=dtype)\n\n\ndef random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n\n    return random_uniform_variable(shape, minval, maxval, dtype, seed)\n\n\ndef random_uniform_variable(shape, low, high,\n                            dtype=None, name=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e3)\n\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    if name is None:\n        name = ''\n\n    scale = (high - low) / 2\n    p = C.parameter(\n        shape,\n        init=C.initializer.uniform(\n            scale,\n            seed=seed),\n        dtype=dtype,\n        name=name)\n    return variable(value=p.value + low + scale)\n\n\ndef random_normal_variable(\n        shape,\n        mean,\n        scale,\n        dtype=None,\n        name=None,\n        seed=None):\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e7)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    if name is None:\n        name = ''\n\n    return C.parameter(\n        shape=shape,\n        init=C.initializer.normal(\n            scale=scale,\n            seed=seed),\n        dtype=dtype,\n        name=name)\n\n\ndef random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if dtype is None:\n        dtype = floatx()\n    for _ in shape:\n        if _ is None:\n            raise ValueError('CNTK Backend: randomness op with '\n                             'dynamic shape is not supported now. '\n                             'Please provide fixed dimension '\n                             'instead of `None`.')\n    # how to apply mean and stddev\n    return random_normal_variable(shape=shape, mean=mean, scale=1.0, seed=seed)\n\n\ndef truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    if seed is None:\n        seed = np.random.randint(1, 10e6)\n    if dtype is None:\n        dtype = np.float32\n    else:\n        dtype = _convert_string_dtype(dtype)\n\n    return C.parameter(\n        shape, init=C.initializer.truncated_normal(\n            stddev, seed=seed), dtype=dtype)\n\n\ndef dtype(x):\n    return _convert_dtype_string(x.dtype)\n\n\ndef zeros(shape, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    ctype = _convert_string_dtype(dtype)\n    return variable(value=np.zeros(shape, ctype), dtype=dtype, name=name)\n\n\ndef ones(shape, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    ctype = _convert_string_dtype(dtype)\n    return variable(value=np.ones(shape, ctype), dtype=dtype, name=name)\n\n\ndef eye(size, dtype=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    return variable(np.eye(size), dtype, name)\n\n\ndef zeros_like(x, dtype=None, name=None):\n    return x * 0\n\n\ndef ones_like(x, dtype=None, name=None):\n    return zeros_like(x) + 1\n\n\ndef count_params(x):\n    for _ in x.shape:\n        if _ == C.InferredDimension or _ == C.FreeDimension:\n            raise ValueError('CNTK backend: `count_params` with dynamic '\n                             'shape is not supported. Please provide '\n                             'fixed dimension instead of `None`.')\n\n    return np.prod(int_shape(x))\n\n\ndef cast(x, dtype):\n    # cntk calculate everything in float, so don't need case from bool / int\n    return x\n\n\ndef dot(x, y):\n    if len(x.shape) > 2 or len(y.shape) > 2:\n        y_shape = int_shape(y)\n        if len(y_shape) > 2:\n            permutation = [len(y_shape) - 2]\n            permutation += list(range(len(y_shape) - 2))\n            permutation += [len(y_shape) - 1]\n            y = C.transpose(y, perm=permutation)\n        return C.times(x, y, len(y_shape) - 1)\n    else:\n        return C.times(x, y)\n\n\ndef batch_dot(x, y, axes=None):\n    x_shape = int_shape(x)\n    y_shape = int_shape(y)\n\n    if isinstance(axes, int):\n        axes = (axes, axes)\n    if axes is None:\n        # behaves like tf.batch_matmul as default\n        axes = [len(x_shape) - 1, len(y_shape) - 2]\n    if b_any([isinstance(a, (list, tuple)) for a in axes]):\n        raise ValueError('Multiple target dimensions are not supported. ' +\n                         'Expected: None, int, (int, int), ' +\n                         'Provided: ' + str(axes))\n\n    if len(x_shape) == 2 and len(y_shape) == 2:\n        if axes[0] == axes[1]:\n            result = sum(x * y, axis=axes[0], keepdims=True)\n            return result if axes[0] == 1 else transpose(result)\n        else:\n            return sum(x * transpose(y), axis=axes[0], keepdims=True)\n    else:\n        if len(y_shape) == 2:\n            y = expand_dims(y)\n\n        normalized_axis = []\n        normalized_axis.append(_normalize_axis(axes[0], x)[0])\n        normalized_axis.append(_normalize_axis(axes[1], y)[0])\n        # transpose\n        i = normalized_axis[0]\n        while i < len(x.shape) - 1:\n            x = C.swapaxes(x, i, i + 1)\n            i += 1\n        i = normalized_axis[1]\n        while i > 0:\n            y = C.swapaxes(y, i, i - 1)\n            i -= 1\n        result = C.times(x, y, output_rank=(len(y.shape) - 1)\n                         if len(y.shape) > 1 else 1)\n        if len(y_shape) == 2:\n            result = squeeze(result, -1)\n        return result\n\n\ndef transpose(x):\n    return C.swapaxes(x, 0, 1)\n\n\ndef gather(reference, indices):\n    # There is a bug in cntk gather op which may cause crash.\n    # We have made a fix but not catched in CNTK 2.1 release.\n    # Will update with gather op in next release\n    if _get_cntk_version() >= 2.2:\n        return C.ops.gather(reference, indices)\n    else:\n        num_classes = reference.shape[0]\n        one_hot_matrix = C.ops.one_hot(indices, num_classes)\n        return C.times(one_hot_matrix, reference, output_rank=len(reference.shape) - 1)\n\n\ndef _remove_dims(x, axis, keepdims=False):\n    if keepdims is False and isinstance(axis, list):\n        # sequence axis is removed by default, so don't need reshape on it\n        reduce_axes = []\n        for a in axis:\n            if isinstance(a, C.Axis) is False:\n                reduce_axes.append(a)\n        return _reshape_dummy_dim(x, reduce_axes)\n    else:\n        if isinstance(axis, list):\n            has_seq = False\n            for a in axis:\n                if isinstance(a, C.Axis):\n                    has_seq = True\n                    break\n            if has_seq:\n                nones = _get_dynamic_axis_num(x)\n                x = expand_dims(x, nones)\n        return x\n\n\ndef max(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_max')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef min(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_min')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef sum(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_sum')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef prod(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_prod')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef logsumexp(x, axis=None, keepdims=False):\n    return log(sum(exp(x), axis=axis, keepdims=keepdims))\n\n\ndef var(x, axis=None, keepdims=False):\n    m = mean(x, axis, keepdims=True)\n    devs_squared = C.square(x - m)\n    return mean(devs_squared, axis=axis, keepdims=keepdims)\n\n\ndef std(x, axis=None, keepdims=False):\n    return C.sqrt(var(x, axis=axis, keepdims=keepdims))\n\n\ndef expand_dims(x, axis=-1):\n    shape = list(int_shape(x))\n    nones = _get_dynamic_axis_num(x)\n    index = axis if axis >= 0 else len(shape) + 1\n    shape.insert(index, 1)\n    new_shape = shape[nones:]\n    new_shape = tuple(\n        [C.InferredDimension if _ is None else _ for _ in new_shape])\n    result = C.reshape(x, new_shape)\n    if index < nones:\n        result._keras_shape = shape\n    return result\n\n\ndef squeeze(x, axis):\n    if isinstance(axis, tuple):\n        axis = list(axis)\n    if not isinstance(axis, list):\n        axis = [axis]\n\n    shape = list(int_shape(x))\n\n    _axis = []\n    for _ in axis:\n        if isinstance(_, int):\n            _axis.append(_ if _ >= 0 else _ + len(shape))\n\n    if len(_axis) == 0:\n        return x\n\n    nones = _get_dynamic_axis_num(x)\n    for _ in sorted(_axis, reverse=True):\n        del shape[_]\n\n    new_shape = shape[nones:]\n    new_shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in new_shape])\n    return C.reshape(x, new_shape)\n\n\ndef tile(x, n):\n    if isinstance(n, int):\n        n = (n,)\n    elif isinstance(n, list):\n        n = tuple(n)\n\n    shape = int_shape(x)\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    # Padding the axis\n    if len(n) < len(shape):\n        n = tuple([1 for _ in range(len(shape) - len(n))]) + n\n\n    if len(n) != len(shape):\n        raise NotImplementedError\n\n    i = num_dynamic_axis\n    for i, rep in enumerate(n):\n        if i >= num_dynamic_axis and shape[i] is not None:\n            tmp = [x] * rep\n            x = C.splice(*tmp, axis=i - num_dynamic_axis)\n        i += 1\n\n    return x\n\n\ndef _normalize_axis(axis, x):\n    shape = int_shape(x)\n    ndim = len(shape)\n\n    nones = _get_dynamic_axis_num(x)\n\n    if nones > ndim:\n        raise ValueError('CNTK Backend: tensor with keras shape: `%s` has '\n                         '%d cntk dynamic axis, this is not expected, please '\n                         'double check the keras shape history.' % (str(shape), nones))\n\n    # Current cntk does not support shape like (1, batch). so using the workaround\n    # here to mapping the correct axis. Will remove this tricky after we add support\n    # in native cntk op\n    cntk_axis = []\n    dynamic_axis_index = 0\n    for i in range(ndim):\n        if shape[i] is None and dynamic_axis_index < nones:\n            cntk_axis.append(x.dynamic_axes[dynamic_axis_index])\n            dynamic_axis_index += 1\n        else:\n            cntk_axis.append(i - dynamic_axis_index)\n\n    if dynamic_axis_index < nones:\n        i = 0\n        while dynamic_axis_index < nones:\n            cntk_axis[i] = x.dynamic_axes[dynamic_axis_index]\n            i += 1\n            dynamic_axis_index += 1\n\n        while i < len(cntk_axis):\n            cntk_axis[i] -= nones\n            i += 1\n\n    if isinstance(axis, tuple):\n        _axis = list(axis)\n    elif isinstance(axis, int):\n        _axis = [axis]\n    elif isinstance(axis, list):\n        _axis = list(axis)\n    else:\n        _axis = axis\n\n    if isinstance(_axis, list):\n        for i, a in enumerate(_axis):\n            if a is not None and a < 0:\n                _axis[i] = (a % ndim)\n            if _axis[i] is not None:\n                _axis[i] = cntk_axis[_axis[i]]\n    else:\n        if _axis is None:\n            _axis = C.Axis.all_axes()\n\n    return _axis\n\n\ndef _reshape_dummy_dim(x, axis):\n    shape = list(x.shape)\n\n    _axis = [_ + len(shape) if _ < 0 else _ for _ in axis]\n\n    if shape.count(C.InferredDimension) > 1 or shape.count(C.FreeDimension) > 1:\n        result = x\n        for index in sorted(_axis, reverse=True):\n            result = C.reshape(result,\n                               shape=(),\n                               begin_axis=index,\n                               end_axis=index + 1)\n        return result\n    else:\n        for index in sorted(_axis, reverse=True):\n            del shape[index]\n\n        shape = [C.InferredDimension if _ == C.FreeDimension else _ for _ in shape]\n        return C.reshape(x, shape)\n\n\ndef mean(x, axis=None, keepdims=False):\n    axis = _normalize_axis(axis, x)\n    output = _reduce_on_axis(x, axis, 'reduce_mean')\n\n    return _remove_dims(output, axis, keepdims)\n\n\ndef any(x, axis=None, keepdims=False):\n    reduce_result = sum(x, axis, keepdims=keepdims)\n    any_matrix = C.element_select(\n        reduce_result,\n        ones_like(reduce_result),\n        zeros_like(reduce_result))\n    if len(reduce_result.shape) == 0 and _get_dynamic_axis_num(x) == 0:\n        return C.reduce_sum(any_matrix)\n    else:\n        return any_matrix\n\n\ndef all(x, axis=None, keepdims=False):\n    reduce_result = prod(x, axis, keepdims=keepdims)\n    all_matrix = C.element_select(\n        reduce_result,\n        ones_like(reduce_result),\n        zeros_like(reduce_result))\n    if len(reduce_result.shape) == 0 and _get_dynamic_axis_num(x) == 0:\n        return C.reduce_sum(all_matrix)\n    else:\n        return all_matrix\n\n\ndef classification_error(target, output, axis=-1):\n    return C.ops.reduce_mean(\n        C.equal(\n            argmax(\n                output,\n                axis=-1),\n            argmax(\n                target,\n                axis=-1)),\n        axis=C.Axis.all_axes())\n\n\ndef argmax(x, axis=-1):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    output = C.ops.argmax(x, axis=axis[0])\n    return _reshape_dummy_dim(output, axis)\n\n\ndef argmin(x, axis=-1):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    output = C.ops.argmin(x, axis=axis[0])\n    return _reshape_dummy_dim(output, axis)\n\n\ndef square(x):\n    return C.square(x)\n\n\ndef abs(x):\n    return C.abs(x)\n\n\ndef sqrt(x):\n    return C.sqrt(x)\n\n\ndef exp(x):\n    return C.exp(x)\n\n\ndef log(x):\n    return C.log(x)\n\n\ndef round(x):\n    return C.round(x)\n\n\ndef sigmoid(x):\n    return C.sigmoid(x)\n\n\ndef sign(x):\n    return x / C.abs(x)\n\n\ndef pow(x, a):\n    return C.pow(x, a)\n\n\ndef clip(x, min_value, max_value):\n    if max_value is not None and max_value < min_value:\n        max_value = min_value\n    if max_value is None:\n        max_value = np.inf\n    if min_value is None:\n        min_value = -np.inf\n    return C.clip(x, min_value, max_value)\n\n\ndef binary_crossentropy(target, output, from_logits=False):\n    if from_logits:\n        output = C.sigmoid(output)\n    output = C.clip(output, epsilon(), 1.0 - epsilon())\n    output = -target * C.log(output) - (1.0 - target) * C.log(1.0 - output)\n    return output\n\n\ndef get_variable_shape(x):\n    return int_shape(x)\n\n\ndef update(x, new_x):\n    return C.assign(x, new_x)\n\n\ndef moving_average_update(variable, value, momentum):\n    return C.assign(variable, variable * momentum + value * (1. - momentum))\n\n\ndef update_add(x, increment):\n    result = x + increment\n    return C.assign(x, result)\n\n\ndef gradients(loss, variables):\n    # cntk does not support gradients as symbolic op,\n    # to hook up with keras model\n    # we will return a constant as place holder, the cntk learner will apply\n    # the gradient during training.\n    global grad_parameter_dict\n    if isinstance(variables, list) is False:\n        variables = [variables]\n    grads = []\n    for v in variables:\n        g = C.constant(0, shape=v.shape, name='keras_grad_placeholder')\n        grads.append(g)\n        grad_parameter_dict[g] = v\n    return grads\n\n\ndef equal(x, y):\n    return C.equal(x, y)\n\n\ndef not_equal(x, y):\n    return C.not_equal(x, y)\n\n\ndef greater(x, y):\n    return C.greater(x, y)\n\n\ndef greater_equal(x, y):\n    return C.greater_equal(x, y)\n\n\ndef less(x, y):\n    return C.less(x, y)\n\n\ndef less_equal(x, y):\n    return C.less_equal(x, y)\n\n\ndef maximum(x, y):\n    return C.element_max(x, y)\n\n\ndef minimum(x, y):\n    return C.element_min(x, y)\n\n\ndef sin(x):\n    return C.sin(x)\n\n\ndef cos(x):\n    return C.cos(x)\n\n\ndef normalize_batch_in_training(x, gamma, beta,\n                                reduction_axes, epsilon=1e-3):\n    if gamma is None:\n        if beta is None:\n            gamma = ones_like(x)\n        else:\n            gamma = ones_like(beta)\n    if beta is None:\n        if gamma is None:\n            beta = zeros_like(x)\n        else:\n            beta = zeros_like(gamma)\n\n    mean, variant = _moments(x, _normalize_axis(reduction_axes, x))\n\n    if sorted(reduction_axes) == list(range(ndim(x)))[:-1]:\n        normalized = batch_normalization(\n            x, mean, variant, beta, gamma, epsilon)\n    else:\n        # need broadcasting\n        target_shape = []\n        x_shape = int_shape(x)\n        # skip the batch axis\n        for axis in range(1, ndim(x)):\n            if axis in reduction_axes:\n                target_shape.append(1)\n                if ndim(gamma) > axis:\n                    gamma = C.reduce_mean(gamma, axis - 1)\n                    beta = C.reduce_mean(beta, axis - 1)\n            else:\n                target_shape.append(x_shape[axis])\n\n        broadcast_mean = C.reshape(mean, target_shape)\n        broadcast_var = C.reshape(variant, target_shape)\n        broadcast_gamma = C.reshape(gamma, target_shape)\n        broadcast_beta = C.reshape(beta, target_shape)\n        normalized = batch_normalization(\n            x,\n            broadcast_mean,\n            broadcast_var,\n            broadcast_beta,\n            broadcast_gamma,\n            epsilon)\n\n    return normalized, mean, variant\n\n\ndef _moments(x, axes=None, shift=None, keep_dims=False):\n    _axes = tuple(axes)\n    if shift is None:\n        shift = x\n        # Compute true mean while keeping the dims for proper broadcasting.\n        for axis in _axes:\n            shift = C.reduce_mean(shift, axis=axis)\n\n    shift = C.stop_gradient(shift)\n    shifted_mean = C.minus(x, shift)\n    for axis in _axes:\n        shifted_mean = C.reduce_mean(shifted_mean, axis=axis)\n\n    variance_mean = C.square(C.minus(x, shift))\n    for axis in _axes:\n        variance_mean = C.reduce_mean(variance_mean, axis=axis)\n\n    variance = C.minus(variance_mean, C.square(shifted_mean))\n    mean = C.plus(shifted_mean, shift)\n\n    if not keep_dims:\n        mean = squeeze(mean, _axes)\n        variance = squeeze(variance, _axes)\n\n    return mean, variance\n\n\ndef batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n    # The mean / var / beta / gamma may be processed by broadcast\n    # so it may have an extra batch axis with 1, it is not needed\n    # in cntk, need to remove those dummy axis.\n    if ndim(mean) == ndim(x) and shape(mean)[0] == 1:\n        mean = _reshape_dummy_dim(mean, [0])\n    if ndim(var) == ndim(x) and shape(var)[0] == 1:\n        var = _reshape_dummy_dim(var, [0])\n\n    if gamma is None:\n        gamma = ones_like(var)\n    elif ndim(gamma) == ndim(x) and shape(gamma)[0] == 1:\n        gamma = _reshape_dummy_dim(gamma, [0])\n\n    if beta is None:\n        beta = zeros_like(mean)\n    elif ndim(beta) == ndim(x) and shape(beta)[0] == 1:\n        beta = _reshape_dummy_dim(beta, [0])\n\n    return (x - mean) / (C.sqrt(var) + epsilon) * gamma + beta\n\n\ndef concatenate(tensors, axis=-1):\n    if len(tensors) == 0:\n        return None\n\n    axis = [axis]\n    axis = _normalize_axis(axis, tensors[0])\n    return C.splice(*tensors, axis=axis[0])\n\n\ndef flatten(x):\n    return reshape(x, (-1,))\n\n\ndef reshape(x, shape):\n    shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in shape])\n    if isinstance(x, C.variables.Parameter):\n        return C.reshape(x, shape)\n    else:\n        num_dynamic_axis = _get_dynamic_axis_num(x)\n\n        if num_dynamic_axis == 1 and len(shape) > 0 and shape[0] == -1:\n            # collapse axis with batch axis\n            if b_any(_ == C.InferredDimension for _ in x.shape) or b_any(\n                    _ == C.FreeDimension for _ in x.shape):\n                warnings.warn(\n                    'Warning: CNTK backend does not support '\n                    'collapse of batch axis with inferred dimension. '\n                    'The reshape did not take place.')\n                return x\n            return _reshape_batch(x, shape)\n        else:\n            # no collapse, then first need to padding the shape\n            if num_dynamic_axis >= len(shape):\n                i = 0\n                while i < len(shape):\n                    if shape[i] is None or shape[i] == -1:\n                        i += 1\n                    else:\n                        break\n                shape = tuple([-1 for _ in range(num_dynamic_axis - i)]) + shape\n\n            new_shape = list(shape)\n            new_shape = new_shape[num_dynamic_axis:]\n            new_shape = [C.InferredDimension if _ is None else _ for _ in new_shape]\n            return C.reshape(x, new_shape)\n\n\ndef permute_dimensions(x, pattern):\n    dims = len(int_shape(x))\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    if isinstance(pattern, list):\n        current_layout = [i for i in range(dims)]\n    else:\n        current_layout = tuple([i for i in range(dims)])\n\n    if num_dynamic_axis > 0 and pattern[:num_dynamic_axis] != current_layout[:num_dynamic_axis]:\n        raise ValueError('CNTK backend: the permute pattern %s '\n                         'requested permute on dynamic axis, '\n                         'which is not supported. Please do permute '\n                         'on static axis.' % pattern)\n\n    axis = list(pattern)\n    axis = axis[num_dynamic_axis:]\n    axis = _normalize_axis(axis, x)\n    return C.transpose(x, axis)\n\n\ndef resize_images(x, height_factor, width_factor, data_format):\n    if data_format == 'channels_first':\n        output = repeat_elements(x, height_factor, axis=2)\n        output = repeat_elements(output, width_factor, axis=3)\n        return output\n    elif data_format == 'channels_last':\n        output = repeat_elements(x, height_factor, axis=1)\n        output = repeat_elements(output, width_factor, axis=2)\n        return output\n    else:\n        raise ValueError('CNTK Backend: Invalid data_format:', data_format)\n\n\ndef resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n    if data_format == 'channels_first':\n        output = repeat_elements(x, depth_factor, axis=2)\n        output = repeat_elements(output, height_factor, axis=3)\n        output = repeat_elements(output, width_factor, axis=4)\n        return output\n    elif data_format == 'channels_last':\n        output = repeat_elements(x, depth_factor, axis=1)\n        output = repeat_elements(output, height_factor, axis=2)\n        output = repeat_elements(output, width_factor, axis=3)\n        return output\n    else:\n        raise ValueError('CNTK Backend: Invalid data_format:', data_format)\n\n\ndef repeat_elements(x, rep, axis):\n    axis = _normalize_axis(axis, x)\n    axis = axis[0]\n    slices = []\n    shape = x.shape\n    i = 0\n    while i < shape[axis]:\n        tmp = C.ops.slice(x, axis, i, i + 1)\n        for _ in range(rep):\n            slices.append(tmp)\n        i += 1\n    return C.splice(*slices, axis=axis)\n\n\ndef repeat(x, n):\n    # this is a workaround for recurrent layer\n    # if n is inferred dimension,\n    # we can't figure out how to repeat it in cntk now\n    # return the same x to take cntk broadcast feature\n    # to make the recurrent layer work.\n    # need to be fixed in GA.\n    if n is C.InferredDimension or n is C.FreeDimension:\n        return x\n    index = 1 - _get_dynamic_axis_num(x)\n    if index < 0 or index > 1:\n        raise NotImplementedError\n\n    new_shape = list(x.shape)\n    new_shape.insert(index, 1)\n    new_shape = tuple(new_shape)\n    x = C.reshape(x, new_shape)\n    temp = [x] * n\n    return C.splice(*temp, axis=index)\n\n\ndef tanh(x):\n    return C.tanh(x)\n\n\ndef _static_rnn(step_function, inputs, initial_states,\n                go_backwards=False, mask=None, constants=None,\n                unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('Input should be at least 3D.')\n\n    # if the second axis is static axis, CNTK will do unroll by default\n    if shape[1] is None:\n        raise ValueError('CNTK Backend: the input of static rnn '\n                         'has shape `%s`, the second axis '\n                         'is not static. If you want to run '\n                         'rnn with non-static axis, please try '\n                         'dynamic rnn with sequence axis.' % shape)\n    if constants is None:\n        constants = []\n\n    if mask is not None:\n        mask_shape = int_shape(mask)\n        if len(mask_shape) == dims - 1:\n            mask = expand_dims(mask)\n\n    nones = _get_dynamic_axis_num(inputs)\n\n    states = tuple(initial_states)\n\n    outputs = []\n\n    time_axis = 1 - nones if nones > 0 else 1\n\n    if go_backwards:\n        i = shape[1] - 1\n        while i >= 0:\n            current = C.ops.slice(inputs, time_axis, i, i + 1)\n            # remove dummy dimension\n            current = squeeze(current, time_axis)\n\n            output, new_states = step_function(\n                current, tuple(states) + tuple(constants))\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n\n            if mask is not None:\n                mask_slice = C.ops.slice(mask, time_axis, i, i + 1)\n                mask_slice = squeeze(mask_slice, time_axis)\n                if len(outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = outputs[-1]\n                output = C.ops.element_select(mask_slice, output, prev_output)\n\n                return_states = []\n                for s, n_s in zip(states, new_states):\n                    return_states.append(\n                        C.ops.element_select(\n                            mask_slice, n_s, s))\n                new_states = return_states\n            outputs.append(output)\n            states = new_states\n            i -= 1\n    else:\n        i = 0\n        while i < shape[1]:\n            current = C.ops.slice(inputs, time_axis, i, i + 1)\n            # remove dummy dimension\n            current = squeeze(current, 1)\n\n            output, new_states = step_function(\n                current, tuple(states) + tuple(constants))\n            if getattr(output, '_uses_learning_phase', False):\n                uses_learning_phase = True\n\n            if mask is not None:\n                mask_slice = C.ops.slice(mask, time_axis, i, i + 1)\n                mask_slice = squeeze(mask_slice, 1)\n                if len(outputs) == 0:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = outputs[-1]\n                output = C.ops.element_select(mask_slice, output, prev_output)\n\n                return_states = []\n                for s, n_s in zip(states, new_states):\n                    return_states.append(\n                        C.ops.element_select(\n                            mask_slice, n_s, s))\n                new_states = return_states\n            outputs.append(output)\n            states = new_states[:len(states)]\n            i += 1\n\n    i = 1\n    # add the time_step axis back\n    final_output = expand_dims(outputs[0], 1)\n    last_output = outputs[0]\n    while i < len(outputs):\n        # add the time_step axis back\n        output_slice = expand_dims(outputs[i], 1)\n        final_output = C.splice(final_output, output_slice, axis=time_axis)\n        last_output = outputs[i]\n        i += 1\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, states\n\n\ndef rnn(step_function, inputs, initial_states,\n        go_backwards=False, mask=None, constants=None,\n        unroll=False, input_length=None):\n\n    shape = int_shape(inputs)\n    dims = len(shape)\n\n    global uses_learning_phase\n    uses_learning_phase = False\n\n    if dims < 3:\n        raise ValueError('CNTK Backend: the input of rnn has only rank %d '\n                         'Need at least rank 3 to run RNN.' % dims)\n\n    if _get_dynamic_axis_num(inputs) == 0 or unroll:\n        return _static_rnn(\n            step_function,\n            inputs,\n            initial_states,\n            go_backwards,\n            mask,\n            constants,\n            unroll,\n            input_length)\n\n    if constants is None:\n        constants = []\n\n    num_time_step = shape[1]\n    if num_time_step is None and not has_seq_axis(inputs):\n        num_time_step = inputs.shape[0]\n\n    initial = []\n    for s in initial_states:\n        if _get_dynamic_axis_num(s) == 0:\n            if hasattr(C, 'to_batch'):\n                initial.append(C.to_batch(s))\n            else:\n                initial.append(C.user_function(ConvertToBatch(s)))\n        else:\n            initial.append(s)\n\n    need_convert = not has_seq_axis(inputs)\n    if go_backwards and need_convert is False:\n        raise NotImplementedError('CNTK Backend: `go_backwards` is not supported with '\n                                  'variable-length sequences. Please specify a '\n                                  'static length for your sequences.')\n\n    rnn_inputs = inputs\n    if need_convert:\n        if go_backwards:\n            rnn_inputs = reverse(rnn_inputs, 1)\n\n        rnn_inputs = C.to_sequence(rnn_inputs)\n\n        rnn_constants = []\n        for constant in constants:\n            if isinstance(constant, list):\n                new_c = []\n                for c in constant:\n                    if _get_dynamic_axis_num(c) == 1:\n                        new_c.append(C.sequence.broadcast_as(c, rnn_inputs))\n                    else:\n                        new_c.append(c)\n                rnn_constants.append(new_c)\n            else:\n                if _get_dynamic_axis_num(constant) == 1:\n                    rnn_constants.append(C.sequence.broadcast_as(constant, rnn_inputs))\n                else:\n                    rnn_constants.append(constant)\n    else:\n        rnn_constants = constants\n\n    if mask is not None and not has_seq_axis(mask):\n        if go_backwards:\n            mask = reverse(mask, 1)\n        if len(int_shape(mask)) == 2:\n            mask = expand_dims(mask)\n        mask = C.to_sequence_like(mask, rnn_inputs)\n\n    states = tuple(initial)\n\n    with C.default_options(axis_offset=1):\n        def _recurrence(x, states, m):\n            # create place holder\n            place_holders = [C.placeholder(dynamic_axes=x.dynamic_axes) for _ in states]\n            past_values = []\n            for s, p in zip(states, place_holders):\n                past_values.append(C.sequence.past_value(p, s))\n            new_output, new_states = step_function(\n                x, tuple(past_values) + tuple(rnn_constants))\n\n            if getattr(new_output, '_uses_learning_phase', False):\n                global uses_learning_phase\n                uses_learning_phase = True\n\n            if m is not None:\n                new_states = [C.element_select(m, n, s) for n, s in zip(new_states, past_values)]\n            n_s = []\n            for o, p in zip(new_states, place_holders):\n                n_s.append(o.replace_placeholders({p: o.output}))\n            if len(n_s) > 0:\n                new_output = n_s[0]\n            return new_output, n_s\n\n        final_output, final_states = _recurrence(rnn_inputs, states, mask)\n        last_output = C.sequence.last(final_output)\n        last_states = [C.sequence.last(s) for s in final_states]\n\n    if need_convert:\n        final_output = C.sequence.unpack(final_output, 0, no_mask_output=True)\n        if num_time_step is not None and num_time_step is not C.FreeDimension:\n            final_output = _reshape_sequence(final_output, num_time_step)\n\n    f_stats = []\n    for l_s, i_s in zip(last_states, initial_states):\n        if _get_dynamic_axis_num(i_s) == 0 and _get_dynamic_axis_num(l_s) == 1:\n            if hasattr(C, 'unpack_batch'):\n                f_stats.append(C.unpack_batch(l_s))\n            else:\n                f_stats.append(C.user_function(ConvertToStatic(l_s, batch_size=i_s.shape[0])))\n        else:\n            f_stats.append(l_s)\n\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, final_output, f_stats\n\n\ndef has_seq_axis(x):\n    return hasattr(x, 'dynamic_axes') and len(x.dynamic_axes) > 1\n\n\ndef l2_normalize(x, axis=None):\n    axis = [axis]\n    axis = _normalize_axis(axis, x)\n    norm = C.sqrt(C.reduce_sum(C.square(x), axis=axis[0]))\n    return x / norm\n\n\ndef hard_sigmoid(x):\n    x = (0.2 * x) + 0.5\n    x = C.clip(x, 0.0, 1.0)\n    return x\n\n\ndef conv1d(x, kernel, strides=1, padding='valid',\n           data_format=None, dilation_rate=1):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    if padding == 'causal':\n        # causal (dilated) convolution:\n        left_pad = dilation_rate * (kernel.shape[0] - 1)\n        x = temporal_padding(x, (left_pad, 0))\n        padding = 'valid'\n\n    if data_format == 'channels_last':\n        x = C.swapaxes(x, 0, 1)\n        kernel = C.swapaxes(kernel, 0, 2)\n\n    padding = _preprocess_border_mode(padding)\n    strides = [strides]\n    x = C.convolution(\n        kernel,\n        x,\n        strides=tuple(strides),\n        auto_padding=[\n            False,\n            padding])\n\n    if data_format == 'channels_last':\n        x = C.swapaxes(x, 0, 1)\n    return x\n\n\ndef conv2d(x, kernel, strides=(1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1)):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(\n            kernel,\n            x,\n            strides,\n            auto_padding=[\n                False,\n                padding,\n                padding])\n    else:\n        assert dilation_rate[0] == dilation_rate[1]\n        assert strides == (1, 1), 'Invalid strides for dilated convolution'\n        x = C.convolution(\n            kernel,\n            x,\n            strides=dilation_rate[0],\n            auto_padding=[\n                False,\n                padding,\n                padding])\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n                     padding='valid', data_format=None, dilation_rate=1):\n    raise NotImplementedError\n\n\ndef separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=dilation_rate[0],\n                          auto_padding=[False, padding, padding])\n        x = C.convolution(pointwise_kernel, x,\n                          strides=(1, 1, 1),\n                          auto_padding=[False])\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n                     data_format=None, dilation_rate=(1, 1)):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    x = _preprocess_conv2d_input(x, data_format)\n    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n                                 (-1, 1) + depthwise_kernel.shape[2:])\n    padding = _preprocess_border_mode(padding)\n    if dilation_rate == (1, 1):\n        strides = (1,) + strides\n        x = C.convolution(depthwise_kernel, x,\n                          strides=strides,\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n    else:\n        if dilation_rate[0] != dilation_rate[1]:\n            raise ValueError('CNTK Backend: non-square dilation_rate is '\n                             'not supported.')\n        if strides != (1, 1):\n            raise ValueError('Invalid strides for dilated convolution')\n        x = C.convolution(depthwise_kernel, x,\n                          strides=dilation_rate[0],\n                          auto_padding=[False, padding, padding],\n                          groups=x.shape[0])\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1, 1)):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = strides + (strides[0],)\n\n    x = C.convolution(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding,\n            padding])\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n                     padding='valid', data_format=None):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    x = _preprocess_conv3d_input(x, data_format)\n    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = (1,) + strides\n    # cntk output_shape does not include batch axis\n    output_shape = output_shape[1:]\n    # in keras2, need handle output shape in different format\n    if data_format == 'channels_last':\n        shape = list(output_shape)\n        shape[0] = output_shape[3]\n        shape[1] = output_shape[0]\n        shape[2] = output_shape[1]\n        shape[3] = output_shape[2]\n        output_shape = tuple(shape)\n\n    x = C.convolution_transpose(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding,\n            padding],\n        output_shape=output_shape)\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef pool2d(x, pool_size, strides=(1, 1),\n           padding='valid', data_format=None,\n           pool_mode='max'):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    padding = _preprocess_border_mode(padding)\n    strides = strides\n    pool_size = pool_size\n    x = _preprocess_conv2d_input(x, data_format)\n    if pool_mode == 'max':\n        x = C.pooling(\n            x,\n            C.MAX_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    elif pool_mode == 'avg':\n        x = C.pooling(\n            x,\n            C.AVG_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    else:\n        raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n           data_format=None, pool_mode='max'):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    padding = _preprocess_border_mode(padding)\n\n    x = _preprocess_conv3d_input(x, data_format)\n\n    if pool_mode == 'max':\n        x = C.pooling(\n            x,\n            C.MAX_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    elif pool_mode == 'avg':\n        x = C.pooling(\n            x,\n            C.AVG_POOLING,\n            pool_size,\n            strides,\n            auto_padding=[padding])\n    else:\n        raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n\n    return _postprocess_conv3d_output(x, data_format)\n\n\ndef relu(x, alpha=0., max_value=None):\n    if alpha != 0.:\n        negative_part = C.relu(-x)\n    x = C.relu(x)\n    if max_value is not None:\n        x = C.clip(x, 0.0, max_value)\n    if alpha != 0.:\n        x -= alpha * negative_part\n    return x\n\n\ndef dropout(x, level, noise_shape=None, seed=None):\n    if level < 0. or level >= 1:\n        raise ValueError('CNTK Backend: Invalid dropout level %s, '\n                         'must be in interval [0, 1].' % level)\n    return C.dropout(x, level)\n\n\ndef batch_flatten(x):\n    # cntk's batch axis is not in shape,\n    # so just flatten all the dim in x.shape\n    dim = np.prod(x.shape)\n    x = C.reshape(x, (-1,))\n    x._keras_shape = (None, dim)\n    return x\n\n\ndef softmax(x, axis=-1):\n    return C.softmax(x, axis=axis)\n\n\ndef softplus(x):\n    return C.softplus(x)\n\n\ndef softsign(x):\n    return x / (1 + C.abs(x))\n\n\ndef categorical_crossentropy(target, output, from_logits=False):\n    if from_logits:\n        result = C.cross_entropy_with_softmax(output, target)\n        # cntk's result shape is (batch, 1), while keras expect (batch, )\n        return C.reshape(result, ())\n    else:\n        # scale preds so that the class probas of each sample sum to 1\n        output /= C.reduce_sum(output, axis=-1)\n        # avoid numerical instability with epsilon clipping\n        output = C.clip(output, epsilon(), 1.0 - epsilon())\n        return -sum(target * C.log(output), axis=-1)\n\n\ndef sparse_categorical_crossentropy(target, output, from_logits=False):\n    target = C.one_hot(target, output.shape[-1])\n    target = C.reshape(target, output.shape)\n    return categorical_crossentropy(target, output, from_logits)\n\n\nclass Function(object):\n\n    def __init__(self, inputs, outputs, updates=[], **kwargs):\n        self.placeholders = inputs\n        self.trainer = None\n        self.unrelated_updates = None\n        self.updates = updates\n        if len(updates) > 0:\n            assert len(outputs) > 0\n            self.loss = outputs[0]\n            # need group update by gradient place holder\n            u_ops = []\n            unrelated_updates = []\n            for update in updates:\n                if isinstance(update, tuple):\n                    if len(update) != 2:\n                        raise NotImplementedError\n                    else:\n                        u = C.assign(update[0], update[1])\n                else:\n                    u = update\n\n                if len(u.arguments) == 0:\n                    u_ops.append(u)\n                else:\n                    unrelated_updates.append(u)\n\n            update_func = C.combine([u.output for u in u_ops])\n\n            grads = update_func.find_all_with_name('keras_grad_placeholder')\n\n            u_list = []\n            p_list = []\n            for g in grads:\n                if g in grad_parameter_dict:\n                    p_list.append(grad_parameter_dict[g])\n                    u_list.append(g)\n                else:\n                    raise ValueError(\n                        'CNTK backend: when constructing trainer, '\n                        'found gradient node `%s` which is not '\n                        'related to any parameters in the model. '\n                        'Please double check how the gradient node '\n                        'is constructed.' % g)\n\n            if len(u_list) > 0:\n                learner = C.cntk_py.universal_learner(p_list, u_list, update_func)\n\n                criterion = (\n                    outputs[0],\n                    outputs[1]) if len(outputs) > 1 else (\n                    outputs[0],\n                )\n                self.trainer = C.trainer.Trainer(\n                    outputs[0], criterion, [learner])\n                self.trainer_output = tuple([f.output for f in criterion])\n            elif len(u_ops) > 0:\n                unrelated_updates.extend(u_ops)\n\n            if len(unrelated_updates) > 0:\n                self.unrelated_updates = C.combine([_.output for _ in unrelated_updates])\n\n        if self.trainer is None:\n            self.metrics_outputs = [f.output for f in outputs]\n            self.metrics_func = C.combine(self.metrics_outputs)\n        # cntk only could handle loss and 1 metric in trainer, for metrics more\n        # than 2, need manual eval\n        elif len(outputs) > 2:\n            self.metrics_outputs = [f.output for f in outputs[2:]]\n            self.metrics_func = C.combine(self.metrics_outputs)\n        else:\n            self.metrics_func = None\n\n    @staticmethod\n    def _is_input_shape_compatible(input, placeholder):\n        if hasattr(input, 'shape') and hasattr(placeholder, 'shape'):\n            num_dynamic = get_num_dynamic_axis(placeholder)\n            input_shape = input.shape[num_dynamic:]\n            placeholder_shape = placeholder.shape\n            for i, p in zip(input_shape, placeholder_shape):\n                if i != p and p != C.InferredDimension and p != C.FreeDimension:\n                    return False\n        return True\n\n    def __call__(self, inputs):\n        global _LEARNING_PHASE_PLACEHOLDER\n        global _LEARNING_PHASE\n        assert isinstance(inputs, (list, tuple))\n        feed_dict = {}\n        for tensor, value in zip(self.placeholders, inputs):\n            # cntk only support calculate on float, do auto cast here\n            if (hasattr(value, 'dtype') and\n               value.dtype != np.float32 and\n               value.dtype != np.float64):\n                value = value.astype(np.float32)\n\n            if tensor == _LEARNING_PHASE_PLACEHOLDER:\n                _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(value)\n            else:\n                # in current version cntk can't support input with variable\n                # length. Will support it in next release.\n                if not self._is_input_shape_compatible(value, tensor):\n                    raise ValueError('CNTK backend: The placeholder has been resolved '\n                                     'to shape `%s`, but input shape is `%s`. Currently '\n                                     'CNTK can not take variable length inputs. Please '\n                                     'pass inputs that have a static shape.'\n                                     % (str(tensor.shape), str(value.shape)))\n            feed_dict[tensor] = value\n\n        updated = []\n        if self.trainer is not None:\n            input_dict = {}\n            for argument in self.loss.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError(\n                        'CNTK backend: argument %s is not found in inputs. '\n                        'Please double check the model and inputs in '\n                        '`train_function`.' % argument.name)\n\n            result = self.trainer.train_minibatch(\n                input_dict, self.trainer_output)\n\n            assert(len(result) == 2)\n            outputs = result[1]\n            for o in self.trainer_output:\n                updated.append(outputs[o])\n\n        if self.metrics_func is not None:\n            input_dict = {}\n            for argument in self.metrics_func.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError('CNTK backend: metrics argument %s '\n                                     'is not found in inputs. Please double '\n                                     'check the model and inputs.' % argument.name)\n            # Some ops (like dropout) won't be applied during \"eval\" in cntk.\n            # They only evaluated in training phase. To make it work, call\n            # \"forward\" method to let cntk know we want to evaluate them.from\n            # But the assign ops won't be executed under this mode, that's why\n            # we need this check.\n            if (self.unrelated_updates is None and\n                    (_LEARNING_PHASE_PLACEHOLDER.value == 1.0 or _LEARNING_PHASE == 1)):\n                _, output_values = self.metrics_func.forward(\n                    input_dict,\n                    self.metrics_func.outputs,\n                    (self.metrics_func.outputs[0],),\n                    as_numpy=False)\n            else:\n                output_values = self.metrics_func.eval(input_dict, as_numpy=False)\n            if isinstance(output_values, dict):\n                for o in self.metrics_outputs:\n                    value = output_values[o]\n                    v = value.asarray()\n                    updated.append(v)\n            else:\n                v = output_values.asarray()\n                for o in self.metrics_outputs:\n                    updated.append(v)\n\n        if self.unrelated_updates is not None:\n            input_dict = {}\n            for argument in self.unrelated_updates.arguments:\n                if argument in feed_dict:\n                    input_dict[argument] = feed_dict[argument]\n                else:\n                    raise ValueError(\n                        'CNTK backend: assign ops argument %s '\n                        'is not found in inputs. Please double '\n                        'check the model and inputs.' % argument.name)\n            self.unrelated_updates.eval(input_dict, as_numpy=False)\n        return updated\n\n\ndef function(inputs, outputs, updates=[], **kwargs):\n    return Function(inputs, outputs, updates=updates, **kwargs)\n\n\ndef temporal_padding(x, padding=(1, 1)):\n    assert len(padding) == 2\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    base_shape = x.shape\n    if num_dynamic_axis > 0:\n        assert len(base_shape) == 2\n        if hasattr(C, 'pad'):\n            x = C.pad(x, pattern=[padding, (0, 0)])\n        else:\n            x = _padding(x, padding, 0)\n    else:\n        assert len(base_shape) == 3\n        if hasattr(C, 'pad'):\n            x = C.pad(x, pattern=[(0, 0), padding, (0, 0)])\n        else:\n            x = _padding(x, padding, 1)\n    return x\n\n\ndef _padding(x, pattern, axis):\n    base_shape = x.shape\n    if b_any([dim < 0 for dim in base_shape]):\n        raise ValueError('CNTK Backend: padding input tensor with '\n                         'shape `%s` contains non-specified dimension, '\n                         'which is not supported. Please give fixed '\n                         'dimension to enable padding.' % base_shape)\n    if pattern[0] > 0:\n        prefix_shape = list(base_shape)\n        prefix_shape[axis] = pattern[0]\n        prefix_shape = tuple(prefix_shape)\n        x = C.splice(C.constant(value=0, shape=prefix_shape), x, axis=axis)\n        base_shape = x.shape\n    if pattern[1] > 0:\n        postfix_shape = list(base_shape)\n        postfix_shape[axis] = pattern[1]\n        postfix_shape = tuple(postfix_shape)\n        x = C.splice(x, C.constant(value=0, shape=postfix_shape), axis=axis)\n    return x\n\n\ndef spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n    assert len(padding) == 2\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    base_shape = x.shape\n    if data_format == 'channels_first':\n        if num_dynamic_axis > 0:\n            assert len(base_shape) == 3\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[[0, 0], list(padding[0]), list(padding[1])])\n            else:\n                x = _padding(x, padding[0], 1)\n                x = _padding(x, padding[1], 2)\n        else:\n            assert len(base_shape) == 4\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[[0, 0], [0, 0], list(padding[0]), list(padding[1])])\n            else:\n                x = _padding(x, padding[0], 2)\n                x = _padding(x, padding[1], 3)\n    else:\n        if num_dynamic_axis > 0:\n            assert len(base_shape) == 3\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[list(padding[0]), list(padding[1]), [0, 0]])\n            else:\n                x = _padding(x, padding[0], 0)\n                x = _padding(x, padding[1], 1)\n        else:\n            assert len(base_shape) == 4\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[[0, 0], list(padding[0]), list(padding[1]), [0, 0]])\n            else:\n                x = _padding(x, padding[0], 1)\n                x = _padding(x, padding[1], 2)\n    return x\n\n\ndef spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n    assert len(padding) == 3\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    assert len(padding[2]) == 2\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    num_dynamic_axis = _get_dynamic_axis_num(x)\n    base_shape = x.shape\n    if data_format == 'channels_first':\n        if num_dynamic_axis > 0:\n            assert len(base_shape) == 4\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[[0, 0], list(padding[0]), list(padding[1]), list(padding[2])])\n            else:\n                x = _padding(x, padding[0], 1)\n                x = _padding(x, padding[1], 2)\n                x = _padding(x, padding[2], 3)\n        else:\n            assert len(base_shape) == 5\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[[0, 0], [0, 0], list(padding[0]), list(padding[1]), list(padding[2])])\n            else:\n                x = _padding(x, padding[0], 2)\n                x = _padding(x, padding[1], 3)\n                x = _padding(x, padding[2], 4)\n    else:\n        if num_dynamic_axis > 0:\n            assert len(base_shape) == 4\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[list(padding[0]), list(padding[1]), list(padding[2]), [0, 0]])\n            else:\n                x = _padding(x, padding[0], 0)\n                x = _padding(x, padding[1], 1)\n                x = _padding(x, padding[2], 2)\n        else:\n            assert len(base_shape) == 5\n            if hasattr(C, 'pad'):\n                x = C.pad(x, pattern=[[0, 0], list(padding[0]), list(padding[1]), list(padding[2]), [0, 0]])\n            else:\n                x = _padding(x, padding[0], 1)\n                x = _padding(x, padding[1], 2)\n                x = _padding(x, padding[2], 3)\n    return x\n\n\ndef one_hot(indices, num_classes):\n    return C.one_hot(indices, num_classes)\n\n\ndef get_value(x):\n    if isinstance(\n            x,\n            C.variables.Parameter) or isinstance(\n            x,\n            C.variables.Constant):\n        return x.value\n    else:\n        return eval(x)\n\n\ndef batch_get_value(xs):\n    result = []\n    for x in xs:\n        if (isinstance(x, C.variables.Parameter) or\n           isinstance(x, C.variables.Constant)):\n            result.append(x.value)\n        else:\n            result.append(eval(x))\n    return result\n\n\ndef set_value(x, value):\n    if (isinstance(x, C.variables.Parameter) or\n       isinstance(x, C.variables.Constant)):\n        if isinstance(value, (float, int)):\n            value = np.full(x.shape, value, dtype=floatx())\n        x.value = value\n    else:\n        raise NotImplementedError\n\n\ndef print_tensor(x, message=''):\n    return C.user_function(\n        LambdaFunc(x,\n                   when=lambda x: True,\n                   execute=lambda x: print(message)))\n\n\ndef batch_set_value(tuples):\n    for t in tuples:\n        x = t[0]\n        value = t[1]\n        if isinstance(value, np.ndarray) is False:\n            value = np.asarray(value)\n        if isinstance(x, C.variables.Parameter):\n            x.value = value\n        else:\n            raise NotImplementedError\n\n\ndef stop_gradient(variables):\n    if isinstance(variables, (list, tuple)):\n        return map(C.stop_gradient, variables)\n    else:\n        return C.stop_gradient(variables)\n\n\ndef switch(condition, then_expression, else_expression):\n    ndim_cond = ndim(condition)\n    ndim_expr = ndim(then_expression)\n    if ndim_cond > ndim_expr:\n        raise ValueError('Rank of condition should be less'\n                         ' than or equal to rank of then and'\n                         ' else expressions. ndim(condition)=' +\n                         str(ndim_cond) + ', ndim(then_expression)'\n                         '=' + str(ndim_expr))\n    elif ndim_cond < ndim_expr:\n        shape_expr = int_shape(then_expression)\n        ndim_diff = ndim_expr - ndim_cond\n        for i in range(ndim_diff):\n            condition = expand_dims(condition)\n            condition = tile(condition, shape_expr[ndim_cond + i])\n    return C.element_select(condition,\n                            then_expression,\n                            else_expression)\n\n\ndef elu(x, alpha=1.):\n    res = C.elu(x)\n    if alpha == 1:\n        return res\n    else:\n        return C.element_select(C.greater(x, 0), res, alpha * res)\n\n\ndef in_top_k(predictions, targets, k):\n    _targets = C.one_hot(targets, predictions.shape[-1])\n    result = C.classification_error(predictions, _targets, topN=k)\n    return 1 - C.reshape(result, shape=())\n\n\ndef conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    x = _preprocess_conv2d_input(x, data_format)\n    kernel = _preprocess_conv2d_kernel(kernel, data_format)\n    padding = _preprocess_border_mode(padding)\n    strides = (1,) + strides\n    # cntk output_shape does not include batch axis\n    output_shape = output_shape[1:]\n    # in keras2, need handle output shape in different format\n    if data_format == 'channels_last':\n        shape = list(output_shape)\n        shape[0] = output_shape[2]\n        shape[1] = output_shape[0]\n        shape[2] = output_shape[1]\n        output_shape = tuple(shape)\n\n    x = C.convolution_transpose(\n        kernel,\n        x,\n        strides,\n        auto_padding=[\n            False,\n            padding,\n            padding],\n        output_shape=output_shape)\n    return _postprocess_conv2d_output(x, data_format)\n\n\ndef identity(x, name=None):\n    if name is None:\n        name = '%s_alias' % x.name\n    return C.alias(x, name=name)\n\n\ndef _preprocess_conv2d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, rows, cols)\n        # TF input shape: (samples, rows, cols, input_depth)\n        x = C.transpose(x, (2, 0, 1))\n    return x\n\n\ndef _preprocess_conv2d_kernel(kernel, data_format):\n    # As of Keras 2.0.0, all kernels are normalized\n    # on the format `(rows, cols, input_depth, depth)`,\n    # independently of `data_format`.\n    # CNTK expects `(depth, input_depth, rows, cols)`.\n    kernel = C.transpose(kernel, (3, 2, 0, 1))\n    return kernel\n\n\ndef _preprocess_border_mode(padding):\n    if padding == 'same':\n        padding = True\n    elif padding == 'valid':\n        padding = False\n    else:\n        raise ValueError('Invalid border mode: ' + str(padding))\n    return padding\n\n\ndef _postprocess_conv2d_output(x, data_format):\n    if data_format == 'channels_last':\n        x = C.transpose(x, (1, 2, 0))\n    return x\n\n\ndef _preprocess_conv3d_input(x, data_format):\n    if data_format == 'channels_last':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, conv_dim1, conv_dim2, conv_dim3)\n        # TF input shape: (samples, conv_dim1, conv_dim2, conv_dim3,\n        # input_depth)\n        x = C.transpose(x, (3, 0, 1, 2))\n    return x\n\n\ndef _preprocess_conv3d_kernel(kernel, dim_ordering):\n    kernel = C.transpose(kernel, (4, 3, 0, 1, 2))\n    return kernel\n\n\ndef _postprocess_conv3d_output(x, dim_ordering):\n    if dim_ordering == 'channels_last':\n        x = C.transpose(x, (1, 2, 3, 0))\n    return x\n\n\ndef _get_dynamic_axis_num(x):\n    if hasattr(x, 'dynamic_axes'):\n        return len(x.dynamic_axes)\n    else:\n        return 0\n\n\ndef _contain_seqence_axis(x):\n    if _get_dynamic_axis_num(x) > 1:\n        return x.dynamic_axes[1] == C.Axis.default_dynamic_axis()\n    else:\n        return False\n\n\ndef get_num_dynamic_axis(x):\n    return _get_dynamic_axis_num(x)\n\n\ndef _reduce_on_axis(x, axis, reduce_fun_name):\n    if isinstance(axis, list):\n        for a in axis:\n            if isinstance(a, C.Axis) \\\n                    and a != C.Axis.default_batch_axis() \\\n                    and hasattr(C.sequence, reduce_fun_name):\n                x = getattr(C.sequence, reduce_fun_name)(x, a)\n            else:\n                x = getattr(C, reduce_fun_name)(x, a)\n    else:\n        x = getattr(C, reduce_fun_name)(x, axis)\n    return x\n\n\ndef _reshape_sequence(x, time_step):\n    tmp_shape = list(int_shape(x))\n    tmp_shape[1] = time_step\n    return reshape(x, tmp_shape)\n\n\ndef local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    stride = strides[0]\n    kernel_shape = int_shape(kernel)\n    output_length, feature_dim, filters = kernel_shape\n\n    xs = []\n    for i in range(output_length):\n        slice_length = slice(i * stride,\n                             i * stride + kernel_size[0])\n        xs.append(reshape(inputs[:, slice_length, :],\n                          (-1, 1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=1)\n    # transpose kernel to output_filters first, to apply broadcast\n    weight = permute_dimensions(kernel, (2, 0, 1))\n    # Shape: (batch, filters, output_length, input_length * kernel_size)\n    output = x_aggregate * weight\n    # Shape: (batch, filters, output_length)\n    output = sum(output, axis=3)\n    # Shape: (batch, output_length, filters)\n    return permute_dimensions(output, (0, 2, 1))\n\n\ndef local_conv2d(inputs,\n                 kernel,\n                 kernel_size,\n                 strides,\n                 output_shape,\n                 data_format=None):\n    if data_format is None:\n        data_format = image_data_format()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('Unknown data_format ' + str(data_format))\n\n    stride_row, stride_col = strides\n    output_row, output_col = output_shape\n    kernel_shape = int_shape(kernel)\n    _, feature_dim, filters = kernel_shape\n    xs = []\n\n    for i in range(output_row):\n        for j in range(output_col):\n            slice_row = slice(i * stride_row,\n                              i * stride_row + kernel_size[0])\n            slice_col = slice(j * stride_col,\n                              j * stride_col + kernel_size[1])\n            if data_format == 'channels_first':\n                xs.append(reshape(inputs[:, :, slice_row, slice_col],\n                                  (-1, 1, feature_dim)))\n            else:\n                xs.append(reshape(inputs[:, slice_row, slice_col, :],\n                                  (-1, 1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=1)\n    # transpose kernel to put filters first\n    weight = permute_dimensions(kernel, (2, 0, 1))\n    # shape: batch, filters, output_length, input_length * kernel_size\n    output = x_aggregate * weight\n    # shape: batch, filters, output_length\n    output = sum(output, axis=3)\n    # shape: batch, filters, row, col\n    output = reshape(output,\n                     (-1, filters, output_row, output_col))\n\n    if data_format == 'channels_last':\n        # shape: batch, row, col, filters\n        output = permute_dimensions(output, (0, 2, 3, 1))\n\n    return output\n\n\ndef reverse(x, axes):\n    if isinstance(axes, int):\n        axes = [axes]\n    cntk_axes = _normalize_axis(axes, x)\n    begin_index = [0 for _ in cntk_axes]\n    end_index = [0 for _ in cntk_axes]\n    strides = [-1 for _ in cntk_axes]\n    return C.slice(x, cntk_axes, begin_index, end_index, strides)\n\n\ndef _reshape_batch(x, shape):\n    # there is a bug in cntk 2.1's unpack_batch implementation\n    if hasattr(C, 'unpack_batch') and _get_cntk_version() >= 2.2:\n        const_a = C.unpack_batch(x)\n        const_a = C.reshape(const_a, shape)\n        return C.to_batch(const_a)\n    else:\n        return C.user_function(ReshapeBatch(x, shape[1:]))\n\n\ndef _get_cntk_version():\n    version = C.__version__\n    if version.endswith('+'):\n        version = version[:-1]\n    # for hot fix, ignore all the . except the first one.\n    if len(version) > 2 and version[1] == '.':\n        version = version[:2] + version[2:].replace('.', '')\n    try:\n        return float(version)\n    except:\n        warnings.warn(\n            'CNTK backend warning: CNTK version not detected. '\n            'Will using CNTK 2.0 GA as default.')\n        return float(2.0)\n\n\nclass ReshapeBatch(C.ops.functions.UserFunction):\n    def __init__(self, input, shape, name='reshape_with_batch'):\n        super(ReshapeBatch, self).__init__([input], as_numpy=False, name=name)\n        self.from_shape = input.shape\n        self.target_shape = shape\n\n    def infer_outputs(self):\n        batch_axis = C.Axis.default_batch_axis()\n        return [\n            C.output_variable(\n                self.target_shape,\n                self.inputs[0].dtype,\n                [batch_axis])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        num_element = arguments.shape()[0] * np.prod(np.asarray(self.from_shape))\n        num_static_element = np.prod(np.asarray(self.target_shape))\n        num_batch = int(num_element / num_static_element)\n        result = arguments.data().as_shape((num_batch,) + self.target_shape)\n        return None, C.cntk_py.Value(result)\n\n    def backward(self, state, root_gradients):\n        grad_array_view = root_gradients.data()\n        num_element = root_gradients.shape()[0] * np.prod(np.asarray(self.target_shape))\n        num_static_element = np.prod(np.asarray(self.from_shape))\n        num_old_batch = int(num_element / num_static_element)\n        return C.cntk_py.Value(\n            grad_array_view.as_shape(\n                (num_old_batch,) + self.from_shape))\n\n\nclass ConvertToBatch(C.ops.functions.UserFunction):\n    \"\"\"Converts input first axis to CNTK batch axis.\n\n    We may introduce this operation in CNTK native\n    implementation later.\n\n    # Arguments\n        inputs: a cntk variable (parameter/constant)\n        name: name of this node\n    \"\"\"\n\n    def __init__(self, input, name='convert_to_batch'):\n        super(ConvertToBatch, self).__init__([input], as_numpy=False, name=name)\n\n    def infer_outputs(self):\n        batch_axis = C.Axis.default_batch_axis()\n        return [\n            C.output_variable(\n                self.inputs[0].shape[1:],\n                self.inputs[0].dtype,\n                [batch_axis])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        return None, C.cntk_py.Value(arguments.data())\n\n    def backward(self, state, root_gradients):\n        return C.cntk_py.Value(root_gradients.data())\n\n\nclass ConvertToStatic(C.ops.functions.UserFunction):\n    \"\"\"Converts input first axis to CNTK static axis.\n\n    We may introduce this operation in CNTK native\n    implementation later.\n\n    # Arguments\n        inputs: a cntk tensor which has batch axis\n        batch_size: size of batch axis.\n        name: name of this node.\n    \"\"\"\n\n    def __init__(self, input, batch_size, name='convert_to_static'):\n        super(ConvertToStatic, self).__init__([input], as_numpy=False, name=name)\n        self.target_shape = (batch_size,) + input.shape\n\n    def infer_outputs(self):\n        return [\n            C.output_variable(\n                self.target_shape,\n                self.inputs[0].dtype,\n                [])]\n\n    def forward(self, arguments, device=None, outputs_to_retain=None):\n        return None, C.cntk_py.Value(arguments.data())\n\n    def backward(self, state, root_gradients):\n        return C.cntk_py.Value(root_gradients.data())\n\n\nclass LambdaFunc(C.ops.functions.UserFunction):\n    def __init__(self,\n                 arg,\n                 when=lambda arg: True,\n                 execute=lambda arg: print(arg),\n                 name=''):\n        self.when = when\n        self.execute = execute\n\n        super(LambdaFunc, self).__init__([arg], name=name)\n\n    def infer_outputs(self):\n        return [\n            C.output_variable(\n                self.inputs[0].shape,\n                self.inputs[0].dtype,\n                self.inputs[0].dynamic_axes)]\n\n    def forward(self, argument, device=None, outputs_to_retain=None):\n        if self.when(argument):\n            self.execute(argument)\n\n        return None, argument\n\n    def backward(self, state, root_gradients):\n        return root_gradients\n",
                "max_stars_repo_path": "deep_learning/keras/keras/backend/cntk_backend.py",
                "max_stars_repo_name": "xpennec/applications",
                "max_stars_count": 21,
                "__cluster__": 33
            }
        ]
    ],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [
        [
            {
                "id": "test_pandas-bug-21",
                "content": "\"\"\"\nData structure for 1-dimensional cross-sectional and time series data\n\"\"\"\nfrom io import StringIO\nfrom shutil import get_terminal_size\nfrom textwrap import dedent\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import lib, properties, reshape, tslibs\nfrom pandas._typing import ArrayLike, Axis, DtypeObj, Label\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import Appender, Substitution, doc\nfrom pandas.util._validators import validate_bool_kwarg, validate_percentile\n\nfrom pandas.core.dtypes.cast import (\n    convert_dtypes,\n    maybe_cast_to_extension_array,\n    validate_numeric_casting,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_platform_int,\n    is_bool,\n    is_categorical_dtype,\n    is_dict_like,\n    is_extension_array_dtype,\n    is_integer,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_scalar,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame,\n    ABCDatetimeIndex,\n    ABCMultiIndex,\n    ABCPeriodIndex,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.inference import is_hashable\nfrom pandas.core.dtypes.missing import (\n    isna,\n    na_value_for_dtype,\n    notna,\n    remove_na_arraylike,\n)\n\nimport pandas as pd\nfrom pandas.core import algorithms, base, generic, nanops, ops\nfrom pandas.core.accessor import CachedAccessor\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.arrays.categorical import CategoricalAccessor\nfrom pandas.core.arrays.sparse import SparseAccessor\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    create_series_with_explicit_dtype,\n    extract_array,\n    is_empty_data,\n    sanitize_array,\n)\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.indexers import unpack_1tuple\nfrom pandas.core.indexes.accessors import CombinedDatetimelikeProperties\nfrom pandas.core.indexes.api import (\n    Float64Index,\n    Index,\n    InvalidIndexError,\n    MultiIndex,\n    ensure_index,\n)\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.datetimes import DatetimeIndex\nfrom pandas.core.indexes.period import PeriodIndex\nfrom pandas.core.indexes.timedeltas import TimedeltaIndex\nfrom pandas.core.indexing import check_bool_indexer\nfrom pandas.core.internals import SingleBlockManager\nfrom pandas.core.strings import StringMethods\nfrom pandas.core.tools.datetimes import to_datetime\n\nimport pandas.io.formats.format as fmt\nimport pandas.plotting\n\nif TYPE_CHECKING:\n    from pandas.core.frame import DataFrame\n    from pandas.core.groupby.generic import SeriesGroupBy\n\n__all__ = [\"Series\"]\n\n_shared_doc_kwargs = dict(\n    axes=\"index\",\n    klass=\"Series\",\n    axes_single_arg=\"{0 or 'index'}\",\n    axis=\"\"\"axis : {0 or 'index'}\n        Parameter needed for compatibility with DataFrame.\"\"\",\n    inplace=\"\"\"inplace : boolean, default False\n        If True, performs operation inplace and returns None.\"\"\",\n    unique=\"np.ndarray\",\n    duplicated=\"Series\",\n    optional_by=\"\",\n    optional_mapper=\"\",\n    optional_labels=\"\",\n    optional_axis=\"\",\n    versionadded_to_excel=\"\\n    .. versionadded:: 0.20.0\\n\",\n)\n\n\ndef _coerce_method(converter):\n    \"\"\"\n    Install the scalar coercion methods.\n    \"\"\"\n\n    def wrapper(self):\n        if len(self) == 1:\n            return converter(self.iloc[0])\n        raise TypeError(f\"cannot convert the series to {converter}\")\n\n    wrapper.__name__ = f\"__{converter.__name__}__\"\n    return wrapper\n\n\n# ----------------------------------------------------------------------\n# Series class\n\n\nclass Series(base.IndexOpsMixin, generic.NDFrame):\n    \"\"\"\n    One-dimensional ndarray with axis labels (including time series).\n\n    Labels need not be unique but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for performing operations involving the index. Statistical\n    methods from ndarray have been overridden to automatically exclude\n    missing data (currently represented as NaN).\n\n    Operations between Series (+, -, /, *, **) align values based on their\n    associated index values-- they need not be the same length. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Series.\n\n        .. versionchanged:: 0.23.0\n           If data is a dict, argument order is maintained for Python 3.6\n           and later.\n\n    index : array-like or Index (1d)\n        Values must be hashable and have the same length as `data`.\n        Non-unique index values are allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If both a dict and index\n        sequence are used, the index will override the keys found in the\n        dict.\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\n        Data type for the output Series. If not specified, this will be\n        inferred from `data`.\n        See the :ref:`user guide <basics.dtypes>` for more usages.\n    name : str, optional\n        The name to give to the Series.\n    copy : bool, default False\n        Copy input data.\n    \"\"\"\n\n    _typ = \"series\"\n\n    _name: Label\n    _metadata: List[str] = [\"name\"]\n    _internal_names_set = {\"index\"} | generic.NDFrame._internal_names_set\n    _accessors = {\"dt\", \"cat\", \"str\", \"sparse\"}\n    _deprecations = (\n        base.IndexOpsMixin._deprecations\n        | generic.NDFrame._deprecations\n        | frozenset([\"compress\", \"ptp\"])\n    )\n\n    # Override cache_readonly bc Series is mutable\n    hasnans = property(\n        base.IndexOpsMixin.hasnans.func, doc=base.IndexOpsMixin.hasnans.__doc__\n    )\n    _mgr: SingleBlockManager\n    div: Callable[[\"Series\", Any], \"Series\"]\n    rdiv: Callable[[\"Series\", Any], \"Series\"]\n\n    # ----------------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self, data=None, index=None, dtype=None, name=None, copy=False, fastpath=False\n    ):\n\n        if (\n            isinstance(data, SingleBlockManager)\n            and index is None\n            and dtype is None\n            and copy is False\n        ):\n            # GH#33357 called with just the SingleBlockManager\n            NDFrame.__init__(self, data)\n            self.name = name\n            return\n\n        # we are called internally, so short-circuit\n        if fastpath:\n\n            # data is an ndarray, index is defined\n            if not isinstance(data, SingleBlockManager):\n                data = SingleBlockManager.from_array(data, index)\n            if copy:\n                data = data.copy()\n            if index is None:\n                index = data.index\n\n        else:\n\n            name = ibase.maybe_extract_name(name, data, type(self))\n\n            if is_empty_data(data) and dtype is None:\n                # gh-17261\n                warnings.warn(\n                    \"The default dtype for empty Series will be 'object' instead \"\n                    \"of 'float64' in a future version. Specify a dtype explicitly \"\n                    \"to silence this warning.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n                # uncomment the line below when removing the DeprecationWarning\n                # dtype = np.dtype(object)\n\n            if index is not None:\n                index = ensure_index(index)\n\n            if data is None:\n                data = {}\n            if dtype is not None:\n                dtype = self._validate_dtype(dtype)\n\n            if isinstance(data, MultiIndex):\n                raise NotImplementedError(\n                    \"initializing a Series from a MultiIndex is not supported\"\n                )\n            elif isinstance(data, Index):\n\n                if dtype is not None:\n                    # astype copies\n                    data = data.astype(dtype)\n                else:\n                    # need to copy to avoid aliasing issues\n                    data = data._values.copy()\n                    if isinstance(data, ABCDatetimeIndex) and data.tz is not None:\n                        # GH#24096 need copy to be deep for datetime64tz case\n                        # TODO: See if we can avoid these copies\n                        data = data._values.copy(deep=True)\n                copy = False\n\n            elif isinstance(data, np.ndarray):\n                if len(data.dtype):\n                    # GH#13296 we are dealing with a compound dtype, which\n                    #  should be treated as 2D\n                    raise ValueError(\n                        \"Cannot construct a Series from an ndarray with \"\n                        \"compound dtype.  Use DataFrame instead.\"\n                    )\n                pass\n            elif isinstance(data, ABCSeries):\n                if index is None:\n                    index = data.index\n                else:\n                    data = data.reindex(index, copy=copy)\n                data = data._mgr\n            elif is_dict_like(data):\n                data, index = self._init_dict(data, index, dtype)\n                dtype = None\n                copy = False\n            elif isinstance(data, SingleBlockManager):\n                if index is None:\n                    index = data.index\n                elif not data.index.equals(index) or copy:\n                    # GH#19275 SingleBlockManager input should only be called\n                    # internally\n                    raise AssertionError(\n                        \"Cannot pass both SingleBlockManager \"\n                        \"`data` argument and a different \"\n                        \"`index` argument. `copy` must be False.\"\n                    )\n\n            elif is_extension_array_dtype(data):\n                pass\n            elif isinstance(data, (set, frozenset)):\n                raise TypeError(f\"'{type(data).__name__}' type is unordered\")\n            else:\n                data = com.maybe_iterable_to_list(data)\n\n            if index is None:\n                if not is_list_like(data):\n                    data = [data]\n                index = ibase.default_index(len(data))\n            elif is_list_like(data):\n\n                # a scalar numpy array is list-like but doesn't\n                # have a proper length\n                try:\n                    if len(index) != len(data):\n                        raise ValueError(\n                            f\"Length of passed values is {len(data)}, \"\n                            f\"index implies {len(index)}.\"\n                        )\n                except TypeError:\n                    pass\n\n            # create/copy the manager\n            if isinstance(data, SingleBlockManager):\n                if dtype is not None:\n                    data = data.astype(dtype=dtype, errors=\"ignore\", copy=copy)\n                elif copy:\n                    data = data.copy()\n            else:\n                data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True)\n\n                data = SingleBlockManager.from_array(data, index)\n\n        generic.NDFrame.__init__(self, data)\n        self.name = name\n        self._set_axis(0, index, fastpath=True)\n\n    def _init_dict(self, data, index=None, dtype=None):\n        \"\"\"\n        Derive the \"_mgr\" and \"index\" attributes of a new Series from a\n        dictionary input.\n\n        Parameters\n        ----------\n        data : dict or dict-like\n            Data used to populate the new Series.\n        index : Index or index-like, default None\n            Index for the new Series: if None, use dict keys.\n        dtype : dtype, default None\n            The dtype for the new Series: if None, infer from data.\n\n        Returns\n        -------\n        _data : BlockManager for the new Series\n        index : index for the new Series\n        \"\"\"\n        # Looking for NaN in dict doesn't work ({np.nan : 1}[float('nan')]\n        # raises KeyError), so we iterate the entire dict, and align\n        if data:\n            keys, values = zip(*data.items())\n            values = list(values)\n        elif index is not None:\n            # fastpath for Series(data=None). Just use broadcasting a scalar\n            # instead of reindexing.\n            values = na_value_for_dtype(dtype)\n            keys = index\n        else:\n            keys, values = [], []\n\n        # Input is now list-like, so rely on \"standard\" construction:\n\n        # TODO: passing np.float64 to not break anything yet. See GH-17261\n        s = create_series_with_explicit_dtype(\n            values, index=keys, dtype=dtype, dtype_if_empty=np.float64\n        )\n\n        # Now we just make sure the order is respected, if any\n        if data and index is not None:\n            s = s.reindex(index, copy=False)\n        return s._mgr, s.index\n\n    # ----------------------------------------------------------------------\n\n    @property\n    def _constructor(self) -> Type[\"Series\"]:\n        return Series\n\n    @property\n    def _constructor_expanddim(self) -> Type[\"DataFrame\"]:\n        from pandas.core.frame import DataFrame\n\n        return DataFrame\n\n    # types\n    @property\n    def _can_hold_na(self):\n        return self._mgr._can_hold_na\n\n    _index = None\n\n    def _set_axis(self, axis: int, labels, fastpath: bool = False) -> None:\n        \"\"\"\n        Override generic, we want to set the _typ here.\n\n        This is called from the cython code when we set the `index` attribute\n        directly, e.g. `series.index = [1, 2, 3]`.\n        \"\"\"\n        if not fastpath:\n            labels = ensure_index(labels)\n\n        is_all_dates = labels.is_all_dates\n        if is_all_dates:\n            if not isinstance(labels, (DatetimeIndex, PeriodIndex, TimedeltaIndex)):\n                try:\n                    labels = DatetimeIndex(labels)\n                    # need to set here because we changed the index\n                    if fastpath:\n                        self._mgr.set_axis(axis, labels)\n                except (tslibs.OutOfBoundsDatetime, ValueError):\n                    # labels may exceeds datetime bounds,\n                    # or not be a DatetimeIndex\n                    pass\n\n        object.__setattr__(self, \"_index\", labels)\n        if not fastpath:\n            # The ensure_index call above ensures we have an Index object\n            self._mgr.set_axis(axis, labels)\n\n    # ndarray compatibility\n    @property\n    def dtype(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._mgr.dtype\n\n    @property\n    def dtypes(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        # DataFrame compatibility\n        return self.dtype\n\n    @property\n    def name(self) -> Label:\n        \"\"\"\n        Return the name of the Series.\n\n        The name of a Series becomes its index or column name if it is used\n        to form a DataFrame. It is also used whenever displaying the Series\n        using the interpreter.\n\n        Returns\n        -------\n        label (hashable object)\n            The name of the Series, also the column name if part of a DataFrame.\n\n        See Also\n        --------\n        Series.rename : Sets the Series name when given a scalar input.\n        Index.name : Corresponding Index property.\n\n        Examples\n        --------\n        The Series name can be set initially when calling the constructor.\n\n        >>> s = pd.Series([1, 2, 3], dtype=np.int64, name='Numbers')\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Numbers, dtype: int64\n        >>> s.name = \"Integers\"\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Integers, dtype: int64\n\n        The name of a Series within a DataFrame is its column name.\n\n        >>> df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],\n        ...                   columns=[\"Odd Numbers\", \"Even Numbers\"])\n        >>> df\n           Odd Numbers  Even Numbers\n        0            1             2\n        1            3             4\n        2            5             6\n        >>> df[\"Even Numbers\"].name\n        'Even Numbers'\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value: Label) -> None:\n        if not is_hashable(value):\n            raise TypeError(\"Series.name must be a hashable type\")\n        object.__setattr__(self, \"_name\", value)\n\n    @property\n    def values(self):\n        \"\"\"\n        Return Series as ndarray or ndarray-like depending on the dtype.\n\n        .. warning::\n\n           We recommend using :attr:`Series.array` or\n           :meth:`Series.to_numpy`, depending on whether you need\n           a reference to the underlying data or a NumPy array.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n\n        See Also\n        --------\n        Series.array : Reference to the underlying data.\n        Series.to_numpy : A NumPy array representing the underlying data.\n\n        Examples\n        --------\n        >>> pd.Series([1, 2, 3]).values\n        array([1, 2, 3])\n\n        >>> pd.Series(list('aabc')).values\n        array(['a', 'a', 'b', 'c'], dtype=object)\n\n        >>> pd.Series(list('aabc')).astype('category').values\n        [a, a, b, c]\n        Categories (3, object): [a, b, c]\n\n        Timezone aware datetime data is converted to UTC:\n\n        >>> pd.Series(pd.date_range('20130101', periods=3,\n        ...                         tz='US/Eastern')).values\n        array(['2013-01-01T05:00:00.000000000',\n               '2013-01-02T05:00:00.000000000',\n               '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')\n        \"\"\"\n        return self._mgr.external_values()\n\n    @property\n    def _values(self):\n        \"\"\"\n        Return the internal repr of this data (defined by Block.interval_values).\n        This are the values as stored in the Block (ndarray or ExtensionArray\n        depending on the Block class), with datetime64[ns] and timedelta64[ns]\n        wrapped in ExtensionArrays to match Index._values behavior.\n\n        Differs from the public ``.values`` for certain data types, because of\n        historical backwards compatibility of the public attribute (e.g. period\n        returns object ndarray and datetimetz a datetime64[ns] ndarray for\n        ``.values`` while it returns an ExtensionArray for ``._values`` in those\n        cases).\n\n        Differs from ``.array`` in that this still returns the numpy array if\n        the Block is backed by a numpy array (except for datetime64 and\n        timedelta64 dtypes), while ``.array`` ensures to always return an\n        ExtensionArray.\n\n        Overview:\n\n        dtype       | values        | _values       | array         |\n        ----------- | ------------- | ------------- | ------------- |\n        Numeric     | ndarray       | ndarray       | PandasArray   |\n        Category    | Categorical   | Categorical   | Categorical   |\n        dt64[ns]    | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        dt64[ns tz] | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        td64[ns]    | ndarray[m8ns] | TimedeltaArray| ndarray[m8ns] |\n        Period      | ndarray[obj]  | PeriodArray   | PeriodArray   |\n        Nullable    | EA            | EA            | EA            |\n\n        \"\"\"\n        return self._mgr.internal_values()\n\n    @Appender(base.IndexOpsMixin.array.__doc__)  # type: ignore\n    @property\n    def array(self) -> ExtensionArray:\n        return self._mgr._block.array_values()\n\n    # ops\n    def ravel(self, order=\"C\"):\n        \"\"\"\n        Return the flattened underlying data as an ndarray.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n            Flattened data of the Series.\n\n        See Also\n        --------\n        numpy.ndarray.ravel : Return a flattened array.\n        \"\"\"\n        return self._values.ravel(order=order)\n\n    def __len__(self) -> int:\n        \"\"\"\n        Return the length of the Series.\n        \"\"\"\n        return len(self._mgr)\n\n    def view(self, dtype=None) -> \"Series\":\n        \"\"\"\n        Create a new view of the Series.\n\n        This function will return a new Series with a view of the same\n        underlying values in memory, optionally reinterpreted with a new data\n        type. The new data type must preserve the same size in bytes as to not\n        cause index misalignment.\n\n        Parameters\n        ----------\n        dtype : data type\n            Data type object or one of their string representations.\n\n        Returns\n        -------\n        Series\n            A new Series object as a view of the same data in memory.\n\n        See Also\n        --------\n        numpy.ndarray.view : Equivalent numpy function to create a new view of\n            the same data in memory.\n\n        Notes\n        -----\n        Series are instantiated with ``dtype=float64`` by default. While\n        ``numpy.ndarray.view()`` will return a view with the same data type as\n        the original array, ``Series.view()`` (without specified dtype)\n        will try using ``float64`` and may fail if the original data type size\n        in bytes is not the same.\n\n        Examples\n        --------\n        >>> s = pd.Series([-2, -1, 0, 1, 2], dtype='int8')\n        >>> s\n        0   -2\n        1   -1\n        2    0\n        3    1\n        4    2\n        dtype: int8\n\n        The 8 bit signed integer representation of `-1` is `0b11111111`, but\n        the same bytes represent 255 if read as an 8 bit unsigned integer:\n\n        >>> us = s.view('uint8')\n        >>> us\n        0    254\n        1    255\n        2      0\n        3      1\n        4      2\n        dtype: uint8\n\n        The views share the same underlying values:\n\n        >>> us[0] = 128\n        >>> s\n        0   -128\n        1     -1\n        2      0\n        3      1\n        4      2\n        dtype: int8\n        \"\"\"\n        return self._constructor(\n            self._values.view(dtype), index=self.index\n        ).__finalize__(self, method=\"view\")\n\n    # ----------------------------------------------------------------------\n    # NDArray Compat\n    _HANDLED_TYPES = (Index, ExtensionArray, np.ndarray)\n\n    def __array_ufunc__(\n        self, ufunc: Callable, method: str, *inputs: Any, **kwargs: Any\n    ):\n        # TODO: handle DataFrame\n        cls = type(self)\n\n        # for binary ops, use our custom dunder methods\n        result = ops.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        # Determine if we should defer.\n        no_defer = (np.ndarray.__array_ufunc__, cls.__array_ufunc__)\n\n        for item in inputs:\n            higher_priority = (\n                hasattr(item, \"__array_priority__\")\n                and item.__array_priority__ > self.__array_priority__\n            )\n            has_array_ufunc = (\n                hasattr(item, \"__array_ufunc__\")\n                and type(item).__array_ufunc__ not in no_defer\n                and not isinstance(item, self._HANDLED_TYPES)\n            )\n            if higher_priority or has_array_ufunc:\n                return NotImplemented\n\n        # align all the inputs.\n        names = [getattr(x, \"name\") for x in inputs if hasattr(x, \"name\")]\n        types = tuple(type(x) for x in inputs)\n        # TODO: dataframe\n        alignable = [x for x, t in zip(inputs, types) if issubclass(t, Series)]\n\n        if len(alignable) > 1:\n            # This triggers alignment.\n            # At the moment, there aren't any ufuncs with more than two inputs\n            # so this ends up just being x1.index | x2.index, but we write\n            # it to handle *args.\n            index = alignable[0].index\n            for s in alignable[1:]:\n                index |= s.index\n            inputs = tuple(\n                x.reindex(index) if issubclass(t, Series) else x\n                for x, t in zip(inputs, types)\n            )\n        else:\n            index = self.index\n\n        inputs = tuple(extract_array(x, extract_numpy=True) for x in inputs)\n        result = getattr(ufunc, method)(*inputs, **kwargs)\n\n        name = names[0] if len(set(names)) == 1 else None\n\n        def construct_return(result):\n            if lib.is_scalar(result):\n                return result\n            elif result.ndim > 1:\n                # e.g. np.subtract.outer\n                if method == \"outer\":\n                    # GH#27198\n                    raise NotImplementedError\n                return result\n            return self._constructor(result, index=index, name=name, copy=False)\n\n        if type(result) is tuple:\n            # multiple return values\n            return tuple(construct_return(x) for x in result)\n        elif method == \"at\":\n            # no return value\n            return None\n        else:\n            return construct_return(result)\n\n    def __array__(self, dtype=None) -> np.ndarray:\n        \"\"\"\n        Return the values as a NumPy array.\n\n        Users should not call this directly. Rather, it is invoked by\n        :func:`numpy.array` and :func:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to use for the resulting NumPy array. By default,\n            the dtype is inferred from the data.\n\n        Returns\n        -------\n        numpy.ndarray\n            The values in the series converted to a :class:`numpy.ndarray`\n            with the specified `dtype`.\n\n        See Also\n        --------\n        array : Create a new array from data.\n        Series.array : Zero-copy view to the array backing the Series.\n        Series.to_numpy : Series method for similar behavior.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1, 2, 3])\n        >>> np.asarray(ser)\n        array([1, 2, 3])\n\n        For timezone-aware data, the timezones may be retained with\n        ``dtype='object'``\n\n        >>> tzser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\n        >>> np.asarray(tzser, dtype=\"object\")\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET', freq='D'),\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET', freq='D')],\n              dtype=object)\n\n        Or the values may be localized to UTC and the tzinfo discarded with\n        ``dtype='datetime64[ns]'``\n\n        >>> np.asarray(tzser, dtype=\"datetime64[ns]\")  # doctest: +ELLIPSIS\n        array(['1999-12-31T23:00:00.000000000', ...],\n              dtype='datetime64[ns]')\n        \"\"\"\n        return np.asarray(self.array, dtype)\n\n    # ----------------------------------------------------------------------\n    # Unary Methods\n\n    # coercion\n    __float__ = _coerce_method(float)\n    __long__ = _coerce_method(int)\n    __int__ = _coerce_method(int)\n\n    # ----------------------------------------------------------------------\n\n    # indexers\n    @property\n    def axes(self) -> List[Index]:\n        \"\"\"\n        Return a list of the row axis labels.\n        \"\"\"\n        return [self.index]\n\n    # ----------------------------------------------------------------------\n    # Indexing Methods\n\n    @Appender(generic.NDFrame.take.__doc__)\n    def take(self, indices, axis=0, is_copy=None, **kwargs) -> \"Series\":\n        if is_copy is not None:\n            warnings.warn(\n                \"is_copy is deprecated and will be removed in a future version. \"\n                \"'take' always returns a copy, so there is no need to specify this.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n        nv.validate_take(tuple(), kwargs)\n\n        indices = ensure_platform_int(indices)\n        new_index = self.index.take(indices)\n        new_values = self._values.take(indices)\n\n        result = self._constructor(new_values, index=new_index, fastpath=True)\n        return result.__finalize__(self, method=\"take\")\n\n    def _take_with_is_copy(self, indices, axis=0):\n        \"\"\"\n        Internal version of the `take` method that sets the `_is_copy`\n        attribute to keep track of the parent dataframe (using in indexing\n        for the SettingWithCopyWarning). For Series this does the same\n        as the public take (it never sets `_is_copy`).\n\n        See the docstring of `take` for full explanation of the parameters.\n        \"\"\"\n        return self.take(indices=indices, axis=axis)\n\n    def _ixs(self, i: int, axis: int = 0):\n        \"\"\"\n        Return the i-th value or values in the Series by location.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        scalar (int) or Series (slice, sequence)\n        \"\"\"\n        return self._values[i]\n\n    def _slice(self, slobj: slice, axis: int = 0) -> \"Series\":\n        # axis kwarg is retained for compat with NDFrame method\n        #  _slice is *always* positional\n        return self._get_values(slobj)\n\n    def __getitem__(self, key):\n        key = com.apply_if_callable(key, self)\n\n        if key is Ellipsis:\n            return self\n\n        key_is_scalar = is_scalar(key)\n        if isinstance(key, (list, tuple)):\n            key = unpack_1tuple(key)\n\n        if is_integer(key) and self.index._should_fallback_to_positional():\n            return self._values[key]\n\n        elif key_is_scalar:\n            return self._get_value(key)\n\n        if (\n            isinstance(key, tuple)\n            and is_hashable(key)\n            and isinstance(self.index, MultiIndex)\n        ):\n            # Otherwise index.get_value will raise InvalidIndexError\n            try:\n                result = self._get_value(key)\n\n                return result\n\n            except KeyError:\n                # We still have the corner case where this tuple is a key\n                #  in the first level of our MultiIndex\n                return self._get_values_tuple(key)\n\n        if is_iterator(key):\n            key = list(key)\n\n        if com.is_bool_indexer(key):\n            key = check_bool_indexer(self.index, key)\n            key = np.asarray(key, dtype=bool)\n            return self._get_values(key)\n\n        return self._get_with(key)\n\n    def _get_with(self, key):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            # _convert_slice_indexer to determin if this slice is positional\n            #  or label based, and if the latter, convert to positional\n            slobj = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            return self._slice(slobj)\n        elif isinstance(key, ABCDataFrame):\n            raise TypeError(\n                \"Indexing a Series with DataFrame is not \"\n                \"supported, use the appropriate DataFrame column\"\n            )\n        elif isinstance(key, tuple):\n            return self._get_values_tuple(key)\n\n        elif not is_list_like(key):\n            # e.g. scalars that aren't recognized by lib.is_scalar, GH#32684\n            return self.loc[key]\n\n        if not isinstance(key, (list, np.ndarray, ExtensionArray, Series, Index)):\n            key = list(key)\n\n        if isinstance(key, Index):\n            key_type = key.inferred_type\n        else:\n            key_type = lib.infer_dtype(key, skipna=False)\n\n        # Note: The key_type == \"boolean\" case should be caught by the\n        #  com.is_bool_indexer check in __getitem__\n        if key_type == \"integer\":\n            # We need to decide whether to treat this as a positional indexer\n            #  (i.e. self.iloc) or label-based (i.e. self.loc)\n            if not self.index._should_fallback_to_positional():\n                return self.loc[key]\n            else:\n                return self.iloc[key]\n\n        if isinstance(key, list):\n            # handle the dup indexing case GH#4246\n            return self.loc[key]\n\n        return self.reindex(key)\n\n    def _get_values_tuple(self, key):\n        # mpl hackaround\n        if com.any_none(*key):\n            # suppress warning from slicing the index with a 2d indexer.\n            # eventually we'll want Series itself to warn.\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\", \"Support for multi-dim\", DeprecationWarning\n                )\n                return self._get_values(key)\n\n        if not isinstance(self.index, MultiIndex):\n            raise ValueError(\"Can only tuple-index with a MultiIndex\")\n\n        # If key is contained, would have returned by now\n        indexer, new_index = self.index.get_loc_level(key)\n        return self._constructor(self._values[indexer], index=new_index).__finalize__(\n            self,\n        )\n\n    def _get_values(self, indexer):\n        try:\n            return self._constructor(self._mgr.get_slice(indexer)).__finalize__(self,)\n        except ValueError:\n            # mpl compat if we look up e.g. ser[:, np.newaxis];\n            #  see tests.series.timeseries.test_mpl_compat_hack\n            return self._values[indexer]\n\n    def _get_value(self, label, takeable: bool = False):\n        \"\"\"\n        Quickly retrieve single value at passed index label.\n\n        Parameters\n        ----------\n        label : object\n        takeable : interpret the index as indexers, default False\n\n        Returns\n        -------\n        scalar value\n        \"\"\"\n        if takeable:\n            return self._values[label]\n\n        # Similar to Index.get_value, but we do not fall back to positional\n        loc = self.index.get_loc(label)\n        return self.index._get_values_for_loc(self, loc, label)\n\n    def __setitem__(self, key, value):\n        key = com.apply_if_callable(key, self)\n        cacher_needs_updating = self._check_is_chained_assignment_possible()\n\n        if key is Ellipsis:\n            key = slice(None)\n\n        try:\n            self._set_with_engine(key, value)\n        except (KeyError, ValueError):\n            values = self._values\n            if is_integer(key) and not self.index.inferred_type == \"integer\":\n                # positional setter\n                values[key] = value\n            else:\n                # GH#12862 adding an new key to the Series\n                self.loc[key] = value\n\n        except TypeError as e:\n            if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):\n                raise ValueError(\"Can only tuple-index with a MultiIndex\") from e\n\n            if com.is_bool_indexer(key):\n                key = check_bool_indexer(self.index, key)\n                key = np.asarray(key, dtype=bool)\n                try:\n                    self._where(~key, value, inplace=True)\n                except InvalidIndexError:\n                    self.iloc[key] = value\n                return\n\n            else:\n                self._set_with(key, value)\n\n        if cacher_needs_updating:\n            self._maybe_update_cacher()\n\n    def _set_with_engine(self, key, value):\n        # fails with AttributeError for IntervalIndex\n        loc = self.index._engine.get_loc(key)\n        validate_numeric_casting(self.dtype, value)\n        self._values[loc] = value\n\n    def _set_with(self, key, value):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            # extract_array so that if we set e.g. ser[-5:] = ser[:5]\n            #  we get the first five values, and not 5 NaNs\n            indexer = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            self.iloc[indexer] = extract_array(value, extract_numpy=True)\n\n        else:\n            assert not isinstance(key, tuple)\n\n            if is_scalar(key):\n                key = [key]\n\n            if isinstance(key, Index):\n                key_type = key.inferred_type\n                key = key._values\n            else:\n                key_type = lib.infer_dtype(key, skipna=False)\n\n            # Note: key_type == \"boolean\" should not occur because that\n            #  should be caught by the is_bool_indexer check in __setitem__\n            if key_type == \"integer\":\n                if not self.index._should_fallback_to_positional():\n                    self.loc[key] = value\n                else:\n                    self.iloc[key] = value\n            else:\n                self.loc[key] = value\n\n    def _set_value(self, label, value, takeable: bool = False):\n        \"\"\"\n        Quickly set single value at passed label.\n\n        If label is not contained, a new object is created with the label\n        placed at the end of the result index.\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed.\n        value : object\n            Scalar value.\n        takeable : interpret the index as indexers, default False\n        \"\"\"\n        try:\n            if takeable:\n                self._values[label] = value\n            else:\n                loc = self.index.get_loc(label)\n                validate_numeric_casting(self.dtype, value)\n                self._values[loc] = value\n        except KeyError:\n\n            # set using a non-recursive method\n            self.loc[label] = value\n\n    # ----------------------------------------------------------------------\n    # Unsorted\n\n    @property\n    def _is_mixed_type(self):\n        return False\n\n    def repeat(self, repeats, axis=None) -> \"Series\":\n        \"\"\"\n        Repeat elements of a Series.\n\n        Returns a new Series where each element of the current Series\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            Series.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        Series\n            Newly created Series with repeated elements.\n\n        See Also\n        --------\n        Index.repeat : Equivalent function for Index.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s\n        0    a\n        1    b\n        2    c\n        dtype: object\n        >>> s.repeat(2)\n        0    a\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        dtype: object\n        >>> s.repeat([1, 2, 3])\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        2    c\n        dtype: object\n        \"\"\"\n        nv.validate_repeat(tuple(), dict(axis=axis))\n        new_index = self.index.repeat(repeats)\n        new_values = self._values.repeat(repeats)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"repeat\"\n        )\n\n    def reset_index(self, level=None, drop=False, name=None, inplace=False):\n        \"\"\"\n        Generate a new DataFrame or Series with the index reset.\n\n        This is useful when the index needs to be treated as a column, or\n        when the index is meaningless and needs to be reset to the default\n        before another operation.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default optional\n            For a Series with a MultiIndex, only remove the specified levels\n            from the index. Removes all levels by default.\n        drop : bool, default False\n            Just reset the index, without inserting it as a column in\n            the new DataFrame.\n        name : object, optional\n            The name to use for the column containing the original Series\n            values. Uses ``self.name`` by default. This argument is ignored\n            when `drop` is True.\n        inplace : bool, default False\n            Modify the Series in place (do not create a new object).\n\n        Returns\n        -------\n        Series or DataFrame\n            When `drop` is False (the default), a DataFrame is returned.\n            The newly created columns will come first in the DataFrame,\n            followed by the original Series values.\n            When `drop` is True, a `Series` is returned.\n            In either case, if ``inplace=True``, no value is returned.\n\n        See Also\n        --------\n        DataFrame.reset_index: Analogous function for DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4], name='foo',\n        ...               index=pd.Index(['a', 'b', 'c', 'd'], name='idx'))\n\n        Generate a DataFrame with default index.\n\n        >>> s.reset_index()\n          idx  foo\n        0   a    1\n        1   b    2\n        2   c    3\n        3   d    4\n\n        To specify the name of the new column use `name`.\n\n        >>> s.reset_index(name='values')\n          idx  values\n        0   a       1\n        1   b       2\n        2   c       3\n        3   d       4\n\n        To generate a new Series with the default set `drop` to True.\n\n        >>> s.reset_index(drop=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        To update the Series in place, without generating a new one\n        set `inplace` to True. Note that it also requires ``drop=True``.\n\n        >>> s.reset_index(inplace=True, drop=True)\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        The `level` parameter is interesting for Series with a multi-level\n        index.\n\n        >>> arrays = [np.array(['bar', 'bar', 'baz', 'baz']),\n        ...           np.array(['one', 'two', 'one', 'two'])]\n        >>> s2 = pd.Series(\n        ...     range(4), name='foo',\n        ...     index=pd.MultiIndex.from_arrays(arrays,\n        ...                                     names=['a', 'b']))\n\n        To remove a specific level from the Index, use `level`.\n\n        >>> s2.reset_index(level='a')\n               a  foo\n        b\n        one  bar    0\n        two  bar    1\n        one  baz    2\n        two  baz    3\n\n        If `level` is not set, all levels are removed from the Index.\n\n        >>> s2.reset_index()\n             a    b  foo\n        0  bar  one    0\n        1  bar  two    1\n        2  baz  one    2\n        3  baz  two    3\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if drop:\n            new_index = ibase.default_index(len(self))\n            if level is not None:\n                if not isinstance(level, (tuple, list)):\n                    level = [level]\n                level = [self.index._get_level_number(lev) for lev in level]\n                if len(level) < self.index.nlevels:\n                    new_index = self.index.droplevel(level)\n\n            if inplace:\n                self.index = new_index\n                # set name if it was passed, otherwise, keep the previous name\n                self.name = name or self.name\n            else:\n                return self._constructor(\n                    self._values.copy(), index=new_index\n                ).__finalize__(self, method=\"reset_index\")\n        elif inplace:\n            raise TypeError(\n                \"Cannot reset_index inplace on a Series to create a DataFrame\"\n            )\n        else:\n            df = self.to_frame(name)\n            return df.reset_index(level=level, drop=drop)\n\n    # ----------------------------------------------------------------------\n    # Rendering Methods\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular Series.\n        \"\"\"\n        buf = StringIO(\"\")\n        width, height = get_terminal_size()\n        max_rows = (\n            height\n            if get_option(\"display.max_rows\") == 0\n            else get_option(\"display.max_rows\")\n        )\n        min_rows = (\n            height\n            if get_option(\"display.max_rows\") == 0\n            else get_option(\"display.min_rows\")\n        )\n        show_dimensions = get_option(\"display.show_dimensions\")\n\n        self.to_string(\n            buf=buf,\n            name=self.name,\n            dtype=self.dtype,\n            min_rows=min_rows,\n            max_rows=max_rows,\n            length=show_dimensions,\n        )\n        result = buf.getvalue()\n\n        return result\n\n    def to_string(\n        self,\n        buf=None,\n        na_rep=\"NaN\",\n        float_format=None,\n        header=True,\n        index=True,\n        length=False,\n        dtype=False,\n        name=False,\n        max_rows=None,\n        min_rows=None,\n    ):\n        \"\"\"\n        Render a string representation of the Series.\n\n        Parameters\n        ----------\n        buf : StringIO-like, optional\n            Buffer to write to.\n        na_rep : str, optional\n            String representation of NaN to use, default 'NaN'.\n        float_format : one-parameter function, optional\n            Formatter function to apply to columns' elements if they are\n            floats, default None.\n        header : bool, default True\n            Add the Series header (index name).\n        index : bool, optional\n            Add index (row) labels, default True.\n        length : bool, default False\n            Add the Series length.\n        dtype : bool, default False\n            Add the Series dtype.\n        name : bool, default False\n            Add the Series name if not None.\n        max_rows : int, optional\n            Maximum number of rows to show before truncating. If None, show\n            all.\n        min_rows : int, optional\n            The number of rows to display in a truncated repr (when number\n            of rows is above `max_rows`).\n\n        Returns\n        -------\n        str or None\n            String representation of Series if ``buf=None``, otherwise None.\n        \"\"\"\n        formatter = fmt.SeriesFormatter(\n            self,\n            name=name,\n            length=length,\n            header=header,\n            index=index,\n            dtype=dtype,\n            na_rep=na_rep,\n            float_format=float_format,\n            min_rows=min_rows,\n            max_rows=max_rows,\n        )\n        result = formatter.to_string()\n\n        # catch contract violations\n        if not isinstance(result, str):\n            raise AssertionError(\n                \"result must be of type str, type \"\n                f\"of result is {repr(type(result).__name__)}\"\n            )\n\n        if buf is None:\n            return result\n        else:\n            try:\n                buf.write(result)\n            except AttributeError:\n                with open(buf, \"w\") as f:\n                    f.write(result)\n\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> s = pd.Series([\"elk\", \"pig\", \"dog\", \"quetzal\"], name=\"animal\")\n        >>> print(s.to_markdown())\n        |    | animal   |\n        |---:|:---------|\n        |  0 | elk      |\n        |  1 | pig      |\n        |  2 | dog      |\n        |  3 | quetzal  |\n        \"\"\"\n    )\n    @Substitution(klass=\"Series\")\n    @Appender(generic._shared_docs[\"to_markdown\"])\n    def to_markdown(\n        self, buf: Optional[IO[str]] = None, mode: Optional[str] = None, **kwargs\n    ) -> Optional[str]:\n        return self.to_frame().to_markdown(buf, mode, **kwargs)\n\n    # ----------------------------------------------------------------------\n\n    def items(self) -> Iterable[Tuple[Label, Any]]:\n        \"\"\"\n        Lazily iterate over (index, value) tuples.\n\n        This method returns an iterable tuple (index, value). This is\n        convenient if you want to create a lazy iterator.\n\n        Returns\n        -------\n        iterable\n            Iterable of tuples containing the (index, value) pairs from a\n            Series.\n\n        See Also\n        --------\n        DataFrame.items : Iterate over (column name, Series) pairs.\n        DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs.\n\n        Examples\n        --------\n        >>> s = pd.Series(['A', 'B', 'C'])\n        >>> for index, value in s.items():\n        ...     print(f\"Index : {index}, Value : {value}\")\n        Index : 0, Value : A\n        Index : 1, Value : B\n        Index : 2, Value : C\n        \"\"\"\n        return zip(iter(self.index), iter(self))\n\n    @Appender(items.__doc__)\n    def iteritems(self) -> Iterable[Tuple[Label, Any]]:\n        return self.items()\n\n    # ----------------------------------------------------------------------\n    # Misc public methods\n\n    def keys(self) -> Index:\n        \"\"\"\n        Return alias for index.\n\n        Returns\n        -------\n        Index\n            Index of the Series.\n        \"\"\"\n        return self.index\n\n    def to_dict(self, into=dict):\n        \"\"\"\n        Convert Series to {label -> value} dict or dict-like object.\n\n        Parameters\n        ----------\n        into : class, default dict\n            The collections.abc.Mapping subclass to use as the return\n            object. Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Key-value representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.to_dict()\n        {0: 1, 1: 2, 2: 3, 3: 4}\n        >>> from collections import OrderedDict, defaultdict\n        >>> s.to_dict(OrderedDict)\n        OrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])\n        >>> dd = defaultdict(list)\n        >>> s.to_dict(dd)\n        defaultdict(<class 'list'>, {0: 1, 1: 2, 2: 3, 3: 4})\n        \"\"\"\n        # GH16122\n        into_c = com.standardize_mapping(into)\n        return into_c(self.items())\n\n    def to_frame(self, name=None) -> \"DataFrame\":\n        \"\"\"\n        Convert Series to DataFrame.\n\n        Parameters\n        ----------\n        name : object, default None\n            The passed name should substitute for the series name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([\"a\", \"b\", \"c\"],\n        ...               name=\"vals\")\n        >>> s.to_frame()\n          vals\n        0    a\n        1    b\n        2    c\n        \"\"\"\n        if name is None:\n            df = self._constructor_expanddim(self)\n        else:\n            df = self._constructor_expanddim({name: self})\n\n        return df\n\n    def _set_name(self, name, inplace=False) -> \"Series\":\n        \"\"\"\n        Set the Series name.\n\n        Parameters\n        ----------\n        name : str\n        inplace : bool\n            Whether to modify `self` directly or return a copy.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        ser = self if inplace else self.copy()\n        ser.name = name\n        return ser\n\n    @Appender(\n        \"\"\"\nExamples\n--------\n>>> ser = pd.Series([390., 350., 30., 20.],\n...                 index=['Falcon', 'Falcon', 'Parrot', 'Parrot'], name=\"Max Speed\")\n>>> ser\nFalcon    390.0\nFalcon    350.0\nParrot     30.0\nParrot     20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby([\"a\", \"b\", \"a\", \"b\"]).mean()\na    210.0\nb    185.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(ser > 100).mean()\nMax Speed\nFalse     25.0\nTrue     370.0\nName: Max Speed, dtype: float64\n\n**Grouping by Indexes**\n\nWe can groupby different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> ser = pd.Series([390., 350., 30., 20.], index=index, name=\"Max Speed\")\n>>> ser\nAnimal  Type\nFalcon  Captive    390.0\n        Wild       350.0\nParrot  Captive     30.0\n        Wild        20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nAnimal\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=\"Type\").mean()\nType\nCaptive    210.0\nWild       185.0\nName: Max Speed, dtype: float64\n\"\"\"\n    )\n    @Appender(generic._shared_docs[\"groupby\"] % _shared_doc_kwargs)\n    def groupby(\n        self,\n        by=None,\n        axis=0,\n        level=None,\n        as_index: bool = True,\n        sort: bool = True,\n        group_keys: bool = True,\n        squeeze: bool = False,\n        observed: bool = False,\n    ) -> \"SeriesGroupBy\":\n        from pandas.core.groupby.generic import SeriesGroupBy\n\n        if level is None and by is None:\n            raise TypeError(\"You have to supply one of 'by' and 'level'\")\n        axis = self._get_axis_number(axis)\n\n        return SeriesGroupBy(\n            obj=self,\n            keys=by,\n            axis=axis,\n            level=level,\n            as_index=as_index,\n            sort=sort,\n            group_keys=group_keys,\n            squeeze=squeeze,\n            observed=observed,\n        )\n\n    # ----------------------------------------------------------------------\n    # Statistics, overridden ndarray methods\n\n    # TODO: integrate bottleneck\n\n    def count(self, level=None):\n        \"\"\"\n        Return number of non-NA/null observations in the Series.\n\n        Parameters\n        ----------\n        level : int or level name, default None\n            If the axis is a MultiIndex (hierarchical), count along a\n            particular level, collapsing into a smaller Series.\n\n        Returns\n        -------\n        int or Series (if level specified)\n            Number of non-null values in the Series.\n\n        See Also\n        --------\n        DataFrame.count : Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.0, 1.0, np.nan])\n        >>> s.count()\n        2\n        \"\"\"\n        if level is None:\n            return notna(self.array).sum()\n\n        if isinstance(level, str):\n            level = self.index._get_level_number(level)\n\n        lev = self.index.levels[level]\n        level_codes = np.array(self.index.codes[level], subok=False, copy=True)\n\n        mask = level_codes == -1\n        if mask.any():\n            level_codes[mask] = cnt = len(lev)\n            lev = lev.insert(cnt, lev._na_value)\n\n        obs = level_codes[notna(self._values)]\n        out = np.bincount(obs, minlength=len(lev) or None)\n        return self._constructor(out, index=lev, dtype=\"int64\").__finalize__(\n            self, method=\"count\"\n        )\n\n    def mode(self, dropna=True) -> \"Series\":\n        \"\"\"\n        Return the mode(s) of the dataset.\n\n        Always returns Series even if only one value is returned.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        Series\n            Modes of the Series in sorted order.\n        \"\"\"\n        # TODO: Add option for bins like value_counts()\n        return algorithms.mode(self, dropna=dropna)\n\n    def unique(self):\n        \"\"\"\n        Return unique values of Series object.\n\n        Uniques are returned in order of appearance. Hash table-based unique,\n        therefore does NOT sort.\n\n        Returns\n        -------\n        ndarray or ExtensionArray\n            The unique values returned as a NumPy array. See Notes.\n\n        See Also\n        --------\n        unique : Top-level unique method for any 1-d array-like object.\n        Index.unique : Return Index with unique values from an Index object.\n\n        Notes\n        -----\n        Returns the unique values as a NumPy array. In case of an\n        extension-array backed Series, a new\n        :class:`~api.extensions.ExtensionArray` of that type with just\n        the unique values is returned. This includes\n\n            * Categorical\n            * Period\n            * Datetime with Timezone\n            * Interval\n            * Sparse\n            * IntegerNA\n\n        See Examples section.\n\n        Examples\n        --------\n        >>> pd.Series([2, 1, 3, 3], name='A').unique()\n        array([2, 1, 3])\n\n        >>> pd.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()\n        array(['2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\n\n        >>> pd.Series([pd.Timestamp('2016-01-01', tz='US/Eastern')\n        ...            for _ in range(3)]).unique()\n        <DatetimeArray>\n        ['2016-01-01 00:00:00-05:00']\n        Length: 1, dtype: datetime64[ns, US/Eastern]\n\n        An unordered Categorical will return categories in the order of\n        appearance.\n\n        >>> pd.Series(pd.Categorical(list('baabc'))).unique()\n        [b, a, c]\n        Categories (3, object): [b, a, c]\n\n        An ordered Categorical preserves the category ordering.\n\n        >>> pd.Series(pd.Categorical(list('baabc'), categories=list('abc'),\n        ...                          ordered=True)).unique()\n        [b, a, c]\n        Categories (3, object): [a < b < c]\n        \"\"\"\n        result = super().unique()\n        return result\n\n    def drop_duplicates(self, keep=\"first\", inplace=False) -> Optional[\"Series\"]:\n        \"\"\"\n        Return Series with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        inplace : bool, default ``False``\n            If ``True``, performs operation inplace and returns None.\n\n        Returns\n        -------\n        Series\n            Series with duplicates dropped.\n\n        See Also\n        --------\n        Index.drop_duplicates : Equivalent method on Index.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Series.duplicated : Related method on Series, indicating duplicate\n            Series values.\n\n        Examples\n        --------\n        Generate a Series with duplicated entries.\n\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'],\n        ...               name='animal')\n        >>> s\n        0      lama\n        1       cow\n        2      lama\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        With the 'keep' parameter, the selection behaviour of duplicated values\n        can be changed. The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> s.drop_duplicates()\n        0      lama\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n\n        The value 'last' for parameter 'keep' keeps the last occurrence for\n        each set of duplicated entries.\n\n        >>> s.drop_duplicates(keep='last')\n        1       cow\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        The value ``False`` for parameter 'keep' discards all sets of\n        duplicated entries. Setting the value of 'inplace' to ``True`` performs\n        the operation inplace and returns ``None``.\n\n        >>> s.drop_duplicates(keep=False, inplace=True)\n        >>> s\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        result = super().drop_duplicates(keep=keep)\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    def duplicated(self, keep=\"first\") -> \"Series\":\n        \"\"\"\n        Indicate duplicate Series values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        Series. Either all duplicates, all except the first or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series\n            Series indicating whether each value has occurred in the\n            preceding values.\n\n        See Also\n        --------\n        Index.duplicated : Equivalent method on pandas.Index.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Series.drop_duplicates : Remove duplicate values from Series.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set on False and all others on True:\n\n        >>> animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> animals.duplicated()\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        which is equivalent to\n\n        >>> animals.duplicated(keep='first')\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> animals.duplicated(keep='last')\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> animals.duplicated(keep=False)\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n        \"\"\"\n        return super().duplicated(keep=keep)\n\n    def idxmin(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the minimum value.\n\n        If multiple values equal the minimum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmin. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the minimum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmin : Return indices of the minimum values\n            along the given axis.\n        DataFrame.idxmin : Return index of first occurrence of minimum\n            over requested axis.\n        Series.idxmax : Return index *label* of the first occurrence\n            of maximum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmin``. This method\n        returns the label of the minimum, while ``ndarray.argmin`` returns\n        the position. To get the position, use ``series.values.argmin()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 1],\n        ...               index=['A', 'B', 'C', 'D'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    1.0\n        dtype: float64\n\n        >>> s.idxmin()\n        'A'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmin(skipna=False)\n        nan\n        \"\"\"\n        skipna = nv.validate_argmin_with_skipna(skipna, args, kwargs)\n        i = nanops.nanargmin(self._values, skipna=skipna)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def idxmax(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the maximum value.\n\n        If multiple values equal the maximum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmax. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the maximum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmax : Return indices of the maximum values\n            along the given axis.\n        DataFrame.idxmax : Return index of first occurrence of maximum\n            over requested axis.\n        Series.idxmin : Return index *label* of the first occurrence\n            of minimum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmax``. This method\n        returns the label of the maximum, while ``ndarray.argmax`` returns\n        the position. To get the position, use ``series.values.argmax()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 3, 4],\n        ...               index=['A', 'B', 'C', 'D', 'E'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    3.0\n        E    4.0\n        dtype: float64\n\n        >>> s.idxmax()\n        'C'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmax(skipna=False)\n        nan\n        \"\"\"\n        skipna = nv.validate_argmax_with_skipna(skipna, args, kwargs)\n        i = nanops.nanargmax(self._values, skipna=skipna)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def round(self, decimals=0, *args, **kwargs) -> \"Series\":\n        \"\"\"\n        Round each value in a Series to the given number of decimals.\n\n        Parameters\n        ----------\n        decimals : int, default 0\n            Number of decimal places to round to. If decimals is negative,\n            it specifies the number of positions to the left of the decimal point.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Series\n            Rounded values of the Series.\n\n        See Also\n        --------\n        numpy.around : Round values of an np.array.\n        DataFrame.round : Round values of a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.1, 1.3, 2.7])\n        >>> s.round()\n        0    0.0\n        1    1.0\n        2    3.0\n        dtype: float64\n        \"\"\"\n        nv.validate_round(args, kwargs)\n        result = self._values.round(decimals)\n        result = self._constructor(result, index=self.index).__finalize__(\n            self, method=\"round\"\n        )\n\n        return result\n\n    def quantile(self, q=0.5, interpolation=\"linear\"):\n        \"\"\"\n        Return value at the given quantile.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            The quantile(s) to compute, which can lie in range: 0 <= q <= 1.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            This optional parameter specifies the interpolation method to use,\n            when the desired quantile lies between two data points `i` and `j`:\n\n                * linear: `i + (j - i) * fraction`, where `fraction` is the\n                  fractional part of the index surrounded by `i` and `j`.\n                * lower: `i`.\n                * higher: `j`.\n                * nearest: `i` or `j` whichever is nearest.\n                * midpoint: (`i` + `j`) / 2.\n\n        Returns\n        -------\n        float or Series\n            If ``q`` is an array, a Series will be returned where the\n            index is ``q`` and the values are the quantiles, otherwise\n            a float will be returned.\n\n        See Also\n        --------\n        core.window.Rolling.quantile : Calculate the rolling quantile.\n        numpy.percentile : Returns the q-th percentile(s) of the array elements.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.quantile(.5)\n        2.5\n        >>> s.quantile([.25, .5, .75])\n        0.25    1.75\n        0.50    2.50\n        0.75    3.25\n        dtype: float64\n        \"\"\"\n        validate_percentile(q)\n\n        # We dispatch to DataFrame so that core.internals only has to worry\n        #  about 2D cases.\n        df = self.to_frame()\n\n        result = df.quantile(q=q, interpolation=interpolation, numeric_only=False)\n        if result.ndim == 2:\n            result = result.iloc[:, 0]\n\n        if is_list_like(q):\n            result.name = self.name\n            return self._constructor(result, index=Float64Index(q), name=self.name)\n        else:\n            # scalar\n            return result.iloc[0]\n\n    def corr(self, other, method=\"pearson\", min_periods=None) -> float:\n        \"\"\"\n        Compute correlation with `other` Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the correlation.\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method used to compute correlation:\n\n            - pearson : Standard correlation coefficient\n            - kendall : Kendall Tau correlation coefficient\n            - spearman : Spearman rank correlation\n            - callable: Callable with input two 1d ndarrays and returning a float.\n\n            .. versionadded:: 0.24.0\n                Note that the returned matrix from corr will have 1 along the\n                diagonals and will be symmetric regardless of the callable's\n                behavior.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n\n        Returns\n        -------\n        float\n            Correlation with other.\n\n        See Also\n        --------\n        DataFrame.corr : Compute pairwise correlation between columns.\n        DataFrame.corrwith : Compute pairwise correlation with another\n            DataFrame or Series.\n\n        Examples\n        --------\n        >>> def histogram_intersection(a, b):\n        ...     v = np.minimum(a, b).sum().round(decimals=1)\n        ...     return v\n        >>> s1 = pd.Series([.2, .0, .6, .2])\n        >>> s2 = pd.Series([.3, .6, .0, .1])\n        >>> s1.corr(s2, method=histogram_intersection)\n        0.3\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n\n        if method in [\"pearson\", \"spearman\", \"kendall\"] or callable(method):\n            return nanops.nancorr(\n                this.values, other.values, method=method, min_periods=min_periods\n            )\n\n        raise ValueError(\n            \"method must be either 'pearson', \"\n            \"'spearman', 'kendall', or a callable, \"\n            f\"'{method}' was supplied\"\n        )\n\n    def cov(self, other, min_periods=None) -> float:\n        \"\"\"\n        Compute covariance with Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the covariance.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n\n        Returns\n        -------\n        float\n            Covariance between Series and other normalized by N-1\n            (unbiased estimator).\n\n        See Also\n        --------\n        DataFrame.cov : Compute pairwise covariance of columns.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([0.90010907, 0.13484424, 0.62036035])\n        >>> s2 = pd.Series([0.12528585, 0.26962463, 0.51111198])\n        >>> s1.cov(s2)\n        -0.01685762652715874\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancov(this.values, other.values, min_periods=min_periods)\n\n    def diff(self, periods: int = 1) -> \"Series\":\n        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of a Series element compared with another\n        element in the Series (default is element in previous row).\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative\n            values.\n\n        Returns\n        -------\n        Series\n            First differences of the Series.\n\n        See Also\n        --------\n        Series.pct_change: Percent change over given number of periods.\n        Series.shift: Shift index by desired number of periods with an\n            optional time freq.\n        DataFrame.diff: First discrete difference of object.\n\n        Notes\n        -----\n        For boolean dtypes, this uses :meth:`operator.xor` rather than\n        :meth:`operator.sub`.\n\n        Examples\n        --------\n        Difference with previous row\n\n        >>> s = pd.Series([1, 1, 2, 3, 5, 8])\n        >>> s.diff()\n        0    NaN\n        1    0.0\n        2    1.0\n        3    1.0\n        4    2.0\n        5    3.0\n        dtype: float64\n\n        Difference with 3rd previous row\n\n        >>> s.diff(periods=3)\n        0    NaN\n        1    NaN\n        2    NaN\n        3    2.0\n        4    4.0\n        5    6.0\n        dtype: float64\n\n        Difference with following row\n\n        >>> s.diff(periods=-1)\n        0    0.0\n        1   -1.0\n        2   -1.0\n        3   -2.0\n        4   -3.0\n        5    NaN\n        dtype: float64\n        \"\"\"\n        result = algorithms.diff(self.array, periods)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"diff\"\n        )\n\n    def autocorr(self, lag=1) -> float:\n        \"\"\"\n        Compute the lag-N autocorrelation.\n\n        This method computes the Pearson correlation between\n        the Series and its shifted self.\n\n        Parameters\n        ----------\n        lag : int, default 1\n            Number of lags to apply before performing autocorrelation.\n\n        Returns\n        -------\n        float\n            The Pearson correlation between self and self.shift(lag).\n\n        See Also\n        --------\n        Series.corr : Compute the correlation between two Series.\n        Series.shift : Shift index by desired number of periods.\n        DataFrame.corr : Compute pairwise correlation of columns.\n        DataFrame.corrwith : Compute pairwise correlation between rows or\n            columns of two DataFrame objects.\n\n        Notes\n        -----\n        If the Pearson correlation is not well defined return 'NaN'.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.25, 0.5, 0.2, -0.05])\n        >>> s.autocorr()  # doctest: +ELLIPSIS\n        0.10355...\n        >>> s.autocorr(lag=2)  # doctest: +ELLIPSIS\n        -0.99999...\n\n        If the Pearson correlation is not well defined, then 'NaN' is returned.\n\n        >>> s = pd.Series([1, 0, 0, 0])\n        >>> s.autocorr()\n        nan\n        \"\"\"\n        return self.corr(self.shift(lag))\n\n    def dot(self, other):\n        \"\"\"\n        Compute the dot product between the Series and the columns of other.\n\n        This method computes the dot product between the Series and another\n        one, or the Series and each columns of a DataFrame, or the Series and\n        each columns of an array.\n\n        It can also be called using `self @ other` in Python >= 3.5.\n\n        Parameters\n        ----------\n        other : Series, DataFrame or array-like\n            The other object to compute the dot product with its columns.\n\n        Returns\n        -------\n        scalar, Series or numpy.ndarray\n            Return the dot product of the Series and other if other is a\n            Series, the Series of the dot product of Series and each rows of\n            other if other is a DataFrame or a numpy.ndarray between the Series\n            and each columns of the numpy array.\n\n        See Also\n        --------\n        DataFrame.dot: Compute the matrix product with the DataFrame.\n        Series.mul: Multiplication of series and other, element-wise.\n\n        Notes\n        -----\n        The Series and other has to share the same index if other is a Series\n        or a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0, 1, 2, 3])\n        >>> other = pd.Series([-1, 2, -3, 4])\n        >>> s.dot(other)\n        8\n        >>> s @ other\n        8\n        >>> df = pd.DataFrame([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(df)\n        0    24\n        1    14\n        dtype: int64\n        >>> arr = np.array([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(arr)\n        array([24, 14])\n        \"\"\"\n        if isinstance(other, (Series, ABCDataFrame)):\n            common = self.index.union(other.index)\n            if len(common) > len(self.index) or len(common) > len(other.index):\n                raise ValueError(\"matrices are not aligned\")\n\n            left = self.reindex(index=common, copy=False)\n            right = other.reindex(index=common, copy=False)\n            lvals = left.values\n            rvals = right.values\n        else:\n            lvals = self.values\n            rvals = np.asarray(other)\n            if lvals.shape[0] != rvals.shape[0]:\n                raise Exception(\n                    f\"Dot product shape mismatch, {lvals.shape} vs {rvals.shape}\"\n                )\n\n        if isinstance(other, ABCDataFrame):\n            return self._constructor(\n                np.dot(lvals, rvals), index=other.columns\n            ).__finalize__(self, method=\"dot\")\n        elif isinstance(other, Series):\n            return np.dot(lvals, rvals)\n        elif isinstance(rvals, np.ndarray):\n            return np.dot(lvals, rvals)\n        else:  # pragma: no cover\n            raise TypeError(f\"unsupported type: {type(other)}\")\n\n    def __matmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(other)\n\n    def __rmatmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(np.transpose(other))\n\n    @doc(base.IndexOpsMixin.searchsorted, klass=\"Series\")\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        return algorithms.searchsorted(self._values, value, side=side, sorter=sorter)\n\n    # -------------------------------------------------------------------\n    # Combination\n\n    def append(self, to_append, ignore_index=False, verify_integrity=False):\n        \"\"\"\n        Concatenate two or more Series.\n\n        Parameters\n        ----------\n        to_append : Series or list/tuple of Series\n            Series to append with self.\n        ignore_index : bool, default False\n            If True, do not use the index labels.\n        verify_integrity : bool, default False\n            If True, raise Exception on creating index with duplicates.\n\n        Returns\n        -------\n        Series\n            Concatenated Series.\n\n        See Also\n        --------\n        concat : General function to concatenate DataFrame or Series objects.\n\n        Notes\n        -----\n        Iteratively appending to a Series can be more computationally intensive\n        than a single concatenate. A better solution is to append values to a\n        list and then concatenate the list with the original Series all at\n        once.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, 2, 3])\n        >>> s2 = pd.Series([4, 5, 6])\n        >>> s3 = pd.Series([4, 5, 6], index=[3, 4, 5])\n        >>> s1.append(s2)\n        0    1\n        1    2\n        2    3\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s1.append(s3)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `ignore_index` set to True:\n\n        >>> s1.append(s2, ignore_index=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `verify_integrity` set to True:\n\n        >>> s1.append(s2, verify_integrity=True)\n        Traceback (most recent call last):\n        ...\n        ValueError: Indexes have overlapping values: [0, 1, 2]\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        if isinstance(to_append, (list, tuple)):\n            to_concat = [self]\n            to_concat.extend(to_append)\n        else:\n            to_concat = [self, to_append]\n        if any(isinstance(x, (ABCDataFrame,)) for x in to_concat[1:]):\n            msg = (\n                f\"to_append should be a Series or list/tuple of Series, \"\n                f\"got DataFrame\"\n            )\n            raise TypeError(msg)\n        return concat(\n            to_concat, ignore_index=ignore_index, verify_integrity=verify_integrity\n        )\n\n    def _binop(self, other, func, level=None, fill_value=None):\n        \"\"\"\n        Perform generic binary operation with optional fill value.\n\n        Parameters\n        ----------\n        other : Series\n        func : binary operator\n        fill_value : float or object\n            Value to substitute for NA/null values. If both Series are NA in a\n            location, the result will be NA regardless of the passed fill value.\n        level : int or level name, default None\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level.\n\n        Returns\n        -------\n        Series\n        \"\"\"\n        if not isinstance(other, Series):\n            raise AssertionError(\"Other operand must be Series\")\n\n        this = self\n\n        if not self.index.equals(other.index):\n            this, other = self.align(other, level=level, join=\"outer\", copy=False)\n\n        this_vals, other_vals = ops.fill_binop(this.values, other.values, fill_value)\n\n        with np.errstate(all=\"ignore\"):\n            result = func(this_vals, other_vals)\n\n        name = ops.get_op_result_name(self, other)\n        ret = this._construct_result(result, name)\n        return ret\n\n    def _construct_result(\n        self, result: Union[ArrayLike, Tuple[ArrayLike, ArrayLike]], name: Label\n    ) -> Union[\"Series\", Tuple[\"Series\", \"Series\"]]:\n        \"\"\"\n        Construct an appropriately-labelled Series from the result of an op.\n\n        Parameters\n        ----------\n        result : ndarray or ExtensionArray\n        name : Label\n\n        Returns\n        -------\n        Series\n            In the case of __divmod__ or __rdivmod__, a 2-tuple of Series.\n        \"\"\"\n        if isinstance(result, tuple):\n            # produced by divmod or rdivmod\n\n            res1 = self._construct_result(result[0], name=name)\n            res2 = self._construct_result(result[1], name=name)\n\n            # GH#33427 assertions to keep mypy happy\n            assert isinstance(res1, Series)\n            assert isinstance(res2, Series)\n            return (res1, res2)\n\n        # We do not pass dtype to ensure that the Series constructor\n        #  does inference in the case where `result` has object-dtype.\n        out = self._constructor(result, index=self.index)\n        out = out.__finalize__(self)\n\n        # Set the result's name after __finalize__ is called because __finalize__\n        #  would set it back to self.name\n        out.name = name\n        return out\n\n    def combine(self, other, func, fill_value=None) -> \"Series\":\n        \"\"\"\n        Combine the Series with a Series or scalar according to `func`.\n\n        Combine the Series and `other` using `func` to perform elementwise\n        selection for combined Series.\n        `fill_value` is assumed when value is missing at some index\n        from one of the two objects being combined.\n\n        Parameters\n        ----------\n        other : Series or scalar\n            The value(s) to be combined with the `Series`.\n        func : function\n            Function that takes two scalars as inputs and returns an element.\n        fill_value : scalar, optional\n            The value to assume when an index is missing from\n            one Series or the other. The default specifies to use the\n            appropriate NaN value for the underlying dtype of the Series.\n\n        Returns\n        -------\n        Series\n            The result of combining the Series with the other object.\n\n        See Also\n        --------\n        Series.combine_first : Combine Series values, choosing the calling\n            Series' values first.\n\n        Examples\n        --------\n        Consider 2 Datasets ``s1`` and ``s2`` containing\n        highest clocked speeds of different birds.\n\n        >>> s1 = pd.Series({'falcon': 330.0, 'eagle': 160.0})\n        >>> s1\n        falcon    330.0\n        eagle     160.0\n        dtype: float64\n        >>> s2 = pd.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})\n        >>> s2\n        falcon    345.0\n        eagle     200.0\n        duck       30.0\n        dtype: float64\n\n        Now, to combine the two datasets and view the highest speeds\n        of the birds across the two datasets\n\n        >>> s1.combine(s2, max)\n        duck        NaN\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n\n        In the previous example, the resulting value for duck is missing,\n        because the maximum of a NaN and a float is a NaN.\n        So, in the example, we set ``fill_value=0``,\n        so the maximum value returned will be the value from some dataset.\n\n        >>> s1.combine(s2, max, fill_value=0)\n        duck       30.0\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n        \"\"\"\n        if fill_value is None:\n            fill_value = na_value_for_dtype(self.dtype, compat=False)\n\n        if isinstance(other, Series):\n            # If other is a Series, result is based on union of Series,\n            # so do this element by element\n            new_index = self.index.union(other.index)\n            new_name = ops.get_op_result_name(self, other)\n            new_values = []\n            for idx in new_index:\n                lv = self.get(idx, fill_value)\n                rv = other.get(idx, fill_value)\n                with np.errstate(all=\"ignore\"):\n                    new_values.append(func(lv, rv))\n        else:\n            # Assume that other is a scalar, so apply the function for\n            # each element in the Series\n            new_index = self.index\n            with np.errstate(all=\"ignore\"):\n                new_values = [func(lv, other) for lv in self._values]\n            new_name = self.name\n\n        if is_categorical_dtype(self.dtype):\n            pass\n        elif is_extension_array_dtype(self.dtype):\n            # TODO: can we do this for only SparseDtype?\n            # The function can return something of any type, so check\n            # if the type is compatible with the calling EA.\n            new_values = maybe_cast_to_extension_array(type(self._values), new_values)\n        return self._constructor(new_values, index=new_index, name=new_name)\n\n    def combine_first(self, other) -> \"Series\":\n        \"\"\"\n        Combine Series values, choosing the calling Series's values first.\n\n        Parameters\n        ----------\n        other : Series\n            The value(s) to be combined with the `Series`.\n\n        Returns\n        -------\n        Series\n            The result of combining the Series with the other object.\n\n        See Also\n        --------\n        Series.combine : Perform elementwise operation on two Series\n            using a given function.\n\n        Notes\n        -----\n        Result index will be the union of the two indexes.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, np.nan])\n        >>> s2 = pd.Series([3, 4])\n        >>> s1.combine_first(s2)\n        0    1.0\n        1    4.0\n        dtype: float64\n        \"\"\"\n        new_index = self.index.union(other.index)\n        this = self.reindex(new_index, copy=False)\n        other = other.reindex(new_index, copy=False)\n        if this.dtype.kind == \"M\" and other.dtype.kind != \"M\":\n            other = to_datetime(other)\n\n        return this.where(notna(this), other)\n\n    def update(self, other) -> None:\n        \"\"\"\n        Modify Series in place using non-NA values from passed\n        Series. Aligns on index.\n\n        Parameters\n        ----------\n        other : Series\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s.update(pd.Series(['d', 'e'], index=[0, 2]))\n        >>> s\n        0    d\n        1    b\n        2    e\n        dtype: object\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6, 7, 8]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        If ``other`` contains NaNs the corresponding values are not updated\n        in the original Series.\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, np.nan, 6]))\n        >>> s\n        0    4\n        1    2\n        2    6\n        dtype: int64\n        \"\"\"\n        other = other.reindex_like(self)\n        mask = notna(other)\n\n        self._mgr = self._mgr.putmask(mask=mask, new=other)\n        self._maybe_update_cacher()\n\n    # ----------------------------------------------------------------------\n    # Reindexing, sorting\n\n    def sort_values(\n        self,\n        axis=0,\n        ascending=True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        ignore_index: bool = False,\n    ):\n        \"\"\"\n        Sort by the values.\n\n        Sort a Series in ascending or descending order by some\n        criterion.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            Axis to direct sorting. The value 'index' is accepted for\n            compatibility with DataFrame.sort_values.\n        ascending : bool, default True\n            If True, sort values in ascending order, otherwise descending.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort' or 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. 'mergesort' is the only stable  algorithm.\n        na_position : {'first' or 'last'}, default 'last'\n            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n            the end.\n        ignore_index : bool, default False\n             If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n             .. versionadded:: 1.0.0\n\n        Returns\n        -------\n        Series\n            Series ordered by values.\n\n        See Also\n        --------\n        Series.sort_index : Sort by the Series indices.\n        DataFrame.sort_values : Sort DataFrame by the values along either axis.\n        DataFrame.sort_index : Sort DataFrame by indices.\n\n        Examples\n        --------\n        >>> s = pd.Series([np.nan, 1, 3, 10, 5])\n        >>> s\n        0     NaN\n        1     1.0\n        2     3.0\n        3     10.0\n        4     5.0\n        dtype: float64\n\n        Sort values ascending order (default behaviour)\n\n        >>> s.sort_values(ascending=True)\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        0     NaN\n        dtype: float64\n\n        Sort values descending order\n\n        >>> s.sort_values(ascending=False)\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values inplace\n\n        >>> s.sort_values(ascending=False, inplace=True)\n        >>> s\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values putting NAs first\n\n        >>> s.sort_values(na_position='first')\n        0     NaN\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        dtype: float64\n\n        Sort a series of strings\n\n        >>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n        >>> s\n        0    z\n        1    b\n        2    d\n        3    a\n        4    c\n        dtype: object\n\n        >>> s.sort_values()\n        3    a\n        1    b\n        4    c\n        2    d\n        0    z\n        dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n\n        # GH 5856/5853\n        if inplace and self._is_cached:\n            raise ValueError(\n                \"This Series is a view of some other array, to \"\n                \"sort in-place you must create a copy\"\n            )\n\n        def _try_kind_sort(arr):\n            # easier to ask forgiveness than permission\n            try:\n                # if kind==mergesort, it can fail for object dtype\n                return arr.argsort(kind=kind)\n            except TypeError:\n                # stable sort not available for object dtype\n                # uses the argsort default quicksort\n                return arr.argsort(kind=\"quicksort\")\n\n        arr = self._values\n        sorted_index = np.empty(len(self), dtype=np.int32)\n\n        bad = isna(arr)\n\n        good = ~bad\n        idx = ibase.default_index(len(self))\n\n        argsorted = _try_kind_sort(arr[good])\n\n        if is_list_like(ascending):\n            if len(ascending) != 1:\n                raise ValueError(\n                    f\"Length of ascending ({len(ascending)}) must be 1 for Series\"\n                )\n            ascending = ascending[0]\n\n        if not is_bool(ascending):\n            raise ValueError(\"ascending must be boolean\")\n\n        if not ascending:\n            argsorted = argsorted[::-1]\n\n        if na_position == \"last\":\n            n = good.sum()\n            sorted_index[:n] = idx[good][argsorted]\n            sorted_index[n:] = idx[bad]\n        elif na_position == \"first\":\n            n = bad.sum()\n            sorted_index[n:] = idx[good][argsorted]\n            sorted_index[:n] = idx[bad]\n        else:\n            raise ValueError(f\"invalid na_position: {na_position}\")\n\n        result = self._constructor(arr[sorted_index], index=self.index[sorted_index])\n\n        if ignore_index:\n            result.index = ibase.default_index(len(sorted_index))\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_values\")\n\n    def sort_index(\n        self,\n        axis=0,\n        level=None,\n        ascending: bool = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        sort_remaining: bool = True,\n        ignore_index: bool = False,\n    ):\n        \"\"\"\n        Sort Series by index labels.\n\n        Returns a new Series sorted by label if `inplace` argument is\n        ``False``, otherwise updates the original series and returns None.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Axis to direct sorting. This can only be 0 for Series.\n        level : int, optional\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level individually.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information.  'mergesort' is the only stable algorithm. For\n            DataFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'last'}, default 'last'\n            If 'first' puts NaNs at the beginning, 'last' puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        Returns\n        -------\n        Series\n            The original Series sorted by the labels.\n\n        See Also\n        --------\n        DataFrame.sort_index: Sort DataFrame by the index.\n        DataFrame.sort_values: Sort DataFrame by the value.\n        Series.sort_values : Sort Series by the value.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])\n        >>> s.sort_index()\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        Sort Descending\n\n        >>> s.sort_index(ascending=False)\n        4    d\n        3    a\n        2    b\n        1    c\n        dtype: object\n\n        Sort Inplace\n\n        >>> s.sort_index(inplace=True)\n        >>> s\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        By default NaNs are put at the end, but use `na_position` to place\n        them at the beginning\n\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])\n        >>> s.sort_index(na_position='first')\n        NaN     d\n         1.0    c\n         2.0    b\n         3.0    a\n        dtype: object\n\n        Specify index level to sort\n\n        >>> arrays = [np.array(['qux', 'qux', 'foo', 'foo',\n        ...                     'baz', 'baz', 'bar', 'bar']),\n        ...           np.array(['two', 'one', 'two', 'one',\n        ...                     'two', 'one', 'two', 'one'])]\n        >>> s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)\n        >>> s.sort_index(level=1)\n        bar  one    8\n        baz  one    6\n        foo  one    4\n        qux  one    2\n        bar  two    7\n        baz  two    5\n        foo  two    3\n        qux  two    1\n        dtype: int64\n\n        Does not sort by remaining levels when sorting by levels\n\n        >>> s.sort_index(level=1, sort_remaining=False)\n        qux  one    2\n        foo  one    4\n        baz  one    6\n        bar  one    8\n        qux  two    1\n        foo  two    3\n        baz  two    5\n        bar  two    7\n        dtype: int64\n        \"\"\"\n        # TODO: this can be combined with DataFrame.sort_index impl as\n        # almost identical\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n        index = self.index\n\n        if level is not None:\n            new_index, indexer = index.sortlevel(\n                level, ascending=ascending, sort_remaining=sort_remaining\n            )\n        elif isinstance(index, MultiIndex):\n            from pandas.core.sorting import lexsort_indexer\n\n            labels = index._sort_levels_monotonic()\n            indexer = lexsort_indexer(\n                labels._get_codes_for_sorting(),\n                orders=ascending,\n                na_position=na_position,\n            )\n        else:\n            from pandas.core.sorting import nargsort\n\n            # Check monotonic-ness before sort an index\n            # GH11080\n            if (ascending and index.is_monotonic_increasing) or (\n                not ascending and index.is_monotonic_decreasing\n            ):\n                if inplace:\n                    return\n                else:\n                    return self.copy()\n\n            indexer = nargsort(\n                index, kind=kind, ascending=ascending, na_position=na_position\n            )\n\n        indexer = ensure_platform_int(indexer)\n        new_index = index.take(indexer)\n        new_index = new_index._sort_levels_monotonic()\n\n        new_values = self._values.take(indexer)\n        result = self._constructor(new_values, index=new_index)\n\n        if ignore_index:\n            result.index = ibase.default_index(len(result))\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_index\")\n\n    def argsort(self, axis=0, kind=\"quicksort\", order=None) -> \"Series\":\n        \"\"\"\n        Override ndarray.argsort. Argsorts the value, omitting NA/null values,\n        and places the result in the same locations as the non-NA values.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Has no effect but is accepted for compatibility with numpy.\n        kind : {'mergesort', 'quicksort', 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See np.sort for more\n            information. 'mergesort' is the only stable algorithm.\n        order : None\n            Has no effect but is accepted for compatibility with numpy.\n\n        Returns\n        -------\n        Series\n            Positions of values within the sort order with -1 indicating\n            nan values.\n\n        See Also\n        --------\n        numpy.ndarray.argsort : Returns the indices that would sort this array.\n        \"\"\"\n        values = self._values\n        mask = isna(values)\n\n        if mask.any():\n            result = Series(-1, index=self.index, name=self.name, dtype=\"int64\")\n            notmask = ~mask\n            result[notmask] = np.argsort(values[notmask], kind=kind)\n            return self._constructor(result, index=self.index).__finalize__(\n                self, method=\"argsort\"\n            )\n        else:\n            return self._constructor(\n                np.argsort(values, kind=kind), index=self.index, dtype=\"int64\"\n            ).__finalize__(self, method=\"argsort\")\n\n    def nlargest(self, n=5, keep=\"first\") -> \"Series\":\n        \"\"\"\n        Return the largest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many descending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n                of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n                order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n                size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` largest values in the Series, sorted in decreasing order.\n\n        See Also\n        --------\n        Series.nsmallest: Get the `n` smallest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values(ascending=False).head(n)`` for small `n`\n        relative to the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Malta\": 434000, \"Maldives\": 434000,\n        ...                         \"Brunei\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Monserat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Monserat        5200\n        dtype: int64\n\n        The `n` largest elements where ``n=5`` by default.\n\n        >>> s.nlargest()\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3``. Default `keep` value is 'first'\n        so Malta will be kept.\n\n        >>> s.nlargest(3)\n        France    65000000\n        Italy     59000000\n        Malta       434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` and keeping the last duplicates.\n        Brunei will be kept since it is the last with value 434000 based on\n        the index order.\n\n        >>> s.nlargest(3, keep='last')\n        France      65000000\n        Italy       59000000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has five elements due to the three duplicates.\n\n        >>> s.nlargest(3, keep='all')\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nlargest()\n\n    def nsmallest(self, n=5, keep=\"first\") -> \"Series\":\n        \"\"\"\n        Return the smallest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many ascending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n                of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n                order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n                size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` smallest values in the Series, sorted in increasing order.\n\n        See Also\n        --------\n        Series.nlargest: Get the `n` largest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values().head(n)`` for small `n` relative to\n        the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Brunei\": 434000, \"Malta\": 434000,\n        ...                         \"Maldives\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Monserat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Brunei        434000\n        Malta         434000\n        Maldives      434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Monserat        5200\n        dtype: int64\n\n        The `n` smallest elements where ``n=5`` by default.\n\n        >>> s.nsmallest()\n        Monserat      5200\n        Nauru        11300\n        Tuvalu       11300\n        Anguilla     11300\n        Iceland     337000\n        dtype: int64\n\n        The `n` smallest elements where ``n=3``. Default `keep` value is\n        'first' so Nauru and Tuvalu will be kept.\n\n        >>> s.nsmallest(3)\n        Monserat     5200\n        Nauru       11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` and keeping the last\n        duplicates. Anguilla and Tuvalu will be kept since they are the last\n        with value 11300 based on the index order.\n\n        >>> s.nsmallest(3, keep='last')\n        Monserat     5200\n        Anguilla    11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has four elements due to the three duplicates.\n\n        >>> s.nsmallest(3, keep='all')\n        Monserat     5200\n        Nauru       11300\n        Tuvalu      11300\n        Anguilla    11300\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nsmallest()\n\n    def swaplevel(self, i=-2, j=-1, copy=True) -> \"Series\":\n        \"\"\"\n        Swap levels i and j in a :class:`MultiIndex`.\n\n        Default is to swap the two innermost levels of the index.\n\n        Parameters\n        ----------\n        i, j : int, str\n            Level of the indices to be swapped. Can pass level name as string.\n        copy : bool, default True\n            Whether to copy underlying data.\n\n        Returns\n        -------\n        Series\n            Series with levels swapped in MultiIndex.\n        \"\"\"\n        assert isinstance(self.index, ABCMultiIndex)\n        new_index = self.index.swaplevel(i, j)\n        return self._constructor(self._values, index=new_index, copy=copy).__finalize__(\n            self, method=\"swaplevel\"\n        )\n\n    def reorder_levels(self, order) -> \"Series\":\n        \"\"\"\n        Rearrange index levels using input order.\n\n        May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int representing new level order\n            Reference level by number or key.\n\n        Returns\n        -------\n        type of caller (new object)\n        \"\"\"\n        if not isinstance(self.index, MultiIndex):  # pragma: no cover\n            raise Exception(\"Can only reorder levels on a hierarchical axis.\")\n\n        result = self.copy()\n        assert isinstance(result.index, ABCMultiIndex)\n        result.index = result.index.reorder_levels(order)\n        return result\n\n    def explode(self) -> \"Series\":\n        \"\"\"\n        Transform each element of a list-like to a row, replicating the\n        index values.\n\n        .. versionadded:: 0.25.0\n\n        Returns\n        -------\n        Series\n            Exploded lists to rows; index will be duplicated for these rows.\n\n        See Also\n        --------\n        Series.str.split : Split string values on specified separator.\n        Series.unstack : Unstack, a.k.a. pivot, Series with MultiIndex\n            to produce DataFrame.\n        DataFrame.melt : Unpivot a DataFrame from wide format to long format.\n        DataFrame.explode : Explode a DataFrame from list-like\n            columns to long format.\n\n        Notes\n        -----\n        This routine will explode list-likes including lists, tuples,\n        Series, and np.ndarray. The result dtype of the subset rows will\n        be object. Scalars will be returned unchanged. Empty list-likes will\n        result in a np.nan for that row.\n\n        Examples\n        --------\n        >>> s = pd.Series([[1, 2, 3], 'foo', [], [3, 4]])\n        >>> s\n        0    [1, 2, 3]\n        1          foo\n        2           []\n        3       [3, 4]\n        dtype: object\n\n        >>> s.explode()\n        0      1\n        0      2\n        0      3\n        1    foo\n        2    NaN\n        3      3\n        3      4\n        dtype: object\n        \"\"\"\n        if not len(self) or not is_object_dtype(self):\n            return self.copy()\n\n        values, counts = reshape.explode(np.asarray(self.array))\n\n        result = Series(values, index=self.index.repeat(counts), name=self.name)\n        return result\n\n    def unstack(self, level=-1, fill_value=None):\n        \"\"\"\n        Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.\n        The level involved will automatically get sorted.\n\n        Parameters\n        ----------\n        level : int, str, or list of these, default last level\n            Level(s) to unstack, can pass level name.\n        fill_value : scalar value, default None\n            Value to use when replacing NaN values.\n\n        Returns\n        -------\n        DataFrame\n            Unstacked Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4],\n        ...               index=pd.MultiIndex.from_product([['one', 'two'],\n        ...                                                 ['a', 'b']]))\n        >>> s\n        one  a    1\n             b    2\n        two  a    3\n             b    4\n        dtype: int64\n\n        >>> s.unstack(level=-1)\n             a  b\n        one  1  2\n        two  3  4\n\n        >>> s.unstack(level=0)\n           one  two\n        a    1    3\n        b    2    4\n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n\n        return unstack(self, level, fill_value)\n\n    # ----------------------------------------------------------------------\n    # function application\n\n    def map(self, arg, na_action=None) -> \"Series\":\n        \"\"\"\n        Map values of Series according to input correspondence.\n\n        Used for substituting each value in a Series with another value,\n        that may be derived from a function, a ``dict`` or\n        a :class:`Series`.\n\n        Parameters\n        ----------\n        arg : function, collections.abc.Mapping subclass or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NaN values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        Series\n            Same index as caller.\n\n        See Also\n        --------\n        Series.apply : For applying more complex functions on a Series.\n        DataFrame.apply : Apply a function row-/column-wise.\n        DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n\n        Notes\n        -----\n        When ``arg`` is a dictionary, values in Series that are not in the\n        dictionary (as keys) are converted to ``NaN``. However, if the\n        dictionary is a ``dict`` subclass that defines ``__missing__`` (i.e.\n        provides a method for default values), then this default is used\n        rather than ``NaN``.\n\n        Examples\n        --------\n        >>> s = pd.Series(['cat', 'dog', np.nan, 'rabbit'])\n        >>> s\n        0      cat\n        1      dog\n        2      NaN\n        3   rabbit\n        dtype: object\n\n        ``map`` accepts a ``dict`` or a ``Series``. Values that are not found\n        in the ``dict`` are converted to ``NaN``, unless the dict has a default\n        value (e.g. ``defaultdict``):\n\n        >>> s.map({'cat': 'kitten', 'dog': 'puppy'})\n        0   kitten\n        1    puppy\n        2      NaN\n        3      NaN\n        dtype: object\n\n        It also accepts a function:\n\n        >>> s.map('I am a {}'.format)\n        0       I am a cat\n        1       I am a dog\n        2       I am a nan\n        3    I am a rabbit\n        dtype: object\n\n        To avoid applying the function to missing values (and keep them as\n        ``NaN``) ``na_action='ignore'`` can be used:\n\n        >>> s.map('I am a {}'.format, na_action='ignore')\n        0     I am a cat\n        1     I am a dog\n        2            NaN\n        3  I am a rabbit\n        dtype: object\n        \"\"\"\n        new_values = super()._map_values(arg, na_action=na_action)\n        return self._constructor(new_values, index=self.index).__finalize__(\n            self, method=\"map\"\n        )\n\n    def _gotitem(self, key, ndim, subset=None) -> \"Series\":\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : 1,2\n            Requested ndim of result.\n        subset : object, default None\n            Subset to act on.\n        \"\"\"\n        return self\n\n    _agg_see_also_doc = dedent(\n        \"\"\"\n    See Also\n    --------\n    Series.apply : Invoke function on a Series.\n    Series.transform : Transform function producing a Series with like indexes.\n    \"\"\"\n    )\n\n    _agg_examples_doc = dedent(\n        \"\"\"\n    Examples\n    --------\n    >>> s = pd.Series([1, 2, 3, 4])\n    >>> s\n    0    1\n    1    2\n    2    3\n    3    4\n    dtype: int64\n\n    >>> s.agg('min')\n    1\n\n    >>> s.agg(['min', 'max'])\n    min   1\n    max   4\n    dtype: int64\n    \"\"\"\n    )\n\n    @Substitution(\n        see_also=_agg_see_also_doc,\n        examples=_agg_examples_doc,\n        versionadded=\"\\n.. versionadded:: 0.20.0\\n\",\n        **_shared_doc_kwargs,\n    )\n    @Appender(generic._shared_docs[\"aggregate\"])\n    def aggregate(self, func, axis=0, *args, **kwargs):\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n        result, how = self._aggregate(func, *args, **kwargs)\n        if result is None:\n\n            # we can be called from an inner function which\n            # passes this meta-data\n            kwargs.pop(\"_axis\", None)\n            kwargs.pop(\"_level\", None)\n\n            # try a regular apply, this evaluates lambdas\n            # row-by-row; however if the lambda is expected a Series\n            # expression, e.g.: lambda x: x-x.quantile(0.25)\n            # this will fail, so we can try a vectorized evaluation\n\n            # we cannot FIRST try the vectorized evaluation, because\n            # then .agg and .apply would have different semantics if the\n            # operation is actually defined on the Series, e.g. str\n            try:\n                result = self.apply(func, *args, **kwargs)\n            except (ValueError, AttributeError, TypeError):\n                result = func(self, *args, **kwargs)\n\n        return result\n\n    agg = aggregate\n\n    @Appender(generic._shared_docs[\"transform\"] % _shared_doc_kwargs)\n    def transform(self, func, axis=0, *args, **kwargs):\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n        return super().transform(func, *args, **kwargs)\n\n    def apply(self, func, convert_dtype=True, args=(), **kwds):\n        \"\"\"\n        Invoke function on values of Series.\n\n        Can be ufunc (a NumPy function that applies to the entire Series)\n        or a Python function that only works on single values.\n\n        Parameters\n        ----------\n        func : function\n            Python function or NumPy ufunc to apply.\n        convert_dtype : bool, default True\n            Try to find better dtype for elementwise function results. If\n            False, leave as dtype=object.\n        args : tuple\n            Positional arguments passed to func after the series value.\n        **kwds\n            Additional keyword arguments passed to func.\n\n        Returns\n        -------\n        Series or DataFrame\n            If func returns a Series object the result will be a DataFrame.\n\n        See Also\n        --------\n        Series.map: For element-wise operations.\n        Series.agg: Only perform aggregating type operations.\n        Series.transform: Only perform transforming type operations.\n\n        Examples\n        --------\n        Create a series with typical summer temperatures for each city.\n\n        >>> s = pd.Series([20, 21, 12],\n        ...               index=['London', 'New York', 'Helsinki'])\n        >>> s\n        London      20\n        New York    21\n        Helsinki    12\n        dtype: int64\n\n        Square the values by defining a function and passing it as an\n        argument to ``apply()``.\n\n        >>> def square(x):\n        ...     return x ** 2\n        >>> s.apply(square)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Square the values by passing an anonymous function as an\n        argument to ``apply()``.\n\n        >>> s.apply(lambda x: x ** 2)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Define a custom function that needs additional positional\n        arguments and pass these additional arguments using the\n        ``args`` keyword.\n\n        >>> def subtract_custom_value(x, custom_value):\n        ...     return x - custom_value\n\n        >>> s.apply(subtract_custom_value, args=(5,))\n        London      15\n        New York    16\n        Helsinki     7\n        dtype: int64\n\n        Define a custom function that takes keyword arguments\n        and pass these arguments to ``apply``.\n\n        >>> def add_custom_values(x, **kwargs):\n        ...     for month in kwargs:\n        ...         x += kwargs[month]\n        ...     return x\n\n        >>> s.apply(add_custom_values, june=30, july=20, august=25)\n        London      95\n        New York    96\n        Helsinki    87\n        dtype: int64\n\n        Use a function from the Numpy library.\n\n        >>> s.apply(np.log)\n        London      2.995732\n        New York    3.044522\n        Helsinki    2.484907\n        dtype: float64\n        \"\"\"\n        if len(self) == 0:\n            return self._constructor(dtype=self.dtype, index=self.index).__finalize__(\n                self, method=\"apply\"\n            )\n\n        # dispatch to agg\n        if isinstance(func, (list, dict)):\n            return self.aggregate(func, *args, **kwds)\n\n        # if we are a string, try to dispatch\n        if isinstance(func, str):\n            return self._try_aggregate_string_function(func, *args, **kwds)\n\n        # handle ufuncs and lambdas\n        if kwds or args and not isinstance(func, np.ufunc):\n\n            def f(x):\n                return func(x, *args, **kwds)\n\n        else:\n            f = func\n\n        with np.errstate(all=\"ignore\"):\n            if isinstance(f, np.ufunc):\n                return f(self)\n\n            # row-wise access\n            if is_extension_array_dtype(self.dtype) and hasattr(self._values, \"map\"):\n                # GH#23179 some EAs do not have `map`\n                mapped = self._values.map(f)\n            else:\n                values = self.astype(object)._values\n                mapped = lib.map_infer(values, f, convert=convert_dtype)\n\n        if len(mapped) and isinstance(mapped[0], Series):\n            # GH 25959 use pd.array instead of tolist\n            # so extension arrays can be used\n            return self._constructor_expanddim(pd.array(mapped), index=self.index)\n        else:\n            return self._constructor(mapped, index=self.index).__finalize__(\n                self, method=\"apply\"\n            )\n\n    def _reduce(\n        self, op, name, axis=0, skipna=True, numeric_only=None, filter_type=None, **kwds\n    ):\n        \"\"\"\n        Perform a reduction operation.\n\n        If we have an ndarray as a value, then simply perform the operation,\n        otherwise delegate to the object.\n        \"\"\"\n        delegate = self._values\n\n        if axis is not None:\n            self._get_axis_number(axis)\n\n        if isinstance(delegate, ExtensionArray):\n            # dispatch to ExtensionArray interface\n            return delegate._reduce(name, skipna=skipna, **kwds)\n\n        else:\n            # dispatch to numpy arrays\n            if numeric_only:\n                raise NotImplementedError(\n                    f\"Series.{name} does not implement numeric_only.\"\n                )\n            with np.errstate(all=\"ignore\"):\n                return op(delegate, skipna=skipna, **kwds)\n\n    def _reindex_indexer(self, new_index, indexer, copy):\n        if indexer is None:\n            if copy:\n                return self.copy()\n            return self\n\n        new_values = algorithms.take_1d(\n            self._values, indexer, allow_fill=True, fill_value=None\n        )\n        return self._constructor(new_values, index=new_index)\n\n    def _needs_reindex_multi(self, axes, method, level):\n        \"\"\"\n        Check if we do need a multi reindex; this is for compat with\n        higher dims.\n        \"\"\"\n        return False\n\n    @doc(NDFrame.align, **_shared_doc_kwargs)\n    def align(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy=True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n        broadcast_axis=None,\n    ):\n        return super().align(\n            other,\n            join=join,\n            axis=axis,\n            level=level,\n            copy=copy,\n            fill_value=fill_value,\n            method=method,\n            limit=limit,\n            fill_axis=fill_axis,\n            broadcast_axis=broadcast_axis,\n        )\n\n    def rename(\n        self,\n        index=None,\n        *,\n        axis=None,\n        copy=True,\n        inplace=False,\n        level=None,\n        errors=\"ignore\",\n    ):\n        \"\"\"\n        Alter Series index labels or name.\n\n        Function / dict values must be unique (1-to-1). Labels not contained in\n        a dict / Series will be left as-is. Extra labels listed don't throw an\n        error.\n\n        Alternatively, change ``Series.name`` with a scalar value.\n\n        See the :ref:`user guide <basics.rename>` for more.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Unused. Accepted for compatibility with DataFrame method only.\n        index : scalar, hashable sequence, dict-like or function, optional\n            Functions or dict-like are transformations to apply to\n            the index.\n            Scalar or hashable sequence-like will alter the ``Series.name``\n            attribute.\n\n        **kwargs\n            Additional keyword arguments passed to the function. Only the\n            \"inplace\" keyword is used.\n\n        Returns\n        -------\n        Series\n            Series with index labels or name altered.\n\n        See Also\n        --------\n        DataFrame.rename : Corresponding DataFrame method.\n        Series.rename_axis : Set the name of the axis.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n        >>> s.rename(\"my_name\")  # scalar, changes Series.name\n        0    1\n        1    2\n        2    3\n        Name: my_name, dtype: int64\n        >>> s.rename(lambda x: x ** 2)  # function, changes labels\n        0    1\n        1    2\n        4    3\n        dtype: int64\n        >>> s.rename({1: 3, 2: 5})  # mapping, changes labels\n        0    1\n        3    2\n        5    3\n        dtype: int64\n        \"\"\"\n        if callable(index) or is_dict_like(index):\n            return super().rename(\n                index, copy=copy, inplace=inplace, level=level, errors=errors\n            )\n        else:\n            return self._set_name(index, inplace=inplace)\n\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n\n        >>> s.set_axis(['a', 'b', 'c'], axis=0)\n        a    1\n        b    2\n        c    3\n        dtype: int64\n    \"\"\"\n    )\n    @Substitution(\n        **_shared_doc_kwargs,\n        extended_summary_sub=\"\",\n        axis_description_sub=\"\",\n        see_also_sub=\"\",\n    )\n    @Appender(generic.NDFrame.set_axis.__doc__)\n    def set_axis(self, labels, axis: Axis = 0, inplace: bool = False):\n        return super().set_axis(labels, axis=axis, inplace=inplace)\n\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(generic.NDFrame.reindex.__doc__)\n    def reindex(self, index=None, **kwargs):\n        return super().reindex(index=index, **kwargs)\n\n    def drop(\n        self,\n        labels=None,\n        axis=0,\n        index=None,\n        columns=None,\n        level=None,\n        inplace=False,\n        errors=\"raise\",\n    ) -> \"Series\":\n        \"\"\"\n        Return Series with specified index labels removed.\n\n        Remove elements of a Series based on specifying the index labels.\n        When using a multi-index, labels on different levels can be removed\n        by specifying the level.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index labels to drop.\n        axis : 0, default 0\n            Redundant for application on Series.\n        index : single label or list-like\n            Redundant for application on Series, but 'index' can be used instead\n            of 'labels'.\n        columns : single label or list-like\n            No change is made to the Series; use 'index' or 'labels' instead.\n        level : int or level name, optional\n            For MultiIndex, level for which the labels will be removed.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and only existing labels are dropped.\n\n        Returns\n        -------\n        Series\n            Series with specified index labels removed.\n\n        Raises\n        ------\n        KeyError\n            If none of the labels are found in the index.\n\n        See Also\n        --------\n        Series.reindex : Return only specified index labels of Series.\n        Series.dropna : Return series without null values.\n        Series.drop_duplicates : Return Series with duplicate values removed.\n        DataFrame.drop : Drop specified labels from rows or columns.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=np.arange(3), index=['A', 'B', 'C'])\n        >>> s\n        A  0\n        B  1\n        C  2\n        dtype: int64\n\n        Drop labels B en C\n\n        >>> s.drop(labels=['B', 'C'])\n        A  0\n        dtype: int64\n\n        Drop 2nd level label in MultiIndex Series\n\n        >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n        ...                              ['speed', 'weight', 'length']],\n        ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> s = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...               index=midx)\n        >>> s\n        lama    speed      45.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        dtype: float64\n\n        >>> s.drop(labels='weight', level=1)\n        lama    speed      45.0\n                length      1.2\n        cow     speed      30.0\n                length      1.5\n        falcon  speed     320.0\n                length      0.3\n        dtype: float64\n        \"\"\"\n        return super().drop(\n            labels=labels,\n            axis=axis,\n            index=index,\n            columns=columns,\n            level=level,\n            inplace=inplace,\n            errors=errors,\n        )\n\n    @doc(NDFrame.fillna, **_shared_doc_kwargs)\n    def fillna(\n        self,\n        value=None,\n        method=None,\n        axis=None,\n        inplace=False,\n        limit=None,\n        downcast=None,\n    ) -> Optional[\"Series\"]:\n        return super().fillna(\n            value=value,\n            method=method,\n            axis=axis,\n            inplace=inplace,\n            limit=limit,\n            downcast=downcast,\n        )\n\n    @doc(NDFrame.replace, **_shared_doc_kwargs)\n    def replace(\n        self,\n        to_replace=None,\n        value=None,\n        inplace=False,\n        limit=None,\n        regex=False,\n        method=\"pad\",\n    ):\n        return super().replace(\n            to_replace=to_replace,\n            value=value,\n            inplace=inplace,\n            limit=limit,\n            regex=regex,\n            method=method,\n        )\n\n    @doc(NDFrame.shift, **_shared_doc_kwargs)\n    def shift(self, periods=1, freq=None, axis=0, fill_value=None) -> \"Series\":\n        return super().shift(\n            periods=periods, freq=freq, axis=axis, fill_value=fill_value\n        )\n\n    def memory_usage(self, index=True, deep=False):\n        \"\"\"\n        Return the memory usage of the Series.\n\n        The memory usage can optionally include the contribution of\n        the index and of elements of `object` dtype.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the Series index.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned value.\n\n        Returns\n        -------\n        int\n            Bytes of memory consumed.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\n            array.\n        DataFrame.memory_usage : Bytes consumed by a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(range(3))\n        >>> s.memory_usage()\n        152\n\n        Not including the index gives the size of the rest of the data, which\n        is necessarily smaller:\n\n        >>> s.memory_usage(index=False)\n        24\n\n        The memory footprint of `object` values is ignored by default:\n\n        >>> s = pd.Series([\"a\", \"b\"])\n        >>> s.values\n        array(['a', 'b'], dtype=object)\n        >>> s.memory_usage()\n        144\n        >>> s.memory_usage(deep=True)\n        260\n        \"\"\"\n        v = super().memory_usage(deep=deep)\n        if index:\n            v += self.index.memory_usage(deep=deep)\n        return v\n\n    def isin(self, values) -> \"Series\":\n        \"\"\"\n        Check whether `values` are contained in Series.\n\n        Return a boolean Series showing whether each element in the Series\n        matches an element in the passed sequence of `values` exactly.\n\n        Parameters\n        ----------\n        values : set or list-like\n            The sequence of values to test. Passing in a single string will\n            raise a ``TypeError``. Instead, turn a single string into a\n            list of one element.\n\n        Returns\n        -------\n        Series\n            Series of booleans indicating if each element is in values.\n\n        Raises\n        ------\n        TypeError\n          * If `values` is a string\n\n        See Also\n        --------\n        DataFrame.isin : Equivalent method on DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama',\n        ...                'hippo'], name='animal')\n        >>> s.isin(['cow', 'lama'])\n        0     True\n        1     True\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n\n        Passing a single string as ``s.isin('lama')`` will raise an error. Use\n        a list of one element instead:\n\n        >>> s.isin(['lama'])\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n        \"\"\"\n        result = algorithms.isin(self, values)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"isin\"\n        )\n\n    def between(self, left, right, inclusive=True) -> \"Series\":\n        \"\"\"\n        Return boolean Series equivalent to left <= series <= right.\n\n        This function returns a boolean vector containing `True` wherever the\n        corresponding Series element is between the boundary values `left` and\n        `right`. NA values are treated as `False`.\n\n        Parameters\n        ----------\n        left : scalar or list-like\n            Left boundary.\n        right : scalar or list-like\n            Right boundary.\n        inclusive : bool, default True\n            Include boundaries.\n\n        Returns\n        -------\n        Series\n            Series representing whether each element is between left and\n            right (inclusive).\n\n        See Also\n        --------\n        Series.gt : Greater than of series and other.\n        Series.lt : Less than of series and other.\n\n        Notes\n        -----\n        This function is equivalent to ``(left <= ser) & (ser <= right)``\n\n        Examples\n        --------\n        >>> s = pd.Series([2, 0, 4, 8, np.nan])\n\n        Boundary values are included by default:\n\n        >>> s.between(1, 4)\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        With `inclusive` set to ``False`` boundary values are excluded:\n\n        >>> s.between(1, 4, inclusive=False)\n        0     True\n        1    False\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        `left` and `right` can be any scalar value:\n\n        >>> s = pd.Series(['Alice', 'Bob', 'Carol', 'Eve'])\n        >>> s.between('Anna', 'Daniel')\n        0    False\n        1     True\n        2     True\n        3    False\n        dtype: bool\n        \"\"\"\n        if inclusive:\n            lmask = self >= left\n            rmask = self <= right\n        else:\n            lmask = self > left\n            rmask = self < right\n\n        return lmask & rmask\n\n    # ----------------------------------------------------------------------\n    # Convert to types that support pd.NA\n\n    def _convert_dtypes(\n        self,\n        infer_objects: bool = True,\n        convert_string: bool = True,\n        convert_integer: bool = True,\n        convert_boolean: bool = True,\n    ) -> \"Series\":\n        input_series = self\n        if infer_objects:\n            input_series = input_series.infer_objects()\n            if is_object_dtype(input_series):\n                input_series = input_series.copy()\n\n        if convert_string or convert_integer or convert_boolean:\n            inferred_dtype = convert_dtypes(\n                input_series._values, convert_string, convert_integer, convert_boolean\n            )\n            try:\n                result = input_series.astype(inferred_dtype)\n            except TypeError:\n                result = input_series.copy()\n        else:\n            result = input_series.copy()\n        return result\n\n    @Appender(generic._shared_docs[\"isna\"] % _shared_doc_kwargs)\n    def isna(self) -> \"Series\":\n        return super().isna()\n\n    @Appender(generic._shared_docs[\"isna\"] % _shared_doc_kwargs)\n    def isnull(self) -> \"Series\":\n        return super().isnull()\n\n    @Appender(generic._shared_docs[\"notna\"] % _shared_doc_kwargs)\n    def notna(self) -> \"Series\":\n        return super().notna()\n\n    @Appender(generic._shared_docs[\"notna\"] % _shared_doc_kwargs)\n    def notnull(self) -> \"Series\":\n        return super().notnull()\n\n    def dropna(self, axis=0, inplace=False, how=None):\n        \"\"\"\n        Return a new Series with missing values removed.\n\n        See the :ref:`User Guide <missing_data>` for more on which values are\n        considered missing, and how to work with missing data.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            There is only one axis to drop values from.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        how : str, optional\n            Not in use. Kept for compatibility.\n\n        Returns\n        -------\n        Series\n            Series with NA entries dropped from it.\n\n        See Also\n        --------\n        Series.isna: Indicate missing values.\n        Series.notna : Indicate existing (non-missing) values.\n        Series.fillna : Replace missing values.\n        DataFrame.dropna : Drop rows or columns which contain NA values.\n        Index.dropna : Drop missing indices.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1., 2., np.nan])\n        >>> ser\n        0    1.0\n        1    2.0\n        2    NaN\n        dtype: float64\n\n        Drop NA values from a Series.\n\n        >>> ser.dropna()\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Keep the Series with valid entries in the same variable.\n\n        >>> ser.dropna(inplace=True)\n        >>> ser\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Empty strings are not considered NA values. ``None`` is considered an\n        NA value.\n\n        >>> ser = pd.Series([np.NaN, 2, pd.NaT, '', None, 'I stay'])\n        >>> ser\n        0       NaN\n        1         2\n        2       NaT\n        3\n        4      None\n        5    I stay\n        dtype: object\n        >>> ser.dropna()\n        1         2\n        3\n        5    I stay\n        dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis or 0)\n\n        if self._can_hold_na:\n            result = remove_na_arraylike(self)\n            if inplace:\n                self._update_inplace(result)\n            else:\n                return result\n        else:\n            if inplace:\n                # do nothing\n                pass\n            else:\n                return self.copy()\n\n    # ----------------------------------------------------------------------\n    # Time series-oriented methods\n\n    def to_timestamp(self, freq=None, how=\"start\", copy=True) -> \"Series\":\n        \"\"\"\n        Cast to DatetimeIndex of Timestamps, at *beginning* of period.\n\n        Parameters\n        ----------\n        freq : str, default frequency of PeriodIndex\n            Desired frequency.\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series with DatetimeIndex\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        assert isinstance(self.index, (ABCDatetimeIndex, ABCPeriodIndex))\n        new_index = self.index.to_timestamp(freq=freq, how=how)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_timestamp\"\n        )\n\n    def to_period(self, freq=None, copy=True) -> \"Series\":\n        \"\"\"\n        Convert Series from DatetimeIndex to PeriodIndex with desired\n        frequency (inferred from index if not passed).\n\n        Parameters\n        ----------\n        freq : str, default None\n            Frequency associated with the PeriodIndex.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series\n            Series with index converted to PeriodIndex.\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        assert isinstance(self.index, ABCDatetimeIndex)\n        new_index = self.index.to_period(freq=freq)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_period\"\n        )\n\n    # ----------------------------------------------------------------------\n    # Add index\n    _AXIS_ORDERS = [\"index\"]\n    _AXIS_NUMBERS = {\"index\": 0}\n    _AXIS_NAMES = {0: \"index\"}\n    _AXIS_REVERSED = False\n    _AXIS_LEN = len(_AXIS_ORDERS)\n    _info_axis_number = 0\n    _info_axis_name = \"index\"\n\n    index: \"Index\" = properties.AxisProperty(\n        axis=0, doc=\"The index (axis labels) of the Series.\"\n    )\n\n    # ----------------------------------------------------------------------\n    # Accessor Methods\n    # ----------------------------------------------------------------------\n    str = CachedAccessor(\"str\", StringMethods)\n    dt = CachedAccessor(\"dt\", CombinedDatetimelikeProperties)\n    cat = CachedAccessor(\"cat\", CategoricalAccessor)\n    plot = CachedAccessor(\"plot\", pandas.plotting.PlotAccessor)\n    sparse = CachedAccessor(\"sparse\", SparseAccessor)\n\n    # ----------------------------------------------------------------------\n    # Add plotting methods to Series\n    hist = pandas.plotting.hist_series\n\n\nSeries._add_numeric_operations()\nSeries._add_series_or_dataframe_operations()\n\n# Add arithmetic!\nops.add_flex_arithmetic_methods(Series)\nops.add_special_arithmetic_methods(Series)\n\n\n\"\"\"\nData structure for 1-dimensional cross-sectional and time series data\n\"\"\"\nfrom io import StringIO\nfrom shutil import get_terminal_size\nfrom textwrap import dedent\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import lib, properties, reshape, tslibs\nfrom pandas._typing import ArrayLike, Axis, DtypeObj, Label\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import Appender, Substitution, doc\nfrom pandas.util._validators import validate_bool_kwarg, validate_percentile\n\nfrom pandas.core.dtypes.cast import (\n    convert_dtypes,\n    maybe_cast_to_extension_array,\n    validate_numeric_casting,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_platform_int,\n    is_bool,\n    is_categorical_dtype,\n    is_dict_like,\n    is_extension_array_dtype,\n    is_integer,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_scalar,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame,\n    ABCDatetimeIndex,\n    ABCMultiIndex,\n    ABCPeriodIndex,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.inference import is_hashable\nfrom pandas.core.dtypes.missing import (\n    isna,\n    na_value_for_dtype,\n    notna,\n    remove_na_arraylike,\n)\n\nimport pandas as pd\nfrom pandas.core import algorithms, base, generic, nanops, ops\nfrom pandas.core.accessor import CachedAccessor\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.arrays.categorical import CategoricalAccessor\nfrom pandas.core.arrays.sparse import SparseAccessor\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    create_series_with_explicit_dtype,\n    extract_array,\n    is_empty_data,\n    sanitize_array,\n)\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.indexers import unpack_1tuple\nfrom pandas.core.indexes.accessors import CombinedDatetimelikeProperties\nfrom pandas.core.indexes.api import (\n    Float64Index,\n    Index,\n    InvalidIndexError,\n    MultiIndex,\n    ensure_index,\n)\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.datetimes import DatetimeIndex\nfrom pandas.core.indexes.period import PeriodIndex\nfrom pandas.core.indexes.timedeltas import TimedeltaIndex\nfrom pandas.core.indexing import check_bool_indexer\nfrom pandas.core.internals import SingleBlockManager\nfrom pandas.core.strings import StringMethods\nfrom pandas.core.tools.datetimes import to_datetime\n\nimport pandas.io.formats.format as fmt\nimport pandas.plotting\n\nif TYPE_CHECKING:\n    from pandas.core.frame import DataFrame\n    from pandas.core.groupby.generic import SeriesGroupBy\n\n__all__ = [\"Series\"]\n\n_shared_doc_kwargs = dict(\n    axes=\"index\",\n    klass=\"Series\",\n    axes_single_arg=\"{0 or 'index'}\",\n    axis=\"\"\"axis : {0 or 'index'}\n        Parameter needed for compatibility with DataFrame.\"\"\",\n    inplace=\"\"\"inplace : boolean, default False\n        If True, performs operation inplace and returns None.\"\"\",\n    unique=\"np.ndarray\",\n    duplicated=\"Series\",\n    optional_by=\"\",\n    optional_mapper=\"\",\n    optional_labels=\"\",\n    optional_axis=\"\",\n    versionadded_to_excel=\"\\n    .. versionadded:: 0.20.0\\n\",\n)\n\n\ndef _coerce_method(converter):\n    \"\"\"\n    Install the scalar coercion methods.\n    \"\"\"\n\n    def wrapper(self):\n        if len(self) == 1:\n            return converter(self.iloc[0])\n        raise TypeError(f\"cannot convert the series to {converter}\")\n\n    wrapper.__name__ = f\"__{converter.__name__}__\"\n    return wrapper\n\n\n# ----------------------------------------------------------------------\n# Series class\n\n\nclass Series(base.IndexOpsMixin, generic.NDFrame):\n    \"\"\"\n    One-dimensional ndarray with axis labels (including time series).\n\n    Labels need not be unique but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for performing operations involving the index. Statistical\n    methods from ndarray have been overridden to automatically exclude\n    missing data (currently represented as NaN).\n\n    Operations between Series (+, -, /, *, **) align values based on their\n    associated index values-- they need not be the same length. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Series.\n\n        .. versionchanged:: 0.23.0\n           If data is a dict, argument order is maintained for Python 3.6\n           and later.\n\n    index : array-like or Index (1d)\n        Values must be hashable and have the same length as `data`.\n        Non-unique index values are allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If both a dict and index\n        sequence are used, the index will override the keys found in the\n        dict.\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\n        Data type for the output Series. If not specified, this will be\n        inferred from `data`.\n        See the :ref:`user guide <basics.dtypes>` for more usages.\n    name : str, optional\n        The name to give to the Series.\n    copy : bool, default False\n        Copy input data.\n    \"\"\"\n\n    _typ = \"series\"\n\n    _name: Label\n    _metadata: List[str] = [\"name\"]\n    _internal_names_set = {\"index\"} | generic.NDFrame._internal_names_set\n    _accessors = {\"dt\", \"cat\", \"str\", \"sparse\"}\n    _deprecations = (\n        base.IndexOpsMixin._deprecations\n        | generic.NDFrame._deprecations\n        | frozenset([\"compress\", \"ptp\"])\n    )\n\n    # Override cache_readonly bc Series is mutable\n    hasnans = property(\n        base.IndexOpsMixin.hasnans.func, doc=base.IndexOpsMixin.hasnans.__doc__\n    )\n    _mgr: SingleBlockManager\n    div: Callable[[\"Series\", Any], \"Series\"]\n    rdiv: Callable[[\"Series\", Any], \"Series\"]\n\n    # ----------------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self, data=None, index=None, dtype=None, name=None, copy=False, fastpath=False\n    ):\n\n        if (\n            isinstance(data, SingleBlockManager)\n            and index is None\n            and dtype is None\n            and copy is False\n        ):\n            # GH#33357 called with just the SingleBlockManager\n            NDFrame.__init__(self, data)\n            self.name = name\n            return\n\n        # we are called internally, so short-circuit\n        if fastpath:\n\n            # data is an ndarray, index is defined\n            if not isinstance(data, SingleBlockManager):\n                data = SingleBlockManager.from_array(data, index)\n            if copy:\n                data = data.copy()\n            if index is None:\n                index = data.index\n\n        else:\n\n            name = ibase.maybe_extract_name(name, data, type(self))\n\n            if is_empty_data(data) and dtype is None:\n                # gh-17261\n                warnings.warn(\n                    \"The default dtype for empty Series will be 'object' instead \"\n                    \"of 'float64' in a future version. Specify a dtype explicitly \"\n                    \"to silence this warning.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n                # uncomment the line below when removing the DeprecationWarning\n                # dtype = np.dtype(object)\n\n            if index is not None:\n                index = ensure_index(index)\n\n            if data is None:\n                data = {}\n            if dtype is not None:\n                dtype = self._validate_dtype(dtype)\n\n            if isinstance(data, MultiIndex):\n                raise NotImplementedError(\n                    \"initializing a Series from a MultiIndex is not supported\"\n                )\n            elif isinstance(data, Index):\n\n                if dtype is not None:\n                    # astype copies\n                    data = data.astype(dtype)\n                else:\n                    # need to copy to avoid aliasing issues\n                    data = data._values.copy()\n                    if isinstance(data, ABCDatetimeIndex) and data.tz is not None:\n                        # GH#24096 need copy to be deep for datetime64tz case\n                        # TODO: See if we can avoid these copies\n                        data = data._values.copy(deep=True)\n                copy = False\n\n            elif isinstance(data, np.ndarray):\n                if len(data.dtype):\n                    # GH#13296 we are dealing with a compound dtype, which\n                    #  should be treated as 2D\n                    raise ValueError(\n                        \"Cannot construct a Series from an ndarray with \"\n                        \"compound dtype.  Use DataFrame instead.\"\n                    )\n                pass\n            elif isinstance(data, ABCSeries):\n                if index is None:\n                    index = data.index\n                else:\n                    data = data.reindex(index, copy=copy)\n                data = data._mgr\n            elif is_dict_like(data):\n                data, index = self._init_dict(data, index, dtype)\n                dtype = None\n                copy = False\n            elif isinstance(data, SingleBlockManager):\n                if index is None:\n                    index = data.index\n                elif not data.index.equals(index) or copy:\n                    # GH#19275 SingleBlockManager input should only be called\n                    # internally\n                    raise AssertionError(\n                        \"Cannot pass both SingleBlockManager \"\n                        \"`data` argument and a different \"\n                        \"`index` argument. `copy` must be False.\"\n                    )\n\n            elif is_extension_array_dtype(data):\n                pass\n            elif isinstance(data, (set, frozenset)):\n                raise TypeError(f\"'{type(data).__name__}' type is unordered\")\n            else:\n                data = com.maybe_iterable_to_list(data)\n\n            if index is None:\n                if not is_list_like(data):\n                    data = [data]\n                index = ibase.default_index(len(data))\n            elif is_list_like(data):\n\n                # a scalar numpy array is list-like but doesn't\n                # have a proper length\n                try:\n                    if len(index) != len(data):\n                        raise ValueError(\n                            f\"Length of passed values is {len(data)}, \"\n                            f\"index implies {len(index)}.\"\n                        )\n                except TypeError:\n                    pass\n\n            # create/copy the manager\n            if isinstance(data, SingleBlockManager):\n                if dtype is not None:\n                    data = data.astype(dtype=dtype, errors=\"ignore\", copy=copy)\n                elif copy:\n                    data = data.copy()\n            else:\n                data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True)\n\n                data = SingleBlockManager.from_array(data, index)\n\n        generic.NDFrame.__init__(self, data)\n        self.name = name\n        self._set_axis(0, index, fastpath=True)\n\n    def _init_dict(self, data, index=None, dtype=None):\n        \"\"\"\n        Derive the \"_mgr\" and \"index\" attributes of a new Series from a\n        dictionary input.\n\n        Parameters\n        ----------\n        data : dict or dict-like\n            Data used to populate the new Series.\n        index : Index or index-like, default None\n            Index for the new Series: if None, use dict keys.\n        dtype : dtype, default None\n            The dtype for the new Series: if None, infer from data.\n\n        Returns\n        -------\n        _data : BlockManager for the new Series\n        index : index for the new Series\n        \"\"\"\n        # Looking for NaN in dict doesn't work ({np.nan : 1}[float('nan')]\n        # raises KeyError), so we iterate the entire dict, and align\n        if data:\n            keys, values = zip(*data.items())\n            values = list(values)\n        elif index is not None:\n            # fastpath for Series(data=None). Just use broadcasting a scalar\n            # instead of reindexing.\n            values = na_value_for_dtype(dtype)\n            keys = index\n        else:\n            keys, values = [], []\n\n        # Input is now list-like, so rely on \"standard\" construction:\n\n        # TODO: passing np.float64 to not break anything yet. See GH-17261\n        s = create_series_with_explicit_dtype(\n            values, index=keys, dtype=dtype, dtype_if_empty=np.float64\n        )\n\n        # Now we just make sure the order is respected, if any\n        if data and index is not None:\n            s = s.reindex(index, copy=False)\n        return s._mgr, s.index\n\n    # ----------------------------------------------------------------------\n\n    @property\n    def _constructor(self) -> Type[\"Series\"]:\n        return Series\n\n    @property\n    def _constructor_expanddim(self) -> Type[\"DataFrame\"]:\n        from pandas.core.frame import DataFrame\n\n        return DataFrame\n\n    # types\n    @property\n    def _can_hold_na(self):\n        return self._mgr._can_hold_na\n\n    _index = None\n\n    def _set_axis(self, axis: int, labels, fastpath: bool = False) -> None:\n        \"\"\"\n        Override generic, we want to set the _typ here.\n\n        This is called from the cython code when we set the `index` attribute\n        directly, e.g. `series.index = [1, 2, 3]`.\n        \"\"\"\n        if not fastpath:\n            labels = ensure_index(labels)\n\n        is_all_dates = labels.is_all_dates\n        if is_all_dates:\n            if not isinstance(labels, (DatetimeIndex, PeriodIndex, TimedeltaIndex)):\n                try:\n                    labels = DatetimeIndex(labels)\n                    # need to set here because we changed the index\n                    if fastpath:\n                        self._mgr.set_axis(axis, labels)\n                except (tslibs.OutOfBoundsDatetime, ValueError):\n                    # labels may exceeds datetime bounds,\n                    # or not be a DatetimeIndex\n                    pass\n\n        object.__setattr__(self, \"_index\", labels)\n        if not fastpath:\n            # The ensure_index call above ensures we have an Index object\n            self._mgr.set_axis(axis, labels)\n\n    # ndarray compatibility\n    @property\n    def dtype(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._mgr.dtype\n\n    @property\n    def dtypes(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        # DataFrame compatibility\n        return self.dtype\n\n    @property\n    def name(self) -> Label:\n        \"\"\"\n        Return the name of the Series.\n\n        The name of a Series becomes its index or column name if it is used\n        to form a DataFrame. It is also used whenever displaying the Series\n        using the interpreter.\n\n        Returns\n        -------\n        label (hashable object)\n            The name of the Series, also the column name if part of a DataFrame.\n\n        See Also\n        --------\n        Series.rename : Sets the Series name when given a scalar input.\n        Index.name : Corresponding Index property.\n\n        Examples\n        --------\n        The Series name can be set initially when calling the constructor.\n\n        >>> s = pd.Series([1, 2, 3], dtype=np.int64, name='Numbers')\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Numbers, dtype: int64\n        >>> s.name = \"Integers\"\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Integers, dtype: int64\n\n        The name of a Series within a DataFrame is its column name.\n\n        >>> df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],\n        ...                   columns=[\"Odd Numbers\", \"Even Numbers\"])\n        >>> df\n           Odd Numbers  Even Numbers\n        0            1             2\n        1            3             4\n        2            5             6\n        >>> df[\"Even Numbers\"].name\n        'Even Numbers'\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value: Label) -> None:\n        if not is_hashable(value):\n            raise TypeError(\"Series.name must be a hashable type\")\n        object.__setattr__(self, \"_name\", value)\n\n    @property\n    def values(self):\n        \"\"\"\n        Return Series as ndarray or ndarray-like depending on the dtype.\n\n        .. warning::\n\n           We recommend using :attr:`Series.array` or\n           :meth:`Series.to_numpy`, depending on whether you need\n           a reference to the underlying data or a NumPy array.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n\n        See Also\n        --------\n        Series.array : Reference to the underlying data.\n        Series.to_numpy : A NumPy array representing the underlying data.\n\n        Examples\n        --------\n        >>> pd.Series([1, 2, 3]).values\n        array([1, 2, 3])\n\n        >>> pd.Series(list('aabc')).values\n        array(['a', 'a', 'b', 'c'], dtype=object)\n\n        >>> pd.Series(list('aabc')).astype('category').values\n        [a, a, b, c]\n        Categories (3, object): [a, b, c]\n\n        Timezone aware datetime data is converted to UTC:\n\n        >>> pd.Series(pd.date_range('20130101', periods=3,\n        ...                         tz='US/Eastern')).values\n        array(['2013-01-01T05:00:00.000000000',\n               '2013-01-02T05:00:00.000000000',\n               '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')\n        \"\"\"\n        return self._mgr.external_values()\n\n    @property\n    def _values(self):\n        \"\"\"\n        Return the internal repr of this data (defined by Block.interval_values).\n        This are the values as stored in the Block (ndarray or ExtensionArray\n        depending on the Block class), with datetime64[ns] and timedelta64[ns]\n        wrapped in ExtensionArrays to match Index._values behavior.\n\n        Differs from the public ``.values`` for certain data types, because of\n        historical backwards compatibility of the public attribute (e.g. period\n        returns object ndarray and datetimetz a datetime64[ns] ndarray for\n        ``.values`` while it returns an ExtensionArray for ``._values`` in those\n        cases).\n\n        Differs from ``.array`` in that this still returns the numpy array if\n        the Block is backed by a numpy array (except for datetime64 and\n        timedelta64 dtypes), while ``.array`` ensures to always return an\n        ExtensionArray.\n\n        Overview:\n\n        dtype       | values        | _values       | array         |\n        ----------- | ------------- | ------------- | ------------- |\n        Numeric     | ndarray       | ndarray       | PandasArray   |\n        Category    | Categorical   | Categorical   | Categorical   |\n        dt64[ns]    | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        dt64[ns tz] | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        td64[ns]    | ndarray[m8ns] | TimedeltaArray| ndarray[m8ns] |\n        Period      | ndarray[obj]  | PeriodArray   | PeriodArray   |\n        Nullable    | EA            | EA            | EA            |\n\n        \"\"\"\n        return self._mgr.internal_values()\n\n    @Appender(base.IndexOpsMixin.array.__doc__)  # type: ignore\n    @property\n    def array(self) -> ExtensionArray:\n        return self._mgr._block.array_values()\n\n    # ops\n    def ravel(self, order=\"C\"):\n        \"\"\"\n        Return the flattened underlying data as an ndarray.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n            Flattened data of the Series.\n\n        See Also\n        --------\n        numpy.ndarray.ravel : Return a flattened array.\n        \"\"\"\n        return self._values.ravel(order=order)\n\n    def __len__(self) -> int:\n        \"\"\"\n        Return the length of the Series.\n        \"\"\"\n        return len(self._mgr)\n\n    def view(self, dtype=None) -> \"Series\":\n        \"\"\"\n        Create a new view of the Series.\n\n        This function will return a new Series with a view of the same\n        underlying values in memory, optionally reinterpreted with a new data\n        type. The new data type must preserve the same size in bytes as to not\n        cause index misalignment.\n\n        Parameters\n        ----------\n        dtype : data type\n            Data type object or one of their string representations.\n\n        Returns\n        -------\n        Series\n            A new Series object as a view of the same data in memory.\n\n        See Also\n        --------\n        numpy.ndarray.view : Equivalent numpy function to create a new view of\n            the same data in memory.\n\n        Notes\n        -----\n        Series are instantiated with ``dtype=float64`` by default. While\n        ``numpy.ndarray.view()`` will return a view with the same data type as\n        the original array, ``Series.view()`` (without specified dtype)\n        will try using ``float64`` and may fail if the original data type size\n        in bytes is not the same.\n\n        Examples\n        --------\n        >>> s = pd.Series([-2, -1, 0, 1, 2], dtype='int8')\n        >>> s\n        0   -2\n        1   -1\n        2    0\n        3    1\n        4    2\n        dtype: int8\n\n        The 8 bit signed integer representation of `-1` is `0b11111111`, but\n        the same bytes represent 255 if read as an 8 bit unsigned integer:\n\n        >>> us = s.view('uint8')\n        >>> us\n        0    254\n        1    255\n        2      0\n        3      1\n        4      2\n        dtype: uint8\n\n        The views share the same underlying values:\n\n        >>> us[0] = 128\n        >>> s\n        0   -128\n        1     -1\n        2      0\n        3      1\n        4      2\n        dtype: int8\n        \"\"\"\n        return self._constructor(\n            self._values.view(dtype), index=self.index\n        ).__finalize__(self, method=\"view\")\n\n    # ----------------------------------------------------------------------\n    # NDArray Compat\n    _HANDLED_TYPES = (Index, ExtensionArray, np.ndarray)\n\n    def __array_ufunc__(\n        self, ufunc: Callable, method: str, *inputs: Any, **kwargs: Any\n    ):\n        # TODO: handle DataFrame\n        cls = type(self)\n\n        # for binary ops, use our custom dunder methods\n        result = ops.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        # Determine if we should defer.\n        no_defer = (np.ndarray.__array_ufunc__, cls.__array_ufunc__)\n\n        for item in inputs:\n            higher_priority = (\n                hasattr(item, \"__array_priority__\")\n                and item.__array_priority__ > self.__array_priority__\n            )\n            has_array_ufunc = (\n                hasattr(item, \"__array_ufunc__\")\n                and type(item).__array_ufunc__ not in no_defer\n                and not isinstance(item, self._HANDLED_TYPES)\n            )\n            if higher_priority or has_array_ufunc:\n                return NotImplemented\n\n        # align all the inputs.\n        names = [getattr(x, \"name\") for x in inputs if hasattr(x, \"name\")]\n        types = tuple(type(x) for x in inputs)\n        # TODO: dataframe\n        alignable = [x for x, t in zip(inputs, types) if issubclass(t, Series)]\n\n        if len(alignable) > 1:\n            # This triggers alignment.\n            # At the moment, there aren't any ufuncs with more than two inputs\n            # so this ends up just being x1.index | x2.index, but we write\n            # it to handle *args.\n            index = alignable[0].index\n            for s in alignable[1:]:\n                index |= s.index\n            inputs = tuple(\n                x.reindex(index) if issubclass(t, Series) else x\n                for x, t in zip(inputs, types)\n            )\n        else:\n            index = self.index\n\n        inputs = tuple(extract_array(x, extract_numpy=True) for x in inputs)\n        result = getattr(ufunc, method)(*inputs, **kwargs)\n\n        name = names[0] if len(set(names)) == 1 else None\n\n        def construct_return(result):\n            if lib.is_scalar(result):\n                return result\n            elif result.ndim > 1:\n                # e.g. np.subtract.outer\n                if method == \"outer\":\n                    # GH#27198\n                    raise NotImplementedError\n                return result\n            return self._constructor(result, index=index, name=name, copy=False)\n\n        if type(result) is tuple:\n            # multiple return values\n            return tuple(construct_return(x) for x in result)\n        elif method == \"at\":\n            # no return value\n            return None\n        else:\n            return construct_return(result)\n\n    def __array__(self, dtype=None) -> np.ndarray:\n        \"\"\"\n        Return the values as a NumPy array.\n\n        Users should not call this directly. Rather, it is invoked by\n        :func:`numpy.array` and :func:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to use for the resulting NumPy array. By default,\n            the dtype is inferred from the data.\n\n        Returns\n        -------\n        numpy.ndarray\n            The values in the series converted to a :class:`numpy.ndarray`\n            with the specified `dtype`.\n\n        See Also\n        --------\n        array : Create a new array from data.\n        Series.array : Zero-copy view to the array backing the Series.\n        Series.to_numpy : Series method for similar behavior.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1, 2, 3])\n        >>> np.asarray(ser)\n        array([1, 2, 3])\n\n        For timezone-aware data, the timezones may be retained with\n        ``dtype='object'``\n\n        >>> tzser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\n        >>> np.asarray(tzser, dtype=\"object\")\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET', freq='D'),\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET', freq='D')],\n              dtype=object)\n\n        Or the values may be localized to UTC and the tzinfo discarded with\n        ``dtype='datetime64[ns]'``\n\n        >>> np.asarray(tzser, dtype=\"datetime64[ns]\")  # doctest: +ELLIPSIS\n        array(['1999-12-31T23:00:00.000000000', ...],\n              dtype='datetime64[ns]')\n        \"\"\"\n        return np.asarray(self.array, dtype)\n\n    # ----------------------------------------------------------------------\n    # Unary Methods\n\n    # coercion\n    __float__ = _coerce_method(float)\n    __long__ = _coerce_method(int)\n    __int__ = _coerce_method(int)\n\n    # ----------------------------------------------------------------------\n\n    # indexers\n    @property\n    def axes(self) -> List[Index]:\n        \"\"\"\n        Return a list of the row axis labels.\n        \"\"\"\n        return [self.index]\n\n    # ----------------------------------------------------------------------\n    # Indexing Methods\n\n    @Appender(generic.NDFrame.take.__doc__)\n    def take(self, indices, axis=0, is_copy=None, **kwargs) -> \"Series\":\n        if is_copy is not None:\n            warnings.warn(\n                \"is_copy is deprecated and will be removed in a future version. \"\n                \"'take' always returns a copy, so there is no need to specify this.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n        nv.validate_take(tuple(), kwargs)\n\n        indices = ensure_platform_int(indices)\n        new_index = self.index.take(indices)\n        new_values = self._values.take(indices)\n\n        result = self._constructor(new_values, index=new_index, fastpath=True)\n        return result.__finalize__(self, method=\"take\")\n\n    def _take_with_is_copy(self, indices, axis=0):\n        \"\"\"\n        Internal version of the `take` method that sets the `_is_copy`\n        attribute to keep track of the parent dataframe (using in indexing\n        for the SettingWithCopyWarning). For Series this does the same\n        as the public take (it never sets `_is_copy`).\n\n        See the docstring of `take` for full explanation of the parameters.\n        \"\"\"\n        return self.take(indices=indices, axis=axis)\n\n    def _ixs(self, i: int, axis: int = 0):\n        \"\"\"\n        Return the i-th value or values in the Series by location.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        scalar (int) or Series (slice, sequence)\n        \"\"\"\n        return self._values[i]\n\n    def _slice(self, slobj: slice, axis: int = 0) -> \"Series\":\n        # axis kwarg is retained for compat with NDFrame method\n        #  _slice is *always* positional\n        return self._get_values(slobj)\n\n    def __getitem__(self, key):\n        key = com.apply_if_callable(key, self)\n\n        if key is Ellipsis:\n            return self\n\n        key_is_scalar = is_scalar(key)\n        if isinstance(key, (list, tuple)):\n            key = unpack_1tuple(key)\n\n        if is_integer(key) and self.index._should_fallback_to_positional():\n            return self._values[key]\n\n        elif key_is_scalar:\n            return self._get_value(key)\n\n        if (\n            isinstance(key, tuple)\n            and is_hashable(key)\n            and isinstance(self.index, MultiIndex)\n        ):\n            # Otherwise index.get_value will raise InvalidIndexError\n            try:\n                result = self._get_value(key)\n\n                return result\n\n            except KeyError:\n                # We still have the corner case where this tuple is a key\n                #  in the first level of our MultiIndex\n                return self._get_values_tuple(key)\n\n        if is_iterator(key):\n            key = list(key)\n\n        if com.is_bool_indexer(key):\n            key = check_bool_indexer(self.index, key)\n            key = np.asarray(key, dtype=bool)\n            return self._get_values(key)\n\n        return self._get_with(key)\n\n    def _get_with(self, key):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            # _convert_slice_indexer to determin if this slice is positional\n            #  or label based, and if the latter, convert to positional\n            slobj = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            return self._slice(slobj)\n        elif isinstance(key, ABCDataFrame):\n            raise TypeError(\n                \"Indexing a Series with DataFrame is not \"\n                \"supported, use the appropriate DataFrame column\"\n            )\n        elif isinstance(key, tuple):\n            return self._get_values_tuple(key)\n\n        elif not is_list_like(key):\n            # e.g. scalars that aren't recognized by lib.is_scalar, GH#32684\n            return self.loc[key]\n\n        if not isinstance(key, (list, np.ndarray, ExtensionArray, Series, Index)):\n            key = list(key)\n\n        if isinstance(key, Index):\n            key_type = key.inferred_type\n        else:\n            key_type = lib.infer_dtype(key, skipna=False)\n\n        # Note: The key_type == \"boolean\" case should be caught by the\n        #  com.is_bool_indexer check in __getitem__\n        if key_type == \"integer\":\n            # We need to decide whether to treat this as a positional indexer\n            #  (i.e. self.iloc) or label-based (i.e. self.loc)\n            if not self.index._should_fallback_to_positional():\n                return self.loc[key]\n            else:\n                return self.iloc[key]\n\n        # handle the dup indexing case GH#4246\n        return self.loc[key]\n\n    def _get_values_tuple(self, key):\n        # mpl hackaround\n        if com.any_none(*key):\n            # suppress warning from slicing the index with a 2d indexer.\n            # eventually we'll want Series itself to warn.\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\", \"Support for multi-dim\", DeprecationWarning\n                )\n                return self._get_values(key)\n\n        if not isinstance(self.index, MultiIndex):\n            raise ValueError(\"Can only tuple-index with a MultiIndex\")\n\n        # If key is contained, would have returned by now\n        indexer, new_index = self.index.get_loc_level(key)\n        return self._constructor(self._values[indexer], index=new_index).__finalize__(\n            self,\n        )\n\n    def _get_values(self, indexer):\n        try:\n            return self._constructor(self._mgr.get_slice(indexer)).__finalize__(self,)\n        except ValueError:\n            # mpl compat if we look up e.g. ser[:, np.newaxis];\n            #  see tests.series.timeseries.test_mpl_compat_hack\n            return self._values[indexer]\n\n    def _get_value(self, label, takeable: bool = False):\n        \"\"\"\n        Quickly retrieve single value at passed index label.\n\n        Parameters\n        ----------\n        label : object\n        takeable : interpret the index as indexers, default False\n\n        Returns\n        -------\n        scalar value\n        \"\"\"\n        if takeable:\n            return self._values[label]\n\n        # Similar to Index.get_value, but we do not fall back to positional\n        loc = self.index.get_loc(label)\n        return self.index._get_values_for_loc(self, loc, label)\n\n    def __setitem__(self, key, value):\n        key = com.apply_if_callable(key, self)\n        cacher_needs_updating = self._check_is_chained_assignment_possible()\n\n        if key is Ellipsis:\n            key = slice(None)\n\n        try:\n            self._set_with_engine(key, value)\n        except (KeyError, ValueError):\n            values = self._values\n            if is_integer(key) and not self.index.inferred_type == \"integer\":\n                # positional setter\n                values[key] = value\n            else:\n                # GH#12862 adding an new key to the Series\n                self.loc[key] = value\n\n        except TypeError as e:\n            if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):\n                raise ValueError(\"Can only tuple-index with a MultiIndex\") from e\n\n            if com.is_bool_indexer(key):\n                key = check_bool_indexer(self.index, key)\n                key = np.asarray(key, dtype=bool)\n                try:\n                    self._where(~key, value, inplace=True)\n                except InvalidIndexError:\n                    self.iloc[key] = value\n                return\n\n            else:\n                self._set_with(key, value)\n\n        if cacher_needs_updating:\n            self._maybe_update_cacher()\n\n    def _set_with_engine(self, key, value):\n        # fails with AttributeError for IntervalIndex\n        loc = self.index._engine.get_loc(key)\n        validate_numeric_casting(self.dtype, value)\n        self._values[loc] = value\n\n    def _set_with(self, key, value):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            # extract_array so that if we set e.g. ser[-5:] = ser[:5]\n            #  we get the first five values, and not 5 NaNs\n            indexer = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            self.iloc[indexer] = extract_array(value, extract_numpy=True)\n\n        else:\n            assert not isinstance(key, tuple)\n\n            if is_scalar(key):\n                key = [key]\n\n            if isinstance(key, Index):\n                key_type = key.inferred_type\n                key = key._values\n            else:\n                key_type = lib.infer_dtype(key, skipna=False)\n\n            # Note: key_type == \"boolean\" should not occur because that\n            #  should be caught by the is_bool_indexer check in __setitem__\n            if key_type == \"integer\":\n                if not self.index._should_fallback_to_positional():\n                    self.loc[key] = value\n                else:\n                    self.iloc[key] = value\n            else:\n                self.loc[key] = value\n\n    def _set_value(self, label, value, takeable: bool = False):\n        \"\"\"\n        Quickly set single value at passed label.\n\n        If label is not contained, a new object is created with the label\n        placed at the end of the result index.\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed.\n        value : object\n            Scalar value.\n        takeable : interpret the index as indexers, default False\n        \"\"\"\n        try:\n            if takeable:\n                self._values[label] = value\n            else:\n                loc = self.index.get_loc(label)\n                validate_numeric_casting(self.dtype, value)\n                self._values[loc] = value\n        except KeyError:\n\n            # set using a non-recursive method\n            self.loc[label] = value\n\n    # ----------------------------------------------------------------------\n    # Unsorted\n\n    @property\n    def _is_mixed_type(self):\n        return False\n\n    def repeat(self, repeats, axis=None) -> \"Series\":\n        \"\"\"\n        Repeat elements of a Series.\n\n        Returns a new Series where each element of the current Series\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            Series.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        Series\n            Newly created Series with repeated elements.\n\n        See Also\n        --------\n        Index.repeat : Equivalent function for Index.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s\n        0    a\n        1    b\n        2    c\n        dtype: object\n        >>> s.repeat(2)\n        0    a\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        dtype: object\n        >>> s.repeat([1, 2, 3])\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        2    c\n        dtype: object\n        \"\"\"\n        nv.validate_repeat(tuple(), dict(axis=axis))\n        new_index = self.index.repeat(repeats)\n        new_values = self._values.repeat(repeats)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"repeat\"\n        )\n\n    def reset_index(self, level=None, drop=False, name=None, inplace=False):\n        \"\"\"\n        Generate a new DataFrame or Series with the index reset.\n\n        This is useful when the index needs to be treated as a column, or\n        when the index is meaningless and needs to be reset to the default\n        before another operation.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default optional\n            For a Series with a MultiIndex, only remove the specified levels\n            from the index. Removes all levels by default.\n        drop : bool, default False\n            Just reset the index, without inserting it as a column in\n            the new DataFrame.\n        name : object, optional\n            The name to use for the column containing the original Series\n            values. Uses ``self.name`` by default. This argument is ignored\n            when `drop` is True.\n        inplace : bool, default False\n            Modify the Series in place (do not create a new object).\n\n        Returns\n        -------\n        Series or DataFrame\n            When `drop` is False (the default), a DataFrame is returned.\n            The newly created columns will come first in the DataFrame,\n            followed by the original Series values.\n            When `drop` is True, a `Series` is returned.\n            In either case, if ``inplace=True``, no value is returned.\n\n        See Also\n        --------\n        DataFrame.reset_index: Analogous function for DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4], name='foo',\n        ...               index=pd.Index(['a', 'b', 'c', 'd'], name='idx'))\n\n        Generate a DataFrame with default index.\n\n        >>> s.reset_index()\n          idx  foo\n        0   a    1\n        1   b    2\n        2   c    3\n        3   d    4\n\n        To specify the name of the new column use `name`.\n\n        >>> s.reset_index(name='values')\n          idx  values\n        0   a       1\n        1   b       2\n        2   c       3\n        3   d       4\n\n        To generate a new Series with the default set `drop` to True.\n\n        >>> s.reset_index(drop=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        To update the Series in place, without generating a new one\n        set `inplace` to True. Note that it also requires ``drop=True``.\n\n        >>> s.reset_index(inplace=True, drop=True)\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        The `level` parameter is interesting for Series with a multi-level\n        index.\n\n        >>> arrays = [np.array(['bar', 'bar', 'baz', 'baz']),\n        ...           np.array(['one', 'two', 'one', 'two'])]\n        >>> s2 = pd.Series(\n        ...     range(4), name='foo',\n        ...     index=pd.MultiIndex.from_arrays(arrays,\n        ...                                     names=['a', 'b']))\n\n        To remove a specific level from the Index, use `level`.\n\n        >>> s2.reset_index(level='a')\n               a  foo\n        b\n        one  bar    0\n        two  bar    1\n        one  baz    2\n        two  baz    3\n\n        If `level` is not set, all levels are removed from the Index.\n\n        >>> s2.reset_index()\n             a    b  foo\n        0  bar  one    0\n        1  bar  two    1\n        2  baz  one    2\n        3  baz  two    3\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if drop:\n            new_index = ibase.default_index(len(self))\n            if level is not None:\n                if not isinstance(level, (tuple, list)):\n                    level = [level]\n                level = [self.index._get_level_number(lev) for lev in level]\n                if len(level) < self.index.nlevels:\n                    new_index = self.index.droplevel(level)\n\n            if inplace:\n                self.index = new_index\n                # set name if it was passed, otherwise, keep the previous name\n                self.name = name or self.name\n            else:\n                return self._constructor(\n                    self._values.copy(), index=new_index\n                ).__finalize__(self, method=\"reset_index\")\n        elif inplace:\n            raise TypeError(\n                \"Cannot reset_index inplace on a Series to create a DataFrame\"\n            )\n        else:\n            df = self.to_frame(name)\n            return df.reset_index(level=level, drop=drop)\n\n    # ----------------------------------------------------------------------\n    # Rendering Methods\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular Series.\n        \"\"\"\n        buf = StringIO(\"\")\n        width, height = get_terminal_size()\n        max_rows = (\n            height\n            if get_option(\"display.max_rows\") == 0\n            else get_option(\"display.max_rows\")\n        )\n        min_rows = (\n            height\n            if get_option(\"display.max_rows\") == 0\n            else get_option(\"display.min_rows\")\n        )\n        show_dimensions = get_option(\"display.show_dimensions\")\n\n        self.to_string(\n            buf=buf,\n            name=self.name,\n            dtype=self.dtype,\n            min_rows=min_rows,\n            max_rows=max_rows,\n            length=show_dimensions,\n        )\n        result = buf.getvalue()\n\n        return result\n\n    def to_string(\n        self,\n        buf=None,\n        na_rep=\"NaN\",\n        float_format=None,\n        header=True,\n        index=True,\n        length=False,\n        dtype=False,\n        name=False,\n        max_rows=None,\n        min_rows=None,\n    ):\n        \"\"\"\n        Render a string representation of the Series.\n\n        Parameters\n        ----------\n        buf : StringIO-like, optional\n            Buffer to write to.\n        na_rep : str, optional\n            String representation of NaN to use, default 'NaN'.\n        float_format : one-parameter function, optional\n            Formatter function to apply to columns' elements if they are\n            floats, default None.\n        header : bool, default True\n            Add the Series header (index name).\n        index : bool, optional\n            Add index (row) labels, default True.\n        length : bool, default False\n            Add the Series length.\n        dtype : bool, default False\n            Add the Series dtype.\n        name : bool, default False\n            Add the Series name if not None.\n        max_rows : int, optional\n            Maximum number of rows to show before truncating. If None, show\n            all.\n        min_rows : int, optional\n            The number of rows to display in a truncated repr (when number\n            of rows is above `max_rows`).\n\n        Returns\n        -------\n        str or None\n            String representation of Series if ``buf=None``, otherwise None.\n        \"\"\"\n        formatter = fmt.SeriesFormatter(\n            self,\n            name=name,\n            length=length,\n            header=header,\n            index=index,\n            dtype=dtype,\n            na_rep=na_rep,\n            float_format=float_format,\n            min_rows=min_rows,\n            max_rows=max_rows,\n        )\n        result = formatter.to_string()\n\n        # catch contract violations\n        if not isinstance(result, str):\n            raise AssertionError(\n                \"result must be of type str, type \"\n                f\"of result is {repr(type(result).__name__)}\"\n            )\n\n        if buf is None:\n            return result\n        else:\n            try:\n                buf.write(result)\n            except AttributeError:\n                with open(buf, \"w\") as f:\n                    f.write(result)\n\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> s = pd.Series([\"elk\", \"pig\", \"dog\", \"quetzal\"], name=\"animal\")\n        >>> print(s.to_markdown())\n        |    | animal   |\n        |---:|:---------|\n        |  0 | elk      |\n        |  1 | pig      |\n        |  2 | dog      |\n        |  3 | quetzal  |\n        \"\"\"\n    )\n    @Substitution(klass=\"Series\")\n    @Appender(generic._shared_docs[\"to_markdown\"])\n    def to_markdown(\n        self, buf: Optional[IO[str]] = None, mode: Optional[str] = None, **kwargs\n    ) -> Optional[str]:\n        return self.to_frame().to_markdown(buf, mode, **kwargs)\n\n    # ----------------------------------------------------------------------\n\n    def items(self) -> Iterable[Tuple[Label, Any]]:\n        \"\"\"\n        Lazily iterate over (index, value) tuples.\n\n        This method returns an iterable tuple (index, value). This is\n        convenient if you want to create a lazy iterator.\n\n        Returns\n        -------\n        iterable\n            Iterable of tuples containing the (index, value) pairs from a\n            Series.\n\n        See Also\n        --------\n        DataFrame.items : Iterate over (column name, Series) pairs.\n        DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs.\n\n        Examples\n        --------\n        >>> s = pd.Series(['A', 'B', 'C'])\n        >>> for index, value in s.items():\n        ...     print(f\"Index : {index}, Value : {value}\")\n        Index : 0, Value : A\n        Index : 1, Value : B\n        Index : 2, Value : C\n        \"\"\"\n        return zip(iter(self.index), iter(self))\n\n    @Appender(items.__doc__)\n    def iteritems(self) -> Iterable[Tuple[Label, Any]]:\n        return self.items()\n\n    # ----------------------------------------------------------------------\n    # Misc public methods\n\n    def keys(self) -> Index:\n        \"\"\"\n        Return alias for index.\n\n        Returns\n        -------\n        Index\n            Index of the Series.\n        \"\"\"\n        return self.index\n\n    def to_dict(self, into=dict):\n        \"\"\"\n        Convert Series to {label -> value} dict or dict-like object.\n\n        Parameters\n        ----------\n        into : class, default dict\n            The collections.abc.Mapping subclass to use as the return\n            object. Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Key-value representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.to_dict()\n        {0: 1, 1: 2, 2: 3, 3: 4}\n        >>> from collections import OrderedDict, defaultdict\n        >>> s.to_dict(OrderedDict)\n        OrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])\n        >>> dd = defaultdict(list)\n        >>> s.to_dict(dd)\n        defaultdict(<class 'list'>, {0: 1, 1: 2, 2: 3, 3: 4})\n        \"\"\"\n        # GH16122\n        into_c = com.standardize_mapping(into)\n        return into_c(self.items())\n\n    def to_frame(self, name=None) -> \"DataFrame\":\n        \"\"\"\n        Convert Series to DataFrame.\n\n        Parameters\n        ----------\n        name : object, default None\n            The passed name should substitute for the series name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([\"a\", \"b\", \"c\"],\n        ...               name=\"vals\")\n        >>> s.to_frame()\n          vals\n        0    a\n        1    b\n        2    c\n        \"\"\"\n        if name is None:\n            df = self._constructor_expanddim(self)\n        else:\n            df = self._constructor_expanddim({name: self})\n\n        return df\n\n    def _set_name(self, name, inplace=False) -> \"Series\":\n        \"\"\"\n        Set the Series name.\n\n        Parameters\n        ----------\n        name : str\n        inplace : bool\n            Whether to modify `self` directly or return a copy.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        ser = self if inplace else self.copy()\n        ser.name = name\n        return ser\n\n    @Appender(\n        \"\"\"\nExamples\n--------\n>>> ser = pd.Series([390., 350., 30., 20.],\n...                 index=['Falcon', 'Falcon', 'Parrot', 'Parrot'], name=\"Max Speed\")\n>>> ser\nFalcon    390.0\nFalcon    350.0\nParrot     30.0\nParrot     20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby([\"a\", \"b\", \"a\", \"b\"]).mean()\na    210.0\nb    185.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(ser > 100).mean()\nMax Speed\nFalse     25.0\nTrue     370.0\nName: Max Speed, dtype: float64\n\n**Grouping by Indexes**\n\nWe can groupby different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> ser = pd.Series([390., 350., 30., 20.], index=index, name=\"Max Speed\")\n>>> ser\nAnimal  Type\nFalcon  Captive    390.0\n        Wild       350.0\nParrot  Captive     30.0\n        Wild        20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nAnimal\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=\"Type\").mean()\nType\nCaptive    210.0\nWild       185.0\nName: Max Speed, dtype: float64\n\"\"\"\n    )\n    @Appender(generic._shared_docs[\"groupby\"] % _shared_doc_kwargs)\n    def groupby(\n        self,\n        by=None,\n        axis=0,\n        level=None,\n        as_index: bool = True,\n        sort: bool = True,\n        group_keys: bool = True,\n        squeeze: bool = False,\n        observed: bool = False,\n    ) -> \"SeriesGroupBy\":\n        from pandas.core.groupby.generic import SeriesGroupBy\n\n        if level is None and by is None:\n            raise TypeError(\"You have to supply one of 'by' and 'level'\")\n        axis = self._get_axis_number(axis)\n\n        return SeriesGroupBy(\n            obj=self,\n            keys=by,\n            axis=axis,\n            level=level,\n            as_index=as_index,\n            sort=sort,\n            group_keys=group_keys,\n            squeeze=squeeze,\n            observed=observed,\n        )\n\n    # ----------------------------------------------------------------------\n    # Statistics, overridden ndarray methods\n\n    # TODO: integrate bottleneck\n\n    def count(self, level=None):\n        \"\"\"\n        Return number of non-NA/null observations in the Series.\n\n        Parameters\n        ----------\n        level : int or level name, default None\n            If the axis is a MultiIndex (hierarchical), count along a\n            particular level, collapsing into a smaller Series.\n\n        Returns\n        -------\n        int or Series (if level specified)\n            Number of non-null values in the Series.\n\n        See Also\n        --------\n        DataFrame.count : Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.0, 1.0, np.nan])\n        >>> s.count()\n        2\n        \"\"\"\n        if level is None:\n            return notna(self.array).sum()\n\n        if isinstance(level, str):\n            level = self.index._get_level_number(level)\n\n        lev = self.index.levels[level]\n        level_codes = np.array(self.index.codes[level], subok=False, copy=True)\n\n        mask = level_codes == -1\n        if mask.any():\n            level_codes[mask] = cnt = len(lev)\n            lev = lev.insert(cnt, lev._na_value)\n\n        obs = level_codes[notna(self._values)]\n        out = np.bincount(obs, minlength=len(lev) or None)\n        return self._constructor(out, index=lev, dtype=\"int64\").__finalize__(\n            self, method=\"count\"\n        )\n\n    def mode(self, dropna=True) -> \"Series\":\n        \"\"\"\n        Return the mode(s) of the dataset.\n\n        Always returns Series even if only one value is returned.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        Series\n            Modes of the Series in sorted order.\n        \"\"\"\n        # TODO: Add option for bins like value_counts()\n        return algorithms.mode(self, dropna=dropna)\n\n    def unique(self):\n        \"\"\"\n        Return unique values of Series object.\n\n        Uniques are returned in order of appearance. Hash table-based unique,\n        therefore does NOT sort.\n\n        Returns\n        -------\n        ndarray or ExtensionArray\n            The unique values returned as a NumPy array. See Notes.\n\n        See Also\n        --------\n        unique : Top-level unique method for any 1-d array-like object.\n        Index.unique : Return Index with unique values from an Index object.\n\n        Notes\n        -----\n        Returns the unique values as a NumPy array. In case of an\n        extension-array backed Series, a new\n        :class:`~api.extensions.ExtensionArray` of that type with just\n        the unique values is returned. This includes\n\n            * Categorical\n            * Period\n            * Datetime with Timezone\n            * Interval\n            * Sparse\n            * IntegerNA\n\n        See Examples section.\n\n        Examples\n        --------\n        >>> pd.Series([2, 1, 3, 3], name='A').unique()\n        array([2, 1, 3])\n\n        >>> pd.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()\n        array(['2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\n\n        >>> pd.Series([pd.Timestamp('2016-01-01', tz='US/Eastern')\n        ...            for _ in range(3)]).unique()\n        <DatetimeArray>\n        ['2016-01-01 00:00:00-05:00']\n        Length: 1, dtype: datetime64[ns, US/Eastern]\n\n        An unordered Categorical will return categories in the order of\n        appearance.\n\n        >>> pd.Series(pd.Categorical(list('baabc'))).unique()\n        [b, a, c]\n        Categories (3, object): [b, a, c]\n\n        An ordered Categorical preserves the category ordering.\n\n        >>> pd.Series(pd.Categorical(list('baabc'), categories=list('abc'),\n        ...                          ordered=True)).unique()\n        [b, a, c]\n        Categories (3, object): [a < b < c]\n        \"\"\"\n        result = super().unique()\n        return result\n\n    def drop_duplicates(self, keep=\"first\", inplace=False) -> Optional[\"Series\"]:\n        \"\"\"\n        Return Series with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        inplace : bool, default ``False``\n            If ``True``, performs operation inplace and returns None.\n\n        Returns\n        -------\n        Series\n            Series with duplicates dropped.\n\n        See Also\n        --------\n        Index.drop_duplicates : Equivalent method on Index.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Series.duplicated : Related method on Series, indicating duplicate\n            Series values.\n\n        Examples\n        --------\n        Generate a Series with duplicated entries.\n\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'],\n        ...               name='animal')\n        >>> s\n        0      lama\n        1       cow\n        2      lama\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        With the 'keep' parameter, the selection behaviour of duplicated values\n        can be changed. The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> s.drop_duplicates()\n        0      lama\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n\n        The value 'last' for parameter 'keep' keeps the last occurrence for\n        each set of duplicated entries.\n\n        >>> s.drop_duplicates(keep='last')\n        1       cow\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        The value ``False`` for parameter 'keep' discards all sets of\n        duplicated entries. Setting the value of 'inplace' to ``True`` performs\n        the operation inplace and returns ``None``.\n\n        >>> s.drop_duplicates(keep=False, inplace=True)\n        >>> s\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        result = super().drop_duplicates(keep=keep)\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    def duplicated(self, keep=\"first\") -> \"Series\":\n        \"\"\"\n        Indicate duplicate Series values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        Series. Either all duplicates, all except the first or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series\n            Series indicating whether each value has occurred in the\n            preceding values.\n\n        See Also\n        --------\n        Index.duplicated : Equivalent method on pandas.Index.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Series.drop_duplicates : Remove duplicate values from Series.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set on False and all others on True:\n\n        >>> animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> animals.duplicated()\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        which is equivalent to\n\n        >>> animals.duplicated(keep='first')\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> animals.duplicated(keep='last')\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> animals.duplicated(keep=False)\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n        \"\"\"\n        return super().duplicated(keep=keep)\n\n    def idxmin(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the minimum value.\n\n        If multiple values equal the minimum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmin. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the minimum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmin : Return indices of the minimum values\n            along the given axis.\n        DataFrame.idxmin : Return index of first occurrence of minimum\n            over requested axis.\n        Series.idxmax : Return index *label* of the first occurrence\n            of maximum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmin``. This method\n        returns the label of the minimum, while ``ndarray.argmin`` returns\n        the position. To get the position, use ``series.values.argmin()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 1],\n        ...               index=['A', 'B', 'C', 'D'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    1.0\n        dtype: float64\n\n        >>> s.idxmin()\n        'A'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmin(skipna=False)\n        nan\n        \"\"\"\n        skipna = nv.validate_argmin_with_skipna(skipna, args, kwargs)\n        i = nanops.nanargmin(self._values, skipna=skipna)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def idxmax(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the maximum value.\n\n        If multiple values equal the maximum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmax. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the maximum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmax : Return indices of the maximum values\n            along the given axis.\n        DataFrame.idxmax : Return index of first occurrence of maximum\n            over requested axis.\n        Series.idxmin : Return index *label* of the first occurrence\n            of minimum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmax``. This method\n        returns the label of the maximum, while ``ndarray.argmax`` returns\n        the position. To get the position, use ``series.values.argmax()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 3, 4],\n        ...               index=['A', 'B', 'C', 'D', 'E'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    3.0\n        E    4.0\n        dtype: float64\n\n        >>> s.idxmax()\n        'C'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmax(skipna=False)\n        nan\n        \"\"\"\n        skipna = nv.validate_argmax_with_skipna(skipna, args, kwargs)\n        i = nanops.nanargmax(self._values, skipna=skipna)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def round(self, decimals=0, *args, **kwargs) -> \"Series\":\n        \"\"\"\n        Round each value in a Series to the given number of decimals.\n\n        Parameters\n        ----------\n        decimals : int, default 0\n            Number of decimal places to round to. If decimals is negative,\n            it specifies the number of positions to the left of the decimal point.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Series\n            Rounded values of the Series.\n\n        See Also\n        --------\n        numpy.around : Round values of an np.array.\n        DataFrame.round : Round values of a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.1, 1.3, 2.7])\n        >>> s.round()\n        0    0.0\n        1    1.0\n        2    3.0\n        dtype: float64\n        \"\"\"\n        nv.validate_round(args, kwargs)\n        result = self._values.round(decimals)\n        result = self._constructor(result, index=self.index).__finalize__(\n            self, method=\"round\"\n        )\n\n        return result\n\n    def quantile(self, q=0.5, interpolation=\"linear\"):\n        \"\"\"\n        Return value at the given quantile.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            The quantile(s) to compute, which can lie in range: 0 <= q <= 1.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            This optional parameter specifies the interpolation method to use,\n            when the desired quantile lies between two data points `i` and `j`:\n\n                * linear: `i + (j - i) * fraction`, where `fraction` is the\n                  fractional part of the index surrounded by `i` and `j`.\n                * lower: `i`.\n                * higher: `j`.\n                * nearest: `i` or `j` whichever is nearest.\n                * midpoint: (`i` + `j`) / 2.\n\n        Returns\n        -------\n        float or Series\n            If ``q`` is an array, a Series will be returned where the\n            index is ``q`` and the values are the quantiles, otherwise\n            a float will be returned.\n\n        See Also\n        --------\n        core.window.Rolling.quantile : Calculate the rolling quantile.\n        numpy.percentile : Returns the q-th percentile(s) of the array elements.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.quantile(.5)\n        2.5\n        >>> s.quantile([.25, .5, .75])\n        0.25    1.75\n        0.50    2.50\n        0.75    3.25\n        dtype: float64\n        \"\"\"\n        validate_percentile(q)\n\n        # We dispatch to DataFrame so that core.internals only has to worry\n        #  about 2D cases.\n        df = self.to_frame()\n\n        result = df.quantile(q=q, interpolation=interpolation, numeric_only=False)\n        if result.ndim == 2:\n            result = result.iloc[:, 0]\n\n        if is_list_like(q):\n            result.name = self.name\n            return self._constructor(result, index=Float64Index(q), name=self.name)\n        else:\n            # scalar\n            return result.iloc[0]\n\n    def corr(self, other, method=\"pearson\", min_periods=None) -> float:\n        \"\"\"\n        Compute correlation with `other` Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the correlation.\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method used to compute correlation:\n\n            - pearson : Standard correlation coefficient\n            - kendall : Kendall Tau correlation coefficient\n            - spearman : Spearman rank correlation\n            - callable: Callable with input two 1d ndarrays and returning a float.\n\n            .. versionadded:: 0.24.0\n                Note that the returned matrix from corr will have 1 along the\n                diagonals and will be symmetric regardless of the callable's\n                behavior.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n\n        Returns\n        -------\n        float\n            Correlation with other.\n\n        See Also\n        --------\n        DataFrame.corr : Compute pairwise correlation between columns.\n        DataFrame.corrwith : Compute pairwise correlation with another\n            DataFrame or Series.\n\n        Examples\n        --------\n        >>> def histogram_intersection(a, b):\n        ...     v = np.minimum(a, b).sum().round(decimals=1)\n        ...     return v\n        >>> s1 = pd.Series([.2, .0, .6, .2])\n        >>> s2 = pd.Series([.3, .6, .0, .1])\n        >>> s1.corr(s2, method=histogram_intersection)\n        0.3\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n\n        if method in [\"pearson\", \"spearman\", \"kendall\"] or callable(method):\n            return nanops.nancorr(\n                this.values, other.values, method=method, min_periods=min_periods\n            )\n\n        raise ValueError(\n            \"method must be either 'pearson', \"\n            \"'spearman', 'kendall', or a callable, \"\n            f\"'{method}' was supplied\"\n        )\n\n    def cov(self, other, min_periods=None) -> float:\n        \"\"\"\n        Compute covariance with Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the covariance.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n\n        Returns\n        -------\n        float\n            Covariance between Series and other normalized by N-1\n            (unbiased estimator).\n\n        See Also\n        --------\n        DataFrame.cov : Compute pairwise covariance of columns.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([0.90010907, 0.13484424, 0.62036035])\n        >>> s2 = pd.Series([0.12528585, 0.26962463, 0.51111198])\n        >>> s1.cov(s2)\n        -0.01685762652715874\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancov(this.values, other.values, min_periods=min_periods)\n\n    def diff(self, periods: int = 1) -> \"Series\":\n        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of a Series element compared with another\n        element in the Series (default is element in previous row).\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative\n            values.\n\n        Returns\n        -------\n        Series\n            First differences of the Series.\n\n        See Also\n        --------\n        Series.pct_change: Percent change over given number of periods.\n        Series.shift: Shift index by desired number of periods with an\n            optional time freq.\n        DataFrame.diff: First discrete difference of object.\n\n        Notes\n        -----\n        For boolean dtypes, this uses :meth:`operator.xor` rather than\n        :meth:`operator.sub`.\n\n        Examples\n        --------\n        Difference with previous row\n\n        >>> s = pd.Series([1, 1, 2, 3, 5, 8])\n        >>> s.diff()\n        0    NaN\n        1    0.0\n        2    1.0\n        3    1.0\n        4    2.0\n        5    3.0\n        dtype: float64\n\n        Difference with 3rd previous row\n\n        >>> s.diff(periods=3)\n        0    NaN\n        1    NaN\n        2    NaN\n        3    2.0\n        4    4.0\n        5    6.0\n        dtype: float64\n\n        Difference with following row\n\n        >>> s.diff(periods=-1)\n        0    0.0\n        1   -1.0\n        2   -1.0\n        3   -2.0\n        4   -3.0\n        5    NaN\n        dtype: float64\n        \"\"\"\n        result = algorithms.diff(self.array, periods)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"diff\"\n        )\n\n    def autocorr(self, lag=1) -> float:\n        \"\"\"\n        Compute the lag-N autocorrelation.\n\n        This method computes the Pearson correlation between\n        the Series and its shifted self.\n\n        Parameters\n        ----------\n        lag : int, default 1\n            Number of lags to apply before performing autocorrelation.\n\n        Returns\n        -------\n        float\n            The Pearson correlation between self and self.shift(lag).\n\n        See Also\n        --------\n        Series.corr : Compute the correlation between two Series.\n        Series.shift : Shift index by desired number of periods.\n        DataFrame.corr : Compute pairwise correlation of columns.\n        DataFrame.corrwith : Compute pairwise correlation between rows or\n            columns of two DataFrame objects.\n\n        Notes\n        -----\n        If the Pearson correlation is not well defined return 'NaN'.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.25, 0.5, 0.2, -0.05])\n        >>> s.autocorr()  # doctest: +ELLIPSIS\n        0.10355...\n        >>> s.autocorr(lag=2)  # doctest: +ELLIPSIS\n        -0.99999...\n\n        If the Pearson correlation is not well defined, then 'NaN' is returned.\n\n        >>> s = pd.Series([1, 0, 0, 0])\n        >>> s.autocorr()\n        nan\n        \"\"\"\n        return self.corr(self.shift(lag))\n\n    def dot(self, other):\n        \"\"\"\n        Compute the dot product between the Series and the columns of other.\n\n        This method computes the dot product between the Series and another\n        one, or the Series and each columns of a DataFrame, or the Series and\n        each columns of an array.\n\n        It can also be called using `self @ other` in Python >= 3.5.\n\n        Parameters\n        ----------\n        other : Series, DataFrame or array-like\n            The other object to compute the dot product with its columns.\n\n        Returns\n        -------\n        scalar, Series or numpy.ndarray\n            Return the dot product of the Series and other if other is a\n            Series, the Series of the dot product of Series and each rows of\n            other if other is a DataFrame or a numpy.ndarray between the Series\n            and each columns of the numpy array.\n\n        See Also\n        --------\n        DataFrame.dot: Compute the matrix product with the DataFrame.\n        Series.mul: Multiplication of series and other, element-wise.\n\n        Notes\n        -----\n        The Series and other has to share the same index if other is a Series\n        or a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0, 1, 2, 3])\n        >>> other = pd.Series([-1, 2, -3, 4])\n        >>> s.dot(other)\n        8\n        >>> s @ other\n        8\n        >>> df = pd.DataFrame([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(df)\n        0    24\n        1    14\n        dtype: int64\n        >>> arr = np.array([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(arr)\n        array([24, 14])\n        \"\"\"\n        if isinstance(other, (Series, ABCDataFrame)):\n            common = self.index.union(other.index)\n            if len(common) > len(self.index) or len(common) > len(other.index):\n                raise ValueError(\"matrices are not aligned\")\n\n            left = self.reindex(index=common, copy=False)\n            right = other.reindex(index=common, copy=False)\n            lvals = left.values\n            rvals = right.values\n        else:\n            lvals = self.values\n            rvals = np.asarray(other)\n            if lvals.shape[0] != rvals.shape[0]:\n                raise Exception(\n                    f\"Dot product shape mismatch, {lvals.shape} vs {rvals.shape}\"\n                )\n\n        if isinstance(other, ABCDataFrame):\n            return self._constructor(\n                np.dot(lvals, rvals), index=other.columns\n            ).__finalize__(self, method=\"dot\")\n        elif isinstance(other, Series):\n            return np.dot(lvals, rvals)\n        elif isinstance(rvals, np.ndarray):\n            return np.dot(lvals, rvals)\n        else:  # pragma: no cover\n            raise TypeError(f\"unsupported type: {type(other)}\")\n\n    def __matmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(other)\n\n    def __rmatmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(np.transpose(other))\n\n    @doc(base.IndexOpsMixin.searchsorted, klass=\"Series\")\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        return algorithms.searchsorted(self._values, value, side=side, sorter=sorter)\n\n    # -------------------------------------------------------------------\n    # Combination\n\n    def append(self, to_append, ignore_index=False, verify_integrity=False):\n        \"\"\"\n        Concatenate two or more Series.\n\n        Parameters\n        ----------\n        to_append : Series or list/tuple of Series\n            Series to append with self.\n        ignore_index : bool, default False\n            If True, do not use the index labels.\n        verify_integrity : bool, default False\n            If True, raise Exception on creating index with duplicates.\n\n        Returns\n        -------\n        Series\n            Concatenated Series.\n\n        See Also\n        --------\n        concat : General function to concatenate DataFrame or Series objects.\n\n        Notes\n        -----\n        Iteratively appending to a Series can be more computationally intensive\n        than a single concatenate. A better solution is to append values to a\n        list and then concatenate the list with the original Series all at\n        once.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, 2, 3])\n        >>> s2 = pd.Series([4, 5, 6])\n        >>> s3 = pd.Series([4, 5, 6], index=[3, 4, 5])\n        >>> s1.append(s2)\n        0    1\n        1    2\n        2    3\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s1.append(s3)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `ignore_index` set to True:\n\n        >>> s1.append(s2, ignore_index=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `verify_integrity` set to True:\n\n        >>> s1.append(s2, verify_integrity=True)\n        Traceback (most recent call last):\n        ...\n        ValueError: Indexes have overlapping values: [0, 1, 2]\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        if isinstance(to_append, (list, tuple)):\n            to_concat = [self]\n            to_concat.extend(to_append)\n        else:\n            to_concat = [self, to_append]\n        if any(isinstance(x, (ABCDataFrame,)) for x in to_concat[1:]):\n            msg = (\n                f\"to_append should be a Series or list/tuple of Series, \"\n                f\"got DataFrame\"\n            )\n            raise TypeError(msg)\n        return concat(\n            to_concat, ignore_index=ignore_index, verify_integrity=verify_integrity\n        )\n\n    def _binop(self, other, func, level=None, fill_value=None):\n        \"\"\"\n        Perform generic binary operation with optional fill value.\n\n        Parameters\n        ----------\n        other : Series\n        func : binary operator\n        fill_value : float or object\n            Value to substitute for NA/null values. If both Series are NA in a\n            location, the result will be NA regardless of the passed fill value.\n        level : int or level name, default None\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level.\n\n        Returns\n        -------\n        Series\n        \"\"\"\n        if not isinstance(other, Series):\n            raise AssertionError(\"Other operand must be Series\")\n\n        this = self\n\n        if not self.index.equals(other.index):\n            this, other = self.align(other, level=level, join=\"outer\", copy=False)\n\n        this_vals, other_vals = ops.fill_binop(this.values, other.values, fill_value)\n\n        with np.errstate(all=\"ignore\"):\n            result = func(this_vals, other_vals)\n\n        name = ops.get_op_result_name(self, other)\n        ret = this._construct_result(result, name)\n        return ret\n\n    def _construct_result(\n        self, result: Union[ArrayLike, Tuple[ArrayLike, ArrayLike]], name: Label\n    ) -> Union[\"Series\", Tuple[\"Series\", \"Series\"]]:\n        \"\"\"\n        Construct an appropriately-labelled Series from the result of an op.\n\n        Parameters\n        ----------\n        result : ndarray or ExtensionArray\n        name : Label\n\n        Returns\n        -------\n        Series\n            In the case of __divmod__ or __rdivmod__, a 2-tuple of Series.\n        \"\"\"\n        if isinstance(result, tuple):\n            # produced by divmod or rdivmod\n\n            res1 = self._construct_result(result[0], name=name)\n            res2 = self._construct_result(result[1], name=name)\n\n            # GH#33427 assertions to keep mypy happy\n            assert isinstance(res1, Series)\n            assert isinstance(res2, Series)\n            return (res1, res2)\n\n        # We do not pass dtype to ensure that the Series constructor\n        #  does inference in the case where `result` has object-dtype.\n        out = self._constructor(result, index=self.index)\n        out = out.__finalize__(self)\n\n        # Set the result's name after __finalize__ is called because __finalize__\n        #  would set it back to self.name\n        out.name = name\n        return out\n\n    def combine(self, other, func, fill_value=None) -> \"Series\":\n        \"\"\"\n        Combine the Series with a Series or scalar according to `func`.\n\n        Combine the Series and `other` using `func` to perform elementwise\n        selection for combined Series.\n        `fill_value` is assumed when value is missing at some index\n        from one of the two objects being combined.\n\n        Parameters\n        ----------\n        other : Series or scalar\n            The value(s) to be combined with the `Series`.\n        func : function\n            Function that takes two scalars as inputs and returns an element.\n        fill_value : scalar, optional\n            The value to assume when an index is missing from\n            one Series or the other. The default specifies to use the\n            appropriate NaN value for the underlying dtype of the Series.\n\n        Returns\n        -------\n        Series\n            The result of combining the Series with the other object.\n\n        See Also\n        --------\n        Series.combine_first : Combine Series values, choosing the calling\n            Series' values first.\n\n        Examples\n        --------\n        Consider 2 Datasets ``s1`` and ``s2`` containing\n        highest clocked speeds of different birds.\n\n        >>> s1 = pd.Series({'falcon': 330.0, 'eagle': 160.0})\n        >>> s1\n        falcon    330.0\n        eagle     160.0\n        dtype: float64\n        >>> s2 = pd.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})\n        >>> s2\n        falcon    345.0\n        eagle     200.0\n        duck       30.0\n        dtype: float64\n\n        Now, to combine the two datasets and view the highest speeds\n        of the birds across the two datasets\n\n        >>> s1.combine(s2, max)\n        duck        NaN\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n\n        In the previous example, the resulting value for duck is missing,\n        because the maximum of a NaN and a float is a NaN.\n        So, in the example, we set ``fill_value=0``,\n        so the maximum value returned will be the value from some dataset.\n\n        >>> s1.combine(s2, max, fill_value=0)\n        duck       30.0\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n        \"\"\"\n        if fill_value is None:\n            fill_value = na_value_for_dtype(self.dtype, compat=False)\n\n        if isinstance(other, Series):\n            # If other is a Series, result is based on union of Series,\n            # so do this element by element\n            new_index = self.index.union(other.index)\n            new_name = ops.get_op_result_name(self, other)\n            new_values = []\n            for idx in new_index:\n                lv = self.get(idx, fill_value)\n                rv = other.get(idx, fill_value)\n                with np.errstate(all=\"ignore\"):\n                    new_values.append(func(lv, rv))\n        else:\n            # Assume that other is a scalar, so apply the function for\n            # each element in the Series\n            new_index = self.index\n            with np.errstate(all=\"ignore\"):\n                new_values = [func(lv, other) for lv in self._values]\n            new_name = self.name\n\n        if is_categorical_dtype(self.dtype):\n            pass\n        elif is_extension_array_dtype(self.dtype):\n            # TODO: can we do this for only SparseDtype?\n            # The function can return something of any type, so check\n            # if the type is compatible with the calling EA.\n            new_values = maybe_cast_to_extension_array(type(self._values), new_values)\n        return self._constructor(new_values, index=new_index, name=new_name)\n\n    def combine_first(self, other) -> \"Series\":\n        \"\"\"\n        Combine Series values, choosing the calling Series's values first.\n\n        Parameters\n        ----------\n        other : Series\n            The value(s) to be combined with the `Series`.\n\n        Returns\n        -------\n        Series\n            The result of combining the Series with the other object.\n\n        See Also\n        --------\n        Series.combine : Perform elementwise operation on two Series\n            using a given function.\n\n        Notes\n        -----\n        Result index will be the union of the two indexes.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, np.nan])\n        >>> s2 = pd.Series([3, 4])\n        >>> s1.combine_first(s2)\n        0    1.0\n        1    4.0\n        dtype: float64\n        \"\"\"\n        new_index = self.index.union(other.index)\n        this = self.reindex(new_index, copy=False)\n        other = other.reindex(new_index, copy=False)\n        if this.dtype.kind == \"M\" and other.dtype.kind != \"M\":\n            other = to_datetime(other)\n\n        return this.where(notna(this), other)\n\n    def update(self, other) -> None:\n        \"\"\"\n        Modify Series in place using non-NA values from passed\n        Series. Aligns on index.\n\n        Parameters\n        ----------\n        other : Series\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s.update(pd.Series(['d', 'e'], index=[0, 2]))\n        >>> s\n        0    d\n        1    b\n        2    e\n        dtype: object\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6, 7, 8]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        If ``other`` contains NaNs the corresponding values are not updated\n        in the original Series.\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, np.nan, 6]))\n        >>> s\n        0    4\n        1    2\n        2    6\n        dtype: int64\n        \"\"\"\n        other = other.reindex_like(self)\n        mask = notna(other)\n\n        self._mgr = self._mgr.putmask(mask=mask, new=other)\n        self._maybe_update_cacher()\n\n    # ----------------------------------------------------------------------\n    # Reindexing, sorting\n\n    def sort_values(\n        self,\n        axis=0,\n        ascending=True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        ignore_index: bool = False,\n    ):\n        \"\"\"\n        Sort by the values.\n\n        Sort a Series in ascending or descending order by some\n        criterion.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            Axis to direct sorting. The value 'index' is accepted for\n            compatibility with DataFrame.sort_values.\n        ascending : bool, default True\n            If True, sort values in ascending order, otherwise descending.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort' or 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. 'mergesort' is the only stable  algorithm.\n        na_position : {'first' or 'last'}, default 'last'\n            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n            the end.\n        ignore_index : bool, default False\n             If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n             .. versionadded:: 1.0.0\n\n        Returns\n        -------\n        Series\n            Series ordered by values.\n\n        See Also\n        --------\n        Series.sort_index : Sort by the Series indices.\n        DataFrame.sort_values : Sort DataFrame by the values along either axis.\n        DataFrame.sort_index : Sort DataFrame by indices.\n\n        Examples\n        --------\n        >>> s = pd.Series([np.nan, 1, 3, 10, 5])\n        >>> s\n        0     NaN\n        1     1.0\n        2     3.0\n        3     10.0\n        4     5.0\n        dtype: float64\n\n        Sort values ascending order (default behaviour)\n\n        >>> s.sort_values(ascending=True)\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        0     NaN\n        dtype: float64\n\n        Sort values descending order\n\n        >>> s.sort_values(ascending=False)\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values inplace\n\n        >>> s.sort_values(ascending=False, inplace=True)\n        >>> s\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values putting NAs first\n\n        >>> s.sort_values(na_position='first')\n        0     NaN\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        dtype: float64\n\n        Sort a series of strings\n\n        >>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n        >>> s\n        0    z\n        1    b\n        2    d\n        3    a\n        4    c\n        dtype: object\n\n        >>> s.sort_values()\n        3    a\n        1    b\n        4    c\n        2    d\n        0    z\n        dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n\n        # GH 5856/5853\n        if inplace and self._is_cached:\n            raise ValueError(\n                \"This Series is a view of some other array, to \"\n                \"sort in-place you must create a copy\"\n            )\n\n        def _try_kind_sort(arr):\n            # easier to ask forgiveness than permission\n            try:\n                # if kind==mergesort, it can fail for object dtype\n                return arr.argsort(kind=kind)\n            except TypeError:\n                # stable sort not available for object dtype\n                # uses the argsort default quicksort\n                return arr.argsort(kind=\"quicksort\")\n\n        arr = self._values\n        sorted_index = np.empty(len(self), dtype=np.int32)\n\n        bad = isna(arr)\n\n        good = ~bad\n        idx = ibase.default_index(len(self))\n\n        argsorted = _try_kind_sort(arr[good])\n\n        if is_list_like(ascending):\n            if len(ascending) != 1:\n                raise ValueError(\n                    f\"Length of ascending ({len(ascending)}) must be 1 for Series\"\n                )\n            ascending = ascending[0]\n\n        if not is_bool(ascending):\n            raise ValueError(\"ascending must be boolean\")\n\n        if not ascending:\n            argsorted = argsorted[::-1]\n\n        if na_position == \"last\":\n            n = good.sum()\n            sorted_index[:n] = idx[good][argsorted]\n            sorted_index[n:] = idx[bad]\n        elif na_position == \"first\":\n            n = bad.sum()\n            sorted_index[n:] = idx[good][argsorted]\n            sorted_index[:n] = idx[bad]\n        else:\n            raise ValueError(f\"invalid na_position: {na_position}\")\n\n        result = self._constructor(arr[sorted_index], index=self.index[sorted_index])\n\n        if ignore_index:\n            result.index = ibase.default_index(len(sorted_index))\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_values\")\n\n    def sort_index(\n        self,\n        axis=0,\n        level=None,\n        ascending: bool = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        sort_remaining: bool = True,\n        ignore_index: bool = False,\n    ):\n        \"\"\"\n        Sort Series by index labels.\n\n        Returns a new Series sorted by label if `inplace` argument is\n        ``False``, otherwise updates the original series and returns None.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Axis to direct sorting. This can only be 0 for Series.\n        level : int, optional\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level individually.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information.  'mergesort' is the only stable algorithm. For\n            DataFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'last'}, default 'last'\n            If 'first' puts NaNs at the beginning, 'last' puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        Returns\n        -------\n        Series\n            The original Series sorted by the labels.\n\n        See Also\n        --------\n        DataFrame.sort_index: Sort DataFrame by the index.\n        DataFrame.sort_values: Sort DataFrame by the value.\n        Series.sort_values : Sort Series by the value.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])\n        >>> s.sort_index()\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        Sort Descending\n\n        >>> s.sort_index(ascending=False)\n        4    d\n        3    a\n        2    b\n        1    c\n        dtype: object\n\n        Sort Inplace\n\n        >>> s.sort_index(inplace=True)\n        >>> s\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        By default NaNs are put at the end, but use `na_position` to place\n        them at the beginning\n\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])\n        >>> s.sort_index(na_position='first')\n        NaN     d\n         1.0    c\n         2.0    b\n         3.0    a\n        dtype: object\n\n        Specify index level to sort\n\n        >>> arrays = [np.array(['qux', 'qux', 'foo', 'foo',\n        ...                     'baz', 'baz', 'bar', 'bar']),\n        ...           np.array(['two', 'one', 'two', 'one',\n        ...                     'two', 'one', 'two', 'one'])]\n        >>> s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)\n        >>> s.sort_index(level=1)\n        bar  one    8\n        baz  one    6\n        foo  one    4\n        qux  one    2\n        bar  two    7\n        baz  two    5\n        foo  two    3\n        qux  two    1\n        dtype: int64\n\n        Does not sort by remaining levels when sorting by levels\n\n        >>> s.sort_index(level=1, sort_remaining=False)\n        qux  one    2\n        foo  one    4\n        baz  one    6\n        bar  one    8\n        qux  two    1\n        foo  two    3\n        baz  two    5\n        bar  two    7\n        dtype: int64\n        \"\"\"\n        # TODO: this can be combined with DataFrame.sort_index impl as\n        # almost identical\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n        index = self.index\n\n        if level is not None:\n            new_index, indexer = index.sortlevel(\n                level, ascending=ascending, sort_remaining=sort_remaining\n            )\n        elif isinstance(index, MultiIndex):\n            from pandas.core.sorting import lexsort_indexer\n\n            labels = index._sort_levels_monotonic()\n            indexer = lexsort_indexer(\n                labels._get_codes_for_sorting(),\n                orders=ascending,\n                na_position=na_position,\n            )\n        else:\n            from pandas.core.sorting import nargsort\n\n            # Check monotonic-ness before sort an index\n            # GH11080\n            if (ascending and index.is_monotonic_increasing) or (\n                not ascending and index.is_monotonic_decreasing\n            ):\n                if inplace:\n                    return\n                else:\n                    return self.copy()\n\n            indexer = nargsort(\n                index, kind=kind, ascending=ascending, na_position=na_position\n            )\n\n        indexer = ensure_platform_int(indexer)\n        new_index = index.take(indexer)\n        new_index = new_index._sort_levels_monotonic()\n\n        new_values = self._values.take(indexer)\n        result = self._constructor(new_values, index=new_index)\n\n        if ignore_index:\n            result.index = ibase.default_index(len(result))\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_index\")\n\n    def argsort(self, axis=0, kind=\"quicksort\", order=None) -> \"Series\":\n        \"\"\"\n        Override ndarray.argsort. Argsorts the value, omitting NA/null values,\n        and places the result in the same locations as the non-NA values.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Has no effect but is accepted for compatibility with numpy.\n        kind : {'mergesort', 'quicksort', 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See np.sort for more\n            information. 'mergesort' is the only stable algorithm.\n        order : None\n            Has no effect but is accepted for compatibility with numpy.\n\n        Returns\n        -------\n        Series\n            Positions of values within the sort order with -1 indicating\n            nan values.\n\n        See Also\n        --------\n        numpy.ndarray.argsort : Returns the indices that would sort this array.\n        \"\"\"\n        values = self._values\n        mask = isna(values)\n\n        if mask.any():\n            result = Series(-1, index=self.index, name=self.name, dtype=\"int64\")\n            notmask = ~mask\n            result[notmask] = np.argsort(values[notmask], kind=kind)\n            return self._constructor(result, index=self.index).__finalize__(\n                self, method=\"argsort\"\n            )\n        else:\n            return self._constructor(\n                np.argsort(values, kind=kind), index=self.index, dtype=\"int64\"\n            ).__finalize__(self, method=\"argsort\")\n\n    def nlargest(self, n=5, keep=\"first\") -> \"Series\":\n        \"\"\"\n        Return the largest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many descending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n                of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n                order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n                size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` largest values in the Series, sorted in decreasing order.\n\n        See Also\n        --------\n        Series.nsmallest: Get the `n` smallest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values(ascending=False).head(n)`` for small `n`\n        relative to the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Malta\": 434000, \"Maldives\": 434000,\n        ...                         \"Brunei\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Monserat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Monserat        5200\n        dtype: int64\n\n        The `n` largest elements where ``n=5`` by default.\n\n        >>> s.nlargest()\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3``. Default `keep` value is 'first'\n        so Malta will be kept.\n\n        >>> s.nlargest(3)\n        France    65000000\n        Italy     59000000\n        Malta       434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` and keeping the last duplicates.\n        Brunei will be kept since it is the last with value 434000 based on\n        the index order.\n\n        >>> s.nlargest(3, keep='last')\n        France      65000000\n        Italy       59000000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has five elements due to the three duplicates.\n\n        >>> s.nlargest(3, keep='all')\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nlargest()\n\n    def nsmallest(self, n=5, keep=\"first\") -> \"Series\":\n        \"\"\"\n        Return the smallest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many ascending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n                of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n                order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n                size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` smallest values in the Series, sorted in increasing order.\n\n        See Also\n        --------\n        Series.nlargest: Get the `n` largest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values().head(n)`` for small `n` relative to\n        the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Brunei\": 434000, \"Malta\": 434000,\n        ...                         \"Maldives\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Monserat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Brunei        434000\n        Malta         434000\n        Maldives      434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Monserat        5200\n        dtype: int64\n\n        The `n` smallest elements where ``n=5`` by default.\n\n        >>> s.nsmallest()\n        Monserat      5200\n        Nauru        11300\n        Tuvalu       11300\n        Anguilla     11300\n        Iceland     337000\n        dtype: int64\n\n        The `n` smallest elements where ``n=3``. Default `keep` value is\n        'first' so Nauru and Tuvalu will be kept.\n\n        >>> s.nsmallest(3)\n        Monserat     5200\n        Nauru       11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` and keeping the last\n        duplicates. Anguilla and Tuvalu will be kept since they are the last\n        with value 11300 based on the index order.\n\n        >>> s.nsmallest(3, keep='last')\n        Monserat     5200\n        Anguilla    11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has four elements due to the three duplicates.\n\n        >>> s.nsmallest(3, keep='all')\n        Monserat     5200\n        Nauru       11300\n        Tuvalu      11300\n        Anguilla    11300\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nsmallest()\n\n    def swaplevel(self, i=-2, j=-1, copy=True) -> \"Series\":\n        \"\"\"\n        Swap levels i and j in a :class:`MultiIndex`.\n\n        Default is to swap the two innermost levels of the index.\n\n        Parameters\n        ----------\n        i, j : int, str\n            Level of the indices to be swapped. Can pass level name as string.\n        copy : bool, default True\n            Whether to copy underlying data.\n\n        Returns\n        -------\n        Series\n            Series with levels swapped in MultiIndex.\n        \"\"\"\n        assert isinstance(self.index, ABCMultiIndex)\n        new_index = self.index.swaplevel(i, j)\n        return self._constructor(self._values, index=new_index, copy=copy).__finalize__(\n            self, method=\"swaplevel\"\n        )\n\n    def reorder_levels(self, order) -> \"Series\":\n        \"\"\"\n        Rearrange index levels using input order.\n\n        May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int representing new level order\n            Reference level by number or key.\n\n        Returns\n        -------\n        type of caller (new object)\n        \"\"\"\n        if not isinstance(self.index, MultiIndex):  # pragma: no cover\n            raise Exception(\"Can only reorder levels on a hierarchical axis.\")\n\n        result = self.copy()\n        assert isinstance(result.index, ABCMultiIndex)\n        result.index = result.index.reorder_levels(order)\n        return result\n\n    def explode(self) -> \"Series\":\n        \"\"\"\n        Transform each element of a list-like to a row, replicating the\n        index values.\n\n        .. versionadded:: 0.25.0\n\n        Returns\n        -------\n        Series\n            Exploded lists to rows; index will be duplicated for these rows.\n\n        See Also\n        --------\n        Series.str.split : Split string values on specified separator.\n        Series.unstack : Unstack, a.k.a. pivot, Series with MultiIndex\n            to produce DataFrame.\n        DataFrame.melt : Unpivot a DataFrame from wide format to long format.\n        DataFrame.explode : Explode a DataFrame from list-like\n            columns to long format.\n\n        Notes\n        -----\n        This routine will explode list-likes including lists, tuples,\n        Series, and np.ndarray. The result dtype of the subset rows will\n        be object. Scalars will be returned unchanged. Empty list-likes will\n        result in a np.nan for that row.\n\n        Examples\n        --------\n        >>> s = pd.Series([[1, 2, 3], 'foo', [], [3, 4]])\n        >>> s\n        0    [1, 2, 3]\n        1          foo\n        2           []\n        3       [3, 4]\n        dtype: object\n\n        >>> s.explode()\n        0      1\n        0      2\n        0      3\n        1    foo\n        2    NaN\n        3      3\n        3      4\n        dtype: object\n        \"\"\"\n        if not len(self) or not is_object_dtype(self):\n            return self.copy()\n\n        values, counts = reshape.explode(np.asarray(self.array))\n\n        result = Series(values, index=self.index.repeat(counts), name=self.name)\n        return result\n\n    def unstack(self, level=-1, fill_value=None):\n        \"\"\"\n        Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.\n        The level involved will automatically get sorted.\n\n        Parameters\n        ----------\n        level : int, str, or list of these, default last level\n            Level(s) to unstack, can pass level name.\n        fill_value : scalar value, default None\n            Value to use when replacing NaN values.\n\n        Returns\n        -------\n        DataFrame\n            Unstacked Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4],\n        ...               index=pd.MultiIndex.from_product([['one', 'two'],\n        ...                                                 ['a', 'b']]))\n        >>> s\n        one  a    1\n             b    2\n        two  a    3\n             b    4\n        dtype: int64\n\n        >>> s.unstack(level=-1)\n             a  b\n        one  1  2\n        two  3  4\n\n        >>> s.unstack(level=0)\n           one  two\n        a    1    3\n        b    2    4\n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n\n        return unstack(self, level, fill_value)\n\n    # ----------------------------------------------------------------------\n    # function application\n\n    def map(self, arg, na_action=None) -> \"Series\":\n        \"\"\"\n        Map values of Series according to input correspondence.\n\n        Used for substituting each value in a Series with another value,\n        that may be derived from a function, a ``dict`` or\n        a :class:`Series`.\n\n        Parameters\n        ----------\n        arg : function, collections.abc.Mapping subclass or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NaN values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        Series\n            Same index as caller.\n\n        See Also\n        --------\n        Series.apply : For applying more complex functions on a Series.\n        DataFrame.apply : Apply a function row-/column-wise.\n        DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n\n        Notes\n        -----\n        When ``arg`` is a dictionary, values in Series that are not in the\n        dictionary (as keys) are converted to ``NaN``. However, if the\n        dictionary is a ``dict`` subclass that defines ``__missing__`` (i.e.\n        provides a method for default values), then this default is used\n        rather than ``NaN``.\n\n        Examples\n        --------\n        >>> s = pd.Series(['cat', 'dog', np.nan, 'rabbit'])\n        >>> s\n        0      cat\n        1      dog\n        2      NaN\n        3   rabbit\n        dtype: object\n\n        ``map`` accepts a ``dict`` or a ``Series``. Values that are not found\n        in the ``dict`` are converted to ``NaN``, unless the dict has a default\n        value (e.g. ``defaultdict``):\n\n        >>> s.map({'cat': 'kitten', 'dog': 'puppy'})\n        0   kitten\n        1    puppy\n        2      NaN\n        3      NaN\n        dtype: object\n\n        It also accepts a function:\n\n        >>> s.map('I am a {}'.format)\n        0       I am a cat\n        1       I am a dog\n        2       I am a nan\n        3    I am a rabbit\n        dtype: object\n\n        To avoid applying the function to missing values (and keep them as\n        ``NaN``) ``na_action='ignore'`` can be used:\n\n        >>> s.map('I am a {}'.format, na_action='ignore')\n        0     I am a cat\n        1     I am a dog\n        2            NaN\n        3  I am a rabbit\n        dtype: object\n        \"\"\"\n        new_values = super()._map_values(arg, na_action=na_action)\n        return self._constructor(new_values, index=self.index).__finalize__(\n            self, method=\"map\"\n        )\n\n    def _gotitem(self, key, ndim, subset=None) -> \"Series\":\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : 1,2\n            Requested ndim of result.\n        subset : object, default None\n            Subset to act on.\n        \"\"\"\n        return self\n\n    _agg_see_also_doc = dedent(\n        \"\"\"\n    See Also\n    --------\n    Series.apply : Invoke function on a Series.\n    Series.transform : Transform function producing a Series with like indexes.\n    \"\"\"\n    )\n\n    _agg_examples_doc = dedent(\n        \"\"\"\n    Examples\n    --------\n    >>> s = pd.Series([1, 2, 3, 4])\n    >>> s\n    0    1\n    1    2\n    2    3\n    3    4\n    dtype: int64\n\n    >>> s.agg('min')\n    1\n\n    >>> s.agg(['min', 'max'])\n    min   1\n    max   4\n    dtype: int64\n    \"\"\"\n    )\n\n    @Substitution(\n        see_also=_agg_see_also_doc,\n        examples=_agg_examples_doc,\n        versionadded=\"\\n.. versionadded:: 0.20.0\\n\",\n        **_shared_doc_kwargs,\n    )\n    @Appender(generic._shared_docs[\"aggregate\"])\n    def aggregate(self, func, axis=0, *args, **kwargs):\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n        result, how = self._aggregate(func, *args, **kwargs)\n        if result is None:\n\n            # we can be called from an inner function which\n            # passes this meta-data\n            kwargs.pop(\"_axis\", None)\n            kwargs.pop(\"_level\", None)\n\n            # try a regular apply, this evaluates lambdas\n            # row-by-row; however if the lambda is expected a Series\n            # expression, e.g.: lambda x: x-x.quantile(0.25)\n            # this will fail, so we can try a vectorized evaluation\n\n            # we cannot FIRST try the vectorized evaluation, because\n            # then .agg and .apply would have different semantics if the\n            # operation is actually defined on the Series, e.g. str\n            try:\n                result = self.apply(func, *args, **kwargs)\n            except (ValueError, AttributeError, TypeError):\n                result = func(self, *args, **kwargs)\n\n        return result\n\n    agg = aggregate\n\n    @Appender(generic._shared_docs[\"transform\"] % _shared_doc_kwargs)\n    def transform(self, func, axis=0, *args, **kwargs):\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n        return super().transform(func, *args, **kwargs)\n\n    def apply(self, func, convert_dtype=True, args=(), **kwds):\n        \"\"\"\n        Invoke function on values of Series.\n\n        Can be ufunc (a NumPy function that applies to the entire Series)\n        or a Python function that only works on single values.\n\n        Parameters\n        ----------\n        func : function\n            Python function or NumPy ufunc to apply.\n        convert_dtype : bool, default True\n            Try to find better dtype for elementwise function results. If\n            False, leave as dtype=object.\n        args : tuple\n            Positional arguments passed to func after the series value.\n        **kwds\n            Additional keyword arguments passed to func.\n\n        Returns\n        -------\n        Series or DataFrame\n            If func returns a Series object the result will be a DataFrame.\n\n        See Also\n        --------\n        Series.map: For element-wise operations.\n        Series.agg: Only perform aggregating type operations.\n        Series.transform: Only perform transforming type operations.\n\n        Examples\n        --------\n        Create a series with typical summer temperatures for each city.\n\n        >>> s = pd.Series([20, 21, 12],\n        ...               index=['London', 'New York', 'Helsinki'])\n        >>> s\n        London      20\n        New York    21\n        Helsinki    12\n        dtype: int64\n\n        Square the values by defining a function and passing it as an\n        argument to ``apply()``.\n\n        >>> def square(x):\n        ...     return x ** 2\n        >>> s.apply(square)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Square the values by passing an anonymous function as an\n        argument to ``apply()``.\n\n        >>> s.apply(lambda x: x ** 2)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Define a custom function that needs additional positional\n        arguments and pass these additional arguments using the\n        ``args`` keyword.\n\n        >>> def subtract_custom_value(x, custom_value):\n        ...     return x - custom_value\n\n        >>> s.apply(subtract_custom_value, args=(5,))\n        London      15\n        New York    16\n        Helsinki     7\n        dtype: int64\n\n        Define a custom function that takes keyword arguments\n        and pass these arguments to ``apply``.\n\n        >>> def add_custom_values(x, **kwargs):\n        ...     for month in kwargs:\n        ...         x += kwargs[month]\n        ...     return x\n\n        >>> s.apply(add_custom_values, june=30, july=20, august=25)\n        London      95\n        New York    96\n        Helsinki    87\n        dtype: int64\n\n        Use a function from the Numpy library.\n\n        >>> s.apply(np.log)\n        London      2.995732\n        New York    3.044522\n        Helsinki    2.484907\n        dtype: float64\n        \"\"\"\n        if len(self) == 0:\n            return self._constructor(dtype=self.dtype, index=self.index).__finalize__(\n                self, method=\"apply\"\n            )\n\n        # dispatch to agg\n        if isinstance(func, (list, dict)):\n            return self.aggregate(func, *args, **kwds)\n\n        # if we are a string, try to dispatch\n        if isinstance(func, str):\n            return self._try_aggregate_string_function(func, *args, **kwds)\n\n        # handle ufuncs and lambdas\n        if kwds or args and not isinstance(func, np.ufunc):\n\n            def f(x):\n                return func(x, *args, **kwds)\n\n        else:\n            f = func\n\n        with np.errstate(all=\"ignore\"):\n            if isinstance(f, np.ufunc):\n                return f(self)\n\n            # row-wise access\n            if is_extension_array_dtype(self.dtype) and hasattr(self._values, \"map\"):\n                # GH#23179 some EAs do not have `map`\n                mapped = self._values.map(f)\n            else:\n                values = self.astype(object)._values\n                mapped = lib.map_infer(values, f, convert=convert_dtype)\n\n        if len(mapped) and isinstance(mapped[0], Series):\n            # GH 25959 use pd.array instead of tolist\n            # so extension arrays can be used\n            return self._constructor_expanddim(pd.array(mapped), index=self.index)\n        else:\n            return self._constructor(mapped, index=self.index).__finalize__(\n                self, method=\"apply\"\n            )\n\n    def _reduce(\n        self, op, name, axis=0, skipna=True, numeric_only=None, filter_type=None, **kwds\n    ):\n        \"\"\"\n        Perform a reduction operation.\n\n        If we have an ndarray as a value, then simply perform the operation,\n        otherwise delegate to the object.\n        \"\"\"\n        delegate = self._values\n\n        if axis is not None:\n            self._get_axis_number(axis)\n\n        if isinstance(delegate, ExtensionArray):\n            # dispatch to ExtensionArray interface\n            return delegate._reduce(name, skipna=skipna, **kwds)\n\n        else:\n            # dispatch to numpy arrays\n            if numeric_only:\n                raise NotImplementedError(\n                    f\"Series.{name} does not implement numeric_only.\"\n                )\n            with np.errstate(all=\"ignore\"):\n                return op(delegate, skipna=skipna, **kwds)\n\n    def _reindex_indexer(self, new_index, indexer, copy):\n        if indexer is None:\n            if copy:\n                return self.copy()\n            return self\n\n        new_values = algorithms.take_1d(\n            self._values, indexer, allow_fill=True, fill_value=None\n        )\n        return self._constructor(new_values, index=new_index)\n\n    def _needs_reindex_multi(self, axes, method, level):\n        \"\"\"\n        Check if we do need a multi reindex; this is for compat with\n        higher dims.\n        \"\"\"\n        return False\n\n    @doc(NDFrame.align, **_shared_doc_kwargs)\n    def align(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy=True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n        broadcast_axis=None,\n    ):\n        return super().align(\n            other,\n            join=join,\n            axis=axis,\n            level=level,\n            copy=copy,\n            fill_value=fill_value,\n            method=method,\n            limit=limit,\n            fill_axis=fill_axis,\n            broadcast_axis=broadcast_axis,\n        )\n\n    def rename(\n        self,\n        index=None,\n        *,\n        axis=None,\n        copy=True,\n        inplace=False,\n        level=None,\n        errors=\"ignore\",\n    ):\n        \"\"\"\n        Alter Series index labels or name.\n\n        Function / dict values must be unique (1-to-1). Labels not contained in\n        a dict / Series will be left as-is. Extra labels listed don't throw an\n        error.\n\n        Alternatively, change ``Series.name`` with a scalar value.\n\n        See the :ref:`user guide <basics.rename>` for more.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Unused. Accepted for compatibility with DataFrame method only.\n        index : scalar, hashable sequence, dict-like or function, optional\n            Functions or dict-like are transformations to apply to\n            the index.\n            Scalar or hashable sequence-like will alter the ``Series.name``\n            attribute.\n\n        **kwargs\n            Additional keyword arguments passed to the function. Only the\n            \"inplace\" keyword is used.\n\n        Returns\n        -------\n        Series\n            Series with index labels or name altered.\n\n        See Also\n        --------\n        DataFrame.rename : Corresponding DataFrame method.\n        Series.rename_axis : Set the name of the axis.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n        >>> s.rename(\"my_name\")  # scalar, changes Series.name\n        0    1\n        1    2\n        2    3\n        Name: my_name, dtype: int64\n        >>> s.rename(lambda x: x ** 2)  # function, changes labels\n        0    1\n        1    2\n        4    3\n        dtype: int64\n        >>> s.rename({1: 3, 2: 5})  # mapping, changes labels\n        0    1\n        3    2\n        5    3\n        dtype: int64\n        \"\"\"\n        if callable(index) or is_dict_like(index):\n            return super().rename(\n                index, copy=copy, inplace=inplace, level=level, errors=errors\n            )\n        else:\n            return self._set_name(index, inplace=inplace)\n\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n\n        >>> s.set_axis(['a', 'b', 'c'], axis=0)\n        a    1\n        b    2\n        c    3\n        dtype: int64\n    \"\"\"\n    )\n    @Substitution(\n        **_shared_doc_kwargs,\n        extended_summary_sub=\"\",\n        axis_description_sub=\"\",\n        see_also_sub=\"\",\n    )\n    @Appender(generic.NDFrame.set_axis.__doc__)\n    def set_axis(self, labels, axis: Axis = 0, inplace: bool = False):\n        return super().set_axis(labels, axis=axis, inplace=inplace)\n\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(generic.NDFrame.reindex.__doc__)\n    def reindex(self, index=None, **kwargs):\n        return super().reindex(index=index, **kwargs)\n\n    def drop(\n        self,\n        labels=None,\n        axis=0,\n        index=None,\n        columns=None,\n        level=None,\n        inplace=False,\n        errors=\"raise\",\n    ) -> \"Series\":\n        \"\"\"\n        Return Series with specified index labels removed.\n\n        Remove elements of a Series based on specifying the index labels.\n        When using a multi-index, labels on different levels can be removed\n        by specifying the level.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index labels to drop.\n        axis : 0, default 0\n            Redundant for application on Series.\n        index : single label or list-like\n            Redundant for application on Series, but 'index' can be used instead\n            of 'labels'.\n        columns : single label or list-like\n            No change is made to the Series; use 'index' or 'labels' instead.\n        level : int or level name, optional\n            For MultiIndex, level for which the labels will be removed.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and only existing labels are dropped.\n\n        Returns\n        -------\n        Series\n            Series with specified index labels removed.\n\n        Raises\n        ------\n        KeyError\n            If none of the labels are found in the index.\n\n        See Also\n        --------\n        Series.reindex : Return only specified index labels of Series.\n        Series.dropna : Return series without null values.\n        Series.drop_duplicates : Return Series with duplicate values removed.\n        DataFrame.drop : Drop specified labels from rows or columns.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=np.arange(3), index=['A', 'B', 'C'])\n        >>> s\n        A  0\n        B  1\n        C  2\n        dtype: int64\n\n        Drop labels B en C\n\n        >>> s.drop(labels=['B', 'C'])\n        A  0\n        dtype: int64\n\n        Drop 2nd level label in MultiIndex Series\n\n        >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n        ...                              ['speed', 'weight', 'length']],\n        ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> s = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...               index=midx)\n        >>> s\n        lama    speed      45.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        dtype: float64\n\n        >>> s.drop(labels='weight', level=1)\n        lama    speed      45.0\n                length      1.2\n        cow     speed      30.0\n                length      1.5\n        falcon  speed     320.0\n                length      0.3\n        dtype: float64\n        \"\"\"\n        return super().drop(\n            labels=labels,\n            axis=axis,\n            index=index,\n            columns=columns,\n            level=level,\n            inplace=inplace,\n            errors=errors,\n        )\n\n    @doc(NDFrame.fillna, **_shared_doc_kwargs)\n    def fillna(\n        self,\n        value=None,\n        method=None,\n        axis=None,\n        inplace=False,\n        limit=None,\n        downcast=None,\n    ) -> Optional[\"Series\"]:\n        return super().fillna(\n            value=value,\n            method=method,\n            axis=axis,\n            inplace=inplace,\n            limit=limit,\n            downcast=downcast,\n        )\n\n    @doc(NDFrame.replace, **_shared_doc_kwargs)\n    def replace(\n        self,\n        to_replace=None,\n        value=None,\n        inplace=False,\n        limit=None,\n        regex=False,\n        method=\"pad\",\n    ):\n        return super().replace(\n            to_replace=to_replace,\n            value=value,\n            inplace=inplace,\n            limit=limit,\n            regex=regex,\n            method=method,\n        )\n\n    @doc(NDFrame.shift, **_shared_doc_kwargs)\n    def shift(self, periods=1, freq=None, axis=0, fill_value=None) -> \"Series\":\n        return super().shift(\n            periods=periods, freq=freq, axis=axis, fill_value=fill_value\n        )\n\n    def memory_usage(self, index=True, deep=False):\n        \"\"\"\n        Return the memory usage of the Series.\n\n        The memory usage can optionally include the contribution of\n        the index and of elements of `object` dtype.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the Series index.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned value.\n\n        Returns\n        -------\n        int\n            Bytes of memory consumed.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\n            array.\n        DataFrame.memory_usage : Bytes consumed by a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(range(3))\n        >>> s.memory_usage()\n        152\n\n        Not including the index gives the size of the rest of the data, which\n        is necessarily smaller:\n\n        >>> s.memory_usage(index=False)\n        24\n\n        The memory footprint of `object` values is ignored by default:\n\n        >>> s = pd.Series([\"a\", \"b\"])\n        >>> s.values\n        array(['a', 'b'], dtype=object)\n        >>> s.memory_usage()\n        144\n        >>> s.memory_usage(deep=True)\n        260\n        \"\"\"\n        v = super().memory_usage(deep=deep)\n        if index:\n            v += self.index.memory_usage(deep=deep)\n        return v\n\n    def isin(self, values) -> \"Series\":\n        \"\"\"\n        Check whether `values` are contained in Series.\n\n        Return a boolean Series showing whether each element in the Series\n        matches an element in the passed sequence of `values` exactly.\n\n        Parameters\n        ----------\n        values : set or list-like\n            The sequence of values to test. Passing in a single string will\n            raise a ``TypeError``. Instead, turn a single string into a\n            list of one element.\n\n        Returns\n        -------\n        Series\n            Series of booleans indicating if each element is in values.\n\n        Raises\n        ------\n        TypeError\n          * If `values` is a string\n\n        See Also\n        --------\n        DataFrame.isin : Equivalent method on DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama',\n        ...                'hippo'], name='animal')\n        >>> s.isin(['cow', 'lama'])\n        0     True\n        1     True\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n\n        Passing a single string as ``s.isin('lama')`` will raise an error. Use\n        a list of one element instead:\n\n        >>> s.isin(['lama'])\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n        \"\"\"\n        result = algorithms.isin(self, values)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"isin\"\n        )\n\n    def between(self, left, right, inclusive=True) -> \"Series\":\n        \"\"\"\n        Return boolean Series equivalent to left <= series <= right.\n\n        This function returns a boolean vector containing `True` wherever the\n        corresponding Series element is between the boundary values `left` and\n        `right`. NA values are treated as `False`.\n\n        Parameters\n        ----------\n        left : scalar or list-like\n            Left boundary.\n        right : scalar or list-like\n            Right boundary.\n        inclusive : bool, default True\n            Include boundaries.\n\n        Returns\n        -------\n        Series\n            Series representing whether each element is between left and\n            right (inclusive).\n\n        See Also\n        --------\n        Series.gt : Greater than of series and other.\n        Series.lt : Less than of series and other.\n\n        Notes\n        -----\n        This function is equivalent to ``(left <= ser) & (ser <= right)``\n\n        Examples\n        --------\n        >>> s = pd.Series([2, 0, 4, 8, np.nan])\n\n        Boundary values are included by default:\n\n        >>> s.between(1, 4)\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        With `inclusive` set to ``False`` boundary values are excluded:\n\n        >>> s.between(1, 4, inclusive=False)\n        0     True\n        1    False\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        `left` and `right` can be any scalar value:\n\n        >>> s = pd.Series(['Alice', 'Bob', 'Carol', 'Eve'])\n        >>> s.between('Anna', 'Daniel')\n        0    False\n        1     True\n        2     True\n        3    False\n        dtype: bool\n        \"\"\"\n        if inclusive:\n            lmask = self >= left\n            rmask = self <= right\n        else:\n            lmask = self > left\n            rmask = self < right\n\n        return lmask & rmask\n\n    # ----------------------------------------------------------------------\n    # Convert to types that support pd.NA\n\n    def _convert_dtypes(\n        self,\n        infer_objects: bool = True,\n        convert_string: bool = True,\n        convert_integer: bool = True,\n        convert_boolean: bool = True,\n    ) -> \"Series\":\n        input_series = self\n        if infer_objects:\n            input_series = input_series.infer_objects()\n            if is_object_dtype(input_series):\n                input_series = input_series.copy()\n\n        if convert_string or convert_integer or convert_boolean:\n            inferred_dtype = convert_dtypes(\n                input_series._values, convert_string, convert_integer, convert_boolean\n            )\n            try:\n                result = input_series.astype(inferred_dtype)\n            except TypeError:\n                result = input_series.copy()\n        else:\n            result = input_series.copy()\n        return result\n\n    @Appender(generic._shared_docs[\"isna\"] % _shared_doc_kwargs)\n    def isna(self) -> \"Series\":\n        return super().isna()\n\n    @Appender(generic._shared_docs[\"isna\"] % _shared_doc_kwargs)\n    def isnull(self) -> \"Series\":\n        return super().isnull()\n\n    @Appender(generic._shared_docs[\"notna\"] % _shared_doc_kwargs)\n    def notna(self) -> \"Series\":\n        return super().notna()\n\n    @Appender(generic._shared_docs[\"notna\"] % _shared_doc_kwargs)\n    def notnull(self) -> \"Series\":\n        return super().notnull()\n\n    def dropna(self, axis=0, inplace=False, how=None):\n        \"\"\"\n        Return a new Series with missing values removed.\n\n        See the :ref:`User Guide <missing_data>` for more on which values are\n        considered missing, and how to work with missing data.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            There is only one axis to drop values from.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        how : str, optional\n            Not in use. Kept for compatibility.\n\n        Returns\n        -------\n        Series\n            Series with NA entries dropped from it.\n\n        See Also\n        --------\n        Series.isna: Indicate missing values.\n        Series.notna : Indicate existing (non-missing) values.\n        Series.fillna : Replace missing values.\n        DataFrame.dropna : Drop rows or columns which contain NA values.\n        Index.dropna : Drop missing indices.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1., 2., np.nan])\n        >>> ser\n        0    1.0\n        1    2.0\n        2    NaN\n        dtype: float64\n\n        Drop NA values from a Series.\n\n        >>> ser.dropna()\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Keep the Series with valid entries in the same variable.\n\n        >>> ser.dropna(inplace=True)\n        >>> ser\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Empty strings are not considered NA values. ``None`` is considered an\n        NA value.\n\n        >>> ser = pd.Series([np.NaN, 2, pd.NaT, '', None, 'I stay'])\n        >>> ser\n        0       NaN\n        1         2\n        2       NaT\n        3\n        4      None\n        5    I stay\n        dtype: object\n        >>> ser.dropna()\n        1         2\n        3\n        5    I stay\n        dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis or 0)\n\n        if self._can_hold_na:\n            result = remove_na_arraylike(self)\n            if inplace:\n                self._update_inplace(result)\n            else:\n                return result\n        else:\n            if inplace:\n                # do nothing\n                pass\n            else:\n                return self.copy()\n\n    # ----------------------------------------------------------------------\n    # Time series-oriented methods\n\n    def to_timestamp(self, freq=None, how=\"start\", copy=True) -> \"Series\":\n        \"\"\"\n        Cast to DatetimeIndex of Timestamps, at *beginning* of period.\n\n        Parameters\n        ----------\n        freq : str, default frequency of PeriodIndex\n            Desired frequency.\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series with DatetimeIndex\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        assert isinstance(self.index, (ABCDatetimeIndex, ABCPeriodIndex))\n        new_index = self.index.to_timestamp(freq=freq, how=how)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_timestamp\"\n        )\n\n    def to_period(self, freq=None, copy=True) -> \"Series\":\n        \"\"\"\n        Convert Series from DatetimeIndex to PeriodIndex with desired\n        frequency (inferred from index if not passed).\n\n        Parameters\n        ----------\n        freq : str, default None\n            Frequency associated with the PeriodIndex.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series\n            Series with index converted to PeriodIndex.\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        assert isinstance(self.index, ABCDatetimeIndex)\n        new_index = self.index.to_period(freq=freq)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_period\"\n        )\n\n    # ----------------------------------------------------------------------\n    # Add index\n    _AXIS_ORDERS = [\"index\"]\n    _AXIS_NUMBERS = {\"index\": 0}\n    _AXIS_NAMES = {0: \"index\"}\n    _AXIS_REVERSED = False\n    _AXIS_LEN = len(_AXIS_ORDERS)\n    _info_axis_number = 0\n    _info_axis_name = \"index\"\n\n    index: \"Index\" = properties.AxisProperty(\n        axis=0, doc=\"The index (axis labels) of the Series.\"\n    )\n\n    # ----------------------------------------------------------------------\n    # Accessor Methods\n    # ----------------------------------------------------------------------\n    str = CachedAccessor(\"str\", StringMethods)\n    dt = CachedAccessor(\"dt\", CombinedDatetimelikeProperties)\n    cat = CachedAccessor(\"cat\", CategoricalAccessor)\n    plot = CachedAccessor(\"plot\", pandas.plotting.PlotAccessor)\n    sparse = CachedAccessor(\"sparse\", SparseAccessor)\n\n    # ----------------------------------------------------------------------\n    # Add plotting methods to Series\n    hist = pandas.plotting.hist_series\n\n\nSeries._add_numeric_operations()\nSeries._add_series_or_dataframe_operations()\n\n# Add arithmetic!\nops.add_flex_arithmetic_methods(Series)\nops.add_special_arithmetic_methods(Series)\n",
                "max_stars_repo_path": "BugsInPy/BugsInPy/temp/projects/pandas/bug-21-fixed/pandas/pandas/core/series.py,BugsInPy/BugsInPy/temp/projects/pandas/bug-21-buggy/pandas/pandas/core/series.py",
                "max_stars_repo_name": "NA",
                "max_stars_count": 0,
                "__cluster__": 79
            },
            {
                "id": "pretrain_python_data_64323",
                "content": "<reponame>sisiyan/pandas<gh_stars>1-10\n\"\"\"\nData structure for 1-dimensional cross-sectional and time series data\n\"\"\"\nfrom __future__ import annotations\n\nfrom io import StringIO\nfrom shutil import get_terminal_size\nfrom textwrap import dedent\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Hashable,\n    Iterable,\n    Sequence,\n    Union,\n    cast,\n    overload,\n)\nimport warnings\nimport weakref\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import (\n    lib,\n    properties,\n    reshape,\n    tslibs,\n)\nfrom pandas._libs.lib import no_default\nfrom pandas._typing import (\n    AggFuncType,\n    ArrayLike,\n    Axis,\n    Dtype,\n    DtypeObj,\n    FillnaOptions,\n    FrameOrSeriesUnion,\n    IndexKeyFunc,\n    NpDtype,\n    SingleManager,\n    StorageOptions,\n    ValueKeyFunc,\n)\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import InvalidIndexError\nfrom pandas.util._decorators import (\n    Appender,\n    Substitution,\n    doc,\n)\nfrom pandas.util._validators import (\n    validate_bool_kwarg,\n    validate_percentile,\n)\n\nfrom pandas.core.dtypes.cast import (\n    convert_dtypes,\n    maybe_box_native,\n    maybe_cast_pointwise_result,\n    validate_numeric_casting,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_platform_int,\n    is_bool,\n    is_dict_like,\n    is_integer,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_scalar,\n    pandas_dtype,\n    validate_all_hashable,\n)\nfrom pandas.core.dtypes.generic import ABCDataFrame\nfrom pandas.core.dtypes.inference import is_hashable\nfrom pandas.core.dtypes.missing import (\n    isna,\n    na_value_for_dtype,\n    notna,\n    remove_na_arraylike,\n)\n\nfrom pandas.core import (\n    algorithms,\n    base,\n    generic,\n    missing,\n    nanops,\n    ops,\n)\nfrom pandas.core.accessor import CachedAccessor\nfrom pandas.core.apply import SeriesApply\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.arrays.categorical import CategoricalAccessor\nfrom pandas.core.arrays.sparse import SparseAccessor\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    create_series_with_explicit_dtype,\n    ensure_wrapped_if_datetimelike,\n    extract_array,\n    is_empty_data,\n    sanitize_array,\n)\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.indexers import (\n    deprecate_ndim_indexing,\n    unpack_1tuple,\n)\nfrom pandas.core.indexes.accessors import CombinedDatetimelikeProperties\nfrom pandas.core.indexes.api import (\n    CategoricalIndex,\n    DatetimeIndex,\n    Float64Index,\n    Index,\n    MultiIndex,\n    PeriodIndex,\n    TimedeltaIndex,\n    ensure_index,\n)\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexing import check_bool_indexer\nfrom pandas.core.internals import (\n    SingleArrayManager,\n    SingleBlockManager,\n)\nfrom pandas.core.shared_docs import _shared_docs\nfrom pandas.core.sorting import (\n    ensure_key_mapped,\n    nargsort,\n)\nfrom pandas.core.strings import StringMethods\nfrom pandas.core.tools.datetimes import to_datetime\n\nimport pandas.io.formats.format as fmt\nimport pandas.plotting\n\nif TYPE_CHECKING:\n    from typing import Literal\n\n    from pandas._typing import (\n        TimedeltaConvertibleTypes,\n        TimestampConvertibleTypes,\n    )\n\n    from pandas.core.frame import DataFrame\n    from pandas.core.groupby.generic import SeriesGroupBy\n    from pandas.core.resample import Resampler\n\n__all__ = [\"Series\"]\n\n_shared_doc_kwargs = {\n    \"axes\": \"index\",\n    \"klass\": \"Series\",\n    \"axes_single_arg\": \"{0 or 'index'}\",\n    \"axis\": \"\"\"axis : {0 or 'index'}\n        Parameter needed for compatibility with DataFrame.\"\"\",\n    \"inplace\": \"\"\"inplace : bool, default False\n        If True, performs operation inplace and returns None.\"\"\",\n    \"unique\": \"np.ndarray\",\n    \"duplicated\": \"Series\",\n    \"optional_by\": \"\",\n    \"optional_mapper\": \"\",\n    \"optional_labels\": \"\",\n    \"optional_axis\": \"\",\n    \"replace_iloc\": \"\"\"\n    This differs from updating with ``.loc`` or ``.iloc``, which require\n    you to specify a location to update with some value.\"\"\",\n}\n\n\ndef _coerce_method(converter):\n    \"\"\"\n    Install the scalar coercion methods.\n    \"\"\"\n\n    def wrapper(self):\n        if len(self) == 1:\n            return converter(self.iloc[0])\n        raise TypeError(f\"cannot convert the series to {converter}\")\n\n    wrapper.__name__ = f\"__{converter.__name__}__\"\n    return wrapper\n\n\n# ----------------------------------------------------------------------\n# Series class\n\n\nclass Series(base.IndexOpsMixin, generic.NDFrame):\n    \"\"\"\n    One-dimensional ndarray with axis labels (including time series).\n\n    Labels need not be unique but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for performing operations involving the index. Statistical\n    methods from ndarray have been overridden to automatically exclude\n    missing data (currently represented as NaN).\n\n    Operations between Series (+, -, /, *, **) align values based on their\n    associated index values-- they need not be the same length. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Series. If data is a dict, argument order is\n        maintained.\n    index : array-like or Index (1d)\n        Values must be hashable and have the same length as `data`.\n        Non-unique index values are allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n        and index is None, then the keys in the data are used as the index. If the\n        index is not None, the resulting Series is reindexed with the index values.\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\n        Data type for the output Series. If not specified, this will be\n        inferred from `data`.\n        See the :ref:`user guide <basics.dtypes>` for more usages.\n    name : str, optional\n        The name to give to the Series.\n    copy : bool, default False\n        Copy input data.\n        If False, the memory location for the values is shared and thus\n        a `view` on the input data will be returned.\n        If True, the memory location for the values is not shared, and a `copy`\n        of the input data will be returned.\n\n    Examples\n    --------\n    Constructing Series from a dictionary with an Index specified\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = pd.Series(data=d, index=['a', 'b', 'c'])\n    >>> ser\n    a   1\n    b   2\n    c   3\n    dtype: int64\n\n    The keys of the dictionary match with the Index values, hence the Index\n    values have no effect.\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = pd.Series(data=d, index=['x', 'y', 'z'])\n    >>> ser\n    x   NaN\n    y   NaN\n    z   NaN\n    dtype: float64\n\n    Note that the Index is first build with the keys from the dictionary.\n    After this the Series is reindexed with the given Index values, hence we\n    get all NaN as a result.\n    \"\"\"\n\n    _typ = \"series\"\n    _HANDLED_TYPES = (Index, ExtensionArray, np.ndarray)\n\n    _name: Hashable\n    _metadata: list[str] = [\"name\"]\n    _internal_names_set = {\"index\"} | generic.NDFrame._internal_names_set\n    _accessors = {\"dt\", \"cat\", \"str\", \"sparse\"}\n    _hidden_attrs = (\n        base.IndexOpsMixin._hidden_attrs\n        | generic.NDFrame._hidden_attrs\n        | frozenset([\"compress\", \"ptp\"])\n    )\n\n    # Override cache_readonly bc Series is mutable\n    # error: Incompatible types in assignment (expression has type \"property\",\n    # base class \"IndexOpsMixin\" defined the type as \"Callable[[IndexOpsMixin], bool]\")\n    hasnans = property(  # type: ignore[assignment]\n        base.IndexOpsMixin.hasnans.func, doc=base.IndexOpsMixin.hasnans.__doc__\n    )\n    __hash__ = generic.NDFrame.__hash__\n    _mgr: SingleManager\n    div: Callable[[Series, Any], Series]\n    rdiv: Callable[[Series, Any], Series]\n\n    # ----------------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self,\n        data=None,\n        index=None,\n        dtype: Dtype | None = None,\n        name=None,\n        copy: bool = False,\n        fastpath: bool = False,\n    ):\n\n        if (\n            isinstance(data, (SingleBlockManager, SingleArrayManager))\n            and index is None\n            and dtype is None\n            and copy is False\n        ):\n            # GH#33357 called with just the SingleBlockManager\n            NDFrame.__init__(self, data)\n            self.name = name\n            return\n\n        # we are called internally, so short-circuit\n        if fastpath:\n\n            # data is an ndarray, index is defined\n            if not isinstance(data, (SingleBlockManager, SingleArrayManager)):\n                manager = get_option(\"mode.data_manager\")\n                if manager == \"block\":\n                    data = SingleBlockManager.from_array(data, index)\n                elif manager == \"array\":\n                    data = SingleArrayManager.from_array(data, index)\n            if copy:\n                data = data.copy()\n            if index is None:\n                index = data.index\n\n        else:\n\n            name = ibase.maybe_extract_name(name, data, type(self))\n\n            if is_empty_data(data) and dtype is None:\n                # gh-17261\n                warnings.warn(\n                    \"The default dtype for empty Series will be 'object' instead \"\n                    \"of 'float64' in a future version. Specify a dtype explicitly \"\n                    \"to silence this warning.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n                # uncomment the line below when removing the DeprecationWarning\n                # dtype = np.dtype(object)\n\n            if index is not None:\n                index = ensure_index(index)\n\n            if data is None:\n                data = {}\n            if dtype is not None:\n                dtype = self._validate_dtype(dtype)\n\n            if isinstance(data, MultiIndex):\n                raise NotImplementedError(\n                    \"initializing a Series from a MultiIndex is not supported\"\n                )\n            elif isinstance(data, Index):\n\n                if dtype is not None:\n                    # astype copies\n                    data = data.astype(dtype)\n                else:\n                    # GH#24096 we need to ensure the index remains immutable\n                    data = data._values.copy()\n                copy = False\n\n            elif isinstance(data, np.ndarray):\n                # error: Argument 1 to \"len\" has incompatible type \"dtype\"; expected\n                # \"Sized\"\n                if len(data.dtype):  # type: ignore[arg-type]\n                    # GH#13296 we are dealing with a compound dtype, which\n                    #  should be treated as 2D\n                    raise ValueError(\n                        \"Cannot construct a Series from an ndarray with \"\n                        \"compound dtype.  Use DataFrame instead.\"\n                    )\n            elif isinstance(data, Series):\n                if index is None:\n                    index = data.index\n                else:\n                    data = data.reindex(index, copy=copy)\n                    copy = False\n                data = data._mgr\n            elif is_dict_like(data):\n                data, index = self._init_dict(data, index, dtype)\n                dtype = None\n                copy = False\n            elif isinstance(data, (SingleBlockManager, SingleArrayManager)):\n                if index is None:\n                    index = data.index\n                elif not data.index.equals(index) or copy:\n                    # GH#19275 SingleBlockManager input should only be called\n                    # internally\n                    raise AssertionError(\n                        \"Cannot pass both SingleBlockManager \"\n                        \"`data` argument and a different \"\n                        \"`index` argument. `copy` must be False.\"\n                    )\n\n            elif isinstance(data, ExtensionArray):\n                pass\n            else:\n                data = com.maybe_iterable_to_list(data)\n\n            if index is None:\n                if not is_list_like(data):\n                    data = [data]\n                index = ibase.default_index(len(data))\n            elif is_list_like(data):\n                com.require_length_match(data, index)\n\n            # create/copy the manager\n            if isinstance(data, (SingleBlockManager, SingleArrayManager)):\n                if dtype is not None:\n                    data = data.astype(dtype=dtype, errors=\"ignore\", copy=copy)\n                elif copy:\n                    data = data.copy()\n            else:\n                data = sanitize_array(data, index, dtype, copy)\n\n                manager = get_option(\"mode.data_manager\")\n                if manager == \"block\":\n                    data = SingleBlockManager.from_array(data, index)\n                elif manager == \"array\":\n                    data = SingleArrayManager.from_array(data, index)\n\n        generic.NDFrame.__init__(self, data)\n        self.name = name\n        self._set_axis(0, index, fastpath=True)\n\n    def _init_dict(self, data, index=None, dtype: Dtype | None = None):\n        \"\"\"\n        Derive the \"_mgr\" and \"index\" attributes of a new Series from a\n        dictionary input.\n\n        Parameters\n        ----------\n        data : dict or dict-like\n            Data used to populate the new Series.\n        index : Index or index-like, default None\n            Index for the new Series: if None, use dict keys.\n        dtype : dtype, default None\n            The dtype for the new Series: if None, infer from data.\n\n        Returns\n        -------\n        _data : BlockManager for the new Series\n        index : index for the new Series\n        \"\"\"\n        # Looking for NaN in dict doesn't work ({np.nan : 1}[float('nan')]\n        # raises KeyError), so we iterate the entire dict, and align\n        if data:\n            # GH:34717, issue was using zip to extract key and values from data.\n            # using generators in effects the performance.\n            # Below is the new way of extracting the keys and values\n\n            keys = tuple(data.keys())\n            values = list(data.values())  # Generating list of values- faster way\n        elif index is not None:\n            # fastpath for Series(data=None). Just use broadcasting a scalar\n            # instead of reindexing.\n            values = na_value_for_dtype(pandas_dtype(dtype))\n            keys = index\n        else:\n            keys, values = (), []\n\n        # Input is now list-like, so rely on \"standard\" construction:\n\n        # TODO: passing np.float64 to not break anything yet. See GH-17261\n        s = create_series_with_explicit_dtype(\n            # error: Argument \"index\" to \"create_series_with_explicit_dtype\" has\n            # incompatible type \"Tuple[Any, ...]\"; expected \"Union[ExtensionArray,\n            # ndarray, Index, None]\"\n            values,\n            index=keys,  # type: ignore[arg-type]\n            dtype=dtype,\n            dtype_if_empty=np.float64,\n        )\n\n        # Now we just make sure the order is respected, if any\n        if data and index is not None:\n            s = s.reindex(index, copy=False)\n        return s._mgr, s.index\n\n    # ----------------------------------------------------------------------\n\n    @property\n    def _constructor(self) -> type[Series]:\n        return Series\n\n    @property\n    def _constructor_expanddim(self) -> type[DataFrame]:\n        \"\"\"\n        Used when a manipulation result has one higher dimension as the\n        original, such as Series.to_frame()\n        \"\"\"\n        from pandas.core.frame import DataFrame\n\n        return DataFrame\n\n    # types\n    @property\n    def _can_hold_na(self) -> bool:\n        return self._mgr._can_hold_na\n\n    _index: Index | None = None\n\n    def _set_axis(self, axis: int, labels, fastpath: bool = False) -> None:\n        \"\"\"\n        Override generic, we want to set the _typ here.\n\n        This is called from the cython code when we set the `index` attribute\n        directly, e.g. `series.index = [1, 2, 3]`.\n        \"\"\"\n        if not fastpath:\n            labels = ensure_index(labels)\n\n        if labels._is_all_dates:\n            deep_labels = labels\n            if isinstance(labels, CategoricalIndex):\n                deep_labels = labels.categories\n\n            if not isinstance(\n                deep_labels, (DatetimeIndex, PeriodIndex, TimedeltaIndex)\n            ):\n                try:\n                    labels = DatetimeIndex(labels)\n                    # need to set here because we changed the index\n                    if fastpath:\n                        self._mgr.set_axis(axis, labels)\n                except (tslibs.OutOfBoundsDatetime, ValueError):\n                    # labels may exceeds datetime bounds,\n                    # or not be a DatetimeIndex\n                    pass\n\n        object.__setattr__(self, \"_index\", labels)\n        if not fastpath:\n            # The ensure_index call above ensures we have an Index object\n            self._mgr.set_axis(axis, labels)\n\n    # ndarray compatibility\n    @property\n    def dtype(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._mgr.dtype\n\n    @property\n    def dtypes(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        # DataFrame compatibility\n        return self.dtype\n\n    @property\n    def name(self) -> Hashable:\n        \"\"\"\n        Return the name of the Series.\n\n        The name of a Series becomes its index or column name if it is used\n        to form a DataFrame. It is also used whenever displaying the Series\n        using the interpreter.\n\n        Returns\n        -------\n        label (hashable object)\n            The name of the Series, also the column name if part of a DataFrame.\n\n        See Also\n        --------\n        Series.rename : Sets the Series name when given a scalar input.\n        Index.name : Corresponding Index property.\n\n        Examples\n        --------\n        The Series name can be set initially when calling the constructor.\n\n        >>> s = pd.Series([1, 2, 3], dtype=np.int64, name='Numbers')\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Numbers, dtype: int64\n        >>> s.name = \"Integers\"\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Integers, dtype: int64\n\n        The name of a Series within a DataFrame is its column name.\n\n        >>> df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],\n        ...                   columns=[\"Odd Numbers\", \"Even Numbers\"])\n        >>> df\n           Odd Numbers  Even Numbers\n        0            1             2\n        1            3             4\n        2            5             6\n        >>> df[\"Even Numbers\"].name\n        'Even Numbers'\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value: Hashable) -> None:\n        validate_all_hashable(value, error_name=f\"{type(self).__name__}.name\")\n        object.__setattr__(self, \"_name\", value)\n\n    @property\n    def values(self):\n        \"\"\"\n        Return Series as ndarray or ndarray-like depending on the dtype.\n\n        .. warning::\n\n           We recommend using :attr:`Series.array` or\n           :meth:`Series.to_numpy`, depending on whether you need\n           a reference to the underlying data or a NumPy array.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n\n        See Also\n        --------\n        Series.array : Reference to the underlying data.\n        Series.to_numpy : A NumPy array representing the underlying data.\n\n        Examples\n        --------\n        >>> pd.Series([1, 2, 3]).values\n        array([1, 2, 3])\n\n        >>> pd.Series(list('aabc')).values\n        array(['a', 'a', 'b', 'c'], dtype=object)\n\n        >>> pd.Series(list('aabc')).astype('category').values\n        ['a', 'a', 'b', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n\n        Timezone aware datetime data is converted to UTC:\n\n        >>> pd.Series(pd.date_range('20130101', periods=3,\n        ...                         tz='US/Eastern')).values\n        array(['2013-01-01T05:00:00.000000000',\n               '2013-01-02T05:00:00.000000000',\n               '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')\n        \"\"\"\n        return self._mgr.external_values()\n\n    @property\n    def _values(self):\n        \"\"\"\n        Return the internal repr of this data (defined by Block.interval_values).\n        This are the values as stored in the Block (ndarray or ExtensionArray\n        depending on the Block class), with datetime64[ns] and timedelta64[ns]\n        wrapped in ExtensionArrays to match Index._values behavior.\n\n        Differs from the public ``.values`` for certain data types, because of\n        historical backwards compatibility of the public attribute (e.g. period\n        returns object ndarray and datetimetz a datetime64[ns] ndarray for\n        ``.values`` while it returns an ExtensionArray for ``._values`` in those\n        cases).\n\n        Differs from ``.array`` in that this still returns the numpy array if\n        the Block is backed by a numpy array (except for datetime64 and\n        timedelta64 dtypes), while ``.array`` ensures to always return an\n        ExtensionArray.\n\n        Overview:\n\n        dtype       | values        | _values       | array         |\n        ----------- | ------------- | ------------- | ------------- |\n        Numeric     | ndarray       | ndarray       | PandasArray   |\n        Category    | Categorical   | Categorical   | Categorical   |\n        dt64[ns]    | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        dt64[ns tz] | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        td64[ns]    | ndarray[m8ns] | TimedeltaArray| ndarray[m8ns] |\n        Period      | ndarray[obj]  | PeriodArray   | PeriodArray   |\n        Nullable    | EA            | EA            | EA            |\n\n        \"\"\"\n        return self._mgr.internal_values()\n\n    # error: Decorated property not supported\n    @Appender(base.IndexOpsMixin.array.__doc__)  # type: ignore[misc]\n    @property\n    def array(self) -> ExtensionArray:\n        return self._mgr.array_values()\n\n    # ops\n    def ravel(self, order=\"C\"):\n        \"\"\"\n        Return the flattened underlying data as an ndarray.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n            Flattened data of the Series.\n\n        See Also\n        --------\n        numpy.ndarray.ravel : Return a flattened array.\n        \"\"\"\n        return self._values.ravel(order=order)\n\n    def __len__(self) -> int:\n        \"\"\"\n        Return the length of the Series.\n        \"\"\"\n        return len(self._mgr)\n\n    def view(self, dtype: Dtype | None = None) -> Series:\n        \"\"\"\n        Create a new view of the Series.\n\n        This function will return a new Series with a view of the same\n        underlying values in memory, optionally reinterpreted with a new data\n        type. The new data type must preserve the same size in bytes as to not\n        cause index misalignment.\n\n        Parameters\n        ----------\n        dtype : data type\n            Data type object or one of their string representations.\n\n        Returns\n        -------\n        Series\n            A new Series object as a view of the same data in memory.\n\n        See Also\n        --------\n        numpy.ndarray.view : Equivalent numpy function to create a new view of\n            the same data in memory.\n\n        Notes\n        -----\n        Series are instantiated with ``dtype=float64`` by default. While\n        ``numpy.ndarray.view()`` will return a view with the same data type as\n        the original array, ``Series.view()`` (without specified dtype)\n        will try using ``float64`` and may fail if the original data type size\n        in bytes is not the same.\n\n        Examples\n        --------\n        >>> s = pd.Series([-2, -1, 0, 1, 2], dtype='int8')\n        >>> s\n        0   -2\n        1   -1\n        2    0\n        3    1\n        4    2\n        dtype: int8\n\n        The 8 bit signed integer representation of `-1` is `0b11111111`, but\n        the same bytes represent 255 if read as an 8 bit unsigned integer:\n\n        >>> us = s.view('uint8')\n        >>> us\n        0    254\n        1    255\n        2      0\n        3      1\n        4      2\n        dtype: uint8\n\n        The views share the same underlying values:\n\n        >>> us[0] = 128\n        >>> s\n        0   -128\n        1     -1\n        2      0\n        3      1\n        4      2\n        dtype: int8\n        \"\"\"\n        return self._constructor(\n            self._values.view(dtype), index=self.index\n        ).__finalize__(self, method=\"view\")\n\n    # ----------------------------------------------------------------------\n    # NDArray Compat\n    _HANDLED_TYPES = (Index, ExtensionArray, np.ndarray)\n\n    def __array__(self, dtype: NpDtype | None = None) -> np.ndarray:\n        \"\"\"\n        Return the values as a NumPy array.\n\n        Users should not call this directly. Rather, it is invoked by\n        :func:`numpy.array` and :func:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to use for the resulting NumPy array. By default,\n            the dtype is inferred from the data.\n\n        Returns\n        -------\n        numpy.ndarray\n            The values in the series converted to a :class:`numpy.ndarray`\n            with the specified `dtype`.\n\n        See Also\n        --------\n        array : Create a new array from data.\n        Series.array : Zero-copy view to the array backing the Series.\n        Series.to_numpy : Series method for similar behavior.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1, 2, 3])\n        >>> np.asarray(ser)\n        array([1, 2, 3])\n\n        For timezone-aware data, the timezones may be retained with\n        ``dtype='object'``\n\n        >>> tzser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\n        >>> np.asarray(tzser, dtype=\"object\")\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET', freq='D'),\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET', freq='D')],\n              dtype=object)\n\n        Or the values may be localized to UTC and the tzinfo discarded with\n        ``dtype='datetime64[ns]'``\n\n        >>> np.asarray(tzser, dtype=\"datetime64[ns]\")  # doctest: +ELLIPSIS\n        array(['1999-12-31T23:00:00.000000000', ...],\n              dtype='datetime64[ns]')\n        \"\"\"\n        return np.asarray(self._values, dtype)\n\n    # ----------------------------------------------------------------------\n    # Unary Methods\n\n    # coercion\n    __float__ = _coerce_method(float)\n    __long__ = _coerce_method(int)\n    __int__ = _coerce_method(int)\n\n    # ----------------------------------------------------------------------\n\n    # indexers\n    @property\n    def axes(self) -> list[Index]:\n        \"\"\"\n        Return a list of the row axis labels.\n        \"\"\"\n        return [self.index]\n\n    # ----------------------------------------------------------------------\n    # Indexing Methods\n\n    @Appender(generic.NDFrame.take.__doc__)\n    def take(self, indices, axis=0, is_copy=None, **kwargs) -> Series:\n        if is_copy is not None:\n            warnings.warn(\n                \"is_copy is deprecated and will be removed in a future version. \"\n                \"'take' always returns a copy, so there is no need to specify this.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n        nv.validate_take((), kwargs)\n\n        indices = ensure_platform_int(indices)\n        new_index = self.index.take(indices)\n        new_values = self._values.take(indices)\n\n        result = self._constructor(new_values, index=new_index, fastpath=True)\n        return result.__finalize__(self, method=\"take\")\n\n    def _take_with_is_copy(self, indices, axis=0) -> Series:\n        \"\"\"\n        Internal version of the `take` method that sets the `_is_copy`\n        attribute to keep track of the parent dataframe (using in indexing\n        for the SettingWithCopyWarning). For Series this does the same\n        as the public take (it never sets `_is_copy`).\n\n        See the docstring of `take` for full explanation of the parameters.\n        \"\"\"\n        return self.take(indices=indices, axis=axis)\n\n    def _ixs(self, i: int, axis: int = 0):\n        \"\"\"\n        Return the i-th value or values in the Series by location.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        scalar (int) or Series (slice, sequence)\n        \"\"\"\n        return self._values[i]\n\n    def _slice(self, slobj: slice, axis: int = 0) -> Series:\n        # axis kwarg is retained for compat with NDFrame method\n        #  _slice is *always* positional\n        return self._get_values(slobj)\n\n    def __getitem__(self, key):\n        key = com.apply_if_callable(key, self)\n\n        if key is Ellipsis:\n            return self\n\n        key_is_scalar = is_scalar(key)\n        if isinstance(key, (list, tuple)):\n            key = unpack_1tuple(key)\n\n        if is_integer(key) and self.index._should_fallback_to_positional():\n            return self._values[key]\n\n        elif key_is_scalar:\n            return self._get_value(key)\n\n        if is_hashable(key):\n            # Otherwise index.get_value will raise InvalidIndexError\n            try:\n                # For labels that don't resolve as scalars like tuples and frozensets\n                result = self._get_value(key)\n\n                return result\n\n            except (KeyError, TypeError):\n                if isinstance(key, tuple) and isinstance(self.index, MultiIndex):\n                    # We still have the corner case where a tuple is a key\n                    # in the first level of our MultiIndex\n                    return self._get_values_tuple(key)\n\n        if is_iterator(key):\n            key = list(key)\n\n        if com.is_bool_indexer(key):\n            key = check_bool_indexer(self.index, key)\n            key = np.asarray(key, dtype=bool)\n            return self._get_values(key)\n\n        return self._get_with(key)\n\n    def _get_with(self, key):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            # _convert_slice_indexer to determine if this slice is positional\n            #  or label based, and if the latter, convert to positional\n            slobj = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            return self._slice(slobj)\n        elif isinstance(key, ABCDataFrame):\n            raise TypeError(\n                \"Indexing a Series with DataFrame is not \"\n                \"supported, use the appropriate DataFrame column\"\n            )\n        elif isinstance(key, tuple):\n            return self._get_values_tuple(key)\n\n        elif not is_list_like(key):\n            # e.g. scalars that aren't recognized by lib.is_scalar, GH#32684\n            return self.loc[key]\n\n        if not isinstance(key, (list, np.ndarray, ExtensionArray, Series, Index)):\n            key = list(key)\n\n        if isinstance(key, Index):\n            key_type = key.inferred_type\n        else:\n            key_type = lib.infer_dtype(key, skipna=False)\n\n        # Note: The key_type == \"boolean\" case should be caught by the\n        #  com.is_bool_indexer check in __getitem__\n        if key_type == \"integer\":\n            # We need to decide whether to treat this as a positional indexer\n            #  (i.e. self.iloc) or label-based (i.e. self.loc)\n            if not self.index._should_fallback_to_positional():\n                return self.loc[key]\n            else:\n                return self.iloc[key]\n\n        # handle the dup indexing case GH#4246\n        return self.loc[key]\n\n    def _get_values_tuple(self, key):\n        # mpl hackaround\n        if com.any_none(*key):\n            result = self._get_values(key)\n            deprecate_ndim_indexing(result, stacklevel=5)\n            return result\n\n        if not isinstance(self.index, MultiIndex):\n            raise KeyError(\"key of type tuple not found and not a MultiIndex\")\n\n        # If key is contained, would have returned by now\n        indexer, new_index = self.index.get_loc_level(key)\n        return self._constructor(self._values[indexer], index=new_index).__finalize__(\n            self\n        )\n\n    def _get_values(self, indexer):\n        try:\n            new_mgr = self._mgr.getitem_mgr(indexer)\n            return self._constructor(new_mgr).__finalize__(self)\n        except ValueError:\n            # mpl compat if we look up e.g. ser[:, np.newaxis];\n            #  see tests.series.timeseries.test_mpl_compat_hack\n            # the asarray is needed to avoid returning a 2D DatetimeArray\n            return np.asarray(self._values[indexer])\n\n    def _get_value(self, label, takeable: bool = False):\n        \"\"\"\n        Quickly retrieve single value at passed index label.\n\n        Parameters\n        ----------\n        label : object\n        takeable : interpret the index as indexers, default False\n\n        Returns\n        -------\n        scalar value\n        \"\"\"\n        if takeable:\n            return self._values[label]\n\n        # Similar to Index.get_value, but we do not fall back to positional\n        loc = self.index.get_loc(label)\n        return self.index._get_values_for_loc(self, loc, label)\n\n    def __setitem__(self, key, value) -> None:\n        key = com.apply_if_callable(key, self)\n        cacher_needs_updating = self._check_is_chained_assignment_possible()\n\n        if key is Ellipsis:\n            key = slice(None)\n\n        try:\n            self._set_with_engine(key, value)\n        except (KeyError, ValueError):\n            values = self._values\n            if is_integer(key) and self.index.inferred_type != \"integer\":\n                # positional setter\n                values[key] = value\n            else:\n                # GH#12862 adding a new key to the Series\n                self.loc[key] = value\n\n        except TypeError as err:\n            if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):\n                raise KeyError(\n                    \"key of type tuple not found and not a MultiIndex\"\n                ) from err\n\n            if com.is_bool_indexer(key):\n                key = check_bool_indexer(self.index, key)\n                key = np.asarray(key, dtype=bool)\n                try:\n                    self._where(~key, value, inplace=True)\n                except InvalidIndexError:\n                    self.iloc[key] = value\n                return\n\n            else:\n                self._set_with(key, value)\n\n        if cacher_needs_updating:\n            self._maybe_update_cacher()\n\n    def _set_with_engine(self, key, value) -> None:\n        # fails with AttributeError for IntervalIndex\n        loc = self.index._engine.get_loc(key)\n        # error: Argument 1 to \"validate_numeric_casting\" has incompatible type\n        # \"Union[dtype, ExtensionDtype]\"; expected \"dtype\"\n        validate_numeric_casting(self.dtype, value)  # type: ignore[arg-type]\n        self._values[loc] = value\n\n    def _set_with(self, key, value):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            indexer = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            return self._set_values(indexer, value)\n\n        else:\n            assert not isinstance(key, tuple)\n\n            if is_scalar(key):\n                key = [key]\n\n            if isinstance(key, Index):\n                key_type = key.inferred_type\n                key = key._values\n            else:\n                key_type = lib.infer_dtype(key, skipna=False)\n\n            # Note: key_type == \"boolean\" should not occur because that\n            #  should be caught by the is_bool_indexer check in __setitem__\n            if key_type == \"integer\":\n                if not self.index._should_fallback_to_positional():\n                    self._set_labels(key, value)\n                else:\n                    self._set_values(key, value)\n            else:\n                self.loc[key] = value\n\n    def _set_labels(self, key, value) -> None:\n        key = com.asarray_tuplesafe(key)\n        indexer: np.ndarray = self.index.get_indexer(key)\n        mask = indexer == -1\n        if mask.any():\n            raise KeyError(f\"{key[mask]} not in index\")\n        self._set_values(indexer, value)\n\n    def _set_values(self, key, value) -> None:\n        if isinstance(key, Series):\n            key = key._values\n\n        self._mgr = self._mgr.setitem(indexer=key, value=value)\n        self._maybe_update_cacher()\n\n    def _set_value(self, label, value, takeable: bool = False):\n        \"\"\"\n        Quickly set single value at passed label.\n\n        If label is not contained, a new object is created with the label\n        placed at the end of the result index.\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed.\n        value : object\n            Scalar value.\n        takeable : interpret the index as indexers, default False\n        \"\"\"\n        if not takeable:\n            try:\n                loc = self.index.get_loc(label)\n            except KeyError:\n                # set using a non-recursive method\n                self.loc[label] = value\n                return\n        else:\n            loc = label\n\n        self._set_values(loc, value)\n\n    # ----------------------------------------------------------------------\n    # Lookup Caching\n\n    @property\n    def _is_cached(self) -> bool:\n        \"\"\"Return boolean indicating if self is cached or not.\"\"\"\n        return getattr(self, \"_cacher\", None) is not None\n\n    def _get_cacher(self):\n        \"\"\"return my cacher or None\"\"\"\n        cacher = getattr(self, \"_cacher\", None)\n        if cacher is not None:\n            cacher = cacher[1]()\n        return cacher\n\n    def _reset_cacher(self) -> None:\n        \"\"\"\n        Reset the cacher.\n        \"\"\"\n        if hasattr(self, \"_cacher\"):\n            # should only get here with self.ndim == 1\n            del self._cacher\n\n    def _set_as_cached(self, item, cacher) -> None:\n        \"\"\"\n        Set the _cacher attribute on the calling object with a weakref to\n        cacher.\n        \"\"\"\n        self._cacher = (item, weakref.ref(cacher))\n\n    def _clear_item_cache(self) -> None:\n        # no-op for Series\n        pass\n\n    def _check_is_chained_assignment_possible(self) -> bool:\n        \"\"\"\n        See NDFrame._check_is_chained_assignment_possible.__doc__\n        \"\"\"\n        if self._is_view and self._is_cached:\n            ref = self._get_cacher()\n            if ref is not None and ref._is_mixed_type:\n                self._check_setitem_copy(stacklevel=4, t=\"referent\", force=True)\n            return True\n        return super()._check_is_chained_assignment_possible()\n\n    def _maybe_update_cacher(\n        self, clear: bool = False, verify_is_copy: bool = True\n    ) -> None:\n        \"\"\"\n        See NDFrame._maybe_update_cacher.__doc__\n        \"\"\"\n        cacher = getattr(self, \"_cacher\", None)\n        if cacher is not None:\n            assert self.ndim == 1\n            ref: DataFrame = cacher[1]()\n\n            # we are trying to reference a dead referent, hence\n            # a copy\n            if ref is None:\n                del self._cacher\n            else:\n                if len(self) == len(ref):\n                    # otherwise, either self or ref has swapped in new arrays\n                    ref._maybe_cache_changed(cacher[0], self)\n                else:\n                    # GH#33675 we have swapped in a new array, so parent\n                    #  reference to self is now invalid\n                    ref._item_cache.pop(cacher[0], None)\n\n        super()._maybe_update_cacher(clear=clear, verify_is_copy=verify_is_copy)\n\n    # ----------------------------------------------------------------------\n    # Unsorted\n\n    @property\n    def _is_mixed_type(self):\n        return False\n\n    def repeat(self, repeats, axis=None) -> Series:\n        \"\"\"\n        Repeat elements of a Series.\n\n        Returns a new Series where each element of the current Series\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            Series.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        Series\n            Newly created Series with repeated elements.\n\n        See Also\n        --------\n        Index.repeat : Equivalent function for Index.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s\n        0    a\n        1    b\n        2    c\n        dtype: object\n        >>> s.repeat(2)\n        0    a\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        dtype: object\n        >>> s.repeat([1, 2, 3])\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        2    c\n        dtype: object\n        \"\"\"\n        nv.validate_repeat((), {\"axis\": axis})\n        new_index = self.index.repeat(repeats)\n        new_values = self._values.repeat(repeats)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"repeat\"\n        )\n\n    def reset_index(self, level=None, drop=False, name=None, inplace=False):\n        \"\"\"\n        Generate a new DataFrame or Series with the index reset.\n\n        This is useful when the index needs to be treated as a column, or\n        when the index is meaningless and needs to be reset to the default\n        before another operation.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default optional\n            For a Series with a MultiIndex, only remove the specified levels\n            from the index. Removes all levels by default.\n        drop : bool, default False\n            Just reset the index, without inserting it as a column in\n            the new DataFrame.\n        name : object, optional\n            The name to use for the column containing the original Series\n            values. Uses ``self.name`` by default. This argument is ignored\n            when `drop` is True.\n        inplace : bool, default False\n            Modify the Series in place (do not create a new object).\n\n        Returns\n        -------\n        Series or DataFrame or None\n            When `drop` is False (the default), a DataFrame is returned.\n            The newly created columns will come first in the DataFrame,\n            followed by the original Series values.\n            When `drop` is True, a `Series` is returned.\n            In either case, if ``inplace=True``, no value is returned.\n\n        See Also\n        --------\n        DataFrame.reset_index: Analogous function for DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4], name='foo',\n        ...               index=pd.Index(['a', 'b', 'c', 'd'], name='idx'))\n\n        Generate a DataFrame with default index.\n\n        >>> s.reset_index()\n          idx  foo\n        0   a    1\n        1   b    2\n        2   c    3\n        3   d    4\n\n        To specify the name of the new column use `name`.\n\n        >>> s.reset_index(name='values')\n          idx  values\n        0   a       1\n        1   b       2\n        2   c       3\n        3   d       4\n\n        To generate a new Series with the default set `drop` to True.\n\n        >>> s.reset_index(drop=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        To update the Series in place, without generating a new one\n        set `inplace` to True. Note that it also requires ``drop=True``.\n\n        >>> s.reset_index(inplace=True, drop=True)\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        The `level` parameter is interesting for Series with a multi-level\n        index.\n\n        >>> arrays = [np.array(['bar', 'bar', 'baz', 'baz']),\n        ...           np.array(['one', 'two', 'one', 'two'])]\n        >>> s2 = pd.Series(\n        ...     range(4), name='foo',\n        ...     index=pd.MultiIndex.from_arrays(arrays,\n        ...                                     names=['a', 'b']))\n\n        To remove a specific level from the Index, use `level`.\n\n        >>> s2.reset_index(level='a')\n               a  foo\n        b\n        one  bar    0\n        two  bar    1\n        one  baz    2\n        two  baz    3\n\n        If `level` is not set, all levels are removed from the Index.\n\n        >>> s2.reset_index()\n             a    b  foo\n        0  bar  one    0\n        1  bar  two    1\n        2  baz  one    2\n        3  baz  two    3\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if drop:\n            new_index = ibase.default_index(len(self))\n            if level is not None:\n                if not isinstance(level, (tuple, list)):\n                    level = [level]\n                level = [self.index._get_level_number(lev) for lev in level]\n                if len(level) < self.index.nlevels:\n                    new_index = self.index.droplevel(level)\n\n            if inplace:\n                self.index = new_index\n                # set name if it was passed, otherwise, keep the previous name\n                self.name = name or self.name\n            else:\n                return self._constructor(\n                    self._values.copy(), index=new_index\n                ).__finalize__(self, method=\"reset_index\")\n        elif inplace:\n            raise TypeError(\n                \"Cannot reset_index inplace on a Series to create a DataFrame\"\n            )\n        else:\n            df = self.to_frame(name)\n            return df.reset_index(level=level, drop=drop)\n\n    # ----------------------------------------------------------------------\n    # Rendering Methods\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular Series.\n        \"\"\"\n        buf = StringIO(\"\")\n        width, height = get_terminal_size()\n        max_rows = (\n            height\n            if get_option(\"display.max_rows\") == 0\n            else get_option(\"display.max_rows\")\n        )\n        min_rows = (\n            height\n            if get_option(\"display.max_rows\") == 0\n            else get_option(\"display.min_rows\")\n        )\n        show_dimensions = get_option(\"display.show_dimensions\")\n\n        self.to_string(\n            buf=buf,\n            name=self.name,\n            dtype=self.dtype,\n            min_rows=min_rows,\n            max_rows=max_rows,\n            length=show_dimensions,\n        )\n        return buf.getvalue()\n\n    def to_string(\n        self,\n        buf=None,\n        na_rep=\"NaN\",\n        float_format=None,\n        header=True,\n        index=True,\n        length=False,\n        dtype=False,\n        name=False,\n        max_rows=None,\n        min_rows=None,\n    ):\n        \"\"\"\n        Render a string representation of the Series.\n\n        Parameters\n        ----------\n        buf : StringIO-like, optional\n            Buffer to write to.\n        na_rep : str, optional\n            String representation of NaN to use, default 'NaN'.\n        float_format : one-parameter function, optional\n            Formatter function to apply to columns' elements if they are\n            floats, default None.\n        header : bool, default True\n            Add the Series header (index name).\n        index : bool, optional\n            Add index (row) labels, default True.\n        length : bool, default False\n            Add the Series length.\n        dtype : bool, default False\n            Add the Series dtype.\n        name : bool, default False\n            Add the Series name if not None.\n        max_rows : int, optional\n            Maximum number of rows to show before truncating. If None, show\n            all.\n        min_rows : int, optional\n            The number of rows to display in a truncated repr (when number\n            of rows is above `max_rows`).\n\n        Returns\n        -------\n        str or None\n            String representation of Series if ``buf=None``, otherwise None.\n        \"\"\"\n        formatter = fmt.SeriesFormatter(\n            self,\n            name=name,\n            length=length,\n            header=header,\n            index=index,\n            dtype=dtype,\n            na_rep=na_rep,\n            float_format=float_format,\n            min_rows=min_rows,\n            max_rows=max_rows,\n        )\n        result = formatter.to_string()\n\n        # catch contract violations\n        if not isinstance(result, str):\n            raise AssertionError(\n                \"result must be of type str, type \"\n                f\"of result is {repr(type(result).__name__)}\"\n            )\n\n        if buf is None:\n            return result\n        else:\n            try:\n                buf.write(result)\n            except AttributeError:\n                with open(buf, \"w\") as f:\n                    f.write(result)\n\n    @doc(\n        klass=_shared_doc_kwargs[\"klass\"],\n        storage_options=generic._shared_docs[\"storage_options\"],\n        examples=dedent(\n            \"\"\"\n            Examples\n            --------\n            >>> s = pd.Series([\"elk\", \"pig\", \"dog\", \"quetzal\"], name=\"animal\")\n            >>> print(s.to_markdown())\n            |    | animal   |\n            |---:|:---------|\n            |  0 | elk      |\n            |  1 | pig      |\n            |  2 | dog      |\n            |  3 | quetzal  |\n            \"\"\"\n        ),\n    )\n    def to_markdown(\n        self,\n        buf: IO[str] | None = None,\n        mode: str = \"wt\",\n        index: bool = True,\n        storage_options: StorageOptions = None,\n        **kwargs,\n    ) -> str | None:\n        \"\"\"\n        Print {klass} in Markdown-friendly format.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        buf : str, Path or StringIO-like, optional, default None\n            Buffer to write to. If None, the output is returned as a string.\n        mode : str, optional\n            Mode in which file is opened, \"wt\" by default.\n        index : bool, optional, default True\n            Add index (row) labels.\n\n            .. versionadded:: 1.1.0\n        {storage_options}\n\n            .. versionadded:: 1.2.0\n\n        **kwargs\n            These parameters will be passed to `tabulate \\\n                <https://pypi.org/project/tabulate>`_.\n\n        Returns\n        -------\n        str\n            {klass} in Markdown-friendly format.\n\n        Notes\n        -----\n        Requires the `tabulate <https://pypi.org/project/tabulate>`_ package.\n\n        Examples\n        --------\n        >>> s = pd.Series([\"elk\", \"pig\", \"dog\", \"quetzal\"], name=\"animal\")\n        >>> print(s.to_markdown())\n        |    | animal   |\n        |---:|:---------|\n        |  0 | elk      |\n        |  1 | pig      |\n        |  2 | dog      |\n        |  3 | quetzal  |\n\n        Output markdown with a tabulate option.\n\n        >>> print(s.to_markdown(tablefmt=\"grid\"))\n        +----+----------+\n        |    | animal   |\n        +====+==========+\n        |  0 | elk      |\n        +----+----------+\n        |  1 | pig      |\n        +----+----------+\n        |  2 | dog      |\n        +----+----------+\n        |  3 | quetzal  |\n        +----+----------+\n        \"\"\"\n        return self.to_frame().to_markdown(\n            buf, mode, index, storage_options=storage_options, **kwargs\n        )\n\n    # ----------------------------------------------------------------------\n\n    def items(self) -> Iterable[tuple[Hashable, Any]]:\n        \"\"\"\n        Lazily iterate over (index, value) tuples.\n\n        This method returns an iterable tuple (index, value). This is\n        convenient if you want to create a lazy iterator.\n\n        Returns\n        -------\n        iterable\n            Iterable of tuples containing the (index, value) pairs from a\n            Series.\n\n        See Also\n        --------\n        DataFrame.items : Iterate over (column name, Series) pairs.\n        DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs.\n\n        Examples\n        --------\n        >>> s = pd.Series(['A', 'B', 'C'])\n        >>> for index, value in s.items():\n        ...     print(f\"Index : {index}, Value : {value}\")\n        Index : 0, Value : A\n        Index : 1, Value : B\n        Index : 2, Value : C\n        \"\"\"\n        return zip(iter(self.index), iter(self))\n\n    @Appender(items.__doc__)\n    def iteritems(self) -> Iterable[tuple[Hashable, Any]]:\n        return self.items()\n\n    # ----------------------------------------------------------------------\n    # Misc public methods\n\n    def keys(self) -> Index:\n        \"\"\"\n        Return alias for index.\n\n        Returns\n        -------\n        Index\n            Index of the Series.\n        \"\"\"\n        return self.index\n\n    def to_dict(self, into=dict):\n        \"\"\"\n        Convert Series to {label -> value} dict or dict-like object.\n\n        Parameters\n        ----------\n        into : class, default dict\n            The collections.abc.Mapping subclass to use as the return\n            object. Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Key-value representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.to_dict()\n        {0: 1, 1: 2, 2: 3, 3: 4}\n        >>> from collections import OrderedDict, defaultdict\n        >>> s.to_dict(OrderedDict)\n        OrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])\n        >>> dd = defaultdict(list)\n        >>> s.to_dict(dd)\n        defaultdict(<class 'list'>, {0: 1, 1: 2, 2: 3, 3: 4})\n        \"\"\"\n        # GH16122\n        into_c = com.standardize_mapping(into)\n        return into_c((k, maybe_box_native(v)) for k, v in self.items())\n\n    def to_frame(self, name=None) -> DataFrame:\n        \"\"\"\n        Convert Series to DataFrame.\n\n        Parameters\n        ----------\n        name : object, default None\n            The passed name should substitute for the series name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([\"a\", \"b\", \"c\"],\n        ...               name=\"vals\")\n        >>> s.to_frame()\n          vals\n        0    a\n        1    b\n        2    c\n        \"\"\"\n        if name is None:\n            df = self._constructor_expanddim(self)\n        else:\n            df = self._constructor_expanddim({name: self})\n\n        return df\n\n    def _set_name(self, name, inplace=False) -> Series:\n        \"\"\"\n        Set the Series name.\n\n        Parameters\n        ----------\n        name : str\n        inplace : bool\n            Whether to modify `self` directly or return a copy.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        ser = self if inplace else self.copy()\n        ser.name = name\n        return ser\n\n    @Appender(\n        \"\"\"\nExamples\n--------\n>>> ser = pd.Series([390., 350., 30., 20.],\n...                 index=['Falcon', 'Falcon', 'Parrot', 'Parrot'], name=\"Max Speed\")\n>>> ser\nFalcon    390.0\nFalcon    350.0\nParrot     30.0\nParrot     20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby([\"a\", \"b\", \"a\", \"b\"]).mean()\na    210.0\nb    185.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(ser > 100).mean()\nMax Speed\nFalse     25.0\nTrue     370.0\nName: Max Speed, dtype: float64\n\n**Grouping by Indexes**\n\nWe can groupby different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> ser = pd.Series([390., 350., 30., 20.], index=index, name=\"Max Speed\")\n>>> ser\nAnimal  Type\nFalcon  Captive    390.0\n        Wild       350.0\nParrot  Captive     30.0\n        Wild        20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nAnimal\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=\"Type\").mean()\nType\nCaptive    210.0\nWild       185.0\nName: Max Speed, dtype: float64\n\nWe can also choose to include `NA` in group keys or not by defining\n`dropna` parameter, the default setting is `True`:\n\n>>> ser = pd.Series([1, 2, 3, 3], index=[\"a\", 'a', 'b', np.nan])\n>>> ser.groupby(level=0).sum()\na    3\nb    3\ndtype: int64\n\n>>> ser.groupby(level=0, dropna=False).sum()\na    3\nb    3\nNaN  3\ndtype: int64\n\n>>> arrays = ['Falcon', 'Falcon', 'Parrot', 'Parrot']\n>>> ser = pd.Series([390., 350., 30., 20.], index=arrays, name=\"Max Speed\")\n>>> ser.groupby([\"a\", \"b\", \"a\", np.nan]).mean()\na    210.0\nb    350.0\nName: Max Speed, dtype: float64\n\n>>> ser.groupby([\"a\", \"b\", \"a\", np.nan], dropna=False).mean()\na    210.0\nb    350.0\nNaN   20.0\nName: Max Speed, dtype: float64\n\"\"\"\n    )\n    @Appender(generic._shared_docs[\"groupby\"] % _shared_doc_kwargs)\n    def groupby(\n        self,\n        by=None,\n        axis=0,\n        level=None,\n        as_index: bool = True,\n        sort: bool = True,\n        group_keys: bool = True,\n        squeeze: bool | lib.NoDefault = no_default,\n        observed: bool = False,\n        dropna: bool = True,\n    ) -> SeriesGroupBy:\n        from pandas.core.groupby.generic import SeriesGroupBy\n\n        if squeeze is not no_default:\n            warnings.warn(\n                (\n                    \"The `squeeze` parameter is deprecated and \"\n                    \"will be removed in a future version.\"\n                ),\n                FutureWarning,\n                stacklevel=2,\n            )\n        else:\n            squeeze = False\n\n        if level is None and by is None:\n            raise TypeError(\"You have to supply one of 'by' and 'level'\")\n        axis = self._get_axis_number(axis)\n\n        # error: Argument \"squeeze\" to \"SeriesGroupBy\" has incompatible type\n        # \"Union[bool, NoDefault]\"; expected \"bool\"\n        return SeriesGroupBy(\n            obj=self,\n            keys=by,\n            axis=axis,\n            level=level,\n            as_index=as_index,\n            sort=sort,\n            group_keys=group_keys,\n            squeeze=squeeze,  # type: ignore[arg-type]\n            observed=observed,\n            dropna=dropna,\n        )\n\n    # ----------------------------------------------------------------------\n    # Statistics, overridden ndarray methods\n\n    # TODO: integrate bottleneck\n\n    def count(self, level=None):\n        \"\"\"\n        Return number of non-NA/null observations in the Series.\n\n        Parameters\n        ----------\n        level : int or level name, default None\n            If the axis is a MultiIndex (hierarchical), count along a\n            particular level, collapsing into a smaller Series.\n\n        Returns\n        -------\n        int or Series (if level specified)\n            Number of non-null values in the Series.\n\n        See Also\n        --------\n        DataFrame.count : Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.0, 1.0, np.nan])\n        >>> s.count()\n        2\n        \"\"\"\n        if level is None:\n            return notna(self._values).sum().astype(\"int64\")\n        else:\n            warnings.warn(\n                \"Using the level keyword in DataFrame and Series aggregations is \"\n                \"deprecated and will be removed in a future version. Use groupby \"\n                \"instead. ser.count(level=1) should use ser.groupby(level=1).count().\",\n                FutureWarning,\n                stacklevel=2,\n            )\n            if not isinstance(self.index, MultiIndex):\n                raise ValueError(\"Series.count level is only valid with a MultiIndex\")\n\n        index = self.index\n        assert isinstance(index, MultiIndex)  # for mypy\n\n        if isinstance(level, str):\n            level = index._get_level_number(level)\n\n        lev = index.levels[level]\n        level_codes = np.array(index.codes[level], subok=False, copy=True)\n\n        mask = level_codes == -1\n        if mask.any():\n            level_codes[mask] = cnt = len(lev)\n            lev = lev.insert(cnt, lev._na_value)\n\n        obs = level_codes[notna(self._values)]\n        out = np.bincount(obs, minlength=len(lev) or None)\n        return self._constructor(out, index=lev, dtype=\"int64\").__finalize__(\n            self, method=\"count\"\n        )\n\n    def mode(self, dropna=True) -> Series:\n        \"\"\"\n        Return the mode(s) of the Series.\n\n        The mode is the value that appears most often. There can be multiple modes.\n\n        Always returns Series even if only one value is returned.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        Series\n            Modes of the Series in sorted order.\n        \"\"\"\n        # TODO: Add option for bins like value_counts()\n        return algorithms.mode(self, dropna=dropna)\n\n    def unique(self) -> ArrayLike:\n        \"\"\"\n        Return unique values of Series object.\n\n        Uniques are returned in order of appearance. Hash table-based unique,\n        therefore does NOT sort.\n\n        Returns\n        -------\n        ndarray or ExtensionArray\n            The unique values returned as a NumPy array. See Notes.\n\n        See Also\n        --------\n        unique : Top-level unique method for any 1-d array-like object.\n        Index.unique : Return Index with unique values from an Index object.\n\n        Notes\n        -----\n        Returns the unique values as a NumPy array. In case of an\n        extension-array backed Series, a new\n        :class:`~api.extensions.ExtensionArray` of that type with just\n        the unique values is returned. This includes\n\n            * Categorical\n            * Period\n            * Datetime with Timezone\n            * Interval\n            * Sparse\n            * IntegerNA\n\n        See Examples section.\n\n        Examples\n        --------\n        >>> pd.Series([2, 1, 3, 3], name='A').unique()\n        array([2, 1, 3])\n\n        >>> pd.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()\n        array(['2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\n\n        >>> pd.Series([pd.Timestamp('2016-01-01', tz='US/Eastern')\n        ...            for _ in range(3)]).unique()\n        <DatetimeArray>\n        ['2016-01-01 00:00:00-05:00']\n        Length: 1, dtype: datetime64[ns, US/Eastern]\n\n        An Categorical will return categories in the order of\n        appearance and with the same dtype.\n\n        >>> pd.Series(pd.Categorical(list('baabc'))).unique()\n        ['b', 'a', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> pd.Series(pd.Categorical(list('baabc'), categories=list('abc'),\n        ...                          ordered=True)).unique()\n        ['b', 'a', 'c']\n        Categories (3, object): ['a' < 'b' < 'c']\n        \"\"\"\n        return super().unique()\n\n    @overload\n    def drop_duplicates(self, keep=..., inplace: Literal[False] = ...) -> Series:\n        ...\n\n    @overload\n    def drop_duplicates(self, keep, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def drop_duplicates(self, *, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def drop_duplicates(self, keep=..., inplace: bool = ...) -> Series | None:\n        ...\n\n    def drop_duplicates(self, keep=\"first\", inplace=False) -> Series | None:\n        \"\"\"\n        Return Series with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        inplace : bool, default ``False``\n            If ``True``, performs operation inplace and returns None.\n\n        Returns\n        -------\n        Series or None\n            Series with duplicates dropped or None if ``inplace=True``.\n\n        See Also\n        --------\n        Index.drop_duplicates : Equivalent method on Index.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Series.duplicated : Related method on Series, indicating duplicate\n            Series values.\n\n        Examples\n        --------\n        Generate a Series with duplicated entries.\n\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'],\n        ...               name='animal')\n        >>> s\n        0      lama\n        1       cow\n        2      lama\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        With the 'keep' parameter, the selection behaviour of duplicated values\n        can be changed. The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> s.drop_duplicates()\n        0      lama\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n\n        The value 'last' for parameter 'keep' keeps the last occurrence for\n        each set of duplicated entries.\n\n        >>> s.drop_duplicates(keep='last')\n        1       cow\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        The value ``False`` for parameter 'keep' discards all sets of\n        duplicated entries. Setting the value of 'inplace' to ``True`` performs\n        the operation inplace and returns ``None``.\n\n        >>> s.drop_duplicates(keep=False, inplace=True)\n        >>> s\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        result = super().drop_duplicates(keep=keep)\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    def duplicated(self, keep=\"first\") -> Series:\n        \"\"\"\n        Indicate duplicate Series values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        Series. Either all duplicates, all except the first or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series[bool]\n            Series indicating whether each value has occurred in the\n            preceding values.\n\n        See Also\n        --------\n        Index.duplicated : Equivalent method on pandas.Index.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Series.drop_duplicates : Remove duplicate values from Series.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set on False and all others on True:\n\n        >>> animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> animals.duplicated()\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        which is equivalent to\n\n        >>> animals.duplicated(keep='first')\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> animals.duplicated(keep='last')\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> animals.duplicated(keep=False)\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n        \"\"\"\n        res = self._duplicated(keep=keep)\n        result = self._constructor(res, index=self.index)\n        return result.__finalize__(self, method=\"duplicated\")\n\n    def idxmin(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the minimum value.\n\n        If multiple values equal the minimum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmin. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the minimum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmin : Return indices of the minimum values\n            along the given axis.\n        DataFrame.idxmin : Return index of first occurrence of minimum\n            over requested axis.\n        Series.idxmax : Return index *label* of the first occurrence\n            of maximum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmin``. This method\n        returns the label of the minimum, while ``ndarray.argmin`` returns\n        the position. To get the position, use ``series.values.argmin()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 1],\n        ...               index=['A', 'B', 'C', 'D'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    1.0\n        dtype: float64\n\n        >>> s.idxmin()\n        'A'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmin(skipna=False)\n        nan\n        \"\"\"\n        i = self.argmin(axis, skipna, *args, **kwargs)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def idxmax(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the maximum value.\n\n        If multiple values equal the maximum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmax. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the maximum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmax : Return indices of the maximum values\n            along the given axis.\n        DataFrame.idxmax : Return index of first occurrence of maximum\n            over requested axis.\n        Series.idxmin : Return index *label* of the first occurrence\n            of minimum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmax``. This method\n        returns the label of the maximum, while ``ndarray.argmax`` returns\n        the position. To get the position, use ``series.values.argmax()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 3, 4],\n        ...               index=['A', 'B', 'C', 'D', 'E'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    3.0\n        E    4.0\n        dtype: float64\n\n        >>> s.idxmax()\n        'C'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmax(skipna=False)\n        nan\n        \"\"\"\n        i = self.argmax(axis, skipna, *args, **kwargs)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def round(self, decimals=0, *args, **kwargs) -> Series:\n        \"\"\"\n        Round each value in a Series to the given number of decimals.\n\n        Parameters\n        ----------\n        decimals : int, default 0\n            Number of decimal places to round to. If decimals is negative,\n            it specifies the number of positions to the left of the decimal point.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Series\n            Rounded values of the Series.\n\n        See Also\n        --------\n        numpy.around : Round values of an np.array.\n        DataFrame.round : Round values of a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.1, 1.3, 2.7])\n        >>> s.round()\n        0    0.0\n        1    1.0\n        2    3.0\n        dtype: float64\n        \"\"\"\n        nv.validate_round(args, kwargs)\n        result = self._values.round(decimals)\n        result = self._constructor(result, index=self.index).__finalize__(\n            self, method=\"round\"\n        )\n\n        return result\n\n    def quantile(self, q=0.5, interpolation=\"linear\"):\n        \"\"\"\n        Return value at the given quantile.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            The quantile(s) to compute, which can lie in range: 0 <= q <= 1.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            This optional parameter specifies the interpolation method to use,\n            when the desired quantile lies between two data points `i` and `j`:\n\n                * linear: `i + (j - i) * fraction`, where `fraction` is the\n                  fractional part of the index surrounded by `i` and `j`.\n                * lower: `i`.\n                * higher: `j`.\n                * nearest: `i` or `j` whichever is nearest.\n                * midpoint: (`i` + `j`) / 2.\n\n        Returns\n        -------\n        float or Series\n            If ``q`` is an array, a Series will be returned where the\n            index is ``q`` and the values are the quantiles, otherwise\n            a float will be returned.\n\n        See Also\n        --------\n        core.window.Rolling.quantile : Calculate the rolling quantile.\n        numpy.percentile : Returns the q-th percentile(s) of the array elements.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.quantile(.5)\n        2.5\n        >>> s.quantile([.25, .5, .75])\n        0.25    1.75\n        0.50    2.50\n        0.75    3.25\n        dtype: float64\n        \"\"\"\n        validate_percentile(q)\n\n        # We dispatch to DataFrame so that core.internals only has to worry\n        #  about 2D cases.\n        df = self.to_frame()\n\n        result = df.quantile(q=q, interpolation=interpolation, numeric_only=False)\n        if result.ndim == 2:\n            result = result.iloc[:, 0]\n\n        if is_list_like(q):\n            result.name = self.name\n            return self._constructor(result, index=Float64Index(q), name=self.name)\n        else:\n            # scalar\n            return result.iloc[0]\n\n    def corr(self, other, method=\"pearson\", min_periods=None) -> float:\n        \"\"\"\n        Compute correlation with `other` Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the correlation.\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method used to compute correlation:\n\n            - pearson : Standard correlation coefficient\n            - kendall : Kendall Tau correlation coefficient\n            - spearman : Spearman rank correlation\n            - callable: Callable with input two 1d ndarrays and returning a float.\n\n            .. versionadded:: 0.24.0\n                Note that the returned matrix from corr will have 1 along the\n                diagonals and will be symmetric regardless of the callable's\n                behavior.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n\n        Returns\n        -------\n        float\n            Correlation with other.\n\n        See Also\n        --------\n        DataFrame.corr : Compute pairwise correlation between columns.\n        DataFrame.corrwith : Compute pairwise correlation with another\n            DataFrame or Series.\n\n        Examples\n        --------\n        >>> def histogram_intersection(a, b):\n        ...     v = np.minimum(a, b).sum().round(decimals=1)\n        ...     return v\n        >>> s1 = pd.Series([.2, .0, .6, .2])\n        >>> s2 = pd.Series([.3, .6, .0, .1])\n        >>> s1.corr(s2, method=histogram_intersection)\n        0.3\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n\n        if method in [\"pearson\", \"spearman\", \"kendall\"] or callable(method):\n            return nanops.nancorr(\n                this.values, other.values, method=method, min_periods=min_periods\n            )\n\n        raise ValueError(\n            \"method must be either 'pearson', \"\n            \"'spearman', 'kendall', or a callable, \"\n            f\"'{method}' was supplied\"\n        )\n\n    def cov(\n        self,\n        other: Series,\n        min_periods: int | None = None,\n        ddof: int | None = 1,\n    ) -> float:\n        \"\"\"\n        Compute covariance with Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the covariance.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n        ddof : int, default 1\n            Delta degrees of freedom.  The divisor used in calculations\n            is ``N - ddof``, where ``N`` represents the number of elements.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        float\n            Covariance between Series and other normalized by N-1\n            (unbiased estimator).\n\n        See Also\n        --------\n        DataFrame.cov : Compute pairwise covariance of columns.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([0.90010907, 0.13484424, 0.62036035])\n        >>> s2 = pd.Series([0.12528585, 0.26962463, 0.51111198])\n        >>> s1.cov(s2)\n        -0.01685762652715874\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancov(\n            this.values, other.values, min_periods=min_periods, ddof=ddof\n        )\n\n    @doc(\n        klass=\"Series\",\n        extra_params=\"\",\n        other_klass=\"DataFrame\",\n        examples=dedent(\n            \"\"\"\n        Difference with previous row\n\n        >>> s = pd.Series([1, 1, 2, 3, 5, 8])\n        >>> s.diff()\n        0    NaN\n        1    0.0\n        2    1.0\n        3    1.0\n        4    2.0\n        5    3.0\n        dtype: float64\n\n        Difference with 3rd previous row\n\n        >>> s.diff(periods=3)\n        0    NaN\n        1    NaN\n        2    NaN\n        3    2.0\n        4    4.0\n        5    6.0\n        dtype: float64\n\n        Difference with following row\n\n        >>> s.diff(periods=-1)\n        0    0.0\n        1   -1.0\n        2   -1.0\n        3   -2.0\n        4   -3.0\n        5    NaN\n        dtype: float64\n\n        Overflow in input dtype\n\n        >>> s = pd.Series([1, 0], dtype=np.uint8)\n        >>> s.diff()\n        0      NaN\n        1    255.0\n        dtype: float64\"\"\"\n        ),\n    )\n    def diff(self, periods: int = 1) -> Series:\n        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of a {klass} element compared with another\n        element in the {klass} (default is element in previous row).\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative\n            values.\n        {extra_params}\n        Returns\n        -------\n        {klass}\n            First differences of the Series.\n\n        See Also\n        --------\n        {klass}.pct_change: Percent change over given number of periods.\n        {klass}.shift: Shift index by desired number of periods with an\n            optional time freq.\n        {other_klass}.diff: First discrete difference of object.\n\n        Notes\n        -----\n        For boolean dtypes, this uses :meth:`operator.xor` rather than\n        :meth:`operator.sub`.\n        The result is calculated according to current dtype in {klass},\n        however dtype of the result is always float64.\n\n        Examples\n        --------\n        {examples}\n        \"\"\"\n        result = algorithms.diff(self._values, periods)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"diff\"\n        )\n\n    def autocorr(self, lag=1) -> float:\n        \"\"\"\n        Compute the lag-N autocorrelation.\n\n        This method computes the Pearson correlation between\n        the Series and its shifted self.\n\n        Parameters\n        ----------\n        lag : int, default 1\n            Number of lags to apply before performing autocorrelation.\n\n        Returns\n        -------\n        float\n            The Pearson correlation between self and self.shift(lag).\n\n        See Also\n        --------\n        Series.corr : Compute the correlation between two Series.\n        Series.shift : Shift index by desired number of periods.\n        DataFrame.corr : Compute pairwise correlation of columns.\n        DataFrame.corrwith : Compute pairwise correlation between rows or\n            columns of two DataFrame objects.\n\n        Notes\n        -----\n        If the Pearson correlation is not well defined return 'NaN'.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.25, 0.5, 0.2, -0.05])\n        >>> s.autocorr()  # doctest: +ELLIPSIS\n        0.10355...\n        >>> s.autocorr(lag=2)  # doctest: +ELLIPSIS\n        -0.99999...\n\n        If the Pearson correlation is not well defined, then 'NaN' is returned.\n\n        >>> s = pd.Series([1, 0, 0, 0])\n        >>> s.autocorr()\n        nan\n        \"\"\"\n        return self.corr(self.shift(lag))\n\n    def dot(self, other):\n        \"\"\"\n        Compute the dot product between the Series and the columns of other.\n\n        This method computes the dot product between the Series and another\n        one, or the Series and each columns of a DataFrame, or the Series and\n        each columns of an array.\n\n        It can also be called using `self @ other` in Python >= 3.5.\n\n        Parameters\n        ----------\n        other : Series, DataFrame or array-like\n            The other object to compute the dot product with its columns.\n\n        Returns\n        -------\n        scalar, Series or numpy.ndarray\n            Return the dot product of the Series and other if other is a\n            Series, the Series of the dot product of Series and each rows of\n            other if other is a DataFrame or a numpy.ndarray between the Series\n            and each columns of the numpy array.\n\n        See Also\n        --------\n        DataFrame.dot: Compute the matrix product with the DataFrame.\n        Series.mul: Multiplication of series and other, element-wise.\n\n        Notes\n        -----\n        The Series and other has to share the same index if other is a Series\n        or a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0, 1, 2, 3])\n        >>> other = pd.Series([-1, 2, -3, 4])\n        >>> s.dot(other)\n        8\n        >>> s @ other\n        8\n        >>> df = pd.DataFrame([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(df)\n        0    24\n        1    14\n        dtype: int64\n        >>> arr = np.array([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(arr)\n        array([24, 14])\n        \"\"\"\n        if isinstance(other, (Series, ABCDataFrame)):\n            common = self.index.union(other.index)\n            if len(common) > len(self.index) or len(common) > len(other.index):\n                raise ValueError(\"matrices are not aligned\")\n\n            left = self.reindex(index=common, copy=False)\n            right = other.reindex(index=common, copy=False)\n            lvals = left.values\n            rvals = right.values\n        else:\n            lvals = self.values\n            rvals = np.asarray(other)\n            if lvals.shape[0] != rvals.shape[0]:\n                raise Exception(\n                    f\"Dot product shape mismatch, {lvals.shape} vs {rvals.shape}\"\n                )\n\n        if isinstance(other, ABCDataFrame):\n            return self._constructor(\n                np.dot(lvals, rvals), index=other.columns\n            ).__finalize__(self, method=\"dot\")\n        elif isinstance(other, Series):\n            return np.dot(lvals, rvals)\n        elif isinstance(rvals, np.ndarray):\n            return np.dot(lvals, rvals)\n        else:  # pragma: no cover\n            raise TypeError(f\"unsupported type: {type(other)}\")\n\n    def __matmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(other)\n\n    def __rmatmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(np.transpose(other))\n\n    @doc(base.IndexOpsMixin.searchsorted, klass=\"Series\")\n    def searchsorted(self, value, side=\"left\", sorter=None) -> np.ndarray:\n        return algorithms.searchsorted(self._values, value, side=side, sorter=sorter)\n\n    # -------------------------------------------------------------------\n    # Combination\n\n    def append(\n        self, to_append, ignore_index: bool = False, verify_integrity: bool = False\n    ):\n        \"\"\"\n        Concatenate two or more Series.\n\n        Parameters\n        ----------\n        to_append : Series or list/tuple of Series\n            Series to append with self.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n        verify_integrity : bool, default False\n            If True, raise Exception on creating index with duplicates.\n\n        Returns\n        -------\n        Series\n            Concatenated Series.\n\n        See Also\n        --------\n        concat : General function to concatenate DataFrame or Series objects.\n\n        Notes\n        -----\n        Iteratively appending to a Series can be more computationally intensive\n        than a single concatenate. A better solution is to append values to a\n        list and then concatenate the list with the original Series all at\n        once.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, 2, 3])\n        >>> s2 = pd.Series([4, 5, 6])\n        >>> s3 = pd.Series([4, 5, 6], index=[3, 4, 5])\n        >>> s1.append(s2)\n        0    1\n        1    2\n        2    3\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s1.append(s3)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `ignore_index` set to True:\n\n        >>> s1.append(s2, ignore_index=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `verify_integrity` set to True:\n\n        >>> s1.append(s2, verify_integrity=True)\n        Traceback (most recent call last):\n        ...\n        ValueError: Indexes have overlapping values: [0, 1, 2]\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        if isinstance(to_append, (list, tuple)):\n            to_concat = [self]\n            to_concat.extend(to_append)\n        else:\n            to_concat = [self, to_append]\n        if any(isinstance(x, (ABCDataFrame,)) for x in to_concat[1:]):\n            msg = \"to_append should be a Series or list/tuple of Series, got DataFrame\"\n            raise TypeError(msg)\n        return concat(\n            to_concat, ignore_index=ignore_index, verify_integrity=verify_integrity\n        )\n\n    def _binop(self, other: Series, func, level=None, fill_value=None):\n        \"\"\"\n        Perform generic binary operation with optional fill value.\n\n        Parameters\n        ----------\n        other : Series\n        func : binary operator\n        fill_value : float or object\n            Value to substitute for NA/null values. If both Series are NA in a\n            location, the result will be NA regardless of the passed fill value.\n        level : int or level name, default None\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level.\n\n        Returns\n        -------\n        Series\n        \"\"\"\n        if not isinstance(other, Series):\n            raise AssertionError(\"Other operand must be Series\")\n\n        this = self\n\n        if not self.index.equals(other.index):\n            this, other = self.align(other, level=level, join=\"outer\", copy=False)\n\n        this_vals, other_vals = ops.fill_binop(this._values, other._values, fill_value)\n\n        with np.errstate(all=\"ignore\"):\n            result = func(this_vals, other_vals)\n\n        name = ops.get_op_result_name(self, other)\n        return this._construct_result(result, name)\n\n    def _construct_result(\n        self, result: ArrayLike | tuple[ArrayLike, ArrayLike], name: Hashable\n    ) -> Series | tuple[Series, Series]:\n        \"\"\"\n        Construct an appropriately-labelled Series from the result of an op.\n\n        Parameters\n        ----------\n        result : ndarray or ExtensionArray\n        name : Label\n\n        Returns\n        -------\n        Series\n            In the case of __divmod__ or __rdivmod__, a 2-tuple of Series.\n        \"\"\"\n        if isinstance(result, tuple):\n            # produced by divmod or rdivmod\n\n            res1 = self._construct_result(result[0], name=name)\n            res2 = self._construct_result(result[1], name=name)\n\n            # GH#33427 assertions to keep mypy happy\n            assert isinstance(res1, Series)\n            assert isinstance(res2, Series)\n            return (res1, res2)\n\n        # We do not pass dtype to ensure that the Series constructor\n        #  does inference in the case where `result` has object-dtype.\n        out = self._constructor(result, index=self.index)\n        out = out.__finalize__(self)\n\n        # Set the result's name after __finalize__ is called because __finalize__\n        #  would set it back to self.name\n        out.name = name\n        return out\n\n    @doc(\n        generic._shared_docs[\"compare\"],\n        \"\"\"\nReturns\n-------\nSeries or DataFrame\n    If axis is 0 or 'index' the result will be a Series.\n    The resulting index will be a MultiIndex with 'self' and 'other'\n    stacked alternately at the inner level.\n\n    If axis is 1 or 'columns' the result will be a DataFrame.\n    It will have two columns namely 'self' and 'other'.\n\nSee Also\n--------\nDataFrame.compare : Compare with another DataFrame and show differences.\n\nNotes\n-----\nMatching NaNs will not appear as a difference.\n\nExamples\n--------\n>>> s1 = pd.Series([\"a\", \"b\", \"c\", \"d\", \"e\"])\n>>> s2 = pd.Series([\"a\", \"a\", \"c\", \"b\", \"e\"])\n\nAlign the differences on columns\n\n>>> s1.compare(s2)\n  self other\n1    b     a\n3    d     b\n\nStack the differences on indices\n\n>>> s1.compare(s2, align_axis=0)\n1  self     b\n   other    a\n3  self     d\n   other    b\ndtype: object\n\nKeep all original rows\n\n>>> s1.compare(s2, keep_shape=True)\n  self other\n0  NaN   NaN\n1    b     a\n2  NaN   NaN\n3    d     b\n4  NaN   NaN\n\nKeep all original rows and also all original values\n\n>>> s1.compare(s2, keep_shape=True, keep_equal=True)\n  self other\n0    a     a\n1    b     a\n2    c     c\n3    d     b\n4    e     e\n\"\"\",\n        klass=_shared_doc_kwargs[\"klass\"],\n    )\n    def compare(\n        self,\n        other: Series,\n        align_axis: Axis = 1,\n        keep_shape: bool = False,\n        keep_equal: bool = False,\n    ) -> FrameOrSeriesUnion:\n        return super().compare(\n            other=other,\n            align_axis=align_axis,\n            keep_shape=keep_shape,\n            keep_equal=keep_equal,\n        )\n\n    def combine(self, other, func, fill_value=None) -> Series:\n        \"\"\"\n        Combine the Series with a Series or scalar according to `func`.\n\n        Combine the Series and `other` using `func` to perform elementwise\n        selection for combined Series.\n        `fill_value` is assumed when value is missing at some index\n        from one of the two objects being combined.\n\n        Parameters\n        ----------\n        other : Series or scalar\n            The value(s) to be combined with the `Series`.\n        func : function\n            Function that takes two scalars as inputs and returns an element.\n        fill_value : scalar, optional\n            The value to assume when an index is missing from\n            one Series or the other. The default specifies to use the\n            appropriate NaN value for the underlying dtype of the Series.\n\n        Returns\n        -------\n        Series\n            The result of combining the Series with the other object.\n\n        See Also\n        --------\n        Series.combine_first : Combine Series values, choosing the calling\n            Series' values first.\n\n        Examples\n        --------\n        Consider 2 Datasets ``s1`` and ``s2`` containing\n        highest clocked speeds of different birds.\n\n        >>> s1 = pd.Series({'falcon': 330.0, 'eagle': 160.0})\n        >>> s1\n        falcon    330.0\n        eagle     160.0\n        dtype: float64\n        >>> s2 = pd.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})\n        >>> s2\n        falcon    345.0\n        eagle     200.0\n        duck       30.0\n        dtype: float64\n\n        Now, to combine the two datasets and view the highest speeds\n        of the birds across the two datasets\n\n        >>> s1.combine(s2, max)\n        duck        NaN\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n\n        In the previous example, the resulting value for duck is missing,\n        because the maximum of a NaN and a float is a NaN.\n        So, in the example, we set ``fill_value=0``,\n        so the maximum value returned will be the value from some dataset.\n\n        >>> s1.combine(s2, max, fill_value=0)\n        duck       30.0\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n        \"\"\"\n        if fill_value is None:\n            fill_value = na_value_for_dtype(self.dtype, compat=False)\n\n        if isinstance(other, Series):\n            # If other is a Series, result is based on union of Series,\n            # so do this element by element\n            new_index = self.index.union(other.index)\n            new_name = ops.get_op_result_name(self, other)\n            new_values = np.empty(len(new_index), dtype=object)\n            for i, idx in enumerate(new_index):\n                lv = self.get(idx, fill_value)\n                rv = other.get(idx, fill_value)\n                with np.errstate(all=\"ignore\"):\n                    new_values[i] = func(lv, rv)\n        else:\n            # Assume that other is a scalar, so apply the function for\n            # each element in the Series\n            new_index = self.index\n            new_values = np.empty(len(new_index), dtype=object)\n            with np.errstate(all=\"ignore\"):\n                new_values[:] = [func(lv, other) for lv in self._values]\n            new_name = self.name\n\n        # try_float=False is to match agg_series\n        npvalues = lib.maybe_convert_objects(new_values, try_float=False)\n        res_values = maybe_cast_pointwise_result(npvalues, self.dtype, same_dtype=False)\n        return self._constructor(res_values, index=new_index, name=new_name)\n\n    def combine_first(self, other) -> Series:\n        \"\"\"\n        Update null elements with value in the same location in 'other'.\n\n        Combine two Series objects by filling null values in one Series with\n        non-null values from the other Series. Result index will be the union\n        of the two indexes.\n\n        Parameters\n        ----------\n        other : Series\n            The value(s) to be used for filling null values.\n\n        Returns\n        -------\n        Series\n            The result of combining the provided Series with the other object.\n\n        See Also\n        --------\n        Series.combine : Perform element-wise operation on two Series\n            using a given function.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, np.nan])\n        >>> s2 = pd.Series([3, 4, 5])\n        >>> s1.combine_first(s2)\n        0    1.0\n        1    4.0\n        2    5.0\n        dtype: float64\n\n        Null values still persist if the location of that null value\n        does not exist in `other`\n\n        >>> s1 = pd.Series({'falcon': np.nan, 'eagle': 160.0})\n        >>> s2 = pd.Series({'eagle': 200.0, 'duck': 30.0})\n        >>> s1.combine_first(s2)\n        duck       30.0\n        eagle     160.0\n        falcon      NaN\n        dtype: float64\n        \"\"\"\n        new_index = self.index.union(other.index)\n        this = self.reindex(new_index, copy=False)\n        other = other.reindex(new_index, copy=False)\n        if this.dtype.kind == \"M\" and other.dtype.kind != \"M\":\n            other = to_datetime(other)\n\n        return this.where(notna(this), other)\n\n    def update(self, other) -> None:\n        \"\"\"\n        Modify Series in place using values from passed Series.\n\n        Uses non-NA values from passed Series to make updates. Aligns\n        on index.\n\n        Parameters\n        ----------\n        other : Series, or object coercible into Series\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s.update(pd.Series(['d', 'e'], index=[0, 2]))\n        >>> s\n        0    d\n        1    b\n        2    e\n        dtype: object\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6, 7, 8]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        If ``other`` contains NaNs the corresponding values are not updated\n        in the original Series.\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, np.nan, 6]))\n        >>> s\n        0    4\n        1    2\n        2    6\n        dtype: int64\n\n        ``other`` can also be a non-Series object type\n        that is coercible into a Series\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update([4, np.nan, 6])\n        >>> s\n        0    4\n        1    2\n        2    6\n        dtype: int64\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update({1: 9})\n        >>> s\n        0    1\n        1    9\n        2    3\n        dtype: int64\n        \"\"\"\n\n        if not isinstance(other, Series):\n            other = Series(other)\n\n        other = other.reindex_like(self)\n        mask = notna(other)\n\n        self._mgr = self._mgr.putmask(mask=mask, new=other)\n        self._maybe_update_cacher()\n\n    # ----------------------------------------------------------------------\n    # Reindexing, sorting\n\n    def sort_values(\n        self,\n        axis=0,\n        ascending: bool | int | Sequence[bool | int] = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        ignore_index: bool = False,\n        key: ValueKeyFunc = None,\n    ):\n        \"\"\"\n        Sort by the values.\n\n        Sort a Series in ascending or descending order by some\n        criterion.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            Axis to direct sorting. The value 'index' is accepted for\n            compatibility with DataFrame.sort_values.\n        ascending : bool or list of bools, default True\n            If True, sort values in ascending order, otherwise descending.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. 'mergesort' and 'stable' are the only stable  algorithms.\n        na_position : {'first' or 'last'}, default 'last'\n            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n            the end.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the series values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect a\n            ``Series`` and return an array-like.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series or None\n            Series ordered by values or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.sort_index : Sort by the Series indices.\n        DataFrame.sort_values : Sort DataFrame by the values along either axis.\n        DataFrame.sort_index : Sort DataFrame by indices.\n\n        Examples\n        --------\n        >>> s = pd.Series([np.nan, 1, 3, 10, 5])\n        >>> s\n        0     NaN\n        1     1.0\n        2     3.0\n        3     10.0\n        4     5.0\n        dtype: float64\n\n        Sort values ascending order (default behaviour)\n\n        >>> s.sort_values(ascending=True)\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        0     NaN\n        dtype: float64\n\n        Sort values descending order\n\n        >>> s.sort_values(ascending=False)\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values inplace\n\n        >>> s.sort_values(ascending=False, inplace=True)\n        >>> s\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values putting NAs first\n\n        >>> s.sort_values(na_position='first')\n        0     NaN\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        dtype: float64\n\n        Sort a series of strings\n\n        >>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n        >>> s\n        0    z\n        1    b\n        2    d\n        3    a\n        4    c\n        dtype: object\n\n        >>> s.sort_values()\n        3    a\n        1    b\n        4    c\n        2    d\n        0    z\n        dtype: object\n\n        Sort using a key function. Your `key` function will be\n        given the ``Series`` of values and should return an array-like.\n\n        >>> s = pd.Series(['a', 'B', 'c', 'D', 'e'])\n        >>> s.sort_values()\n        1    B\n        3    D\n        0    a\n        2    c\n        4    e\n        dtype: object\n        >>> s.sort_values(key=lambda x: x.str.lower())\n        0    a\n        1    B\n        2    c\n        3    D\n        4    e\n        dtype: object\n\n        NumPy ufuncs work well here. For example, we can\n        sort by the ``sin`` of the value\n\n        >>> s = pd.Series([-4, -2, 0, 2, 4])\n        >>> s.sort_values(key=np.sin)\n        1   -2\n        4    4\n        2    0\n        0   -4\n        3    2\n        dtype: int64\n\n        More complicated user-defined functions can be used,\n        as long as they expect a Series and return an array-like\n\n        >>> s.sort_values(key=lambda x: (np.tan(x.cumsum())))\n        0   -4\n        3    2\n        4    4\n        1   -2\n        2    0\n        dtype: int64\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n\n        # GH 5856/5853\n        if inplace and self._is_cached:\n            raise ValueError(\n                \"This Series is a view of some other array, to \"\n                \"sort in-place you must create a copy\"\n            )\n\n        if is_list_like(ascending):\n            ascending = cast(Sequence[Union[bool, int]], ascending)\n            if len(ascending) != 1:\n                raise ValueError(\n                    f\"Length of ascending ({len(ascending)}) must be 1 for Series\"\n                )\n            ascending = ascending[0]\n\n        if not is_bool(ascending):\n            raise ValueError(\"ascending must be boolean\")\n\n        if na_position not in [\"first\", \"last\"]:\n            raise ValueError(f\"invalid na_position: {na_position}\")\n\n        # GH 35922. Make sorting stable by leveraging nargsort\n        values_to_sort = ensure_key_mapped(self, key)._values if key else self._values\n        sorted_index = nargsort(values_to_sort, kind, bool(ascending), na_position)\n\n        result = self._constructor(\n            self._values[sorted_index], index=self.index[sorted_index]\n        )\n\n        if ignore_index:\n            result.index = ibase.default_index(len(sorted_index))\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_values\")\n\n    def sort_index(\n        self,\n        axis=0,\n        level=None,\n        ascending: bool | int | Sequence[bool | int] = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        sort_remaining: bool = True,\n        ignore_index: bool = False,\n        key: IndexKeyFunc = None,\n    ):\n        \"\"\"\n        Sort Series by index labels.\n\n        Returns a new Series sorted by label if `inplace` argument is\n        ``False``, otherwise updates the original series and returns None.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Axis to direct sorting. This can only be 0 for Series.\n        level : int, optional\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list-like of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level individually.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. 'mergesort' and 'stable' are the only stable algorithms. For\n            DataFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'last'}, default 'last'\n            If 'first' puts NaNs at the beginning, 'last' puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series or None\n            The original Series sorted by the labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.sort_index: Sort DataFrame by the index.\n        DataFrame.sort_values: Sort DataFrame by the value.\n        Series.sort_values : Sort Series by the value.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])\n        >>> s.sort_index()\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        Sort Descending\n\n        >>> s.sort_index(ascending=False)\n        4    d\n        3    a\n        2    b\n        1    c\n        dtype: object\n\n        Sort Inplace\n\n        >>> s.sort_index(inplace=True)\n        >>> s\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        By default NaNs are put at the end, but use `na_position` to place\n        them at the beginning\n\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])\n        >>> s.sort_index(na_position='first')\n        NaN     d\n         1.0    c\n         2.0    b\n         3.0    a\n        dtype: object\n\n        Specify index level to sort\n\n        >>> arrays = [np.array(['qux', 'qux', 'foo', 'foo',\n        ...                     'baz', 'baz', 'bar', 'bar']),\n        ...           np.array(['two', 'one', 'two', 'one',\n        ...                     'two', 'one', 'two', 'one'])]\n        >>> s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)\n        >>> s.sort_index(level=1)\n        bar  one    8\n        baz  one    6\n        foo  one    4\n        qux  one    2\n        bar  two    7\n        baz  two    5\n        foo  two    3\n        qux  two    1\n        dtype: int64\n\n        Does not sort by remaining levels when sorting by levels\n\n        >>> s.sort_index(level=1, sort_remaining=False)\n        qux  one    2\n        foo  one    4\n        baz  one    6\n        bar  one    8\n        qux  two    1\n        foo  two    3\n        baz  two    5\n        bar  two    7\n        dtype: int64\n\n        Apply a key function before sorting\n\n        >>> s = pd.Series([1, 2, 3, 4], index=['A', 'b', 'C', 'd'])\n        >>> s.sort_index(key=lambda x : x.str.lower())\n        A    1\n        b    2\n        C    3\n        d    4\n        dtype: int64\n        \"\"\"\n\n        return super().sort_index(\n            axis,\n            level,\n            ascending,\n            inplace,\n            kind,\n            na_position,\n            sort_remaining,\n            ignore_index,\n            key,\n        )\n\n    def argsort(self, axis=0, kind=\"quicksort\", order=None) -> Series:\n        \"\"\"\n        Return the integer indices that would sort the Series values.\n\n        Override ndarray.argsort. Argsorts the value, omitting NA/null values,\n        and places the result in the same locations as the non-NA values.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Has no effect but is accepted for compatibility with numpy.\n        kind : {'mergesort', 'quicksort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See :func:`numpy.sort` for more\n            information. 'mergesort' and 'stable' are the only stable algorithms.\n        order : None\n            Has no effect but is accepted for compatibility with numpy.\n\n        Returns\n        -------\n        Series[np.intp]\n            Positions of values within the sort order with -1 indicating\n            nan values.\n\n        See Also\n        --------\n        numpy.ndarray.argsort : Returns the indices that would sort this array.\n        \"\"\"\n        values = self._values\n        mask = isna(values)\n\n        if mask.any():\n            result = Series(-1, index=self.index, name=self.name, dtype=\"int64\")\n            notmask = ~mask\n            result[notmask] = np.argsort(values[notmask], kind=kind)\n            return self._constructor(result, index=self.index).__finalize__(\n                self, method=\"argsort\"\n            )\n        else:\n            return self._constructor(\n                np.argsort(values, kind=kind), index=self.index, dtype=\"int64\"\n            ).__finalize__(self, method=\"argsort\")\n\n    def nlargest(self, n=5, keep=\"first\") -> Series:\n        \"\"\"\n        Return the largest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many descending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n                of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n                order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n                size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` largest values in the Series, sorted in decreasing order.\n\n        See Also\n        --------\n        Series.nsmallest: Get the `n` smallest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values(ascending=False).head(n)`` for small `n`\n        relative to the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Malta\": 434000, \"Maldives\": 434000,\n        ...                         \"Brunei\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Montserrat      5200\n        dtype: int64\n\n        The `n` largest elements where ``n=5`` by default.\n\n        >>> s.nlargest()\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3``. Default `keep` value is 'first'\n        so Malta will be kept.\n\n        >>> s.nlargest(3)\n        France    65000000\n        Italy     59000000\n        Malta       434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` and keeping the last duplicates.\n        Brunei will be kept since it is the last with value 434000 based on\n        the index order.\n\n        >>> s.nlargest(3, keep='last')\n        France      65000000\n        Italy       59000000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has five elements due to the three duplicates.\n\n        >>> s.nlargest(3, keep='all')\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nlargest()\n\n    def nsmallest(self, n: int = 5, keep: str = \"first\") -> Series:\n        \"\"\"\n        Return the smallest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many ascending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n                of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n                order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n                size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` smallest values in the Series, sorted in increasing order.\n\n        See Also\n        --------\n        Series.nlargest: Get the `n` largest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values().head(n)`` for small `n` relative to\n        the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Brunei\": 434000, \"Malta\": 434000,\n        ...                         \"Maldives\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Brunei        434000\n        Malta         434000\n        Maldives      434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Montserrat      5200\n        dtype: int64\n\n        The `n` smallest elements where ``n=5`` by default.\n\n        >>> s.nsmallest()\n        Montserrat    5200\n        Nauru        11300\n        Tuvalu       11300\n        Anguilla     11300\n        Iceland     337000\n        dtype: int64\n\n        The `n` smallest elements where ``n=3``. Default `keep` value is\n        'first' so Nauru and Tuvalu will be kept.\n\n        >>> s.nsmallest(3)\n        Montserrat   5200\n        Nauru       11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` and keeping the last\n        duplicates. Anguilla and Tuvalu will be kept since they are the last\n        with value 11300 based on the index order.\n\n        >>> s.nsmallest(3, keep='last')\n        Montserrat   5200\n        Anguilla    11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has four elements due to the three duplicates.\n\n        >>> s.nsmallest(3, keep='all')\n        Montserrat   5200\n        Nauru       11300\n        Tuvalu      11300\n        Anguilla    11300\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nsmallest()\n\n    def swaplevel(self, i=-2, j=-1, copy=True) -> Series:\n        \"\"\"\n        Swap levels i and j in a :class:`MultiIndex`.\n\n        Default is to swap the two innermost levels of the index.\n\n        Parameters\n        ----------\n        i, j : int, str\n            Level of the indices to be swapped. Can pass level name as string.\n        copy : bool, default True\n            Whether to copy underlying data.\n\n        Returns\n        -------\n        Series\n            Series with levels swapped in MultiIndex.\n        \"\"\"\n        assert isinstance(self.index, MultiIndex)\n        new_index = self.index.swaplevel(i, j)\n        return self._constructor(self._values, index=new_index, copy=copy).__finalize__(\n            self, method=\"swaplevel\"\n        )\n\n    def reorder_levels(self, order) -> Series:\n        \"\"\"\n        Rearrange index levels using input order.\n\n        May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int representing new level order\n            Reference level by number or key.\n\n        Returns\n        -------\n        type of caller (new object)\n        \"\"\"\n        if not isinstance(self.index, MultiIndex):  # pragma: no cover\n            raise Exception(\"Can only reorder levels on a hierarchical axis.\")\n\n        result = self.copy()\n        assert isinstance(result.index, MultiIndex)\n        result.index = result.index.reorder_levels(order)\n        return result\n\n    def explode(self, ignore_index: bool = False) -> Series:\n        \"\"\"\n        Transform each element of a list-like to a row.\n\n        .. versionadded:: 0.25.0\n\n        Parameters\n        ----------\n        ignore_index : bool, default False\n            If True, the resulting index will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series\n            Exploded lists to rows; index will be duplicated for these rows.\n\n        See Also\n        --------\n        Series.str.split : Split string values on specified separator.\n        Series.unstack : Unstack, a.k.a. pivot, Series with MultiIndex\n            to produce DataFrame.\n        DataFrame.melt : Unpivot a DataFrame from wide format to long format.\n        DataFrame.explode : Explode a DataFrame from list-like\n            columns to long format.\n\n        Notes\n        -----\n        This routine will explode list-likes including lists, tuples, sets,\n        Series, and np.ndarray. The result dtype of the subset rows will\n        be object. Scalars will be returned unchanged, and empty list-likes will\n        result in a np.nan for that row. In addition, the ordering of elements in\n        the output will be non-deterministic when exploding sets.\n\n        Examples\n        --------\n        >>> s = pd.Series([[1, 2, 3], 'foo', [], [3, 4]])\n        >>> s\n        0    [1, 2, 3]\n        1          foo\n        2           []\n        3       [3, 4]\n        dtype: object\n\n        >>> s.explode()\n        0      1\n        0      2\n        0      3\n        1    foo\n        2    NaN\n        3      3\n        3      4\n        dtype: object\n        \"\"\"\n        if not len(self) or not is_object_dtype(self):\n            result = self.copy()\n            return result.reset_index(drop=True) if ignore_index else result\n\n        values, counts = reshape.explode(np.asarray(self._values))\n\n        if ignore_index:\n            index = ibase.default_index(len(values))\n        else:\n            index = self.index.repeat(counts)\n\n        return self._constructor(values, index=index, name=self.name)\n\n    def unstack(self, level=-1, fill_value=None) -> DataFrame:\n        \"\"\"\n        Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.\n\n        Parameters\n        ----------\n        level : int, str, or list of these, default last level\n            Level(s) to unstack, can pass level name.\n        fill_value : scalar value, default None\n            Value to use when replacing NaN values.\n\n        Returns\n        -------\n        DataFrame\n            Unstacked Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4],\n        ...               index=pd.MultiIndex.from_product([['one', 'two'],\n        ...                                                 ['a', 'b']]))\n        >>> s\n        one  a    1\n             b    2\n        two  a    3\n             b    4\n        dtype: int64\n\n        >>> s.unstack(level=-1)\n             a  b\n        one  1  2\n        two  3  4\n\n        >>> s.unstack(level=0)\n           one  two\n        a    1    3\n        b    2    4\n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n\n        return unstack(self, level, fill_value)\n\n    # ----------------------------------------------------------------------\n    # function application\n\n    def map(self, arg, na_action=None) -> Series:\n        \"\"\"\n        Map values of Series according to input correspondence.\n\n        Used for substituting each value in a Series with another value,\n        that may be derived from a function, a ``dict`` or\n        a :class:`Series`.\n\n        Parameters\n        ----------\n        arg : function, collections.abc.Mapping subclass or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NaN values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        Series\n            Same index as caller.\n\n        See Also\n        --------\n        Series.apply : For applying more complex functions on a Series.\n        DataFrame.apply : Apply a function row-/column-wise.\n        DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n\n        Notes\n        -----\n        When ``arg`` is a dictionary, values in Series that are not in the\n        dictionary (as keys) are converted to ``NaN``. However, if the\n        dictionary is a ``dict`` subclass that defines ``__missing__`` (i.e.\n        provides a method for default values), then this default is used\n        rather than ``NaN``.\n\n        Examples\n        --------\n        >>> s = pd.Series(['cat', 'dog', np.nan, 'rabbit'])\n        >>> s\n        0      cat\n        1      dog\n        2      NaN\n        3   rabbit\n        dtype: object\n\n        ``map`` accepts a ``dict`` or a ``Series``. Values that are not found\n        in the ``dict`` are converted to ``NaN``, unless the dict has a default\n        value (e.g. ``defaultdict``):\n\n        >>> s.map({'cat': 'kitten', 'dog': 'puppy'})\n        0   kitten\n        1    puppy\n        2      NaN\n        3      NaN\n        dtype: object\n\n        It also accepts a function:\n\n        >>> s.map('I am a {}'.format)\n        0       I am a cat\n        1       I am a dog\n        2       I am a nan\n        3    I am a rabbit\n        dtype: object\n\n        To avoid applying the function to missing values (and keep them as\n        ``NaN``) ``na_action='ignore'`` can be used:\n\n        >>> s.map('I am a {}'.format, na_action='ignore')\n        0     I am a cat\n        1     I am a dog\n        2            NaN\n        3  I am a rabbit\n        dtype: object\n        \"\"\"\n        new_values = super()._map_values(arg, na_action=na_action)\n        return self._constructor(new_values, index=self.index).__finalize__(\n            self, method=\"map\"\n        )\n\n    def _gotitem(self, key, ndim, subset=None) -> Series:\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : {1, 2}\n            Requested ndim of result.\n        subset : object, default None\n            Subset to act on.\n        \"\"\"\n        return self\n\n    _agg_see_also_doc = dedent(\n        \"\"\"\n    See Also\n    --------\n    Series.apply : Invoke function on a Series.\n    Series.transform : Transform function producing a Series with like indexes.\n    \"\"\"\n    )\n\n    _agg_examples_doc = dedent(\n        \"\"\"\n    Examples\n    --------\n    >>> s = pd.Series([1, 2, 3, 4])\n    >>> s\n    0    1\n    1    2\n    2    3\n    3    4\n    dtype: int64\n\n    >>> s.agg('min')\n    1\n\n    >>> s.agg(['min', 'max'])\n    min   1\n    max   4\n    dtype: int64\n    \"\"\"\n    )\n\n    @doc(\n        generic._shared_docs[\"aggregate\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        axis=_shared_doc_kwargs[\"axis\"],\n        see_also=_agg_see_also_doc,\n        examples=_agg_examples_doc,\n    )\n    def aggregate(self, func=None, axis=0, *args, **kwargs):\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n\n        # if func is None, will switch to user-provided \"named aggregation\" kwargs\n        if func is None:\n            func = dict(kwargs.items())\n\n        op = SeriesApply(self, func, convert_dtype=False, args=args, kwargs=kwargs)\n        result = op.agg()\n        return result\n\n    agg = aggregate\n\n    @doc(\n        _shared_docs[\"transform\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        axis=_shared_doc_kwargs[\"axis\"],\n    )\n    def transform(\n        self, func: AggFuncType, axis: Axis = 0, *args, **kwargs\n    ) -> FrameOrSeriesUnion:\n        # Validate axis argument\n        self._get_axis_number(axis)\n        result = SeriesApply(\n            self, func=func, convert_dtype=True, args=args, kwargs=kwargs\n        ).transform()\n        return result\n\n    def apply(\n        self,\n        func: AggFuncType,\n        convert_dtype: bool = True,\n        args: tuple[Any, ...] = (),\n        **kwargs,\n    ) -> FrameOrSeriesUnion:\n        \"\"\"\n        Invoke function on values of Series.\n\n        Can be ufunc (a NumPy function that applies to the entire Series)\n        or a Python function that only works on single values.\n\n        Parameters\n        ----------\n        func : function\n            Python function or NumPy ufunc to apply.\n        convert_dtype : bool, default True\n            Try to find better dtype for elementwise function results. If\n            False, leave as dtype=object. Note that the dtype is always\n            preserved for extension array dtypes, such as Categorical.\n        args : tuple\n            Positional arguments passed to func after the series value.\n        **kwargs\n            Additional keyword arguments passed to func.\n\n        Returns\n        -------\n        Series or DataFrame\n            If func returns a Series object the result will be a DataFrame.\n\n        See Also\n        --------\n        Series.map: For element-wise operations.\n        Series.agg: Only perform aggregating type operations.\n        Series.transform: Only perform transforming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n        for more details.\n\n        Examples\n        --------\n        Create a series with typical summer temperatures for each city.\n\n        >>> s = pd.Series([20, 21, 12],\n        ...               index=['London', 'New York', 'Helsinki'])\n        >>> s\n        London      20\n        New York    21\n        Helsinki    12\n        dtype: int64\n\n        Square the values by defining a function and passing it as an\n        argument to ``apply()``.\n\n        >>> def square(x):\n        ...     return x ** 2\n        >>> s.apply(square)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Square the values by passing an anonymous function as an\n        argument to ``apply()``.\n\n        >>> s.apply(lambda x: x ** 2)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Define a custom function that needs additional positional\n        arguments and pass these additional arguments using the\n        ``args`` keyword.\n\n        >>> def subtract_custom_value(x, custom_value):\n        ...     return x - custom_value\n\n        >>> s.apply(subtract_custom_value, args=(5,))\n        London      15\n        New York    16\n        Helsinki     7\n        dtype: int64\n\n        Define a custom function that takes keyword arguments\n        and pass these arguments to ``apply``.\n\n        >>> def add_custom_values(x, **kwargs):\n        ...     for month in kwargs:\n        ...         x += kwargs[month]\n        ...     return x\n\n        >>> s.apply(add_custom_values, june=30, july=20, august=25)\n        London      95\n        New York    96\n        Helsinki    87\n        dtype: int64\n\n        Use a function from the Numpy library.\n\n        >>> s.apply(np.log)\n        London      2.995732\n        New York    3.044522\n        Helsinki    2.484907\n        dtype: float64\n        \"\"\"\n        return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n\n    def _reduce(\n        self,\n        op,\n        name: str,\n        *,\n        axis=0,\n        skipna=True,\n        numeric_only=None,\n        filter_type=None,\n        **kwds,\n    ):\n        \"\"\"\n        Perform a reduction operation.\n\n        If we have an ndarray as a value, then simply perform the operation,\n        otherwise delegate to the object.\n        \"\"\"\n        delegate = self._values\n\n        if axis is not None:\n            self._get_axis_number(axis)\n\n        if isinstance(delegate, ExtensionArray):\n            # dispatch to ExtensionArray interface\n            return delegate._reduce(name, skipna=skipna, **kwds)\n\n        else:\n            # dispatch to numpy arrays\n            if numeric_only:\n                raise NotImplementedError(\n                    f\"Series.{name} does not implement numeric_only.\"\n                )\n            with np.errstate(all=\"ignore\"):\n                return op(delegate, skipna=skipna, **kwds)\n\n    def _reindex_indexer(\n        self, new_index: Index | None, indexer: np.ndarray | None, copy: bool\n    ) -> Series:\n        # Note: new_index is None iff indexer is None\n        # if not None, indexer is np.intp\n        if indexer is None:\n            if copy:\n                return self.copy()\n            return self\n\n        new_values = algorithms.take_nd(\n            self._values, indexer, allow_fill=True, fill_value=None\n        )\n        return self._constructor(new_values, index=new_index)\n\n    def _needs_reindex_multi(self, axes, method, level) -> bool:\n        \"\"\"\n        Check if we do need a multi reindex; this is for compat with\n        higher dims.\n        \"\"\"\n        return False\n\n    # error: Cannot determine type of 'align'\n    @doc(\n        NDFrame.align,  # type: ignore[has-type]\n        klass=_shared_doc_kwargs[\"klass\"],\n        axes_single_arg=_shared_doc_kwargs[\"axes_single_arg\"],\n    )\n    def align(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy=True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n        broadcast_axis=None,\n    ):\n        return super().align(\n            other,\n            join=join,\n            axis=axis,\n            level=level,\n            copy=copy,\n            fill_value=fill_value,\n            method=method,\n            limit=limit,\n            fill_axis=fill_axis,\n            broadcast_axis=broadcast_axis,\n        )\n\n    def rename(\n        self,\n        index=None,\n        *,\n        axis=None,\n        copy=True,\n        inplace=False,\n        level=None,\n        errors=\"ignore\",\n    ):\n        \"\"\"\n        Alter Series index labels or name.\n\n        Function / dict values must be unique (1-to-1). Labels not contained in\n        a dict / Series will be left as-is. Extra labels listed don't throw an\n        error.\n\n        Alternatively, change ``Series.name`` with a scalar value.\n\n        See the :ref:`user guide <basics.rename>` for more.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Unused. Accepted for compatibility with DataFrame method only.\n        index : scalar, hashable sequence, dict-like or function, optional\n            Functions or dict-like are transformations to apply to\n            the index.\n            Scalar or hashable sequence-like will alter the ``Series.name``\n            attribute.\n\n        **kwargs\n            Additional keyword arguments passed to the function. Only the\n            \"inplace\" keyword is used.\n\n        Returns\n        -------\n        Series or None\n            Series with index labels or name altered or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.rename : Corresponding DataFrame method.\n        Series.rename_axis : Set the name of the axis.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n        >>> s.rename(\"my_name\")  # scalar, changes Series.name\n        0    1\n        1    2\n        2    3\n        Name: my_name, dtype: int64\n        >>> s.rename(lambda x: x ** 2)  # function, changes labels\n        0    1\n        1    2\n        4    3\n        dtype: int64\n        >>> s.rename({1: 3, 2: 5})  # mapping, changes labels\n        0    1\n        3    2\n        5    3\n        dtype: int64\n        \"\"\"\n        if callable(index) or is_dict_like(index):\n            return super().rename(\n                index, copy=copy, inplace=inplace, level=level, errors=errors\n            )\n        else:\n            return self._set_name(index, inplace=inplace)\n\n    @overload\n    def set_axis(\n        self, labels, axis: Axis = ..., inplace: Literal[False] = ...\n    ) -> Series:\n        ...\n\n    @overload\n    def set_axis(self, labels, axis: Axis, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def set_axis(self, labels, *, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def set_axis(self, labels, axis: Axis = ..., inplace: bool = ...) -> Series | None:\n        ...\n\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n\n        >>> s.set_axis(['a', 'b', 'c'], axis=0)\n        a    1\n        b    2\n        c    3\n        dtype: int64\n    \"\"\"\n    )\n    @Substitution(\n        **_shared_doc_kwargs,\n        extended_summary_sub=\"\",\n        axis_description_sub=\"\",\n        see_also_sub=\"\",\n    )\n    @Appender(generic.NDFrame.set_axis.__doc__)\n    def set_axis(self, labels, axis: Axis = 0, inplace: bool = False):\n        return super().set_axis(labels, axis=axis, inplace=inplace)\n\n    # error: Cannot determine type of 'reindex'\n    @doc(\n        NDFrame.reindex,  # type: ignore[has-type]\n        klass=_shared_doc_kwargs[\"klass\"],\n        axes=_shared_doc_kwargs[\"axes\"],\n        optional_labels=_shared_doc_kwargs[\"optional_labels\"],\n        optional_axis=_shared_doc_kwargs[\"optional_axis\"],\n    )\n    def reindex(self, index=None, **kwargs):\n        return super().reindex(index=index, **kwargs)\n\n    def drop(\n        self,\n        labels=None,\n        axis=0,\n        index=None,\n        columns=None,\n        level=None,\n        inplace=False,\n        errors=\"raise\",\n    ) -> Series:\n        \"\"\"\n        Return Series with specified index labels removed.\n\n        Remove elements of a Series based on specifying the index labels.\n        When using a multi-index, labels on different levels can be removed\n        by specifying the level.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index labels to drop.\n        axis : 0, default 0\n            Redundant for application on Series.\n        index : single label or list-like\n            Redundant for application on Series, but 'index' can be used instead\n            of 'labels'.\n        columns : single label or list-like\n            No change is made to the Series; use 'index' or 'labels' instead.\n        level : int or level name, optional\n            For MultiIndex, level for which the labels will be removed.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and only existing labels are dropped.\n\n        Returns\n        -------\n        Series or None\n            Series with specified index labels removed or None if ``inplace=True``.\n\n        Raises\n        ------\n        KeyError\n            If none of the labels are found in the index.\n\n        See Also\n        --------\n        Series.reindex : Return only specified index labels of Series.\n        Series.dropna : Return series without null values.\n        Series.drop_duplicates : Return Series with duplicate values removed.\n        DataFrame.drop : Drop specified labels from rows or columns.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=np.arange(3), index=['A', 'B', 'C'])\n        >>> s\n        A  0\n        B  1\n        C  2\n        dtype: int64\n\n        Drop labels B en C\n\n        >>> s.drop(labels=['B', 'C'])\n        A  0\n        dtype: int64\n\n        Drop 2nd level label in MultiIndex Series\n\n        >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n        ...                              ['speed', 'weight', 'length']],\n        ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> s = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...               index=midx)\n        >>> s\n        lama    speed      45.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        dtype: float64\n\n        >>> s.drop(labels='weight', level=1)\n        lama    speed      45.0\n                length      1.2\n        cow     speed      30.0\n                length      1.5\n        falcon  speed     320.0\n                length      0.3\n        dtype: float64\n        \"\"\"\n        return super().drop(\n            labels=labels,\n            axis=axis,\n            index=index,\n            columns=columns,\n            level=level,\n            inplace=inplace,\n            errors=errors,\n        )\n\n    @overload\n    def fillna(\n        self,\n        value=...,\n        method: FillnaOptions | None = ...,\n        axis: Axis | None = ...,\n        inplace: Literal[False] = ...,\n        limit=...,\n        downcast=...,\n    ) -> Series:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        method: FillnaOptions | None,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        method: FillnaOptions | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        method: FillnaOptions | None,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        *,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        method: FillnaOptions | None,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value=...,\n        method: FillnaOptions | None = ...,\n        axis: Axis | None = ...,\n        inplace: bool = ...,\n        limit=...,\n        downcast=...,\n    ) -> Series | None:\n        ...\n\n    # error: Cannot determine type of 'fillna'\n    @doc(NDFrame.fillna, **_shared_doc_kwargs)  # type: ignore[has-type]\n    def fillna(\n        self,\n        value: object | ArrayLike | None = None,\n        method: FillnaOptions | None = None,\n        axis=None,\n        inplace=False,\n        limit=None,\n        downcast=None,\n    ) -> Series | None:\n        return super().fillna(\n            value=value,\n            method=method,\n            axis=axis,\n            inplace=inplace,\n            limit=limit,\n            downcast=downcast,\n        )\n\n    def pop(self, item: Hashable) -> Any:\n        \"\"\"\n        Return item and drops from series. Raise KeyError if not found.\n\n        Parameters\n        ----------\n        item : label\n            Index of the element that needs to be removed.\n\n        Returns\n        -------\n        Value that is popped from series.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1,2,3])\n\n        >>> ser.pop(0)\n        1\n\n        >>> ser\n        1    2\n        2    3\n        dtype: int64\n        \"\"\"\n        return super().pop(item=item)\n\n    # error: Cannot determine type of 'replace'\n    @doc(\n        NDFrame.replace,  # type: ignore[has-type]\n        klass=_shared_doc_kwargs[\"klass\"],\n        inplace=_shared_doc_kwargs[\"inplace\"],\n        replace_iloc=_shared_doc_kwargs[\"replace_iloc\"],\n    )\n    def replace(\n        self,\n        to_replace=None,\n        value=None,\n        inplace=False,\n        limit=None,\n        regex=False,\n        method=\"pad\",\n    ):\n        return super().replace(\n            to_replace=to_replace,\n            value=value,\n            inplace=inplace,\n            limit=limit,\n            regex=regex,\n            method=method,\n        )\n\n    def _replace_single(self, to_replace, method: str, inplace: bool, limit):\n        \"\"\"\n        Replaces values in a Series using the fill method specified when no\n        replacement value is given in the replace method\n        \"\"\"\n\n        orig_dtype = self.dtype\n        result = self if inplace else self.copy()\n        fill_f = missing.get_fill_func(method)\n\n        mask = missing.mask_missing(result.values, to_replace)\n        values, _ = fill_f(result.values, limit=limit, mask=mask)\n\n        if values.dtype == orig_dtype and inplace:\n            return\n\n        result = self._constructor(values, index=self.index, dtype=self.dtype)\n        result = result.__finalize__(self)\n\n        if inplace:\n            self._update_inplace(result)\n            return\n\n        return result\n\n    # error: Cannot determine type of 'shift'\n    @doc(NDFrame.shift, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def shift(self, periods=1, freq=None, axis=0, fill_value=None) -> Series:\n        return super().shift(\n            periods=periods, freq=freq, axis=axis, fill_value=fill_value\n        )\n\n    def memory_usage(self, index: bool = True, deep: bool = False) -> int:\n        \"\"\"\n        Return the memory usage of the Series.\n\n        The memory usage can optionally include the contribution of\n        the index and of elements of `object` dtype.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the Series index.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned value.\n\n        Returns\n        -------\n        int\n            Bytes of memory consumed.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\n            array.\n        DataFrame.memory_usage : Bytes consumed by a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(range(3))\n        >>> s.memory_usage()\n        152\n\n        Not including the index gives the size of the rest of the data, which\n        is necessarily smaller:\n\n        >>> s.memory_usage(index=False)\n        24\n\n        The memory footprint of `object` values is ignored by default:\n\n        >>> s = pd.Series([\"a\", \"b\"])\n        >>> s.values\n        array(['a', 'b'], dtype=object)\n        >>> s.memory_usage()\n        144\n        >>> s.memory_usage(deep=True)\n        244\n        \"\"\"\n        v = self._memory_usage(deep=deep)\n        if index:\n            v += self.index.memory_usage(deep=deep)\n        return v\n\n    def isin(self, values) -> Series:\n        \"\"\"\n        Whether elements in Series are contained in `values`.\n\n        Return a boolean Series showing whether each element in the Series\n        matches an element in the passed sequence of `values` exactly.\n\n        Parameters\n        ----------\n        values : set or list-like\n            The sequence of values to test. Passing in a single string will\n            raise a ``TypeError``. Instead, turn a single string into a\n            list of one element.\n\n        Returns\n        -------\n        Series\n            Series of booleans indicating if each element is in values.\n\n        Raises\n        ------\n        TypeError\n          * If `values` is a string\n\n        See Also\n        --------\n        DataFrame.isin : Equivalent method on DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama',\n        ...                'hippo'], name='animal')\n        >>> s.isin(['cow', 'lama'])\n        0     True\n        1     True\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n\n        Passing a single string as ``s.isin('lama')`` will raise an error. Use\n        a list of one element instead:\n\n        >>> s.isin(['lama'])\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n\n        Strings and integers are distinct and are therefore not comparable:\n\n        >>> pd.Series([1]).isin(['1'])\n        0    False\n        dtype: bool\n        >>> pd.Series([1.1]).isin(['1.1'])\n        0    False\n        dtype: bool\n        \"\"\"\n        result = algorithms.isin(self._values, values)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"isin\"\n        )\n\n    def between(self, left, right, inclusive=True) -> Series:\n        \"\"\"\n        Return boolean Series equivalent to left <= series <= right.\n\n        This function returns a boolean vector containing `True` wherever the\n        corresponding Series element is between the boundary values `left` and\n        `right`. NA values are treated as `False`.\n\n        Parameters\n        ----------\n        left : scalar or list-like\n            Left boundary.\n        right : scalar or list-like\n            Right boundary.\n        inclusive : bool, default True\n            Include boundaries.\n\n        Returns\n        -------\n        Series\n            Series representing whether each element is between left and\n            right (inclusive).\n\n        See Also\n        --------\n        Series.gt : Greater than of series and other.\n        Series.lt : Less than of series and other.\n\n        Notes\n        -----\n        This function is equivalent to ``(left <= ser) & (ser <= right)``\n\n        Examples\n        --------\n        >>> s = pd.Series([2, 0, 4, 8, np.nan])\n\n        Boundary values are included by default:\n\n        >>> s.between(1, 4)\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        With `inclusive` set to ``False`` boundary values are excluded:\n\n        >>> s.between(1, 4, inclusive=False)\n        0     True\n        1    False\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        `left` and `right` can be any scalar value:\n\n        >>> s = pd.Series(['Alice', 'Bob', 'Carol', 'Eve'])\n        >>> s.between('Anna', 'Daniel')\n        0    False\n        1     True\n        2     True\n        3    False\n        dtype: bool\n        \"\"\"\n        if inclusive:\n            lmask = self >= left\n            rmask = self <= right\n        else:\n            lmask = self > left\n            rmask = self < right\n\n        return lmask & rmask\n\n    # ----------------------------------------------------------------------\n    # Convert to types that support pd.NA\n\n    def _convert_dtypes(\n        self,\n        infer_objects: bool = True,\n        convert_string: bool = True,\n        convert_integer: bool = True,\n        convert_boolean: bool = True,\n        convert_floating: bool = True,\n    ) -> Series:\n        input_series = self\n        if infer_objects:\n            input_series = input_series.infer_objects()\n            if is_object_dtype(input_series):\n                input_series = input_series.copy()\n\n        if convert_string or convert_integer or convert_boolean or convert_floating:\n            inferred_dtype = convert_dtypes(\n                input_series._values,\n                convert_string,\n                convert_integer,\n                convert_boolean,\n                convert_floating,\n            )\n            try:\n                result = input_series.astype(inferred_dtype)\n            except TypeError:\n                result = input_series.copy()\n        else:\n            result = input_series.copy()\n        return result\n\n    # error: Cannot determine type of 'isna'\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def isna(self) -> Series:\n        return generic.NDFrame.isna(self)\n\n    # error: Cannot determine type of 'isna'\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def isnull(self) -> Series:\n        return super().isnull()\n\n    # error: Cannot determine type of 'notna'\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def notna(self) -> Series:\n        return super().notna()\n\n    # error: Cannot determine type of 'notna'\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def notnull(self) -> Series:\n        return super().notnull()\n\n    def dropna(self, axis=0, inplace=False, how=None):\n        \"\"\"\n        Return a new Series with missing values removed.\n\n        See the :ref:`User Guide <missing_data>` for more on which values are\n        considered missing, and how to work with missing data.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            There is only one axis to drop values from.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        how : str, optional\n            Not in use. Kept for compatibility.\n\n        Returns\n        -------\n        Series or None\n            Series with NA entries dropped from it or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.isna: Indicate missing values.\n        Series.notna : Indicate existing (non-missing) values.\n        Series.fillna : Replace missing values.\n        DataFrame.dropna : Drop rows or columns which contain NA values.\n        Index.dropna : Drop missing indices.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1., 2., np.nan])\n        >>> ser\n        0    1.0\n        1    2.0\n        2    NaN\n        dtype: float64\n\n        Drop NA values from a Series.\n\n        >>> ser.dropna()\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Keep the Series with valid entries in the same variable.\n\n        >>> ser.dropna(inplace=True)\n        >>> ser\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Empty strings are not considered NA values. ``None`` is considered an\n        NA value.\n\n        >>> ser = pd.Series([np.NaN, 2, pd.NaT, '', None, 'I stay'])\n        >>> ser\n        0       NaN\n        1         2\n        2       NaT\n        3\n        4      None\n        5    I stay\n        dtype: object\n        >>> ser.dropna()\n        1         2\n        3\n        5    I stay\n        dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis or 0)\n\n        if self._can_hold_na:\n            result = remove_na_arraylike(self)\n            if inplace:\n                self._update_inplace(result)\n            else:\n                return result\n        else:\n            if inplace:\n                # do nothing\n                pass\n            else:\n                return self.copy()\n\n    # ----------------------------------------------------------------------\n    # Time series-oriented methods\n\n    # error: Cannot determine type of 'asfreq'\n    @doc(NDFrame.asfreq, **_shared_doc_kwargs)  # type: ignore[has-type]\n    def asfreq(\n        self,\n        freq,\n        method=None,\n        how: str | None = None,\n        normalize: bool = False,\n        fill_value=None,\n    ) -> Series:\n        return super().asfreq(\n            freq=freq,\n            method=method,\n            how=how,\n            normalize=normalize,\n            fill_value=fill_value,\n        )\n\n    # error: Cannot determine type of 'resample'\n    @doc(NDFrame.resample, **_shared_doc_kwargs)  # type: ignore[has-type]\n    def resample(\n        self,\n        rule,\n        axis=0,\n        closed: str | None = None,\n        label: str | None = None,\n        convention: str = \"start\",\n        kind: str | None = None,\n        loffset=None,\n        base: int | None = None,\n        on=None,\n        level=None,\n        origin: str | TimestampConvertibleTypes = \"start_day\",\n        offset: TimedeltaConvertibleTypes | None = None,\n    ) -> Resampler:\n        return super().resample(\n            rule=rule,\n            axis=axis,\n            closed=closed,\n            label=label,\n            convention=convention,\n            kind=kind,\n            loffset=loffset,\n            base=base,\n            on=on,\n            level=level,\n            origin=origin,\n            offset=offset,\n        )\n\n    def to_timestamp(self, freq=None, how=\"start\", copy=True) -> Series:\n        \"\"\"\n        Cast to DatetimeIndex of Timestamps, at *beginning* of period.\n\n        Parameters\n        ----------\n        freq : str, default frequency of PeriodIndex\n            Desired frequency.\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series with DatetimeIndex\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        if not isinstance(self.index, PeriodIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\n        new_index = self.index.to_timestamp(freq=freq, how=how)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_timestamp\"\n        )\n\n    def to_period(self, freq=None, copy=True) -> Series:\n        \"\"\"\n        Convert Series from DatetimeIndex to PeriodIndex.\n\n        Parameters\n        ----------\n        freq : str, default None\n            Frequency associated with the PeriodIndex.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series\n            Series with index converted to PeriodIndex.\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\n        new_index = self.index.to_period(freq=freq)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_period\"\n        )\n\n    # ----------------------------------------------------------------------\n    # Add index\n    _AXIS_ORDERS = [\"index\"]\n    _AXIS_REVERSED = False\n    _AXIS_LEN = len(_AXIS_ORDERS)\n    _info_axis_number = 0\n    _info_axis_name = \"index\"\n\n    index: Index = properties.AxisProperty(\n        axis=0, doc=\"The index (axis labels) of the Series.\"\n    )\n\n    # ----------------------------------------------------------------------\n    # Accessor Methods\n    # ----------------------------------------------------------------------\n    str = CachedAccessor(\"str\", StringMethods)\n    dt = CachedAccessor(\"dt\", CombinedDatetimelikeProperties)\n    cat = CachedAccessor(\"cat\", CategoricalAccessor)\n    plot = CachedAccessor(\"plot\", pandas.plotting.PlotAccessor)\n    sparse = CachedAccessor(\"sparse\", SparseAccessor)\n\n    # ----------------------------------------------------------------------\n    # Add plotting methods to Series\n    hist = pandas.plotting.hist_series\n\n    # ----------------------------------------------------------------------\n    # Template-Based Arithmetic/Comparison Methods\n\n    def _cmp_method(self, other, op):\n        res_name = ops.get_op_result_name(self, other)\n\n        if isinstance(other, Series) and not self._indexed_same(other):\n            raise ValueError(\"Can only compare identically-labeled Series objects\")\n\n        lvalues = self._values\n        rvalues = extract_array(other, extract_numpy=True, extract_range=True)\n\n        with np.errstate(all=\"ignore\"):\n            res_values = ops.comparison_op(lvalues, rvalues, op)\n\n        return self._construct_result(res_values, name=res_name)\n\n    def _logical_method(self, other, op):\n        res_name = ops.get_op_result_name(self, other)\n        self, other = ops.align_method_SERIES(self, other, align_asobject=True)\n\n        lvalues = self._values\n        rvalues = extract_array(other, extract_numpy=True, extract_range=True)\n\n        res_values = ops.logical_op(lvalues, rvalues, op)\n        return self._construct_result(res_values, name=res_name)\n\n    def _arith_method(self, other, op):\n        res_name = ops.get_op_result_name(self, other)\n        self, other = ops.align_method_SERIES(self, other)\n\n        lvalues = self._values\n        rvalues = extract_array(other, extract_numpy=True, extract_range=True)\n        rvalues = ops.maybe_prepare_scalar_for_op(rvalues, lvalues.shape)\n        rvalues = ensure_wrapped_if_datetimelike(rvalues)\n\n        with np.errstate(all=\"ignore\"):\n            result = ops.arithmetic_op(lvalues, rvalues, op)\n\n        return self._construct_result(result, name=res_name)\n\n\nSeries._add_numeric_operations()\n\n# Add arithmetic!\nops.add_flex_arithmetic_methods(Series)\n",
                "max_stars_repo_path": "pandas/core/series.py",
                "max_stars_repo_name": "sisiyan/pandas",
                "max_stars_count": 1,
                "__cluster__": 79
            }
        ],
        [
            {
                "id": "test_pandas-bug-3",
                "content": "\"\"\"\nData structure for 1-dimensional cross-sectional and time series data\n\"\"\"\nfrom io import StringIO\nfrom shutil import get_terminal_size\nfrom textwrap import dedent\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import lib, properties, reshape, tslibs\nfrom pandas._typing import ArrayLike, Axis, DtypeObj, IndexKeyFunc, Label, ValueKeyFunc\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import Appender, Substitution, doc\nfrom pandas.util._validators import validate_bool_kwarg, validate_percentile\n\nfrom pandas.core.dtypes.cast import (\n    convert_dtypes,\n    maybe_cast_to_extension_array,\n    validate_numeric_casting,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_platform_int,\n    is_bool,\n    is_categorical_dtype,\n    is_dict_like,\n    is_extension_array_dtype,\n    is_integer,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_scalar,\n)\nfrom pandas.core.dtypes.generic import ABCDataFrame\nfrom pandas.core.dtypes.inference import is_hashable\nfrom pandas.core.dtypes.missing import (\n    isna,\n    na_value_for_dtype,\n    notna,\n    remove_na_arraylike,\n)\n\nimport pandas as pd\nfrom pandas.core import algorithms, base, generic, nanops, ops\nfrom pandas.core.accessor import CachedAccessor\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.arrays.categorical import CategoricalAccessor\nfrom pandas.core.arrays.sparse import SparseAccessor\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    create_series_with_explicit_dtype,\n    extract_array,\n    is_empty_data,\n    sanitize_array,\n)\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.indexers import unpack_1tuple\nfrom pandas.core.indexes.accessors import CombinedDatetimelikeProperties\nfrom pandas.core.indexes.api import (\n    Float64Index,\n    Index,\n    InvalidIndexError,\n    MultiIndex,\n    ensure_index,\n)\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.datetimes import DatetimeIndex\nfrom pandas.core.indexes.period import PeriodIndex\nfrom pandas.core.indexes.timedeltas import TimedeltaIndex\nfrom pandas.core.indexing import check_bool_indexer\nfrom pandas.core.internals import SingleBlockManager\nfrom pandas.core.sorting import ensure_key_mapped\nfrom pandas.core.strings import StringMethods\nfrom pandas.core.tools.datetimes import to_datetime\n\nimport pandas.io.formats.format as fmt\nimport pandas.plotting\n\nif TYPE_CHECKING:\n    from pandas.core.frame import DataFrame\n    from pandas.core.groupby.generic import SeriesGroupBy\n\n__all__ = [\"Series\"]\n\n_shared_doc_kwargs = dict(\n    axes=\"index\",\n    klass=\"Series\",\n    axes_single_arg=\"{0 or 'index'}\",\n    axis=\"\"\"axis : {0 or 'index'}\n        Parameter needed for compatibility with DataFrame.\"\"\",\n    inplace=\"\"\"inplace : boolean, default False\n        If True, performs operation inplace and returns None.\"\"\",\n    unique=\"np.ndarray\",\n    duplicated=\"Series\",\n    optional_by=\"\",\n    optional_mapper=\"\",\n    optional_labels=\"\",\n    optional_axis=\"\",\n    versionadded_to_excel=\"\\n    .. versionadded:: 0.20.0\\n\",\n)\n\n\ndef _coerce_method(converter):\n    \"\"\"\n    Install the scalar coercion methods.\n    \"\"\"\n\n    def wrapper(self):\n        if len(self) == 1:\n            return converter(self.iloc[0])\n        raise TypeError(f\"cannot convert the series to {converter}\")\n\n    wrapper.__name__ = f\"__{converter.__name__}__\"\n    return wrapper\n\n\n# ----------------------------------------------------------------------\n# Series class\n\n\nclass Series(base.IndexOpsMixin, generic.NDFrame):\n    \"\"\"\n    One-dimensional ndarray with axis labels (including time series).\n\n    Labels need not be unique but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for performing operations involving the index. Statistical\n    methods from ndarray have been overridden to automatically exclude\n    missing data (currently represented as NaN).\n\n    Operations between Series (+, -, /, *, **) align values based on their\n    associated index values-- they need not be the same length. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Series.\n\n        .. versionchanged:: 0.23.0\n           If data is a dict, argument order is maintained for Python 3.6\n           and later.\n\n    index : array-like or Index (1d)\n        Values must be hashable and have the same length as `data`.\n        Non-unique index values are allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If both a dict and index\n        sequence are used, the index will override the keys found in the\n        dict.\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\n        Data type for the output Series. If not specified, this will be\n        inferred from `data`.\n        See the :ref:`user guide <basics.dtypes>` for more usages.\n    name : str, optional\n        The name to give to the Series.\n    copy : bool, default False\n        Copy input data.\n    \"\"\"\n\n    _typ = \"series\"\n\n    _name: Label\n    _metadata: List[str] = [\"name\"]\n    _internal_names_set = {\"index\"} | generic.NDFrame._internal_names_set\n    _accessors = {\"dt\", \"cat\", \"str\", \"sparse\"}\n    _deprecations = (\n        base.IndexOpsMixin._deprecations\n        | generic.NDFrame._deprecations\n        | frozenset([\"compress\", \"ptp\"])\n    )\n\n    # Override cache_readonly bc Series is mutable\n    hasnans = property(\n        base.IndexOpsMixin.hasnans.func, doc=base.IndexOpsMixin.hasnans.__doc__\n    )\n    _mgr: SingleBlockManager\n    div: Callable[[\"Series\", Any], \"Series\"]\n    rdiv: Callable[[\"Series\", Any], \"Series\"]\n\n    # ----------------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self, data=None, index=None, dtype=None, name=None, copy=False, fastpath=False\n    ):\n\n        if (\n            isinstance(data, SingleBlockManager)\n            and index is None\n            and dtype is None\n            and copy is False\n        ):\n            # GH#33357 called with just the SingleBlockManager\n            NDFrame.__init__(self, data)\n            self.name = name\n            return\n\n        # we are called internally, so short-circuit\n        if fastpath:\n\n            # data is an ndarray, index is defined\n            if not isinstance(data, SingleBlockManager):\n                data = SingleBlockManager.from_array(data, index)\n            if copy:\n                data = data.copy()\n            if index is None:\n                index = data.index\n\n        else:\n\n            name = ibase.maybe_extract_name(name, data, type(self))\n\n            if is_empty_data(data) and dtype is None:\n                # gh-17261\n                warnings.warn(\n                    \"The default dtype for empty Series will be 'object' instead \"\n                    \"of 'float64' in a future version. Specify a dtype explicitly \"\n                    \"to silence this warning.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n                # uncomment the line below when removing the DeprecationWarning\n                # dtype = np.dtype(object)\n\n            if index is not None:\n                index = ensure_index(index)\n\n            if data is None:\n                data = {}\n            if dtype is not None:\n                dtype = self._validate_dtype(dtype)\n\n            if isinstance(data, MultiIndex):\n                raise NotImplementedError(\n                    \"initializing a Series from a MultiIndex is not supported\"\n                )\n            elif isinstance(data, Index):\n\n                if dtype is not None:\n                    # astype copies\n                    data = data.astype(dtype)\n                else:\n                    # GH#24096 we need to ensure the index remains immutable\n                    data = data._values.copy()\n                copy = False\n\n            elif isinstance(data, np.ndarray):\n                if len(data.dtype):\n                    # GH#13296 we are dealing with a compound dtype, which\n                    #  should be treated as 2D\n                    raise ValueError(\n                        \"Cannot construct a Series from an ndarray with \"\n                        \"compound dtype.  Use DataFrame instead.\"\n                    )\n            elif isinstance(data, Series):\n                if index is None:\n                    index = data.index\n                else:\n                    data = data.reindex(index, copy=copy)\n                    copy = False\n                data = data._mgr\n            elif is_dict_like(data):\n                data, index = self._init_dict(data, index, dtype)\n                dtype = None\n                copy = False\n            elif isinstance(data, SingleBlockManager):\n                if index is None:\n                    index = data.index\n                elif not data.index.equals(index) or copy:\n                    # GH#19275 SingleBlockManager input should only be called\n                    # internally\n                    raise AssertionError(\n                        \"Cannot pass both SingleBlockManager \"\n                        \"`data` argument and a different \"\n                        \"`index` argument. `copy` must be False.\"\n                    )\n\n            elif is_extension_array_dtype(data):\n                pass\n            elif isinstance(data, (set, frozenset)):\n                raise TypeError(f\"'{type(data).__name__}' type is unordered\")\n            else:\n                data = com.maybe_iterable_to_list(data)\n\n            if index is None:\n                if not is_list_like(data):\n                    data = [data]\n                index = ibase.default_index(len(data))\n            elif is_list_like(data):\n\n                # a scalar numpy array is list-like but doesn't\n                # have a proper length\n                try:\n                    if len(index) != len(data):\n                        raise ValueError(\n                            f\"Length of passed values is {len(data)}, \"\n                            f\"index implies {len(index)}.\"\n                        )\n                except TypeError:\n                    pass\n\n            # create/copy the manager\n            if isinstance(data, SingleBlockManager):\n                if dtype is not None:\n                    data = data.astype(dtype=dtype, errors=\"ignore\", copy=copy)\n                elif copy:\n                    data = data.copy()\n            else:\n                data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True)\n\n                data = SingleBlockManager.from_array(data, index)\n\n        generic.NDFrame.__init__(self, data)\n        self.name = name\n        self._set_axis(0, index, fastpath=True)\n\n    def _init_dict(self, data, index=None, dtype=None):\n        \"\"\"\n        Derive the \"_mgr\" and \"index\" attributes of a new Series from a\n        dictionary input.\n\n        Parameters\n        ----------\n        data : dict or dict-like\n            Data used to populate the new Series.\n        index : Index or index-like, default None\n            Index for the new Series: if None, use dict keys.\n        dtype : dtype, default None\n            The dtype for the new Series: if None, infer from data.\n\n        Returns\n        -------\n        _data : BlockManager for the new Series\n        index : index for the new Series\n        \"\"\"\n        # Looking for NaN in dict doesn't work ({np.nan : 1}[float('nan')]\n        # raises KeyError), so we iterate the entire dict, and align\n        if data:\n            keys, values = zip(*data.items())\n            values = list(values)\n        elif index is not None:\n            # fastpath for Series(data=None). Just use broadcasting a scalar\n            # instead of reindexing.\n            values = na_value_for_dtype(dtype)\n            keys = index\n        else:\n            keys, values = [], []\n\n        # Input is now list-like, so rely on \"standard\" construction:\n\n        # TODO: passing np.float64 to not break anything yet. See GH-17261\n        s = create_series_with_explicit_dtype(\n            values, index=keys, dtype=dtype, dtype_if_empty=np.float64\n        )\n\n        # Now we just make sure the order is respected, if any\n        if data and index is not None:\n            s = s.reindex(index, copy=False)\n        return s._mgr, s.index\n\n    # ----------------------------------------------------------------------\n\n    @property\n    def _constructor(self) -> Type[\"Series\"]:\n        return Series\n\n    @property\n    def _constructor_expanddim(self) -> Type[\"DataFrame\"]:\n        from pandas.core.frame import DataFrame\n\n        return DataFrame\n\n    # types\n    @property\n    def _can_hold_na(self):\n        return self._mgr._can_hold_na\n\n    _index = None\n\n    def _set_axis(self, axis: int, labels, fastpath: bool = False) -> None:\n        \"\"\"\n        Override generic, we want to set the _typ here.\n\n        This is called from the cython code when we set the `index` attribute\n        directly, e.g. `series.index = [1, 2, 3]`.\n        \"\"\"\n        if not fastpath:\n            labels = ensure_index(labels)\n\n        is_all_dates = labels.is_all_dates\n        if is_all_dates:\n            if not isinstance(labels, (DatetimeIndex, PeriodIndex, TimedeltaIndex)):\n                try:\n                    labels = DatetimeIndex(labels)\n                    # need to set here because we changed the index\n                    if fastpath:\n                        self._mgr.set_axis(axis, labels)\n                except (tslibs.OutOfBoundsDatetime, ValueError):\n                    # labels may exceeds datetime bounds,\n                    # or not be a DatetimeIndex\n                    pass\n\n        object.__setattr__(self, \"_index\", labels)\n        if not fastpath:\n            # The ensure_index call above ensures we have an Index object\n            self._mgr.set_axis(axis, labels)\n\n    # ndarray compatibility\n    @property\n    def dtype(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._mgr.dtype\n\n    @property\n    def dtypes(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        # DataFrame compatibility\n        return self.dtype\n\n    @property\n    def name(self) -> Label:\n        \"\"\"\n        Return the name of the Series.\n\n        The name of a Series becomes its index or column name if it is used\n        to form a DataFrame. It is also used whenever displaying the Series\n        using the interpreter.\n\n        Returns\n        -------\n        label (hashable object)\n            The name of the Series, also the column name if part of a DataFrame.\n\n        See Also\n        --------\n        Series.rename : Sets the Series name when given a scalar input.\n        Index.name : Corresponding Index property.\n\n        Examples\n        --------\n        The Series name can be set initially when calling the constructor.\n\n        >>> s = pd.Series([1, 2, 3], dtype=np.int64, name='Numbers')\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Numbers, dtype: int64\n        >>> s.name = \"Integers\"\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Integers, dtype: int64\n\n        The name of a Series within a DataFrame is its column name.\n\n        >>> df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],\n        ...                   columns=[\"Odd Numbers\", \"Even Numbers\"])\n        >>> df\n           Odd Numbers  Even Numbers\n        0            1             2\n        1            3             4\n        2            5             6\n        >>> df[\"Even Numbers\"].name\n        'Even Numbers'\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value: Label) -> None:\n        if not is_hashable(value):\n            raise TypeError(\"Series.name must be a hashable type\")\n        object.__setattr__(self, \"_name\", value)\n\n    @property\n    def values(self):\n        \"\"\"\n        Return Series as ndarray or ndarray-like depending on the dtype.\n\n        .. warning::\n\n           We recommend using :attr:`Series.array` or\n           :meth:`Series.to_numpy`, depending on whether you need\n           a reference to the underlying data or a NumPy array.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n\n        See Also\n        --------\n        Series.array : Reference to the underlying data.\n        Series.to_numpy : A NumPy array representing the underlying data.\n\n        Examples\n        --------\n        >>> pd.Series([1, 2, 3]).values\n        array([1, 2, 3])\n\n        >>> pd.Series(list('aabc')).values\n        array(['a', 'a', 'b', 'c'], dtype=object)\n\n        >>> pd.Series(list('aabc')).astype('category').values\n        [a, a, b, c]\n        Categories (3, object): [a, b, c]\n\n        Timezone aware datetime data is converted to UTC:\n\n        >>> pd.Series(pd.date_range('20130101', periods=3,\n        ...                         tz='US/Eastern')).values\n        array(['2013-01-01T05:00:00.000000000',\n               '2013-01-02T05:00:00.000000000',\n               '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')\n        \"\"\"\n        return self._mgr.external_values()\n\n    @property\n    def _values(self):\n        \"\"\"\n        Return the internal repr of this data (defined by Block.interval_values).\n        This are the values as stored in the Block (ndarray or ExtensionArray\n        depending on the Block class), with datetime64[ns] and timedelta64[ns]\n        wrapped in ExtensionArrays to match Index._values behavior.\n\n        Differs from the public ``.values`` for certain data types, because of\n        historical backwards compatibility of the public attribute (e.g. period\n        returns object ndarray and datetimetz a datetime64[ns] ndarray for\n        ``.values`` while it returns an ExtensionArray for ``._values`` in those\n        cases).\n\n        Differs from ``.array`` in that this still returns the numpy array if\n        the Block is backed by a numpy array (except for datetime64 and\n        timedelta64 dtypes), while ``.array`` ensures to always return an\n        ExtensionArray.\n\n        Overview:\n\n        dtype       | values        | _values       | array         |\n        ----------- | ------------- | ------------- | ------------- |\n        Numeric     | ndarray       | ndarray       | PandasArray   |\n        Category    | Categorical   | Categorical   | Categorical   |\n        dt64[ns]    | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        dt64[ns tz] | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        td64[ns]    | ndarray[m8ns] | TimedeltaArray| ndarray[m8ns] |\n        Period      | ndarray[obj]  | PeriodArray   | PeriodArray   |\n        Nullable    | EA            | EA            | EA            |\n\n        \"\"\"\n        return self._mgr.internal_values()\n\n    @Appender(base.IndexOpsMixin.array.__doc__)  # type: ignore\n    @property\n    def array(self) -> ExtensionArray:\n        return self._mgr._block.array_values()\n\n    # ops\n    def ravel(self, order=\"C\"):\n        \"\"\"\n        Return the flattened underlying data as an ndarray.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n            Flattened data of the Series.\n\n        See Also\n        --------\n        numpy.ndarray.ravel : Return a flattened array.\n        \"\"\"\n        return self._values.ravel(order=order)\n\n    def __len__(self) -> int:\n        \"\"\"\n        Return the length of the Series.\n        \"\"\"\n        return len(self._mgr)\n\n    def view(self, dtype=None) -> \"Series\":\n        \"\"\"\n        Create a new view of the Series.\n\n        This function will return a new Series with a view of the same\n        underlying values in memory, optionally reinterpreted with a new data\n        type. The new data type must preserve the same size in bytes as to not\n        cause index misalignment.\n\n        Parameters\n        ----------\n        dtype : data type\n            Data type object or one of their string representations.\n\n        Returns\n        -------\n        Series\n            A new Series object as a view of the same data in memory.\n\n        See Also\n        --------\n        numpy.ndarray.view : Equivalent numpy function to create a new view of\n            the same data in memory.\n\n        Notes\n        -----\n        Series are instantiated with ``dtype=float64`` by default. While\n        ``numpy.ndarray.view()`` will return a view with the same data type as\n        the original array, ``Series.view()`` (without specified dtype)\n        will try using ``float64`` and may fail if the original data type size\n        in bytes is not the same.\n\n        Examples\n        --------\n        >>> s = pd.Series([-2, -1, 0, 1, 2], dtype='int8')\n        >>> s\n        0   -2\n        1   -1\n        2    0\n        3    1\n        4    2\n        dtype: int8\n\n        The 8 bit signed integer representation of `-1` is `0b11111111`, but\n        the same bytes represent 255 if read as an 8 bit unsigned integer:\n\n        >>> us = s.view('uint8')\n        >>> us\n        0    254\n        1    255\n        2      0\n        3      1\n        4      2\n        dtype: uint8\n\n        The views share the same underlying values:\n\n        >>> us[0] = 128\n        >>> s\n        0   -128\n        1     -1\n        2      0\n        3      1\n        4      2\n        dtype: int8\n        \"\"\"\n        return self._constructor(\n            self._values.view(dtype), index=self.index\n        ).__finalize__(self, method=\"view\")\n\n    # ----------------------------------------------------------------------\n    # NDArray Compat\n    _HANDLED_TYPES = (Index, ExtensionArray, np.ndarray)\n\n    def __array_ufunc__(\n        self, ufunc: Callable, method: str, *inputs: Any, **kwargs: Any\n    ):\n        # TODO: handle DataFrame\n        cls = type(self)\n\n        # for binary ops, use our custom dunder methods\n        result = ops.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        # Determine if we should defer.\n        no_defer = (np.ndarray.__array_ufunc__, cls.__array_ufunc__)\n\n        for item in inputs:\n            higher_priority = (\n                hasattr(item, \"__array_priority__\")\n                and item.__array_priority__ > self.__array_priority__\n            )\n            has_array_ufunc = (\n                hasattr(item, \"__array_ufunc__\")\n                and type(item).__array_ufunc__ not in no_defer\n                and not isinstance(item, self._HANDLED_TYPES)\n            )\n            if higher_priority or has_array_ufunc:\n                return NotImplemented\n\n        # align all the inputs.\n        names = [getattr(x, \"name\") for x in inputs if hasattr(x, \"name\")]\n        types = tuple(type(x) for x in inputs)\n        # TODO: dataframe\n        alignable = [x for x, t in zip(inputs, types) if issubclass(t, Series)]\n\n        if len(alignable) > 1:\n            # This triggers alignment.\n            # At the moment, there aren't any ufuncs with more than two inputs\n            # so this ends up just being x1.index | x2.index, but we write\n            # it to handle *args.\n            index = alignable[0].index\n            for s in alignable[1:]:\n                index |= s.index\n            inputs = tuple(\n                x.reindex(index) if issubclass(t, Series) else x\n                for x, t in zip(inputs, types)\n            )\n        else:\n            index = self.index\n\n        inputs = tuple(extract_array(x, extract_numpy=True) for x in inputs)\n        result = getattr(ufunc, method)(*inputs, **kwargs)\n\n        name = names[0] if len(set(names)) == 1 else None\n\n        def construct_return(result):\n            if lib.is_scalar(result):\n                return result\n            elif result.ndim > 1:\n                # e.g. np.subtract.outer\n                if method == \"outer\":\n                    # GH#27198\n                    raise NotImplementedError\n                return result\n            return self._constructor(result, index=index, name=name, copy=False)\n\n        if type(result) is tuple:\n            # multiple return values\n            return tuple(construct_return(x) for x in result)\n        elif method == \"at\":\n            # no return value\n            return None\n        else:\n            return construct_return(result)\n\n    def __array__(self, dtype=None) -> np.ndarray:\n        \"\"\"\n        Return the values as a NumPy array.\n\n        Users should not call this directly. Rather, it is invoked by\n        :func:`numpy.array` and :func:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to use for the resulting NumPy array. By default,\n            the dtype is inferred from the data.\n\n        Returns\n        -------\n        numpy.ndarray\n            The values in the series converted to a :class:`numpy.ndarray`\n            with the specified `dtype`.\n\n        See Also\n        --------\n        array : Create a new array from data.\n        Series.array : Zero-copy view to the array backing the Series.\n        Series.to_numpy : Series method for similar behavior.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1, 2, 3])\n        >>> np.asarray(ser)\n        array([1, 2, 3])\n\n        For timezone-aware data, the timezones may be retained with\n        ``dtype='object'``\n\n        >>> tzser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\n        >>> np.asarray(tzser, dtype=\"object\")\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET', freq='D'),\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET', freq='D')],\n              dtype=object)\n\n        Or the values may be localized to UTC and the tzinfo discarded with\n        ``dtype='datetime64[ns]'``\n\n        >>> np.asarray(tzser, dtype=\"datetime64[ns]\")  # doctest: +ELLIPSIS\n        array(['1999-12-31T23:00:00.000000000', ...],\n              dtype='datetime64[ns]')\n        \"\"\"\n        return np.asarray(self.array, dtype)\n\n    # ----------------------------------------------------------------------\n    # Unary Methods\n\n    # coercion\n    __float__ = _coerce_method(float)\n    __long__ = _coerce_method(int)\n    __int__ = _coerce_method(int)\n\n    # ----------------------------------------------------------------------\n\n    # indexers\n    @property\n    def axes(self) -> List[Index]:\n        \"\"\"\n        Return a list of the row axis labels.\n        \"\"\"\n        return [self.index]\n\n    # ----------------------------------------------------------------------\n    # Indexing Methods\n\n    @Appender(generic.NDFrame.take.__doc__)\n    def take(self, indices, axis=0, is_copy=None, **kwargs) -> \"Series\":\n        if is_copy is not None:\n            warnings.warn(\n                \"is_copy is deprecated and will be removed in a future version. \"\n                \"'take' always returns a copy, so there is no need to specify this.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n        nv.validate_take(tuple(), kwargs)\n\n        indices = ensure_platform_int(indices)\n        new_index = self.index.take(indices)\n        new_values = self._values.take(indices)\n\n        result = self._constructor(new_values, index=new_index, fastpath=True)\n        return result.__finalize__(self, method=\"take\")\n\n    def _take_with_is_copy(self, indices, axis=0):\n        \"\"\"\n        Internal version of the `take` method that sets the `_is_copy`\n        attribute to keep track of the parent dataframe (using in indexing\n        for the SettingWithCopyWarning). For Series this does the same\n        as the public take (it never sets `_is_copy`).\n\n        See the docstring of `take` for full explanation of the parameters.\n        \"\"\"\n        return self.take(indices=indices, axis=axis)\n\n    def _ixs(self, i: int, axis: int = 0):\n        \"\"\"\n        Return the i-th value or values in the Series by location.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        scalar (int) or Series (slice, sequence)\n        \"\"\"\n        return self._values[i]\n\n    def _slice(self, slobj: slice, axis: int = 0) -> \"Series\":\n        # axis kwarg is retained for compat with NDFrame method\n        #  _slice is *always* positional\n        return self._get_values(slobj)\n\n    def __getitem__(self, key):\n        key = com.apply_if_callable(key, self)\n\n        if key is Ellipsis:\n            return self\n\n        key_is_scalar = is_scalar(key)\n        if isinstance(key, (list, tuple)):\n            key = unpack_1tuple(key)\n\n        if is_integer(key) and self.index._should_fallback_to_positional():\n            return self._values[key]\n\n        elif key_is_scalar:\n            return self._get_value(key)\n\n        if (\n            isinstance(key, tuple)\n            and is_hashable(key)\n            and isinstance(self.index, MultiIndex)\n        ):\n            # Otherwise index.get_value will raise InvalidIndexError\n            try:\n                result = self._get_value(key)\n\n                return result\n\n            except KeyError:\n                # We still have the corner case where this tuple is a key\n                #  in the first level of our MultiIndex\n                return self._get_values_tuple(key)\n\n        if is_iterator(key):\n            key = list(key)\n\n        if com.is_bool_indexer(key):\n            key = check_bool_indexer(self.index, key)\n            key = np.asarray(key, dtype=bool)\n            return self._get_values(key)\n\n        return self._get_with(key)\n\n    def _get_with(self, key):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            # _convert_slice_indexer to determine if this slice is positional\n            #  or label based, and if the latter, convert to positional\n            slobj = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            return self._slice(slobj)\n        elif isinstance(key, ABCDataFrame):\n            raise TypeError(\n                \"Indexing a Series with DataFrame is not \"\n                \"supported, use the appropriate DataFrame column\"\n            )\n        elif isinstance(key, tuple):\n            return self._get_values_tuple(key)\n\n        elif not is_list_like(key):\n            # e.g. scalars that aren't recognized by lib.is_scalar, GH#32684\n            return self.loc[key]\n\n        if not isinstance(key, (list, np.ndarray, ExtensionArray, Series, Index)):\n            key = list(key)\n\n        if isinstance(key, Index):\n            key_type = key.inferred_type\n        else:\n            key_type = lib.infer_dtype(key, skipna=False)\n\n        # Note: The key_type == \"boolean\" case should be caught by the\n        #  com.is_bool_indexer check in __getitem__\n        if key_type == \"integer\":\n            # We need to decide whether to treat this as a positional indexer\n            #  (i.e. self.iloc) or label-based (i.e. self.loc)\n            if not self.index._should_fallback_to_positional():\n                return self.loc[key]\n            else:\n                return self.iloc[key]\n\n        # handle the dup indexing case GH#4246\n        return self.loc[key]\n\n    def _get_values_tuple(self, key):\n        # mpl hackaround\n        if com.any_none(*key):\n            # suppress warning from slicing the index with a 2d indexer.\n            # eventually we'll want Series itself to warn.\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\", \"Support for multi-dim\", DeprecationWarning\n                )\n                return self._get_values(key)\n\n        if not isinstance(self.index, MultiIndex):\n            raise ValueError(\"Can only tuple-index with a MultiIndex\")\n\n        # If key is contained, would have returned by now\n        indexer, new_index = self.index.get_loc_level(key)\n        return self._constructor(self._values[indexer], index=new_index).__finalize__(\n            self,\n        )\n\n    def _get_values(self, indexer):\n        try:\n            return self._constructor(self._mgr.get_slice(indexer)).__finalize__(self,)\n        except ValueError:\n            # mpl compat if we look up e.g. ser[:, np.newaxis];\n            #  see tests.series.timeseries.test_mpl_compat_hack\n            return self._values[indexer]\n\n    def _get_value(self, label, takeable: bool = False):\n        \"\"\"\n        Quickly retrieve single value at passed index label.\n\n        Parameters\n        ----------\n        label : object\n        takeable : interpret the index as indexers, default False\n\n        Returns\n        -------\n        scalar value\n        \"\"\"\n        if takeable:\n            return self._values[label]\n\n        # Similar to Index.get_value, but we do not fall back to positional\n        loc = self.index.get_loc(label)\n        return self.index._get_values_for_loc(self, loc, label)\n\n    def __setitem__(self, key, value):\n        key = com.apply_if_callable(key, self)\n        cacher_needs_updating = self._check_is_chained_assignment_possible()\n\n        if key is Ellipsis:\n            key = slice(None)\n\n        try:\n            self._set_with_engine(key, value)\n        except (KeyError, ValueError):\n            values = self._values\n            if is_integer(key) and not self.index.inferred_type == \"integer\":\n                # positional setter\n                values[key] = value\n            else:\n                # GH#12862 adding an new key to the Series\n                self.loc[key] = value\n\n        except TypeError as e:\n            if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):\n                raise ValueError(\"Can only tuple-index with a MultiIndex\") from e\n\n            if com.is_bool_indexer(key):\n                key = check_bool_indexer(self.index, key)\n                key = np.asarray(key, dtype=bool)\n                try:\n                    self._where(~key, value, inplace=True)\n                except InvalidIndexError:\n                    self.iloc[key] = value\n                return\n\n            else:\n                self._set_with(key, value)\n\n        if cacher_needs_updating:\n            self._maybe_update_cacher()\n\n    def _set_with_engine(self, key, value):\n        # fails with AttributeError for IntervalIndex\n        loc = self.index._engine.get_loc(key)\n        validate_numeric_casting(self.dtype, value)\n        self._values[loc] = value\n\n    def _set_with(self, key, value):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            # extract_array so that if we set e.g. ser[-5:] = ser[:5]\n            #  we get the first five values, and not 5 NaNs\n            indexer = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            self.iloc[indexer] = extract_array(value, extract_numpy=True)\n\n        else:\n            assert not isinstance(key, tuple)\n\n            if is_scalar(key):\n                key = [key]\n\n            if isinstance(key, Index):\n                key_type = key.inferred_type\n                key = key._values\n            else:\n                key_type = lib.infer_dtype(key, skipna=False)\n\n            # Note: key_type == \"boolean\" should not occur because that\n            #  should be caught by the is_bool_indexer check in __setitem__\n            if key_type == \"integer\":\n                if not self.index._should_fallback_to_positional():\n                    self.loc[key] = value\n                else:\n                    self.iloc[key] = value\n            else:\n                self.loc[key] = value\n\n    def _set_value(self, label, value, takeable: bool = False):\n        \"\"\"\n        Quickly set single value at passed label.\n\n        If label is not contained, a new object is created with the label\n        placed at the end of the result index.\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed.\n        value : object\n            Scalar value.\n        takeable : interpret the index as indexers, default False\n        \"\"\"\n        try:\n            if takeable:\n                self._values[label] = value\n            else:\n                loc = self.index.get_loc(label)\n                validate_numeric_casting(self.dtype, value)\n                self._values[loc] = value\n        except KeyError:\n\n            # set using a non-recursive method\n            self.loc[label] = value\n\n    # ----------------------------------------------------------------------\n    # Unsorted\n\n    @property\n    def _is_mixed_type(self):\n        return False\n\n    def repeat(self, repeats, axis=None) -> \"Series\":\n        \"\"\"\n        Repeat elements of a Series.\n\n        Returns a new Series where each element of the current Series\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            Series.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        Series\n            Newly created Series with repeated elements.\n\n        See Also\n        --------\n        Index.repeat : Equivalent function for Index.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s\n        0    a\n        1    b\n        2    c\n        dtype: object\n        >>> s.repeat(2)\n        0    a\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        dtype: object\n        >>> s.repeat([1, 2, 3])\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        2    c\n        dtype: object\n        \"\"\"\n        nv.validate_repeat(tuple(), dict(axis=axis))\n        new_index = self.index.repeat(repeats)\n        new_values = self._values.repeat(repeats)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"repeat\"\n        )\n\n    def reset_index(self, level=None, drop=False, name=None, inplace=False):\n        \"\"\"\n        Generate a new DataFrame or Series with the index reset.\n\n        This is useful when the index needs to be treated as a column, or\n        when the index is meaningless and needs to be reset to the default\n        before another operation.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default optional\n            For a Series with a MultiIndex, only remove the specified levels\n            from the index. Removes all levels by default.\n        drop : bool, default False\n            Just reset the index, without inserting it as a column in\n            the new DataFrame.\n        name : object, optional\n            The name to use for the column containing the original Series\n            values. Uses ``self.name`` by default. This argument is ignored\n            when `drop` is True.\n        inplace : bool, default False\n            Modify the Series in place (do not create a new object).\n\n        Returns\n        -------\n        Series or DataFrame\n            When `drop` is False (the default), a DataFrame is returned.\n            The newly created columns will come first in the DataFrame,\n            followed by the original Series values.\n            When `drop` is True, a `Series` is returned.\n            In either case, if ``inplace=True``, no value is returned.\n\n        See Also\n        --------\n        DataFrame.reset_index: Analogous function for DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4], name='foo',\n        ...               index=pd.Index(['a', 'b', 'c', 'd'], name='idx'))\n\n        Generate a DataFrame with default index.\n\n        >>> s.reset_index()\n          idx  foo\n        0   a    1\n        1   b    2\n        2   c    3\n        3   d    4\n\n        To specify the name of the new column use `name`.\n\n        >>> s.reset_index(name='values')\n          idx  values\n        0   a       1\n        1   b       2\n        2   c       3\n        3   d       4\n\n        To generate a new Series with the default set `drop` to True.\n\n        >>> s.reset_index(drop=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        To update the Series in place, without generating a new one\n        set `inplace` to True. Note that it also requires ``drop=True``.\n\n        >>> s.reset_index(inplace=True, drop=True)\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        The `level` parameter is interesting for Series with a multi-level\n        index.\n\n        >>> arrays = [np.array(['bar', 'bar', 'baz', 'baz']),\n        ...           np.array(['one', 'two', 'one', 'two'])]\n        >>> s2 = pd.Series(\n        ...     range(4), name='foo',\n        ...     index=pd.MultiIndex.from_arrays(arrays,\n        ...                                     names=['a', 'b']))\n\n        To remove a specific level from the Index, use `level`.\n\n        >>> s2.reset_index(level='a')\n               a  foo\n        b\n        one  bar    0\n        two  bar    1\n        one  baz    2\n        two  baz    3\n\n        If `level` is not set, all levels are removed from the Index.\n\n        >>> s2.reset_index()\n             a    b  foo\n        0  bar  one    0\n        1  bar  two    1\n        2  baz  one    2\n        3  baz  two    3\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if drop:\n            new_index = ibase.default_index(len(self))\n            if level is not None:\n                if not isinstance(level, (tuple, list)):\n                    level = [level]\n                level = [self.index._get_level_number(lev) for lev in level]\n                if len(level) < self.index.nlevels:\n                    new_index = self.index.droplevel(level)\n\n            if inplace:\n                self.index = new_index\n                # set name if it was passed, otherwise, keep the previous name\n                self.name = name or self.name\n            else:\n                return self._constructor(\n                    self._values.copy(), index=new_index\n                ).__finalize__(self, method=\"reset_index\")\n        elif inplace:\n            raise TypeError(\n                \"Cannot reset_index inplace on a Series to create a DataFrame\"\n            )\n        else:\n            df = self.to_frame(name)\n            return df.reset_index(level=level, drop=drop)\n\n    # ----------------------------------------------------------------------\n    # Rendering Methods\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular Series.\n        \"\"\"\n        buf = StringIO(\"\")\n        width, height = get_terminal_size()\n        max_rows = (\n            height\n            if get_option(\"display.max_rows\") == 0\n            else get_option(\"display.max_rows\")\n        )\n        min_rows = (\n            height\n            if get_option(\"display.max_rows\") == 0\n            else get_option(\"display.min_rows\")\n        )\n        show_dimensions = get_option(\"display.show_dimensions\")\n\n        self.to_string(\n            buf=buf,\n            name=self.name,\n            dtype=self.dtype,\n            min_rows=min_rows,\n            max_rows=max_rows,\n            length=show_dimensions,\n        )\n        result = buf.getvalue()\n\n        return result\n\n    def to_string(\n        self,\n        buf=None,\n        na_rep=\"NaN\",\n        float_format=None,\n        header=True,\n        index=True,\n        length=False,\n        dtype=False,\n        name=False,\n        max_rows=None,\n        min_rows=None,\n    ):\n        \"\"\"\n        Render a string representation of the Series.\n\n        Parameters\n        ----------\n        buf : StringIO-like, optional\n            Buffer to write to.\n        na_rep : str, optional\n            String representation of NaN to use, default 'NaN'.\n        float_format : one-parameter function, optional\n            Formatter function to apply to columns' elements if they are\n            floats, default None.\n        header : bool, default True\n            Add the Series header (index name).\n        index : bool, optional\n            Add index (row) labels, default True.\n        length : bool, default False\n            Add the Series length.\n        dtype : bool, default False\n            Add the Series dtype.\n        name : bool, default False\n            Add the Series name if not None.\n        max_rows : int, optional\n            Maximum number of rows to show before truncating. If None, show\n            all.\n        min_rows : int, optional\n            The number of rows to display in a truncated repr (when number\n            of rows is above `max_rows`).\n\n        Returns\n        -------\n        str or None\n            String representation of Series if ``buf=None``, otherwise None.\n        \"\"\"\n        formatter = fmt.SeriesFormatter(\n            self,\n            name=name,\n            length=length,\n            header=header,\n            index=index,\n            dtype=dtype,\n            na_rep=na_rep,\n            float_format=float_format,\n            min_rows=min_rows,\n            max_rows=max_rows,\n        )\n        result = formatter.to_string()\n\n        # catch contract violations\n        if not isinstance(result, str):\n            raise AssertionError(\n                \"result must be of type str, type \"\n                f\"of result is {repr(type(result).__name__)}\"\n            )\n\n        if buf is None:\n            return result\n        else:\n            try:\n                buf.write(result)\n            except AttributeError:\n                with open(buf, \"w\") as f:\n                    f.write(result)\n\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> s = pd.Series([\"elk\", \"pig\", \"dog\", \"quetzal\"], name=\"animal\")\n        >>> print(s.to_markdown())\n        |    | animal   |\n        |---:|:---------|\n        |  0 | elk      |\n        |  1 | pig      |\n        |  2 | dog      |\n        |  3 | quetzal  |\n        \"\"\"\n    )\n    @Substitution(klass=\"Series\")\n    @Appender(generic._shared_docs[\"to_markdown\"])\n    def to_markdown(\n        self, buf: Optional[IO[str]] = None, mode: Optional[str] = None, **kwargs\n    ) -> Optional[str]:\n        return self.to_frame().to_markdown(buf, mode, **kwargs)\n\n    # ----------------------------------------------------------------------\n\n    def items(self) -> Iterable[Tuple[Label, Any]]:\n        \"\"\"\n        Lazily iterate over (index, value) tuples.\n\n        This method returns an iterable tuple (index, value). This is\n        convenient if you want to create a lazy iterator.\n\n        Returns\n        -------\n        iterable\n            Iterable of tuples containing the (index, value) pairs from a\n            Series.\n\n        See Also\n        --------\n        DataFrame.items : Iterate over (column name, Series) pairs.\n        DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs.\n\n        Examples\n        --------\n        >>> s = pd.Series(['A', 'B', 'C'])\n        >>> for index, value in s.items():\n        ...     print(f\"Index : {index}, Value : {value}\")\n        Index : 0, Value : A\n        Index : 1, Value : B\n        Index : 2, Value : C\n        \"\"\"\n        return zip(iter(self.index), iter(self))\n\n    @Appender(items.__doc__)\n    def iteritems(self) -> Iterable[Tuple[Label, Any]]:\n        return self.items()\n\n    # ----------------------------------------------------------------------\n    # Misc public methods\n\n    def keys(self) -> Index:\n        \"\"\"\n        Return alias for index.\n\n        Returns\n        -------\n        Index\n            Index of the Series.\n        \"\"\"\n        return self.index\n\n    def to_dict(self, into=dict):\n        \"\"\"\n        Convert Series to {label -> value} dict or dict-like object.\n\n        Parameters\n        ----------\n        into : class, default dict\n            The collections.abc.Mapping subclass to use as the return\n            object. Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Key-value representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.to_dict()\n        {0: 1, 1: 2, 2: 3, 3: 4}\n        >>> from collections import OrderedDict, defaultdict\n        >>> s.to_dict(OrderedDict)\n        OrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])\n        >>> dd = defaultdict(list)\n        >>> s.to_dict(dd)\n        defaultdict(<class 'list'>, {0: 1, 1: 2, 2: 3, 3: 4})\n        \"\"\"\n        # GH16122\n        into_c = com.standardize_mapping(into)\n        return into_c(self.items())\n\n    def to_frame(self, name=None) -> \"DataFrame\":\n        \"\"\"\n        Convert Series to DataFrame.\n\n        Parameters\n        ----------\n        name : object, default None\n            The passed name should substitute for the series name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([\"a\", \"b\", \"c\"],\n        ...               name=\"vals\")\n        >>> s.to_frame()\n          vals\n        0    a\n        1    b\n        2    c\n        \"\"\"\n        if name is None:\n            df = self._constructor_expanddim(self)\n        else:\n            df = self._constructor_expanddim({name: self})\n\n        return df\n\n    def _set_name(self, name, inplace=False) -> \"Series\":\n        \"\"\"\n        Set the Series name.\n\n        Parameters\n        ----------\n        name : str\n        inplace : bool\n            Whether to modify `self` directly or return a copy.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        ser = self if inplace else self.copy()\n        ser.name = name\n        return ser\n\n    @Appender(\n        \"\"\"\nExamples\n--------\n>>> ser = pd.Series([390., 350., 30., 20.],\n...                 index=['Falcon', 'Falcon', 'Parrot', 'Parrot'], name=\"Max Speed\")\n>>> ser\nFalcon    390.0\nFalcon    350.0\nParrot     30.0\nParrot     20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby([\"a\", \"b\", \"a\", \"b\"]).mean()\na    210.0\nb    185.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(ser > 100).mean()\nMax Speed\nFalse     25.0\nTrue     370.0\nName: Max Speed, dtype: float64\n\n**Grouping by Indexes**\n\nWe can groupby different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> ser = pd.Series([390., 350., 30., 20.], index=index, name=\"Max Speed\")\n>>> ser\nAnimal  Type\nFalcon  Captive    390.0\n        Wild       350.0\nParrot  Captive     30.0\n        Wild        20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nAnimal\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=\"Type\").mean()\nType\nCaptive    210.0\nWild       185.0\nName: Max Speed, dtype: float64\n\nWe can also choose to include `NA` in group keys or not by defining\n`dropna` parameter, the default setting is `True`:\n\n>>> ser = pd.Series([1, 2, 3, 3], index=[\"a\", 'a', 'b', np.nan])\n>>> ser.groupby(level=0).sum()\na    3\nb    3\ndtype: int64\n\n>>> ser.groupby(level=0, dropna=False).sum()\na    3\nb    3\nNaN  3\ndtype: int64\n\n>>> arrays = ['Falcon', 'Falcon', 'Parrot', 'Parrot']\n>>> ser = pd.Series([390., 350., 30., 20.], index=arrays, name=\"Max Speed\")\n>>> ser.groupby([\"a\", \"b\", \"a\", np.nan]).mean()\na    210.0\nb    350.0\nName: Max Speed, dtype: float64\n\n>>> ser.groupby([\"a\", \"b\", \"a\", np.nan], dropna=False).mean()\na    210.0\nb    350.0\nNaN   20.0\nName: Max Speed, dtype: float64\n\"\"\"\n    )\n    @Appender(generic._shared_docs[\"groupby\"] % _shared_doc_kwargs)\n    def groupby(\n        self,\n        by=None,\n        axis=0,\n        level=None,\n        as_index: bool = True,\n        sort: bool = True,\n        group_keys: bool = True,\n        squeeze: bool = False,\n        observed: bool = False,\n        dropna: bool = True,\n    ) -> \"SeriesGroupBy\":\n        from pandas.core.groupby.generic import SeriesGroupBy\n\n        if level is None and by is None:\n            raise TypeError(\"You have to supply one of 'by' and 'level'\")\n        axis = self._get_axis_number(axis)\n\n        return SeriesGroupBy(\n            obj=self,\n            keys=by,\n            axis=axis,\n            level=level,\n            as_index=as_index,\n            sort=sort,\n            group_keys=group_keys,\n            squeeze=squeeze,\n            observed=observed,\n            dropna=dropna,\n        )\n\n    # ----------------------------------------------------------------------\n    # Statistics, overridden ndarray methods\n\n    # TODO: integrate bottleneck\n\n    def count(self, level=None):\n        \"\"\"\n        Return number of non-NA/null observations in the Series.\n\n        Parameters\n        ----------\n        level : int or level name, default None\n            If the axis is a MultiIndex (hierarchical), count along a\n            particular level, collapsing into a smaller Series.\n\n        Returns\n        -------\n        int or Series (if level specified)\n            Number of non-null values in the Series.\n\n        See Also\n        --------\n        DataFrame.count : Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.0, 1.0, np.nan])\n        >>> s.count()\n        2\n        \"\"\"\n        if level is None:\n            return notna(self.array).sum()\n\n        if isinstance(level, str):\n            level = self.index._get_level_number(level)\n\n        lev = self.index.levels[level]\n        level_codes = np.array(self.index.codes[level], subok=False, copy=True)\n\n        mask = level_codes == -1\n        if mask.any():\n            level_codes[mask] = cnt = len(lev)\n            lev = lev.insert(cnt, lev._na_value)\n\n        obs = level_codes[notna(self._values)]\n        out = np.bincount(obs, minlength=len(lev) or None)\n        return self._constructor(out, index=lev, dtype=\"int64\").__finalize__(\n            self, method=\"count\"\n        )\n\n    def mode(self, dropna=True) -> \"Series\":\n        \"\"\"\n        Return the mode(s) of the dataset.\n\n        Always returns Series even if only one value is returned.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        Series\n            Modes of the Series in sorted order.\n        \"\"\"\n        # TODO: Add option for bins like value_counts()\n        return algorithms.mode(self, dropna=dropna)\n\n    def unique(self):\n        \"\"\"\n        Return unique values of Series object.\n\n        Uniques are returned in order of appearance. Hash table-based unique,\n        therefore does NOT sort.\n\n        Returns\n        -------\n        ndarray or ExtensionArray\n            The unique values returned as a NumPy array. See Notes.\n\n        See Also\n        --------\n        unique : Top-level unique method for any 1-d array-like object.\n        Index.unique : Return Index with unique values from an Index object.\n\n        Notes\n        -----\n        Returns the unique values as a NumPy array. In case of an\n        extension-array backed Series, a new\n        :class:`~api.extensions.ExtensionArray` of that type with just\n        the unique values is returned. This includes\n\n            * Categorical\n            * Period\n            * Datetime with Timezone\n            * Interval\n            * Sparse\n            * IntegerNA\n\n        See Examples section.\n\n        Examples\n        --------\n        >>> pd.Series([2, 1, 3, 3], name='A').unique()\n        array([2, 1, 3])\n\n        >>> pd.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()\n        array(['2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\n\n        >>> pd.Series([pd.Timestamp('2016-01-01', tz='US/Eastern')\n        ...            for _ in range(3)]).unique()\n        <DatetimeArray>\n        ['2016-01-01 00:00:00-05:00']\n        Length: 1, dtype: datetime64[ns, US/Eastern]\n\n        An unordered Categorical will return categories in the order of\n        appearance.\n\n        >>> pd.Series(pd.Categorical(list('baabc'))).unique()\n        [b, a, c]\n        Categories (3, object): [b, a, c]\n\n        An ordered Categorical preserves the category ordering.\n\n        >>> pd.Series(pd.Categorical(list('baabc'), categories=list('abc'),\n        ...                          ordered=True)).unique()\n        [b, a, c]\n        Categories (3, object): [a < b < c]\n        \"\"\"\n        result = super().unique()\n        return result\n\n    def drop_duplicates(self, keep=\"first\", inplace=False) -> Optional[\"Series\"]:\n        \"\"\"\n        Return Series with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        inplace : bool, default ``False``\n            If ``True``, performs operation inplace and returns None.\n\n        Returns\n        -------\n        Series\n            Series with duplicates dropped.\n\n        See Also\n        --------\n        Index.drop_duplicates : Equivalent method on Index.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Series.duplicated : Related method on Series, indicating duplicate\n            Series values.\n\n        Examples\n        --------\n        Generate a Series with duplicated entries.\n\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'],\n        ...               name='animal')\n        >>> s\n        0      lama\n        1       cow\n        2      lama\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        With the 'keep' parameter, the selection behaviour of duplicated values\n        can be changed. The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> s.drop_duplicates()\n        0      lama\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n\n        The value 'last' for parameter 'keep' keeps the last occurrence for\n        each set of duplicated entries.\n\n        >>> s.drop_duplicates(keep='last')\n        1       cow\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        The value ``False`` for parameter 'keep' discards all sets of\n        duplicated entries. Setting the value of 'inplace' to ``True`` performs\n        the operation inplace and returns ``None``.\n\n        >>> s.drop_duplicates(keep=False, inplace=True)\n        >>> s\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        result = super().drop_duplicates(keep=keep)\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    def duplicated(self, keep=\"first\") -> \"Series\":\n        \"\"\"\n        Indicate duplicate Series values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        Series. Either all duplicates, all except the first or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series\n            Series indicating whether each value has occurred in the\n            preceding values.\n\n        See Also\n        --------\n        Index.duplicated : Equivalent method on pandas.Index.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Series.drop_duplicates : Remove duplicate values from Series.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set on False and all others on True:\n\n        >>> animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> animals.duplicated()\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        which is equivalent to\n\n        >>> animals.duplicated(keep='first')\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> animals.duplicated(keep='last')\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> animals.duplicated(keep=False)\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n        \"\"\"\n        return super().duplicated(keep=keep)\n\n    def idxmin(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the minimum value.\n\n        If multiple values equal the minimum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmin. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the minimum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmin : Return indices of the minimum values\n            along the given axis.\n        DataFrame.idxmin : Return index of first occurrence of minimum\n            over requested axis.\n        Series.idxmax : Return index *label* of the first occurrence\n            of maximum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmin``. This method\n        returns the label of the minimum, while ``ndarray.argmin`` returns\n        the position. To get the position, use ``series.values.argmin()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 1],\n        ...               index=['A', 'B', 'C', 'D'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    1.0\n        dtype: float64\n\n        >>> s.idxmin()\n        'A'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmin(skipna=False)\n        nan\n        \"\"\"\n        skipna = nv.validate_argmin_with_skipna(skipna, args, kwargs)\n        i = nanops.nanargmin(self._values, skipna=skipna)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def idxmax(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the maximum value.\n\n        If multiple values equal the maximum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmax. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the maximum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmax : Return indices of the maximum values\n            along the given axis.\n        DataFrame.idxmax : Return index of first occurrence of maximum\n            over requested axis.\n        Series.idxmin : Return index *label* of the first occurrence\n            of minimum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmax``. This method\n        returns the label of the maximum, while ``ndarray.argmax`` returns\n        the position. To get the position, use ``series.values.argmax()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 3, 4],\n        ...               index=['A', 'B', 'C', 'D', 'E'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    3.0\n        E    4.0\n        dtype: float64\n\n        >>> s.idxmax()\n        'C'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmax(skipna=False)\n        nan\n        \"\"\"\n        skipna = nv.validate_argmax_with_skipna(skipna, args, kwargs)\n        i = nanops.nanargmax(self._values, skipna=skipna)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def round(self, decimals=0, *args, **kwargs) -> \"Series\":\n        \"\"\"\n        Round each value in a Series to the given number of decimals.\n\n        Parameters\n        ----------\n        decimals : int, default 0\n            Number of decimal places to round to. If decimals is negative,\n            it specifies the number of positions to the left of the decimal point.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Series\n            Rounded values of the Series.\n\n        See Also\n        --------\n        numpy.around : Round values of an np.array.\n        DataFrame.round : Round values of a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.1, 1.3, 2.7])\n        >>> s.round()\n        0    0.0\n        1    1.0\n        2    3.0\n        dtype: float64\n        \"\"\"\n        nv.validate_round(args, kwargs)\n        result = self._values.round(decimals)\n        result = self._constructor(result, index=self.index).__finalize__(\n            self, method=\"round\"\n        )\n\n        return result\n\n    def quantile(self, q=0.5, interpolation=\"linear\"):\n        \"\"\"\n        Return value at the given quantile.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            The quantile(s) to compute, which can lie in range: 0 <= q <= 1.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            This optional parameter specifies the interpolation method to use,\n            when the desired quantile lies between two data points `i` and `j`:\n\n                * linear: `i + (j - i) * fraction`, where `fraction` is the\n                  fractional part of the index surrounded by `i` and `j`.\n                * lower: `i`.\n                * higher: `j`.\n                * nearest: `i` or `j` whichever is nearest.\n                * midpoint: (`i` + `j`) / 2.\n\n        Returns\n        -------\n        float or Series\n            If ``q`` is an array, a Series will be returned where the\n            index is ``q`` and the values are the quantiles, otherwise\n            a float will be returned.\n\n        See Also\n        --------\n        core.window.Rolling.quantile : Calculate the rolling quantile.\n        numpy.percentile : Returns the q-th percentile(s) of the array elements.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.quantile(.5)\n        2.5\n        >>> s.quantile([.25, .5, .75])\n        0.25    1.75\n        0.50    2.50\n        0.75    3.25\n        dtype: float64\n        \"\"\"\n        validate_percentile(q)\n\n        # We dispatch to DataFrame so that core.internals only has to worry\n        #  about 2D cases.\n        df = self.to_frame()\n\n        result = df.quantile(q=q, interpolation=interpolation, numeric_only=False)\n        if result.ndim == 2:\n            result = result.iloc[:, 0]\n\n        if is_list_like(q):\n            result.name = self.name\n            return self._constructor(result, index=Float64Index(q), name=self.name)\n        else:\n            # scalar\n            return result.iloc[0]\n\n    def corr(self, other, method=\"pearson\", min_periods=None) -> float:\n        \"\"\"\n        Compute correlation with `other` Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the correlation.\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method used to compute correlation:\n\n            - pearson : Standard correlation coefficient\n            - kendall : Kendall Tau correlation coefficient\n            - spearman : Spearman rank correlation\n            - callable: Callable with input two 1d ndarrays and returning a float.\n\n            .. versionadded:: 0.24.0\n                Note that the returned matrix from corr will have 1 along the\n                diagonals and will be symmetric regardless of the callable's\n                behavior.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n\n        Returns\n        -------\n        float\n            Correlation with other.\n\n        See Also\n        --------\n        DataFrame.corr : Compute pairwise correlation between columns.\n        DataFrame.corrwith : Compute pairwise correlation with another\n            DataFrame or Series.\n\n        Examples\n        --------\n        >>> def histogram_intersection(a, b):\n        ...     v = np.minimum(a, b).sum().round(decimals=1)\n        ...     return v\n        >>> s1 = pd.Series([.2, .0, .6, .2])\n        >>> s2 = pd.Series([.3, .6, .0, .1])\n        >>> s1.corr(s2, method=histogram_intersection)\n        0.3\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n\n        if method in [\"pearson\", \"spearman\", \"kendall\"] or callable(method):\n            return nanops.nancorr(\n                this.values, other.values, method=method, min_periods=min_periods\n            )\n\n        raise ValueError(\n            \"method must be either 'pearson', \"\n            \"'spearman', 'kendall', or a callable, \"\n            f\"'{method}' was supplied\"\n        )\n\n    def cov(self, other, min_periods=None) -> float:\n        \"\"\"\n        Compute covariance with Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the covariance.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n\n        Returns\n        -------\n        float\n            Covariance between Series and other normalized by N-1\n            (unbiased estimator).\n\n        See Also\n        --------\n        DataFrame.cov : Compute pairwise covariance of columns.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([0.90010907, 0.13484424, 0.62036035])\n        >>> s2 = pd.Series([0.12528585, 0.26962463, 0.51111198])\n        >>> s1.cov(s2)\n        -0.01685762652715874\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancov(this.values, other.values, min_periods=min_periods)\n\n    def diff(self, periods: int = 1) -> \"Series\":\n        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of a Series element compared with another\n        element in the Series (default is element in previous row).\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative\n            values.\n\n        Returns\n        -------\n        Series\n            First differences of the Series.\n\n        See Also\n        --------\n        Series.pct_change: Percent change over given number of periods.\n        Series.shift: Shift index by desired number of periods with an\n            optional time freq.\n        DataFrame.diff: First discrete difference of object.\n\n        Notes\n        -----\n        For boolean dtypes, this uses :meth:`operator.xor` rather than\n        :meth:`operator.sub`.\n\n        Examples\n        --------\n        Difference with previous row\n\n        >>> s = pd.Series([1, 1, 2, 3, 5, 8])\n        >>> s.diff()\n        0    NaN\n        1    0.0\n        2    1.0\n        3    1.0\n        4    2.0\n        5    3.0\n        dtype: float64\n\n        Difference with 3rd previous row\n\n        >>> s.diff(periods=3)\n        0    NaN\n        1    NaN\n        2    NaN\n        3    2.0\n        4    4.0\n        5    6.0\n        dtype: float64\n\n        Difference with following row\n\n        >>> s.diff(periods=-1)\n        0    0.0\n        1   -1.0\n        2   -1.0\n        3   -2.0\n        4   -3.0\n        5    NaN\n        dtype: float64\n        \"\"\"\n        result = algorithms.diff(self.array, periods)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"diff\"\n        )\n\n    def autocorr(self, lag=1) -> float:\n        \"\"\"\n        Compute the lag-N autocorrelation.\n\n        This method computes the Pearson correlation between\n        the Series and its shifted self.\n\n        Parameters\n        ----------\n        lag : int, default 1\n            Number of lags to apply before performing autocorrelation.\n\n        Returns\n        -------\n        float\n            The Pearson correlation between self and self.shift(lag).\n\n        See Also\n        --------\n        Series.corr : Compute the correlation between two Series.\n        Series.shift : Shift index by desired number of periods.\n        DataFrame.corr : Compute pairwise correlation of columns.\n        DataFrame.corrwith : Compute pairwise correlation between rows or\n            columns of two DataFrame objects.\n\n        Notes\n        -----\n        If the Pearson correlation is not well defined return 'NaN'.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.25, 0.5, 0.2, -0.05])\n        >>> s.autocorr()  # doctest: +ELLIPSIS\n        0.10355...\n        >>> s.autocorr(lag=2)  # doctest: +ELLIPSIS\n        -0.99999...\n\n        If the Pearson correlation is not well defined, then 'NaN' is returned.\n\n        >>> s = pd.Series([1, 0, 0, 0])\n        >>> s.autocorr()\n        nan\n        \"\"\"\n        return self.corr(self.shift(lag))\n\n    def dot(self, other):\n        \"\"\"\n        Compute the dot product between the Series and the columns of other.\n\n        This method computes the dot product between the Series and another\n        one, or the Series and each columns of a DataFrame, or the Series and\n        each columns of an array.\n\n        It can also be called using `self @ other` in Python >= 3.5.\n\n        Parameters\n        ----------\n        other : Series, DataFrame or array-like\n            The other object to compute the dot product with its columns.\n\n        Returns\n        -------\n        scalar, Series or numpy.ndarray\n            Return the dot product of the Series and other if other is a\n            Series, the Series of the dot product of Series and each rows of\n            other if other is a DataFrame or a numpy.ndarray between the Series\n            and each columns of the numpy array.\n\n        See Also\n        --------\n        DataFrame.dot: Compute the matrix product with the DataFrame.\n        Series.mul: Multiplication of series and other, element-wise.\n\n        Notes\n        -----\n        The Series and other has to share the same index if other is a Series\n        or a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0, 1, 2, 3])\n        >>> other = pd.Series([-1, 2, -3, 4])\n        >>> s.dot(other)\n        8\n        >>> s @ other\n        8\n        >>> df = pd.DataFrame([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(df)\n        0    24\n        1    14\n        dtype: int64\n        >>> arr = np.array([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(arr)\n        array([24, 14])\n        \"\"\"\n        if isinstance(other, (Series, ABCDataFrame)):\n            common = self.index.union(other.index)\n            if len(common) > len(self.index) or len(common) > len(other.index):\n                raise ValueError(\"matrices are not aligned\")\n\n            left = self.reindex(index=common, copy=False)\n            right = other.reindex(index=common, copy=False)\n            lvals = left.values\n            rvals = right.values\n        else:\n            lvals = self.values\n            rvals = np.asarray(other)\n            if lvals.shape[0] != rvals.shape[0]:\n                raise Exception(\n                    f\"Dot product shape mismatch, {lvals.shape} vs {rvals.shape}\"\n                )\n\n        if isinstance(other, ABCDataFrame):\n            return self._constructor(\n                np.dot(lvals, rvals), index=other.columns\n            ).__finalize__(self, method=\"dot\")\n        elif isinstance(other, Series):\n            return np.dot(lvals, rvals)\n        elif isinstance(rvals, np.ndarray):\n            return np.dot(lvals, rvals)\n        else:  # pragma: no cover\n            raise TypeError(f\"unsupported type: {type(other)}\")\n\n    def __matmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(other)\n\n    def __rmatmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(np.transpose(other))\n\n    @doc(base.IndexOpsMixin.searchsorted, klass=\"Series\")\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        return algorithms.searchsorted(self._values, value, side=side, sorter=sorter)\n\n    # -------------------------------------------------------------------\n    # Combination\n\n    def append(self, to_append, ignore_index=False, verify_integrity=False):\n        \"\"\"\n        Concatenate two or more Series.\n\n        Parameters\n        ----------\n        to_append : Series or list/tuple of Series\n            Series to append with self.\n        ignore_index : bool, default False\n            If True, do not use the index labels.\n        verify_integrity : bool, default False\n            If True, raise Exception on creating index with duplicates.\n\n        Returns\n        -------\n        Series\n            Concatenated Series.\n\n        See Also\n        --------\n        concat : General function to concatenate DataFrame or Series objects.\n\n        Notes\n        -----\n        Iteratively appending to a Series can be more computationally intensive\n        than a single concatenate. A better solution is to append values to a\n        list and then concatenate the list with the original Series all at\n        once.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, 2, 3])\n        >>> s2 = pd.Series([4, 5, 6])\n        >>> s3 = pd.Series([4, 5, 6], index=[3, 4, 5])\n        >>> s1.append(s2)\n        0    1\n        1    2\n        2    3\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s1.append(s3)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `ignore_index` set to True:\n\n        >>> s1.append(s2, ignore_index=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `verify_integrity` set to True:\n\n        >>> s1.append(s2, verify_integrity=True)\n        Traceback (most recent call last):\n        ...\n        ValueError: Indexes have overlapping values: [0, 1, 2]\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        if isinstance(to_append, (list, tuple)):\n            to_concat = [self]\n            to_concat.extend(to_append)\n        else:\n            to_concat = [self, to_append]\n        if any(isinstance(x, (ABCDataFrame,)) for x in to_concat[1:]):\n            msg = \"to_append should be a Series or list/tuple of Series, got DataFrame\"\n            raise TypeError(msg)\n        return concat(\n            to_concat, ignore_index=ignore_index, verify_integrity=verify_integrity\n        )\n\n    def _binop(self, other, func, level=None, fill_value=None):\n        \"\"\"\n        Perform generic binary operation with optional fill value.\n\n        Parameters\n        ----------\n        other : Series\n        func : binary operator\n        fill_value : float or object\n            Value to substitute for NA/null values. If both Series are NA in a\n            location, the result will be NA regardless of the passed fill value.\n        level : int or level name, default None\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level.\n\n        Returns\n        -------\n        Series\n        \"\"\"\n        if not isinstance(other, Series):\n            raise AssertionError(\"Other operand must be Series\")\n\n        this = self\n\n        if not self.index.equals(other.index):\n            this, other = self.align(other, level=level, join=\"outer\", copy=False)\n\n        this_vals, other_vals = ops.fill_binop(this.values, other.values, fill_value)\n\n        with np.errstate(all=\"ignore\"):\n            result = func(this_vals, other_vals)\n\n        name = ops.get_op_result_name(self, other)\n        ret = this._construct_result(result, name)\n        return ret\n\n    def _construct_result(\n        self, result: Union[ArrayLike, Tuple[ArrayLike, ArrayLike]], name: Label\n    ) -> Union[\"Series\", Tuple[\"Series\", \"Series\"]]:\n        \"\"\"\n        Construct an appropriately-labelled Series from the result of an op.\n\n        Parameters\n        ----------\n        result : ndarray or ExtensionArray\n        name : Label\n\n        Returns\n        -------\n        Series\n            In the case of __divmod__ or __rdivmod__, a 2-tuple of Series.\n        \"\"\"\n        if isinstance(result, tuple):\n            # produced by divmod or rdivmod\n\n            res1 = self._construct_result(result[0], name=name)\n            res2 = self._construct_result(result[1], name=name)\n\n            # GH#33427 assertions to keep mypy happy\n            assert isinstance(res1, Series)\n            assert isinstance(res2, Series)\n            return (res1, res2)\n\n        # We do not pass dtype to ensure that the Series constructor\n        #  does inference in the case where `result` has object-dtype.\n        out = self._constructor(result, index=self.index)\n        out = out.__finalize__(self)\n\n        # Set the result's name after __finalize__ is called because __finalize__\n        #  would set it back to self.name\n        out.name = name\n        return out\n\n    def combine(self, other, func, fill_value=None) -> \"Series\":\n        \"\"\"\n        Combine the Series with a Series or scalar according to `func`.\n\n        Combine the Series and `other` using `func` to perform elementwise\n        selection for combined Series.\n        `fill_value` is assumed when value is missing at some index\n        from one of the two objects being combined.\n\n        Parameters\n        ----------\n        other : Series or scalar\n            The value(s) to be combined with the `Series`.\n        func : function\n            Function that takes two scalars as inputs and returns an element.\n        fill_value : scalar, optional\n            The value to assume when an index is missing from\n            one Series or the other. The default specifies to use the\n            appropriate NaN value for the underlying dtype of the Series.\n\n        Returns\n        -------\n        Series\n            The result of combining the Series with the other object.\n\n        See Also\n        --------\n        Series.combine_first : Combine Series values, choosing the calling\n            Series' values first.\n\n        Examples\n        --------\n        Consider 2 Datasets ``s1`` and ``s2`` containing\n        highest clocked speeds of different birds.\n\n        >>> s1 = pd.Series({'falcon': 330.0, 'eagle': 160.0})\n        >>> s1\n        falcon    330.0\n        eagle     160.0\n        dtype: float64\n        >>> s2 = pd.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})\n        >>> s2\n        falcon    345.0\n        eagle     200.0\n        duck       30.0\n        dtype: float64\n\n        Now, to combine the two datasets and view the highest speeds\n        of the birds across the two datasets\n\n        >>> s1.combine(s2, max)\n        duck        NaN\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n\n        In the previous example, the resulting value for duck is missing,\n        because the maximum of a NaN and a float is a NaN.\n        So, in the example, we set ``fill_value=0``,\n        so the maximum value returned will be the value from some dataset.\n\n        >>> s1.combine(s2, max, fill_value=0)\n        duck       30.0\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n        \"\"\"\n        if fill_value is None:\n            fill_value = na_value_for_dtype(self.dtype, compat=False)\n\n        if isinstance(other, Series):\n            # If other is a Series, result is based on union of Series,\n            # so do this element by element\n            new_index = self.index.union(other.index)\n            new_name = ops.get_op_result_name(self, other)\n            new_values = []\n            for idx in new_index:\n                lv = self.get(idx, fill_value)\n                rv = other.get(idx, fill_value)\n                with np.errstate(all=\"ignore\"):\n                    new_values.append(func(lv, rv))\n        else:\n            # Assume that other is a scalar, so apply the function for\n            # each element in the Series\n            new_index = self.index\n            with np.errstate(all=\"ignore\"):\n                new_values = [func(lv, other) for lv in self._values]\n            new_name = self.name\n\n        if is_categorical_dtype(self.dtype):\n            pass\n        elif is_extension_array_dtype(self.dtype):\n            # TODO: can we do this for only SparseDtype?\n            # The function can return something of any type, so check\n            # if the type is compatible with the calling EA.\n            new_values = maybe_cast_to_extension_array(type(self._values), new_values)\n        return self._constructor(new_values, index=new_index, name=new_name)\n\n    def combine_first(self, other) -> \"Series\":\n        \"\"\"\n        Combine Series values, choosing the calling Series's values first.\n\n        Parameters\n        ----------\n        other : Series\n            The value(s) to be combined with the `Series`.\n\n        Returns\n        -------\n        Series\n            The result of combining the Series with the other object.\n\n        See Also\n        --------\n        Series.combine : Perform elementwise operation on two Series\n            using a given function.\n\n        Notes\n        -----\n        Result index will be the union of the two indexes.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, np.nan])\n        >>> s2 = pd.Series([3, 4])\n        >>> s1.combine_first(s2)\n        0    1.0\n        1    4.0\n        dtype: float64\n        \"\"\"\n        new_index = self.index.union(other.index)\n        this = self.reindex(new_index, copy=False)\n        other = other.reindex(new_index, copy=False)\n        if this.dtype.kind == \"M\" and other.dtype.kind != \"M\":\n            other = to_datetime(other)\n\n        return this.where(notna(this), other)\n\n    def update(self, other) -> None:\n        \"\"\"\n        Modify Series in place using non-NA values from passed\n        Series. Aligns on index.\n\n        Parameters\n        ----------\n        other : Series, or object coercible into Series\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s.update(pd.Series(['d', 'e'], index=[0, 2]))\n        >>> s\n        0    d\n        1    b\n        2    e\n        dtype: object\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6, 7, 8]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        If ``other`` contains NaNs the corresponding values are not updated\n        in the original Series.\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, np.nan, 6]))\n        >>> s\n        0    4\n        1    2\n        2    6\n        dtype: int64\n\n        ``other`` can also be a non-Series object type\n        that is coercible into a Series\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update([4, np.nan, 6])\n        >>> s\n        0    4\n        1    2\n        2    6\n        dtype: int64\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update({1: 9})\n        >>> s\n        0    1\n        1    9\n        2    3\n        dtype: int64\n        \"\"\"\n\n        if not isinstance(other, Series):\n            other = Series(other)\n\n        other = other.reindex_like(self)\n        mask = notna(other)\n\n        self._mgr = self._mgr.putmask(mask=mask, new=other)\n        self._maybe_update_cacher()\n\n    # ----------------------------------------------------------------------\n    # Reindexing, sorting\n\n    def sort_values(\n        self,\n        axis=0,\n        ascending=True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        ignore_index: bool = False,\n        key: ValueKeyFunc = None,\n    ):\n        \"\"\"\n        Sort by the values.\n\n        Sort a Series in ascending or descending order by some\n        criterion.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            Axis to direct sorting. The value 'index' is accepted for\n            compatibility with DataFrame.sort_values.\n        ascending : bool, default True\n            If True, sort values in ascending order, otherwise descending.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort' or 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. 'mergesort' is the only stable  algorithm.\n        na_position : {'first' or 'last'}, default 'last'\n            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n            the end.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the series values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect a\n            ``Series`` and return an array-like.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series\n            Series ordered by values.\n\n        See Also\n        --------\n        Series.sort_index : Sort by the Series indices.\n        DataFrame.sort_values : Sort DataFrame by the values along either axis.\n        DataFrame.sort_index : Sort DataFrame by indices.\n\n        Examples\n        --------\n        >>> s = pd.Series([np.nan, 1, 3, 10, 5])\n        >>> s\n        0     NaN\n        1     1.0\n        2     3.0\n        3     10.0\n        4     5.0\n        dtype: float64\n\n        Sort values ascending order (default behaviour)\n\n        >>> s.sort_values(ascending=True)\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        0     NaN\n        dtype: float64\n\n        Sort values descending order\n\n        >>> s.sort_values(ascending=False)\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values inplace\n\n        >>> s.sort_values(ascending=False, inplace=True)\n        >>> s\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values putting NAs first\n\n        >>> s.sort_values(na_position='first')\n        0     NaN\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        dtype: float64\n\n        Sort a series of strings\n\n        >>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n        >>> s\n        0    z\n        1    b\n        2    d\n        3    a\n        4    c\n        dtype: object\n\n        >>> s.sort_values()\n        3    a\n        1    b\n        4    c\n        2    d\n        0    z\n        dtype: object\n\n        Sort using a key function. Your `key` function will be\n        given the ``Series`` of values and should return an array-like.\n\n        >>> s = pd.Series(['a', 'B', 'c', 'D', 'e'])\n        >>> s.sort_values()\n        1    B\n        3    D\n        0    a\n        2    c\n        4    e\n        dtype: object\n        >>> s.sort_values(key=lambda x: x.str.lower())\n        0    a\n        1    B\n        2    c\n        3    D\n        4    e\n        dtype: object\n\n        NumPy ufuncs work well here. For example, we can\n        sort by the ``sin`` of the value\n\n        >>> s = pd.Series([-4, -2, 0, 2, 4])\n        >>> s.sort_values(key=np.sin)\n        1   -2\n        4    4\n        2    0\n        0   -4\n        3    2\n        dtype: int64\n\n        More complicated user-defined functions can be used,\n        as long as they expect a Series and return an array-like\n\n        >>> s.sort_values(key=lambda x: (np.tan(x.cumsum())))\n        0   -4\n        3    2\n        4    4\n        1   -2\n        2    0\n        dtype: int64\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n\n        # GH 5856/5853\n        if inplace and self._is_cached:\n            raise ValueError(\n                \"This Series is a view of some other array, to \"\n                \"sort in-place you must create a copy\"\n            )\n\n        def _try_kind_sort(arr):\n            arr = ensure_key_mapped(arr, key)\n            arr = getattr(arr, \"_values\", arr)\n\n            # easier to ask forgiveness than permission\n            try:\n                # if kind==mergesort, it can fail for object dtype\n                return arr.argsort(kind=kind)\n            except TypeError:\n                # stable sort not available for object dtype\n                # uses the argsort default quicksort\n                return arr.argsort(kind=\"quicksort\")\n\n        arr = self._values\n        sorted_index = np.empty(len(self), dtype=np.int32)\n\n        bad = isna(arr)\n\n        good = ~bad\n        idx = ibase.default_index(len(self))\n\n        argsorted = _try_kind_sort(self[good])\n\n        if is_list_like(ascending):\n            if len(ascending) != 1:\n                raise ValueError(\n                    f\"Length of ascending ({len(ascending)}) must be 1 for Series\"\n                )\n            ascending = ascending[0]\n\n        if not is_bool(ascending):\n            raise ValueError(\"ascending must be boolean\")\n\n        if not ascending:\n            argsorted = argsorted[::-1]\n\n        if na_position == \"last\":\n            n = good.sum()\n            sorted_index[:n] = idx[good][argsorted]\n            sorted_index[n:] = idx[bad]\n        elif na_position == \"first\":\n            n = bad.sum()\n            sorted_index[n:] = idx[good][argsorted]\n            sorted_index[:n] = idx[bad]\n        else:\n            raise ValueError(f\"invalid na_position: {na_position}\")\n\n        result = self._constructor(arr[sorted_index], index=self.index[sorted_index])\n\n        if ignore_index:\n            result.index = ibase.default_index(len(sorted_index))\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_values\")\n\n    def sort_index(\n        self,\n        axis=0,\n        level=None,\n        ascending: bool = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        sort_remaining: bool = True,\n        ignore_index: bool = False,\n        key: IndexKeyFunc = None,\n    ):\n        \"\"\"\n        Sort Series by index labels.\n\n        Returns a new Series sorted by label if `inplace` argument is\n        ``False``, otherwise updates the original series and returns None.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Axis to direct sorting. This can only be 0 for Series.\n        level : int, optional\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level individually.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information.  'mergesort' is the only stable algorithm. For\n            DataFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'last'}, default 'last'\n            If 'first' puts NaNs at the beginning, 'last' puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series\n            The original Series sorted by the labels.\n\n        See Also\n        --------\n        DataFrame.sort_index: Sort DataFrame by the index.\n        DataFrame.sort_values: Sort DataFrame by the value.\n        Series.sort_values : Sort Series by the value.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])\n        >>> s.sort_index()\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        Sort Descending\n\n        >>> s.sort_index(ascending=False)\n        4    d\n        3    a\n        2    b\n        1    c\n        dtype: object\n\n        Sort Inplace\n\n        >>> s.sort_index(inplace=True)\n        >>> s\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        By default NaNs are put at the end, but use `na_position` to place\n        them at the beginning\n\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])\n        >>> s.sort_index(na_position='first')\n        NaN     d\n         1.0    c\n         2.0    b\n         3.0    a\n        dtype: object\n\n        Specify index level to sort\n\n        >>> arrays = [np.array(['qux', 'qux', 'foo', 'foo',\n        ...                     'baz', 'baz', 'bar', 'bar']),\n        ...           np.array(['two', 'one', 'two', 'one',\n        ...                     'two', 'one', 'two', 'one'])]\n        >>> s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)\n        >>> s.sort_index(level=1)\n        bar  one    8\n        baz  one    6\n        foo  one    4\n        qux  one    2\n        bar  two    7\n        baz  two    5\n        foo  two    3\n        qux  two    1\n        dtype: int64\n\n        Does not sort by remaining levels when sorting by levels\n\n        >>> s.sort_index(level=1, sort_remaining=False)\n        qux  one    2\n        foo  one    4\n        baz  one    6\n        bar  one    8\n        qux  two    1\n        foo  two    3\n        baz  two    5\n        bar  two    7\n        dtype: int64\n\n        Apply a key function before sorting\n\n        >>> s = pd.Series([1, 2, 3, 4], index=['A', 'b', 'C', 'd'])\n        >>> s.sort_index(key=lambda x : x.str.lower())\n        A    1\n        b    2\n        C    3\n        d    4\n        dtype: int64\n        \"\"\"\n\n        # TODO: this can be combined with DataFrame.sort_index impl as\n        # almost identical\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n        index = ensure_key_mapped(self.index, key, levels=level)\n\n        if level is not None:\n            new_index, indexer = index.sortlevel(\n                level, ascending=ascending, sort_remaining=sort_remaining\n            )\n\n        elif isinstance(index, MultiIndex):\n            from pandas.core.sorting import lexsort_indexer\n\n            labels = index._sort_levels_monotonic()\n\n            indexer = lexsort_indexer(\n                labels._get_codes_for_sorting(),\n                orders=ascending,\n                na_position=na_position,\n            )\n        else:\n            from pandas.core.sorting import nargsort\n\n            # Check monotonic-ness before sort an index\n            # GH11080\n            if (ascending and index.is_monotonic_increasing) or (\n                not ascending and index.is_monotonic_decreasing\n            ):\n                if inplace:\n                    return\n                else:\n                    return self.copy()\n\n            indexer = nargsort(\n                index, kind=kind, ascending=ascending, na_position=na_position\n            )\n\n        indexer = ensure_platform_int(indexer)\n        new_index = self.index.take(indexer)\n        new_index = new_index._sort_levels_monotonic()\n\n        new_values = self._values.take(indexer)\n        result = self._constructor(new_values, index=new_index)\n\n        if ignore_index:\n            result.index = ibase.default_index(len(result))\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_index\")\n\n    def argsort(self, axis=0, kind=\"quicksort\", order=None) -> \"Series\":\n        \"\"\"\n        Override ndarray.argsort. Argsorts the value, omitting NA/null values,\n        and places the result in the same locations as the non-NA values.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Has no effect but is accepted for compatibility with numpy.\n        kind : {'mergesort', 'quicksort', 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See np.sort for more\n            information. 'mergesort' is the only stable algorithm.\n        order : None\n            Has no effect but is accepted for compatibility with numpy.\n\n        Returns\n        -------\n        Series\n            Positions of values within the sort order with -1 indicating\n            nan values.\n\n        See Also\n        --------\n        numpy.ndarray.argsort : Returns the indices that would sort this array.\n        \"\"\"\n        values = self._values\n        mask = isna(values)\n\n        if mask.any():\n            result = Series(-1, index=self.index, name=self.name, dtype=\"int64\")\n            notmask = ~mask\n            result[notmask] = np.argsort(values[notmask], kind=kind)\n            return self._constructor(result, index=self.index).__finalize__(\n                self, method=\"argsort\"\n            )\n        else:\n            return self._constructor(\n                np.argsort(values, kind=kind), index=self.index, dtype=\"int64\"\n            ).__finalize__(self, method=\"argsort\")\n\n    def nlargest(self, n=5, keep=\"first\") -> \"Series\":\n        \"\"\"\n        Return the largest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many descending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n                of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n                order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n                size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` largest values in the Series, sorted in decreasing order.\n\n        See Also\n        --------\n        Series.nsmallest: Get the `n` smallest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values(ascending=False).head(n)`` for small `n`\n        relative to the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Malta\": 434000, \"Maldives\": 434000,\n        ...                         \"Brunei\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Montserrat      5200\n        dtype: int64\n\n        The `n` largest elements where ``n=5`` by default.\n\n        >>> s.nlargest()\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3``. Default `keep` value is 'first'\n        so Malta will be kept.\n\n        >>> s.nlargest(3)\n        France    65000000\n        Italy     59000000\n        Malta       434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` and keeping the last duplicates.\n        Brunei will be kept since it is the last with value 434000 based on\n        the index order.\n\n        >>> s.nlargest(3, keep='last')\n        France      65000000\n        Italy       59000000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has five elements due to the three duplicates.\n\n        >>> s.nlargest(3, keep='all')\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nlargest()\n\n    def nsmallest(self, n=5, keep=\"first\") -> \"Series\":\n        \"\"\"\n        Return the smallest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many ascending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n                of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n                order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n                size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` smallest values in the Series, sorted in increasing order.\n\n        See Also\n        --------\n        Series.nlargest: Get the `n` largest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values().head(n)`` for small `n` relative to\n        the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Brunei\": 434000, \"Malta\": 434000,\n        ...                         \"Maldives\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Brunei        434000\n        Malta         434000\n        Maldives      434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Montserrat      5200\n        dtype: int64\n\n        The `n` smallest elements where ``n=5`` by default.\n\n        >>> s.nsmallest()\n        Montserrat    5200\n        Nauru        11300\n        Tuvalu       11300\n        Anguilla     11300\n        Iceland     337000\n        dtype: int64\n\n        The `n` smallest elements where ``n=3``. Default `keep` value is\n        'first' so Nauru and Tuvalu will be kept.\n\n        >>> s.nsmallest(3)\n        Montserrat   5200\n        Nauru       11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` and keeping the last\n        duplicates. Anguilla and Tuvalu will be kept since they are the last\n        with value 11300 based on the index order.\n\n        >>> s.nsmallest(3, keep='last')\n        Montserrat   5200\n        Anguilla    11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has four elements due to the three duplicates.\n\n        >>> s.nsmallest(3, keep='all')\n        Montserrat   5200\n        Nauru       11300\n        Tuvalu      11300\n        Anguilla    11300\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nsmallest()\n\n    def swaplevel(self, i=-2, j=-1, copy=True) -> \"Series\":\n        \"\"\"\n        Swap levels i and j in a :class:`MultiIndex`.\n\n        Default is to swap the two innermost levels of the index.\n\n        Parameters\n        ----------\n        i, j : int, str\n            Level of the indices to be swapped. Can pass level name as string.\n        copy : bool, default True\n            Whether to copy underlying data.\n\n        Returns\n        -------\n        Series\n            Series with levels swapped in MultiIndex.\n        \"\"\"\n        assert isinstance(self.index, MultiIndex)\n        new_index = self.index.swaplevel(i, j)\n        return self._constructor(self._values, index=new_index, copy=copy).__finalize__(\n            self, method=\"swaplevel\"\n        )\n\n    def reorder_levels(self, order) -> \"Series\":\n        \"\"\"\n        Rearrange index levels using input order.\n\n        May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int representing new level order\n            Reference level by number or key.\n\n        Returns\n        -------\n        type of caller (new object)\n        \"\"\"\n        if not isinstance(self.index, MultiIndex):  # pragma: no cover\n            raise Exception(\"Can only reorder levels on a hierarchical axis.\")\n\n        result = self.copy()\n        assert isinstance(result.index, MultiIndex)\n        result.index = result.index.reorder_levels(order)\n        return result\n\n    def explode(self) -> \"Series\":\n        \"\"\"\n        Transform each element of a list-like to a row, replicating the\n        index values.\n\n        .. versionadded:: 0.25.0\n\n        Returns\n        -------\n        Series\n            Exploded lists to rows; index will be duplicated for these rows.\n\n        See Also\n        --------\n        Series.str.split : Split string values on specified separator.\n        Series.unstack : Unstack, a.k.a. pivot, Series with MultiIndex\n            to produce DataFrame.\n        DataFrame.melt : Unpivot a DataFrame from wide format to long format.\n        DataFrame.explode : Explode a DataFrame from list-like\n            columns to long format.\n\n        Notes\n        -----\n        This routine will explode list-likes including lists, tuples,\n        Series, and np.ndarray. The result dtype of the subset rows will\n        be object. Scalars will be returned unchanged. Empty list-likes will\n        result in a np.nan for that row.\n\n        Examples\n        --------\n        >>> s = pd.Series([[1, 2, 3], 'foo', [], [3, 4]])\n        >>> s\n        0    [1, 2, 3]\n        1          foo\n        2           []\n        3       [3, 4]\n        dtype: object\n\n        >>> s.explode()\n        0      1\n        0      2\n        0      3\n        1    foo\n        2    NaN\n        3      3\n        3      4\n        dtype: object\n        \"\"\"\n        if not len(self) or not is_object_dtype(self):\n            return self.copy()\n\n        values, counts = reshape.explode(np.asarray(self.array))\n\n        result = self._constructor(\n            values, index=self.index.repeat(counts), name=self.name\n        )\n        return result\n\n    def unstack(self, level=-1, fill_value=None):\n        \"\"\"\n        Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.\n        The level involved will automatically get sorted.\n\n        Parameters\n        ----------\n        level : int, str, or list of these, default last level\n            Level(s) to unstack, can pass level name.\n        fill_value : scalar value, default None\n            Value to use when replacing NaN values.\n\n        Returns\n        -------\n        DataFrame\n            Unstacked Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4],\n        ...               index=pd.MultiIndex.from_product([['one', 'two'],\n        ...                                                 ['a', 'b']]))\n        >>> s\n        one  a    1\n             b    2\n        two  a    3\n             b    4\n        dtype: int64\n\n        >>> s.unstack(level=-1)\n             a  b\n        one  1  2\n        two  3  4\n\n        >>> s.unstack(level=0)\n           one  two\n        a    1    3\n        b    2    4\n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n\n        return unstack(self, level, fill_value)\n\n    # ----------------------------------------------------------------------\n    # function application\n\n    def map(self, arg, na_action=None) -> \"Series\":\n        \"\"\"\n        Map values of Series according to input correspondence.\n\n        Used for substituting each value in a Series with another value,\n        that may be derived from a function, a ``dict`` or\n        a :class:`Series`.\n\n        Parameters\n        ----------\n        arg : function, collections.abc.Mapping subclass or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NaN values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        Series\n            Same index as caller.\n\n        See Also\n        --------\n        Series.apply : For applying more complex functions on a Series.\n        DataFrame.apply : Apply a function row-/column-wise.\n        DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n\n        Notes\n        -----\n        When ``arg`` is a dictionary, values in Series that are not in the\n        dictionary (as keys) are converted to ``NaN``. However, if the\n        dictionary is a ``dict`` subclass that defines ``__missing__`` (i.e.\n        provides a method for default values), then this default is used\n        rather than ``NaN``.\n\n        Examples\n        --------\n        >>> s = pd.Series(['cat', 'dog', np.nan, 'rabbit'])\n        >>> s\n        0      cat\n        1      dog\n        2      NaN\n        3   rabbit\n        dtype: object\n\n        ``map`` accepts a ``dict`` or a ``Series``. Values that are not found\n        in the ``dict`` are converted to ``NaN``, unless the dict has a default\n        value (e.g. ``defaultdict``):\n\n        >>> s.map({'cat': 'kitten', 'dog': 'puppy'})\n        0   kitten\n        1    puppy\n        2      NaN\n        3      NaN\n        dtype: object\n\n        It also accepts a function:\n\n        >>> s.map('I am a {}'.format)\n        0       I am a cat\n        1       I am a dog\n        2       I am a nan\n        3    I am a rabbit\n        dtype: object\n\n        To avoid applying the function to missing values (and keep them as\n        ``NaN``) ``na_action='ignore'`` can be used:\n\n        >>> s.map('I am a {}'.format, na_action='ignore')\n        0     I am a cat\n        1     I am a dog\n        2            NaN\n        3  I am a rabbit\n        dtype: object\n        \"\"\"\n        new_values = super()._map_values(arg, na_action=na_action)\n        return self._constructor(new_values, index=self.index).__finalize__(\n            self, method=\"map\"\n        )\n\n    def _gotitem(self, key, ndim, subset=None) -> \"Series\":\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : 1,2\n            Requested ndim of result.\n        subset : object, default None\n            Subset to act on.\n        \"\"\"\n        return self\n\n    _agg_see_also_doc = dedent(\n        \"\"\"\n    See Also\n    --------\n    Series.apply : Invoke function on a Series.\n    Series.transform : Transform function producing a Series with like indexes.\n    \"\"\"\n    )\n\n    _agg_examples_doc = dedent(\n        \"\"\"\n    Examples\n    --------\n    >>> s = pd.Series([1, 2, 3, 4])\n    >>> s\n    0    1\n    1    2\n    2    3\n    3    4\n    dtype: int64\n\n    >>> s.agg('min')\n    1\n\n    >>> s.agg(['min', 'max'])\n    min   1\n    max   4\n    dtype: int64\n    \"\"\"\n    )\n\n    @Substitution(\n        see_also=_agg_see_also_doc,\n        examples=_agg_examples_doc,\n        versionadded=\"\\n.. versionadded:: 0.20.0\\n\",\n        **_shared_doc_kwargs,\n    )\n    @Appender(generic._shared_docs[\"aggregate\"])\n    def aggregate(self, func, axis=0, *args, **kwargs):\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n        result, how = self._aggregate(func, *args, **kwargs)\n        if result is None:\n\n            # we can be called from an inner function which\n            # passes this meta-data\n            kwargs.pop(\"_axis\", None)\n            kwargs.pop(\"_level\", None)\n\n            # try a regular apply, this evaluates lambdas\n            # row-by-row; however if the lambda is expected a Series\n            # expression, e.g.: lambda x: x-x.quantile(0.25)\n            # this will fail, so we can try a vectorized evaluation\n\n            # we cannot FIRST try the vectorized evaluation, because\n            # then .agg and .apply would have different semantics if the\n            # operation is actually defined on the Series, e.g. str\n            try:\n                result = self.apply(func, *args, **kwargs)\n            except (ValueError, AttributeError, TypeError):\n                result = func(self, *args, **kwargs)\n\n        return result\n\n    agg = aggregate\n\n    @Appender(generic._shared_docs[\"transform\"] % _shared_doc_kwargs)\n    def transform(self, func, axis=0, *args, **kwargs):\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n        return super().transform(func, *args, **kwargs)\n\n    def apply(self, func, convert_dtype=True, args=(), **kwds):\n        \"\"\"\n        Invoke function on values of Series.\n\n        Can be ufunc (a NumPy function that applies to the entire Series)\n        or a Python function that only works on single values.\n\n        Parameters\n        ----------\n        func : function\n            Python function or NumPy ufunc to apply.\n        convert_dtype : bool, default True\n            Try to find better dtype for elementwise function results. If\n            False, leave as dtype=object.\n        args : tuple\n            Positional arguments passed to func after the series value.\n        **kwds\n            Additional keyword arguments passed to func.\n\n        Returns\n        -------\n        Series or DataFrame\n            If func returns a Series object the result will be a DataFrame.\n\n        See Also\n        --------\n        Series.map: For element-wise operations.\n        Series.agg: Only perform aggregating type operations.\n        Series.transform: Only perform transforming type operations.\n\n        Examples\n        --------\n        Create a series with typical summer temperatures for each city.\n\n        >>> s = pd.Series([20, 21, 12],\n        ...               index=['London', 'New York', 'Helsinki'])\n        >>> s\n        London      20\n        New York    21\n        Helsinki    12\n        dtype: int64\n\n        Square the values by defining a function and passing it as an\n        argument to ``apply()``.\n\n        >>> def square(x):\n        ...     return x ** 2\n        >>> s.apply(square)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Square the values by passing an anonymous function as an\n        argument to ``apply()``.\n\n        >>> s.apply(lambda x: x ** 2)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Define a custom function that needs additional positional\n        arguments and pass these additional arguments using the\n        ``args`` keyword.\n\n        >>> def subtract_custom_value(x, custom_value):\n        ...     return x - custom_value\n\n        >>> s.apply(subtract_custom_value, args=(5,))\n        London      15\n        New York    16\n        Helsinki     7\n        dtype: int64\n\n        Define a custom function that takes keyword arguments\n        and pass these arguments to ``apply``.\n\n        >>> def add_custom_values(x, **kwargs):\n        ...     for month in kwargs:\n        ...         x += kwargs[month]\n        ...     return x\n\n        >>> s.apply(add_custom_values, june=30, july=20, august=25)\n        London      95\n        New York    96\n        Helsinki    87\n        dtype: int64\n\n        Use a function from the Numpy library.\n\n        >>> s.apply(np.log)\n        London      2.995732\n        New York    3.044522\n        Helsinki    2.484907\n        dtype: float64\n        \"\"\"\n        if len(self) == 0:\n            return self._constructor(dtype=self.dtype, index=self.index).__finalize__(\n                self, method=\"apply\"\n            )\n\n        # dispatch to agg\n        if isinstance(func, (list, dict)):\n            return self.aggregate(func, *args, **kwds)\n\n        # if we are a string, try to dispatch\n        if isinstance(func, str):\n            return self._try_aggregate_string_function(func, *args, **kwds)\n\n        # handle ufuncs and lambdas\n        if kwds or args and not isinstance(func, np.ufunc):\n\n            def f(x):\n                return func(x, *args, **kwds)\n\n        else:\n            f = func\n\n        with np.errstate(all=\"ignore\"):\n            if isinstance(f, np.ufunc):\n                return f(self)\n\n            # row-wise access\n            if is_extension_array_dtype(self.dtype) and hasattr(self._values, \"map\"):\n                # GH#23179 some EAs do not have `map`\n                mapped = self._values.map(f)\n            else:\n                values = self.astype(object)._values\n                mapped = lib.map_infer(values, f, convert=convert_dtype)\n\n        if len(mapped) and isinstance(mapped[0], Series):\n            # GH 25959 use pd.array instead of tolist\n            # so extension arrays can be used\n            return self._constructor_expanddim(pd.array(mapped), index=self.index)\n        else:\n            return self._constructor(mapped, index=self.index).__finalize__(\n                self, method=\"apply\"\n            )\n\n    def _reduce(\n        self, op, name, axis=0, skipna=True, numeric_only=None, filter_type=None, **kwds\n    ):\n        \"\"\"\n        Perform a reduction operation.\n\n        If we have an ndarray as a value, then simply perform the operation,\n        otherwise delegate to the object.\n        \"\"\"\n        delegate = self._values\n\n        if axis is not None:\n            self._get_axis_number(axis)\n\n        if isinstance(delegate, ExtensionArray):\n            # dispatch to ExtensionArray interface\n            return delegate._reduce(name, skipna=skipna, **kwds)\n\n        else:\n            # dispatch to numpy arrays\n            if numeric_only:\n                raise NotImplementedError(\n                    f\"Series.{name} does not implement numeric_only.\"\n                )\n            with np.errstate(all=\"ignore\"):\n                return op(delegate, skipna=skipna, **kwds)\n\n    def _reindex_indexer(self, new_index, indexer, copy):\n        if indexer is None:\n            if copy:\n                return self.copy()\n            return self\n\n        new_values = algorithms.take_1d(\n            self._values, indexer, allow_fill=True, fill_value=None\n        )\n        return self._constructor(new_values, index=new_index)\n\n    def _needs_reindex_multi(self, axes, method, level):\n        \"\"\"\n        Check if we do need a multi reindex; this is for compat with\n        higher dims.\n        \"\"\"\n        return False\n\n    @doc(NDFrame.align, **_shared_doc_kwargs)\n    def align(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy=True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n        broadcast_axis=None,\n    ):\n        return super().align(\n            other,\n            join=join,\n            axis=axis,\n            level=level,\n            copy=copy,\n            fill_value=fill_value,\n            method=method,\n            limit=limit,\n            fill_axis=fill_axis,\n            broadcast_axis=broadcast_axis,\n        )\n\n    def rename(\n        self,\n        index=None,\n        *,\n        axis=None,\n        copy=True,\n        inplace=False,\n        level=None,\n        errors=\"ignore\",\n    ):\n        \"\"\"\n        Alter Series index labels or name.\n\n        Function / dict values must be unique (1-to-1). Labels not contained in\n        a dict / Series will be left as-is. Extra labels listed don't throw an\n        error.\n\n        Alternatively, change ``Series.name`` with a scalar value.\n\n        See the :ref:`user guide <basics.rename>` for more.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Unused. Accepted for compatibility with DataFrame method only.\n        index : scalar, hashable sequence, dict-like or function, optional\n            Functions or dict-like are transformations to apply to\n            the index.\n            Scalar or hashable sequence-like will alter the ``Series.name``\n            attribute.\n\n        **kwargs\n            Additional keyword arguments passed to the function. Only the\n            \"inplace\" keyword is used.\n\n        Returns\n        -------\n        Series\n            Series with index labels or name altered.\n\n        See Also\n        --------\n        DataFrame.rename : Corresponding DataFrame method.\n        Series.rename_axis : Set the name of the axis.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n        >>> s.rename(\"my_name\")  # scalar, changes Series.name\n        0    1\n        1    2\n        2    3\n        Name: my_name, dtype: int64\n        >>> s.rename(lambda x: x ** 2)  # function, changes labels\n        0    1\n        1    2\n        4    3\n        dtype: int64\n        >>> s.rename({1: 3, 2: 5})  # mapping, changes labels\n        0    1\n        3    2\n        5    3\n        dtype: int64\n        \"\"\"\n        if callable(index) or is_dict_like(index):\n            return super().rename(\n                index, copy=copy, inplace=inplace, level=level, errors=errors\n            )\n        else:\n            return self._set_name(index, inplace=inplace)\n\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n\n        >>> s.set_axis(['a', 'b', 'c'], axis=0)\n        a    1\n        b    2\n        c    3\n        dtype: int64\n    \"\"\"\n    )\n    @Substitution(\n        **_shared_doc_kwargs,\n        extended_summary_sub=\"\",\n        axis_description_sub=\"\",\n        see_also_sub=\"\",\n    )\n    @Appender(generic.NDFrame.set_axis.__doc__)\n    def set_axis(self, labels, axis: Axis = 0, inplace: bool = False):\n        return super().set_axis(labels, axis=axis, inplace=inplace)\n\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(generic.NDFrame.reindex.__doc__)\n    def reindex(self, index=None, **kwargs):\n        return super().reindex(index=index, **kwargs)\n\n    def drop(\n        self,\n        labels=None,\n        axis=0,\n        index=None,\n        columns=None,\n        level=None,\n        inplace=False,\n        errors=\"raise\",\n    ) -> \"Series\":\n        \"\"\"\n        Return Series with specified index labels removed.\n\n        Remove elements of a Series based on specifying the index labels.\n        When using a multi-index, labels on different levels can be removed\n        by specifying the level.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index labels to drop.\n        axis : 0, default 0\n            Redundant for application on Series.\n        index : single label or list-like\n            Redundant for application on Series, but 'index' can be used instead\n            of 'labels'.\n        columns : single label or list-like\n            No change is made to the Series; use 'index' or 'labels' instead.\n        level : int or level name, optional\n            For MultiIndex, level for which the labels will be removed.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and only existing labels are dropped.\n\n        Returns\n        -------\n        Series\n            Series with specified index labels removed.\n\n        Raises\n        ------\n        KeyError\n            If none of the labels are found in the index.\n\n        See Also\n        --------\n        Series.reindex : Return only specified index labels of Series.\n        Series.dropna : Return series without null values.\n        Series.drop_duplicates : Return Series with duplicate values removed.\n        DataFrame.drop : Drop specified labels from rows or columns.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=np.arange(3), index=['A', 'B', 'C'])\n        >>> s\n        A  0\n        B  1\n        C  2\n        dtype: int64\n\n        Drop labels B en C\n\n        >>> s.drop(labels=['B', 'C'])\n        A  0\n        dtype: int64\n\n        Drop 2nd level label in MultiIndex Series\n\n        >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n        ...                              ['speed', 'weight', 'length']],\n        ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> s = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...               index=midx)\n        >>> s\n        lama    speed      45.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        dtype: float64\n\n        >>> s.drop(labels='weight', level=1)\n        lama    speed      45.0\n                length      1.2\n        cow     speed      30.0\n                length      1.5\n        falcon  speed     320.0\n                length      0.3\n        dtype: float64\n        \"\"\"\n        return super().drop(\n            labels=labels,\n            axis=axis,\n            index=index,\n            columns=columns,\n            level=level,\n            inplace=inplace,\n            errors=errors,\n        )\n\n    @doc(NDFrame.fillna, **_shared_doc_kwargs)\n    def fillna(\n        self,\n        value=None,\n        method=None,\n        axis=None,\n        inplace=False,\n        limit=None,\n        downcast=None,\n    ) -> Optional[\"Series\"]:\n        return super().fillna(\n            value=value,\n            method=method,\n            axis=axis,\n            inplace=inplace,\n            limit=limit,\n            downcast=downcast,\n        )\n\n    @doc(NDFrame.replace, **_shared_doc_kwargs)\n    def replace(\n        self,\n        to_replace=None,\n        value=None,\n        inplace=False,\n        limit=None,\n        regex=False,\n        method=\"pad\",\n    ):\n        return super().replace(\n            to_replace=to_replace,\n            value=value,\n            inplace=inplace,\n            limit=limit,\n            regex=regex,\n            method=method,\n        )\n\n    @doc(NDFrame.shift, **_shared_doc_kwargs)\n    def shift(self, periods=1, freq=None, axis=0, fill_value=None) -> \"Series\":\n        return super().shift(\n            periods=periods, freq=freq, axis=axis, fill_value=fill_value\n        )\n\n    def memory_usage(self, index=True, deep=False):\n        \"\"\"\n        Return the memory usage of the Series.\n\n        The memory usage can optionally include the contribution of\n        the index and of elements of `object` dtype.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the Series index.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned value.\n\n        Returns\n        -------\n        int\n            Bytes of memory consumed.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\n            array.\n        DataFrame.memory_usage : Bytes consumed by a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(range(3))\n        >>> s.memory_usage()\n        152\n\n        Not including the index gives the size of the rest of the data, which\n        is necessarily smaller:\n\n        >>> s.memory_usage(index=False)\n        24\n\n        The memory footprint of `object` values is ignored by default:\n\n        >>> s = pd.Series([\"a\", \"b\"])\n        >>> s.values\n        array(['a', 'b'], dtype=object)\n        >>> s.memory_usage()\n        144\n        >>> s.memory_usage(deep=True)\n        260\n        \"\"\"\n        v = super().memory_usage(deep=deep)\n        if index:\n            v += self.index.memory_usage(deep=deep)\n        return v\n\n    def isin(self, values) -> \"Series\":\n        \"\"\"\n        Check whether `values` are contained in Series.\n\n        Return a boolean Series showing whether each element in the Series\n        matches an element in the passed sequence of `values` exactly.\n\n        Parameters\n        ----------\n        values : set or list-like\n            The sequence of values to test. Passing in a single string will\n            raise a ``TypeError``. Instead, turn a single string into a\n            list of one element.\n\n        Returns\n        -------\n        Series\n            Series of booleans indicating if each element is in values.\n\n        Raises\n        ------\n        TypeError\n          * If `values` is a string\n\n        See Also\n        --------\n        DataFrame.isin : Equivalent method on DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama',\n        ...                'hippo'], name='animal')\n        >>> s.isin(['cow', 'lama'])\n        0     True\n        1     True\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n\n        Passing a single string as ``s.isin('lama')`` will raise an error. Use\n        a list of one element instead:\n\n        >>> s.isin(['lama'])\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n        \"\"\"\n        result = algorithms.isin(self, values)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"isin\"\n        )\n\n    def between(self, left, right, inclusive=True) -> \"Series\":\n        \"\"\"\n        Return boolean Series equivalent to left <= series <= right.\n\n        This function returns a boolean vector containing `True` wherever the\n        corresponding Series element is between the boundary values `left` and\n        `right`. NA values are treated as `False`.\n\n        Parameters\n        ----------\n        left : scalar or list-like\n            Left boundary.\n        right : scalar or list-like\n            Right boundary.\n        inclusive : bool, default True\n            Include boundaries.\n\n        Returns\n        -------\n        Series\n            Series representing whether each element is between left and\n            right (inclusive).\n\n        See Also\n        --------\n        Series.gt : Greater than of series and other.\n        Series.lt : Less than of series and other.\n\n        Notes\n        -----\n        This function is equivalent to ``(left <= ser) & (ser <= right)``\n\n        Examples\n        --------\n        >>> s = pd.Series([2, 0, 4, 8, np.nan])\n\n        Boundary values are included by default:\n\n        >>> s.between(1, 4)\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        With `inclusive` set to ``False`` boundary values are excluded:\n\n        >>> s.between(1, 4, inclusive=False)\n        0     True\n        1    False\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        `left` and `right` can be any scalar value:\n\n        >>> s = pd.Series(['Alice', 'Bob', 'Carol', 'Eve'])\n        >>> s.between('Anna', 'Daniel')\n        0    False\n        1     True\n        2     True\n        3    False\n        dtype: bool\n        \"\"\"\n        if inclusive:\n            lmask = self >= left\n            rmask = self <= right\n        else:\n            lmask = self > left\n            rmask = self < right\n\n        return lmask & rmask\n\n    # ----------------------------------------------------------------------\n    # Convert to types that support pd.NA\n\n    def _convert_dtypes(\n        self,\n        infer_objects: bool = True,\n        convert_string: bool = True,\n        convert_integer: bool = True,\n        convert_boolean: bool = True,\n    ) -> \"Series\":\n        input_series = self\n        if infer_objects:\n            input_series = input_series.infer_objects()\n            if is_object_dtype(input_series):\n                input_series = input_series.copy()\n\n        if convert_string or convert_integer or convert_boolean:\n            inferred_dtype = convert_dtypes(\n                input_series._values, convert_string, convert_integer, convert_boolean\n            )\n            try:\n                result = input_series.astype(inferred_dtype)\n            except TypeError:\n                result = input_series.copy()\n        else:\n            result = input_series.copy()\n        return result\n\n    @Appender(generic._shared_docs[\"isna\"] % _shared_doc_kwargs)\n    def isna(self) -> \"Series\":\n        return super().isna()\n\n    @Appender(generic._shared_docs[\"isna\"] % _shared_doc_kwargs)\n    def isnull(self) -> \"Series\":\n        return super().isnull()\n\n    @Appender(generic._shared_docs[\"notna\"] % _shared_doc_kwargs)\n    def notna(self) -> \"Series\":\n        return super().notna()\n\n    @Appender(generic._shared_docs[\"notna\"] % _shared_doc_kwargs)\n    def notnull(self) -> \"Series\":\n        return super().notnull()\n\n    def dropna(self, axis=0, inplace=False, how=None):\n        \"\"\"\n        Return a new Series with missing values removed.\n\n        See the :ref:`User Guide <missing_data>` for more on which values are\n        considered missing, and how to work with missing data.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            There is only one axis to drop values from.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        how : str, optional\n            Not in use. Kept for compatibility.\n\n        Returns\n        -------\n        Series\n            Series with NA entries dropped from it.\n\n        See Also\n        --------\n        Series.isna: Indicate missing values.\n        Series.notna : Indicate existing (non-missing) values.\n        Series.fillna : Replace missing values.\n        DataFrame.dropna : Drop rows or columns which contain NA values.\n        Index.dropna : Drop missing indices.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1., 2., np.nan])\n        >>> ser\n        0    1.0\n        1    2.0\n        2    NaN\n        dtype: float64\n\n        Drop NA values from a Series.\n\n        >>> ser.dropna()\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Keep the Series with valid entries in the same variable.\n\n        >>> ser.dropna(inplace=True)\n        >>> ser\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Empty strings are not considered NA values. ``None`` is considered an\n        NA value.\n\n        >>> ser = pd.Series([np.NaN, 2, pd.NaT, '', None, 'I stay'])\n        >>> ser\n        0       NaN\n        1         2\n        2       NaT\n        3\n        4      None\n        5    I stay\n        dtype: object\n        >>> ser.dropna()\n        1         2\n        3\n        5    I stay\n        dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis or 0)\n\n        if self._can_hold_na:\n            result = remove_na_arraylike(self)\n            if inplace:\n                self._update_inplace(result)\n            else:\n                return result\n        else:\n            if inplace:\n                # do nothing\n                pass\n            else:\n                return self.copy()\n\n    # ----------------------------------------------------------------------\n    # Time series-oriented methods\n\n    def to_timestamp(self, freq=None, how=\"start\", copy=True) -> \"Series\":\n        \"\"\"\n        Cast to DatetimeIndex of Timestamps, at *beginning* of period.\n\n        Parameters\n        ----------\n        freq : str, default frequency of PeriodIndex\n            Desired frequency.\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series with DatetimeIndex\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        assert isinstance(self.index, PeriodIndex)\n        new_index = self.index.to_timestamp(freq=freq, how=how)  # type: ignore\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_timestamp\"\n        )\n\n    def to_period(self, freq=None, copy=True) -> \"Series\":\n        \"\"\"\n        Convert Series from DatetimeIndex to PeriodIndex with desired\n        frequency (inferred from index if not passed).\n\n        Parameters\n        ----------\n        freq : str, default None\n            Frequency associated with the PeriodIndex.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series\n            Series with index converted to PeriodIndex.\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        assert isinstance(self.index, DatetimeIndex)\n        new_index = self.index.to_period(freq=freq)  # type: ignore\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_period\"\n        )\n\n    # ----------------------------------------------------------------------\n    # Add index\n    _AXIS_ORDERS = [\"index\"]\n    _AXIS_REVERSED = False\n    _AXIS_LEN = len(_AXIS_ORDERS)\n    _info_axis_number = 0\n    _info_axis_name = \"index\"\n\n    index: \"Index\" = properties.AxisProperty(\n        axis=0, doc=\"The index (axis labels) of the Series.\"\n    )\n\n    # ----------------------------------------------------------------------\n    # Accessor Methods\n    # ----------------------------------------------------------------------\n    str = CachedAccessor(\"str\", StringMethods)\n    dt = CachedAccessor(\"dt\", CombinedDatetimelikeProperties)\n    cat = CachedAccessor(\"cat\", CategoricalAccessor)\n    plot = CachedAccessor(\"plot\", pandas.plotting.PlotAccessor)\n    sparse = CachedAccessor(\"sparse\", SparseAccessor)\n\n    # ----------------------------------------------------------------------\n    # Add plotting methods to Series\n    hist = pandas.plotting.hist_series\n\n\nSeries._add_numeric_operations()\nSeries._add_series_or_dataframe_operations()\n\n# Add arithmetic!\nops.add_flex_arithmetic_methods(Series)\nops.add_special_arithmetic_methods(Series)\n\n\n\"\"\"\nData structure for 1-dimensional cross-sectional and time series data\n\"\"\"\nfrom io import StringIO\nfrom shutil import get_terminal_size\nfrom textwrap import dedent\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import lib, properties, reshape, tslibs\nfrom pandas._typing import ArrayLike, Axis, DtypeObj, IndexKeyFunc, Label, ValueKeyFunc\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import Appender, Substitution, doc\nfrom pandas.util._validators import validate_bool_kwarg, validate_percentile\n\nfrom pandas.core.dtypes.cast import (\n    convert_dtypes,\n    maybe_cast_to_extension_array,\n    validate_numeric_casting,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_platform_int,\n    is_bool,\n    is_categorical_dtype,\n    is_dict_like,\n    is_extension_array_dtype,\n    is_integer,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_scalar,\n)\nfrom pandas.core.dtypes.generic import ABCDataFrame\nfrom pandas.core.dtypes.inference import is_hashable\nfrom pandas.core.dtypes.missing import (\n    isna,\n    na_value_for_dtype,\n    notna,\n    remove_na_arraylike,\n)\n\nimport pandas as pd\nfrom pandas.core import algorithms, base, generic, nanops, ops\nfrom pandas.core.accessor import CachedAccessor\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.arrays.categorical import CategoricalAccessor\nfrom pandas.core.arrays.sparse import SparseAccessor\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    create_series_with_explicit_dtype,\n    extract_array,\n    is_empty_data,\n    sanitize_array,\n)\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.indexers import unpack_1tuple\nfrom pandas.core.indexes.accessors import CombinedDatetimelikeProperties\nfrom pandas.core.indexes.api import (\n    Float64Index,\n    Index,\n    InvalidIndexError,\n    MultiIndex,\n    ensure_index,\n)\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.datetimes import DatetimeIndex\nfrom pandas.core.indexes.period import PeriodIndex\nfrom pandas.core.indexes.timedeltas import TimedeltaIndex\nfrom pandas.core.indexing import check_bool_indexer\nfrom pandas.core.internals import SingleBlockManager\nfrom pandas.core.sorting import ensure_key_mapped\nfrom pandas.core.strings import StringMethods\nfrom pandas.core.tools.datetimes import to_datetime\n\nimport pandas.io.formats.format as fmt\nimport pandas.plotting\n\nif TYPE_CHECKING:\n    from pandas.core.frame import DataFrame\n    from pandas.core.groupby.generic import SeriesGroupBy\n\n__all__ = [\"Series\"]\n\n_shared_doc_kwargs = dict(\n    axes=\"index\",\n    klass=\"Series\",\n    axes_single_arg=\"{0 or 'index'}\",\n    axis=\"\"\"axis : {0 or 'index'}\n        Parameter needed for compatibility with DataFrame.\"\"\",\n    inplace=\"\"\"inplace : boolean, default False\n        If True, performs operation inplace and returns None.\"\"\",\n    unique=\"np.ndarray\",\n    duplicated=\"Series\",\n    optional_by=\"\",\n    optional_mapper=\"\",\n    optional_labels=\"\",\n    optional_axis=\"\",\n    versionadded_to_excel=\"\\n    .. versionadded:: 0.20.0\\n\",\n)\n\n\ndef _coerce_method(converter):\n    \"\"\"\n    Install the scalar coercion methods.\n    \"\"\"\n\n    def wrapper(self):\n        if len(self) == 1:\n            return converter(self.iloc[0])\n        raise TypeError(f\"cannot convert the series to {converter}\")\n\n    wrapper.__name__ = f\"__{converter.__name__}__\"\n    return wrapper\n\n\n# ----------------------------------------------------------------------\n# Series class\n\n\nclass Series(base.IndexOpsMixin, generic.NDFrame):\n    \"\"\"\n    One-dimensional ndarray with axis labels (including time series).\n\n    Labels need not be unique but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for performing operations involving the index. Statistical\n    methods from ndarray have been overridden to automatically exclude\n    missing data (currently represented as NaN).\n\n    Operations between Series (+, -, /, *, **) align values based on their\n    associated index values-- they need not be the same length. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Series.\n\n        .. versionchanged:: 0.23.0\n           If data is a dict, argument order is maintained for Python 3.6\n           and later.\n\n    index : array-like or Index (1d)\n        Values must be hashable and have the same length as `data`.\n        Non-unique index values are allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If both a dict and index\n        sequence are used, the index will override the keys found in the\n        dict.\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\n        Data type for the output Series. If not specified, this will be\n        inferred from `data`.\n        See the :ref:`user guide <basics.dtypes>` for more usages.\n    name : str, optional\n        The name to give to the Series.\n    copy : bool, default False\n        Copy input data.\n    \"\"\"\n\n    _typ = \"series\"\n\n    _name: Label\n    _metadata: List[str] = [\"name\"]\n    _internal_names_set = {\"index\"} | generic.NDFrame._internal_names_set\n    _accessors = {\"dt\", \"cat\", \"str\", \"sparse\"}\n    _deprecations = (\n        base.IndexOpsMixin._deprecations\n        | generic.NDFrame._deprecations\n        | frozenset([\"compress\", \"ptp\"])\n    )\n\n    # Override cache_readonly bc Series is mutable\n    hasnans = property(\n        base.IndexOpsMixin.hasnans.func, doc=base.IndexOpsMixin.hasnans.__doc__\n    )\n    _mgr: SingleBlockManager\n    div: Callable[[\"Series\", Any], \"Series\"]\n    rdiv: Callable[[\"Series\", Any], \"Series\"]\n\n    # ----------------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self, data=None, index=None, dtype=None, name=None, copy=False, fastpath=False\n    ):\n\n        if (\n            isinstance(data, SingleBlockManager)\n            and index is None\n            and dtype is None\n            and copy is False\n        ):\n            # GH#33357 called with just the SingleBlockManager\n            NDFrame.__init__(self, data)\n            self.name = name\n            return\n\n        # we are called internally, so short-circuit\n        if fastpath:\n\n            # data is an ndarray, index is defined\n            if not isinstance(data, SingleBlockManager):\n                data = SingleBlockManager.from_array(data, index)\n            if copy:\n                data = data.copy()\n            if index is None:\n                index = data.index\n\n        else:\n\n            name = ibase.maybe_extract_name(name, data, type(self))\n\n            if is_empty_data(data) and dtype is None:\n                # gh-17261\n                warnings.warn(\n                    \"The default dtype for empty Series will be 'object' instead \"\n                    \"of 'float64' in a future version. Specify a dtype explicitly \"\n                    \"to silence this warning.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n                # uncomment the line below when removing the DeprecationWarning\n                # dtype = np.dtype(object)\n\n            if index is not None:\n                index = ensure_index(index)\n\n            if data is None:\n                data = {}\n            if dtype is not None:\n                dtype = self._validate_dtype(dtype)\n\n            if isinstance(data, MultiIndex):\n                raise NotImplementedError(\n                    \"initializing a Series from a MultiIndex is not supported\"\n                )\n            elif isinstance(data, Index):\n\n                if dtype is not None:\n                    # astype copies\n                    data = data.astype(dtype)\n                else:\n                    # GH#24096 we need to ensure the index remains immutable\n                    data = data._values.copy()\n                copy = False\n\n            elif isinstance(data, np.ndarray):\n                if len(data.dtype):\n                    # GH#13296 we are dealing with a compound dtype, which\n                    #  should be treated as 2D\n                    raise ValueError(\n                        \"Cannot construct a Series from an ndarray with \"\n                        \"compound dtype.  Use DataFrame instead.\"\n                    )\n            elif isinstance(data, Series):\n                if index is None:\n                    index = data.index\n                else:\n                    data = data.reindex(index, copy=copy)\n                    copy = False\n                data = data._mgr\n            elif is_dict_like(data):\n                data, index = self._init_dict(data, index, dtype)\n                dtype = None\n                copy = False\n            elif isinstance(data, SingleBlockManager):\n                if index is None:\n                    index = data.index\n                elif not data.index.equals(index) or copy:\n                    # GH#19275 SingleBlockManager input should only be called\n                    # internally\n                    raise AssertionError(\n                        \"Cannot pass both SingleBlockManager \"\n                        \"`data` argument and a different \"\n                        \"`index` argument. `copy` must be False.\"\n                    )\n\n            elif is_extension_array_dtype(data):\n                pass\n            elif isinstance(data, (set, frozenset)):\n                raise TypeError(f\"'{type(data).__name__}' type is unordered\")\n            else:\n                data = com.maybe_iterable_to_list(data)\n\n            if index is None:\n                if not is_list_like(data):\n                    data = [data]\n                index = ibase.default_index(len(data))\n            elif is_list_like(data):\n\n                # a scalar numpy array is list-like but doesn't\n                # have a proper length\n                try:\n                    if len(index) != len(data):\n                        raise ValueError(\n                            f\"Length of passed values is {len(data)}, \"\n                            f\"index implies {len(index)}.\"\n                        )\n                except TypeError:\n                    pass\n\n            # create/copy the manager\n            if isinstance(data, SingleBlockManager):\n                if dtype is not None:\n                    data = data.astype(dtype=dtype, errors=\"ignore\", copy=copy)\n                elif copy:\n                    data = data.copy()\n            else:\n                data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True)\n\n                data = SingleBlockManager.from_array(data, index)\n\n        generic.NDFrame.__init__(self, data)\n        self.name = name\n        self._set_axis(0, index, fastpath=True)\n\n    def _init_dict(self, data, index=None, dtype=None):\n        \"\"\"\n        Derive the \"_mgr\" and \"index\" attributes of a new Series from a\n        dictionary input.\n\n        Parameters\n        ----------\n        data : dict or dict-like\n            Data used to populate the new Series.\n        index : Index or index-like, default None\n            Index for the new Series: if None, use dict keys.\n        dtype : dtype, default None\n            The dtype for the new Series: if None, infer from data.\n\n        Returns\n        -------\n        _data : BlockManager for the new Series\n        index : index for the new Series\n        \"\"\"\n        # Looking for NaN in dict doesn't work ({np.nan : 1}[float('nan')]\n        # raises KeyError), so we iterate the entire dict, and align\n        if data:\n            keys, values = zip(*data.items())\n            values = list(values)\n        elif index is not None:\n            # fastpath for Series(data=None). Just use broadcasting a scalar\n            # instead of reindexing.\n            values = na_value_for_dtype(dtype)\n            keys = index\n        else:\n            keys, values = [], []\n\n        # Input is now list-like, so rely on \"standard\" construction:\n\n        # TODO: passing np.float64 to not break anything yet. See GH-17261\n        s = create_series_with_explicit_dtype(\n            values, index=keys, dtype=dtype, dtype_if_empty=np.float64\n        )\n\n        # Now we just make sure the order is respected, if any\n        if data and index is not None:\n            s = s.reindex(index, copy=False)\n        return s._mgr, s.index\n\n    # ----------------------------------------------------------------------\n\n    @property\n    def _constructor(self) -> Type[\"Series\"]:\n        return Series\n\n    @property\n    def _constructor_expanddim(self) -> Type[\"DataFrame\"]:\n        from pandas.core.frame import DataFrame\n\n        return DataFrame\n\n    # types\n    @property\n    def _can_hold_na(self):\n        return self._mgr._can_hold_na\n\n    _index = None\n\n    def _set_axis(self, axis: int, labels, fastpath: bool = False) -> None:\n        \"\"\"\n        Override generic, we want to set the _typ here.\n\n        This is called from the cython code when we set the `index` attribute\n        directly, e.g. `series.index = [1, 2, 3]`.\n        \"\"\"\n        if not fastpath:\n            labels = ensure_index(labels)\n\n        is_all_dates = labels.is_all_dates\n        if is_all_dates:\n            if not isinstance(labels, (DatetimeIndex, PeriodIndex, TimedeltaIndex)):\n                try:\n                    labels = DatetimeIndex(labels)\n                    # need to set here because we changed the index\n                    if fastpath:\n                        self._mgr.set_axis(axis, labels)\n                except (tslibs.OutOfBoundsDatetime, ValueError):\n                    # labels may exceeds datetime bounds,\n                    # or not be a DatetimeIndex\n                    pass\n\n        object.__setattr__(self, \"_index\", labels)\n        if not fastpath:\n            # The ensure_index call above ensures we have an Index object\n            self._mgr.set_axis(axis, labels)\n\n    # ndarray compatibility\n    @property\n    def dtype(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._mgr.dtype\n\n    @property\n    def dtypes(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        # DataFrame compatibility\n        return self.dtype\n\n    @property\n    def name(self) -> Label:\n        \"\"\"\n        Return the name of the Series.\n\n        The name of a Series becomes its index or column name if it is used\n        to form a DataFrame. It is also used whenever displaying the Series\n        using the interpreter.\n\n        Returns\n        -------\n        label (hashable object)\n            The name of the Series, also the column name if part of a DataFrame.\n\n        See Also\n        --------\n        Series.rename : Sets the Series name when given a scalar input.\n        Index.name : Corresponding Index property.\n\n        Examples\n        --------\n        The Series name can be set initially when calling the constructor.\n\n        >>> s = pd.Series([1, 2, 3], dtype=np.int64, name='Numbers')\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Numbers, dtype: int64\n        >>> s.name = \"Integers\"\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Integers, dtype: int64\n\n        The name of a Series within a DataFrame is its column name.\n\n        >>> df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],\n        ...                   columns=[\"Odd Numbers\", \"Even Numbers\"])\n        >>> df\n           Odd Numbers  Even Numbers\n        0            1             2\n        1            3             4\n        2            5             6\n        >>> df[\"Even Numbers\"].name\n        'Even Numbers'\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value: Label) -> None:\n        if not is_hashable(value):\n            raise TypeError(\"Series.name must be a hashable type\")\n        object.__setattr__(self, \"_name\", value)\n\n    @property\n    def values(self):\n        \"\"\"\n        Return Series as ndarray or ndarray-like depending on the dtype.\n\n        .. warning::\n\n           We recommend using :attr:`Series.array` or\n           :meth:`Series.to_numpy`, depending on whether you need\n           a reference to the underlying data or a NumPy array.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n\n        See Also\n        --------\n        Series.array : Reference to the underlying data.\n        Series.to_numpy : A NumPy array representing the underlying data.\n\n        Examples\n        --------\n        >>> pd.Series([1, 2, 3]).values\n        array([1, 2, 3])\n\n        >>> pd.Series(list('aabc')).values\n        array(['a', 'a', 'b', 'c'], dtype=object)\n\n        >>> pd.Series(list('aabc')).astype('category').values\n        [a, a, b, c]\n        Categories (3, object): [a, b, c]\n\n        Timezone aware datetime data is converted to UTC:\n\n        >>> pd.Series(pd.date_range('20130101', periods=3,\n        ...                         tz='US/Eastern')).values\n        array(['2013-01-01T05:00:00.000000000',\n               '2013-01-02T05:00:00.000000000',\n               '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')\n        \"\"\"\n        return self._mgr.external_values()\n\n    @property\n    def _values(self):\n        \"\"\"\n        Return the internal repr of this data (defined by Block.interval_values).\n        This are the values as stored in the Block (ndarray or ExtensionArray\n        depending on the Block class), with datetime64[ns] and timedelta64[ns]\n        wrapped in ExtensionArrays to match Index._values behavior.\n\n        Differs from the public ``.values`` for certain data types, because of\n        historical backwards compatibility of the public attribute (e.g. period\n        returns object ndarray and datetimetz a datetime64[ns] ndarray for\n        ``.values`` while it returns an ExtensionArray for ``._values`` in those\n        cases).\n\n        Differs from ``.array`` in that this still returns the numpy array if\n        the Block is backed by a numpy array (except for datetime64 and\n        timedelta64 dtypes), while ``.array`` ensures to always return an\n        ExtensionArray.\n\n        Overview:\n\n        dtype       | values        | _values       | array         |\n        ----------- | ------------- | ------------- | ------------- |\n        Numeric     | ndarray       | ndarray       | PandasArray   |\n        Category    | Categorical   | Categorical   | Categorical   |\n        dt64[ns]    | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        dt64[ns tz] | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        td64[ns]    | ndarray[m8ns] | TimedeltaArray| ndarray[m8ns] |\n        Period      | ndarray[obj]  | PeriodArray   | PeriodArray   |\n        Nullable    | EA            | EA            | EA            |\n\n        \"\"\"\n        return self._mgr.internal_values()\n\n    @Appender(base.IndexOpsMixin.array.__doc__)  # type: ignore\n    @property\n    def array(self) -> ExtensionArray:\n        return self._mgr._block.array_values()\n\n    # ops\n    def ravel(self, order=\"C\"):\n        \"\"\"\n        Return the flattened underlying data as an ndarray.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n            Flattened data of the Series.\n\n        See Also\n        --------\n        numpy.ndarray.ravel : Return a flattened array.\n        \"\"\"\n        return self._values.ravel(order=order)\n\n    def __len__(self) -> int:\n        \"\"\"\n        Return the length of the Series.\n        \"\"\"\n        return len(self._mgr)\n\n    def view(self, dtype=None) -> \"Series\":\n        \"\"\"\n        Create a new view of the Series.\n\n        This function will return a new Series with a view of the same\n        underlying values in memory, optionally reinterpreted with a new data\n        type. The new data type must preserve the same size in bytes as to not\n        cause index misalignment.\n\n        Parameters\n        ----------\n        dtype : data type\n            Data type object or one of their string representations.\n\n        Returns\n        -------\n        Series\n            A new Series object as a view of the same data in memory.\n\n        See Also\n        --------\n        numpy.ndarray.view : Equivalent numpy function to create a new view of\n            the same data in memory.\n\n        Notes\n        -----\n        Series are instantiated with ``dtype=float64`` by default. While\n        ``numpy.ndarray.view()`` will return a view with the same data type as\n        the original array, ``Series.view()`` (without specified dtype)\n        will try using ``float64`` and may fail if the original data type size\n        in bytes is not the same.\n\n        Examples\n        --------\n        >>> s = pd.Series([-2, -1, 0, 1, 2], dtype='int8')\n        >>> s\n        0   -2\n        1   -1\n        2    0\n        3    1\n        4    2\n        dtype: int8\n\n        The 8 bit signed integer representation of `-1` is `0b11111111`, but\n        the same bytes represent 255 if read as an 8 bit unsigned integer:\n\n        >>> us = s.view('uint8')\n        >>> us\n        0    254\n        1    255\n        2      0\n        3      1\n        4      2\n        dtype: uint8\n\n        The views share the same underlying values:\n\n        >>> us[0] = 128\n        >>> s\n        0   -128\n        1     -1\n        2      0\n        3      1\n        4      2\n        dtype: int8\n        \"\"\"\n        return self._constructor(\n            self._values.view(dtype), index=self.index\n        ).__finalize__(self, method=\"view\")\n\n    # ----------------------------------------------------------------------\n    # NDArray Compat\n    _HANDLED_TYPES = (Index, ExtensionArray, np.ndarray)\n\n    def __array_ufunc__(\n        self, ufunc: Callable, method: str, *inputs: Any, **kwargs: Any\n    ):\n        # TODO: handle DataFrame\n        cls = type(self)\n\n        # for binary ops, use our custom dunder methods\n        result = ops.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        # Determine if we should defer.\n        no_defer = (np.ndarray.__array_ufunc__, cls.__array_ufunc__)\n\n        for item in inputs:\n            higher_priority = (\n                hasattr(item, \"__array_priority__\")\n                and item.__array_priority__ > self.__array_priority__\n            )\n            has_array_ufunc = (\n                hasattr(item, \"__array_ufunc__\")\n                and type(item).__array_ufunc__ not in no_defer\n                and not isinstance(item, self._HANDLED_TYPES)\n            )\n            if higher_priority or has_array_ufunc:\n                return NotImplemented\n\n        # align all the inputs.\n        names = [getattr(x, \"name\") for x in inputs if hasattr(x, \"name\")]\n        types = tuple(type(x) for x in inputs)\n        # TODO: dataframe\n        alignable = [x for x, t in zip(inputs, types) if issubclass(t, Series)]\n\n        if len(alignable) > 1:\n            # This triggers alignment.\n            # At the moment, there aren't any ufuncs with more than two inputs\n            # so this ends up just being x1.index | x2.index, but we write\n            # it to handle *args.\n            index = alignable[0].index\n            for s in alignable[1:]:\n                index |= s.index\n            inputs = tuple(\n                x.reindex(index) if issubclass(t, Series) else x\n                for x, t in zip(inputs, types)\n            )\n        else:\n            index = self.index\n\n        inputs = tuple(extract_array(x, extract_numpy=True) for x in inputs)\n        result = getattr(ufunc, method)(*inputs, **kwargs)\n\n        name = names[0] if len(set(names)) == 1 else None\n\n        def construct_return(result):\n            if lib.is_scalar(result):\n                return result\n            elif result.ndim > 1:\n                # e.g. np.subtract.outer\n                if method == \"outer\":\n                    # GH#27198\n                    raise NotImplementedError\n                return result\n            return self._constructor(result, index=index, name=name, copy=False)\n\n        if type(result) is tuple:\n            # multiple return values\n            return tuple(construct_return(x) for x in result)\n        elif method == \"at\":\n            # no return value\n            return None\n        else:\n            return construct_return(result)\n\n    def __array__(self, dtype=None) -> np.ndarray:\n        \"\"\"\n        Return the values as a NumPy array.\n\n        Users should not call this directly. Rather, it is invoked by\n        :func:`numpy.array` and :func:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to use for the resulting NumPy array. By default,\n            the dtype is inferred from the data.\n\n        Returns\n        -------\n        numpy.ndarray\n            The values in the series converted to a :class:`numpy.ndarray`\n            with the specified `dtype`.\n\n        See Also\n        --------\n        array : Create a new array from data.\n        Series.array : Zero-copy view to the array backing the Series.\n        Series.to_numpy : Series method for similar behavior.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1, 2, 3])\n        >>> np.asarray(ser)\n        array([1, 2, 3])\n\n        For timezone-aware data, the timezones may be retained with\n        ``dtype='object'``\n\n        >>> tzser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\n        >>> np.asarray(tzser, dtype=\"object\")\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET', freq='D'),\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET', freq='D')],\n              dtype=object)\n\n        Or the values may be localized to UTC and the tzinfo discarded with\n        ``dtype='datetime64[ns]'``\n\n        >>> np.asarray(tzser, dtype=\"datetime64[ns]\")  # doctest: +ELLIPSIS\n        array(['1999-12-31T23:00:00.000000000', ...],\n              dtype='datetime64[ns]')\n        \"\"\"\n        return np.asarray(self.array, dtype)\n\n    # ----------------------------------------------------------------------\n    # Unary Methods\n\n    # coercion\n    __float__ = _coerce_method(float)\n    __long__ = _coerce_method(int)\n    __int__ = _coerce_method(int)\n\n    # ----------------------------------------------------------------------\n\n    # indexers\n    @property\n    def axes(self) -> List[Index]:\n        \"\"\"\n        Return a list of the row axis labels.\n        \"\"\"\n        return [self.index]\n\n    # ----------------------------------------------------------------------\n    # Indexing Methods\n\n    @Appender(generic.NDFrame.take.__doc__)\n    def take(self, indices, axis=0, is_copy=None, **kwargs) -> \"Series\":\n        if is_copy is not None:\n            warnings.warn(\n                \"is_copy is deprecated and will be removed in a future version. \"\n                \"'take' always returns a copy, so there is no need to specify this.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n        nv.validate_take(tuple(), kwargs)\n\n        indices = ensure_platform_int(indices)\n        new_index = self.index.take(indices)\n        new_values = self._values.take(indices)\n\n        result = self._constructor(new_values, index=new_index, fastpath=True)\n        return result.__finalize__(self, method=\"take\")\n\n    def _take_with_is_copy(self, indices, axis=0):\n        \"\"\"\n        Internal version of the `take` method that sets the `_is_copy`\n        attribute to keep track of the parent dataframe (using in indexing\n        for the SettingWithCopyWarning). For Series this does the same\n        as the public take (it never sets `_is_copy`).\n\n        See the docstring of `take` for full explanation of the parameters.\n        \"\"\"\n        return self.take(indices=indices, axis=axis)\n\n    def _ixs(self, i: int, axis: int = 0):\n        \"\"\"\n        Return the i-th value or values in the Series by location.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        scalar (int) or Series (slice, sequence)\n        \"\"\"\n        return self._values[i]\n\n    def _slice(self, slobj: slice, axis: int = 0) -> \"Series\":\n        # axis kwarg is retained for compat with NDFrame method\n        #  _slice is *always* positional\n        return self._get_values(slobj)\n\n    def __getitem__(self, key):\n        key = com.apply_if_callable(key, self)\n\n        if key is Ellipsis:\n            return self\n\n        key_is_scalar = is_scalar(key)\n        if isinstance(key, (list, tuple)):\n            key = unpack_1tuple(key)\n\n        if is_integer(key) and self.index._should_fallback_to_positional():\n            return self._values[key]\n\n        elif key_is_scalar:\n            return self._get_value(key)\n\n        if (\n            isinstance(key, tuple)\n            and is_hashable(key)\n            and isinstance(self.index, MultiIndex)\n        ):\n            # Otherwise index.get_value will raise InvalidIndexError\n            try:\n                result = self._get_value(key)\n\n                return result\n\n            except KeyError:\n                # We still have the corner case where this tuple is a key\n                #  in the first level of our MultiIndex\n                return self._get_values_tuple(key)\n\n        if is_iterator(key):\n            key = list(key)\n\n        if com.is_bool_indexer(key):\n            key = check_bool_indexer(self.index, key)\n            key = np.asarray(key, dtype=bool)\n            return self._get_values(key)\n\n        return self._get_with(key)\n\n    def _get_with(self, key):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            # _convert_slice_indexer to determine if this slice is positional\n            #  or label based, and if the latter, convert to positional\n            slobj = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            return self._slice(slobj)\n        elif isinstance(key, ABCDataFrame):\n            raise TypeError(\n                \"Indexing a Series with DataFrame is not \"\n                \"supported, use the appropriate DataFrame column\"\n            )\n        elif isinstance(key, tuple):\n            return self._get_values_tuple(key)\n\n        elif not is_list_like(key):\n            # e.g. scalars that aren't recognized by lib.is_scalar, GH#32684\n            return self.loc[key]\n\n        if not isinstance(key, (list, np.ndarray, ExtensionArray, Series, Index)):\n            key = list(key)\n\n        if isinstance(key, Index):\n            key_type = key.inferred_type\n        else:\n            key_type = lib.infer_dtype(key, skipna=False)\n\n        # Note: The key_type == \"boolean\" case should be caught by the\n        #  com.is_bool_indexer check in __getitem__\n        if key_type == \"integer\":\n            # We need to decide whether to treat this as a positional indexer\n            #  (i.e. self.iloc) or label-based (i.e. self.loc)\n            if not self.index._should_fallback_to_positional():\n                return self.loc[key]\n            else:\n                return self.iloc[key]\n\n        # handle the dup indexing case GH#4246\n        return self.loc[key]\n\n    def _get_values_tuple(self, key):\n        # mpl hackaround\n        if com.any_none(*key):\n            # suppress warning from slicing the index with a 2d indexer.\n            # eventually we'll want Series itself to warn.\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\", \"Support for multi-dim\", DeprecationWarning\n                )\n                return self._get_values(key)\n\n        if not isinstance(self.index, MultiIndex):\n            raise ValueError(\"Can only tuple-index with a MultiIndex\")\n\n        # If key is contained, would have returned by now\n        indexer, new_index = self.index.get_loc_level(key)\n        return self._constructor(self._values[indexer], index=new_index).__finalize__(\n            self,\n        )\n\n    def _get_values(self, indexer):\n        try:\n            return self._constructor(self._mgr.get_slice(indexer)).__finalize__(self,)\n        except ValueError:\n            # mpl compat if we look up e.g. ser[:, np.newaxis];\n            #  see tests.series.timeseries.test_mpl_compat_hack\n            return self._values[indexer]\n\n    def _get_value(self, label, takeable: bool = False):\n        \"\"\"\n        Quickly retrieve single value at passed index label.\n\n        Parameters\n        ----------\n        label : object\n        takeable : interpret the index as indexers, default False\n\n        Returns\n        -------\n        scalar value\n        \"\"\"\n        if takeable:\n            return self._values[label]\n\n        # Similar to Index.get_value, but we do not fall back to positional\n        loc = self.index.get_loc(label)\n        return self.index._get_values_for_loc(self, loc, label)\n\n    def __setitem__(self, key, value):\n        key = com.apply_if_callable(key, self)\n        cacher_needs_updating = self._check_is_chained_assignment_possible()\n\n        if key is Ellipsis:\n            key = slice(None)\n\n        try:\n            self._set_with_engine(key, value)\n        except (KeyError, ValueError):\n            values = self._values\n            if is_integer(key) and not self.index.inferred_type == \"integer\":\n                # positional setter\n                values[key] = value\n            else:\n                # GH#12862 adding an new key to the Series\n                self.loc[key] = value\n\n        except TypeError as e:\n            if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):\n                raise ValueError(\"Can only tuple-index with a MultiIndex\") from e\n\n            if com.is_bool_indexer(key):\n                key = check_bool_indexer(self.index, key)\n                key = np.asarray(key, dtype=bool)\n                try:\n                    self._where(~key, value, inplace=True)\n                except InvalidIndexError:\n                    self.iloc[key] = value\n                return\n\n            else:\n                self._set_with(key, value)\n\n        if cacher_needs_updating:\n            self._maybe_update_cacher()\n\n    def _set_with_engine(self, key, value):\n        # fails with AttributeError for IntervalIndex\n        loc = self.index._engine.get_loc(key)\n        validate_numeric_casting(self.dtype, value)\n        self._values[loc] = value\n\n    def _set_with(self, key, value):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            # extract_array so that if we set e.g. ser[-5:] = ser[:5]\n            #  we get the first five values, and not 5 NaNs\n            indexer = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            self.iloc[indexer] = extract_array(value, extract_numpy=True)\n\n        else:\n            assert not isinstance(key, tuple)\n\n            if is_scalar(key):\n                key = [key]\n\n            if isinstance(key, Index):\n                key_type = key.inferred_type\n                key = key._values\n            else:\n                key_type = lib.infer_dtype(key, skipna=False)\n\n            # Note: key_type == \"boolean\" should not occur because that\n            #  should be caught by the is_bool_indexer check in __setitem__\n            if key_type == \"integer\":\n                if not self.index._should_fallback_to_positional():\n                    self.loc[key] = value\n                else:\n                    self.iloc[key] = value\n            else:\n                self.loc[key] = value\n\n    def _set_value(self, label, value, takeable: bool = False):\n        \"\"\"\n        Quickly set single value at passed label.\n\n        If label is not contained, a new object is created with the label\n        placed at the end of the result index.\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed.\n        value : object\n            Scalar value.\n        takeable : interpret the index as indexers, default False\n        \"\"\"\n        try:\n            if takeable:\n                self._values[label] = value\n            else:\n                loc = self.index.get_loc(label)\n                validate_numeric_casting(self.dtype, value)\n                self._values[loc] = value\n        except KeyError:\n\n            # set using a non-recursive method\n            self.loc[label] = value\n\n    # ----------------------------------------------------------------------\n    # Unsorted\n\n    @property\n    def _is_mixed_type(self):\n        return False\n\n    def repeat(self, repeats, axis=None) -> \"Series\":\n        \"\"\"\n        Repeat elements of a Series.\n\n        Returns a new Series where each element of the current Series\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            Series.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        Series\n            Newly created Series with repeated elements.\n\n        See Also\n        --------\n        Index.repeat : Equivalent function for Index.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s\n        0    a\n        1    b\n        2    c\n        dtype: object\n        >>> s.repeat(2)\n        0    a\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        dtype: object\n        >>> s.repeat([1, 2, 3])\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        2    c\n        dtype: object\n        \"\"\"\n        nv.validate_repeat(tuple(), dict(axis=axis))\n        new_index = self.index.repeat(repeats)\n        new_values = self._values.repeat(repeats)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"repeat\"\n        )\n\n    def reset_index(self, level=None, drop=False, name=None, inplace=False):\n        \"\"\"\n        Generate a new DataFrame or Series with the index reset.\n\n        This is useful when the index needs to be treated as a column, or\n        when the index is meaningless and needs to be reset to the default\n        before another operation.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default optional\n            For a Series with a MultiIndex, only remove the specified levels\n            from the index. Removes all levels by default.\n        drop : bool, default False\n            Just reset the index, without inserting it as a column in\n            the new DataFrame.\n        name : object, optional\n            The name to use for the column containing the original Series\n            values. Uses ``self.name`` by default. This argument is ignored\n            when `drop` is True.\n        inplace : bool, default False\n            Modify the Series in place (do not create a new object).\n\n        Returns\n        -------\n        Series or DataFrame\n            When `drop` is False (the default), a DataFrame is returned.\n            The newly created columns will come first in the DataFrame,\n            followed by the original Series values.\n            When `drop` is True, a `Series` is returned.\n            In either case, if ``inplace=True``, no value is returned.\n\n        See Also\n        --------\n        DataFrame.reset_index: Analogous function for DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4], name='foo',\n        ...               index=pd.Index(['a', 'b', 'c', 'd'], name='idx'))\n\n        Generate a DataFrame with default index.\n\n        >>> s.reset_index()\n          idx  foo\n        0   a    1\n        1   b    2\n        2   c    3\n        3   d    4\n\n        To specify the name of the new column use `name`.\n\n        >>> s.reset_index(name='values')\n          idx  values\n        0   a       1\n        1   b       2\n        2   c       3\n        3   d       4\n\n        To generate a new Series with the default set `drop` to True.\n\n        >>> s.reset_index(drop=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        To update the Series in place, without generating a new one\n        set `inplace` to True. Note that it also requires ``drop=True``.\n\n        >>> s.reset_index(inplace=True, drop=True)\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        The `level` parameter is interesting for Series with a multi-level\n        index.\n\n        >>> arrays = [np.array(['bar', 'bar', 'baz', 'baz']),\n        ...           np.array(['one', 'two', 'one', 'two'])]\n        >>> s2 = pd.Series(\n        ...     range(4), name='foo',\n        ...     index=pd.MultiIndex.from_arrays(arrays,\n        ...                                     names=['a', 'b']))\n\n        To remove a specific level from the Index, use `level`.\n\n        >>> s2.reset_index(level='a')\n               a  foo\n        b\n        one  bar    0\n        two  bar    1\n        one  baz    2\n        two  baz    3\n\n        If `level` is not set, all levels are removed from the Index.\n\n        >>> s2.reset_index()\n             a    b  foo\n        0  bar  one    0\n        1  bar  two    1\n        2  baz  one    2\n        3  baz  two    3\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if drop:\n            new_index = ibase.default_index(len(self))\n            if level is not None:\n                if not isinstance(level, (tuple, list)):\n                    level = [level]\n                level = [self.index._get_level_number(lev) for lev in level]\n                if len(level) < self.index.nlevels:\n                    new_index = self.index.droplevel(level)\n\n            if inplace:\n                self.index = new_index\n                # set name if it was passed, otherwise, keep the previous name\n                self.name = name or self.name\n            else:\n                return self._constructor(\n                    self._values.copy(), index=new_index\n                ).__finalize__(self, method=\"reset_index\")\n        elif inplace:\n            raise TypeError(\n                \"Cannot reset_index inplace on a Series to create a DataFrame\"\n            )\n        else:\n            df = self.to_frame(name)\n            return df.reset_index(level=level, drop=drop)\n\n    # ----------------------------------------------------------------------\n    # Rendering Methods\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular Series.\n        \"\"\"\n        buf = StringIO(\"\")\n        width, height = get_terminal_size()\n        max_rows = (\n            height\n            if get_option(\"display.max_rows\") == 0\n            else get_option(\"display.max_rows\")\n        )\n        min_rows = (\n            height\n            if get_option(\"display.max_rows\") == 0\n            else get_option(\"display.min_rows\")\n        )\n        show_dimensions = get_option(\"display.show_dimensions\")\n\n        self.to_string(\n            buf=buf,\n            name=self.name,\n            dtype=self.dtype,\n            min_rows=min_rows,\n            max_rows=max_rows,\n            length=show_dimensions,\n        )\n        result = buf.getvalue()\n\n        return result\n\n    def to_string(\n        self,\n        buf=None,\n        na_rep=\"NaN\",\n        float_format=None,\n        header=True,\n        index=True,\n        length=False,\n        dtype=False,\n        name=False,\n        max_rows=None,\n        min_rows=None,\n    ):\n        \"\"\"\n        Render a string representation of the Series.\n\n        Parameters\n        ----------\n        buf : StringIO-like, optional\n            Buffer to write to.\n        na_rep : str, optional\n            String representation of NaN to use, default 'NaN'.\n        float_format : one-parameter function, optional\n            Formatter function to apply to columns' elements if they are\n            floats, default None.\n        header : bool, default True\n            Add the Series header (index name).\n        index : bool, optional\n            Add index (row) labels, default True.\n        length : bool, default False\n            Add the Series length.\n        dtype : bool, default False\n            Add the Series dtype.\n        name : bool, default False\n            Add the Series name if not None.\n        max_rows : int, optional\n            Maximum number of rows to show before truncating. If None, show\n            all.\n        min_rows : int, optional\n            The number of rows to display in a truncated repr (when number\n            of rows is above `max_rows`).\n\n        Returns\n        -------\n        str or None\n            String representation of Series if ``buf=None``, otherwise None.\n        \"\"\"\n        formatter = fmt.SeriesFormatter(\n            self,\n            name=name,\n            length=length,\n            header=header,\n            index=index,\n            dtype=dtype,\n            na_rep=na_rep,\n            float_format=float_format,\n            min_rows=min_rows,\n            max_rows=max_rows,\n        )\n        result = formatter.to_string()\n\n        # catch contract violations\n        if not isinstance(result, str):\n            raise AssertionError(\n                \"result must be of type str, type \"\n                f\"of result is {repr(type(result).__name__)}\"\n            )\n\n        if buf is None:\n            return result\n        else:\n            try:\n                buf.write(result)\n            except AttributeError:\n                with open(buf, \"w\") as f:\n                    f.write(result)\n\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> s = pd.Series([\"elk\", \"pig\", \"dog\", \"quetzal\"], name=\"animal\")\n        >>> print(s.to_markdown())\n        |    | animal   |\n        |---:|:---------|\n        |  0 | elk      |\n        |  1 | pig      |\n        |  2 | dog      |\n        |  3 | quetzal  |\n        \"\"\"\n    )\n    @Substitution(klass=\"Series\")\n    @Appender(generic._shared_docs[\"to_markdown\"])\n    def to_markdown(\n        self, buf: Optional[IO[str]] = None, mode: Optional[str] = None, **kwargs\n    ) -> Optional[str]:\n        return self.to_frame().to_markdown(buf, mode, **kwargs)\n\n    # ----------------------------------------------------------------------\n\n    def items(self) -> Iterable[Tuple[Label, Any]]:\n        \"\"\"\n        Lazily iterate over (index, value) tuples.\n\n        This method returns an iterable tuple (index, value). This is\n        convenient if you want to create a lazy iterator.\n\n        Returns\n        -------\n        iterable\n            Iterable of tuples containing the (index, value) pairs from a\n            Series.\n\n        See Also\n        --------\n        DataFrame.items : Iterate over (column name, Series) pairs.\n        DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs.\n\n        Examples\n        --------\n        >>> s = pd.Series(['A', 'B', 'C'])\n        >>> for index, value in s.items():\n        ...     print(f\"Index : {index}, Value : {value}\")\n        Index : 0, Value : A\n        Index : 1, Value : B\n        Index : 2, Value : C\n        \"\"\"\n        return zip(iter(self.index), iter(self))\n\n    @Appender(items.__doc__)\n    def iteritems(self) -> Iterable[Tuple[Label, Any]]:\n        return self.items()\n\n    # ----------------------------------------------------------------------\n    # Misc public methods\n\n    def keys(self) -> Index:\n        \"\"\"\n        Return alias for index.\n\n        Returns\n        -------\n        Index\n            Index of the Series.\n        \"\"\"\n        return self.index\n\n    def to_dict(self, into=dict):\n        \"\"\"\n        Convert Series to {label -> value} dict or dict-like object.\n\n        Parameters\n        ----------\n        into : class, default dict\n            The collections.abc.Mapping subclass to use as the return\n            object. Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Key-value representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.to_dict()\n        {0: 1, 1: 2, 2: 3, 3: 4}\n        >>> from collections import OrderedDict, defaultdict\n        >>> s.to_dict(OrderedDict)\n        OrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])\n        >>> dd = defaultdict(list)\n        >>> s.to_dict(dd)\n        defaultdict(<class 'list'>, {0: 1, 1: 2, 2: 3, 3: 4})\n        \"\"\"\n        # GH16122\n        into_c = com.standardize_mapping(into)\n        return into_c(self.items())\n\n    def to_frame(self, name=None) -> \"DataFrame\":\n        \"\"\"\n        Convert Series to DataFrame.\n\n        Parameters\n        ----------\n        name : object, default None\n            The passed name should substitute for the series name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([\"a\", \"b\", \"c\"],\n        ...               name=\"vals\")\n        >>> s.to_frame()\n          vals\n        0    a\n        1    b\n        2    c\n        \"\"\"\n        if name is None:\n            df = self._constructor_expanddim(self)\n        else:\n            df = self._constructor_expanddim({name: self})\n\n        return df\n\n    def _set_name(self, name, inplace=False) -> \"Series\":\n        \"\"\"\n        Set the Series name.\n\n        Parameters\n        ----------\n        name : str\n        inplace : bool\n            Whether to modify `self` directly or return a copy.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        ser = self if inplace else self.copy()\n        ser.name = name\n        return ser\n\n    @Appender(\n        \"\"\"\nExamples\n--------\n>>> ser = pd.Series([390., 350., 30., 20.],\n...                 index=['Falcon', 'Falcon', 'Parrot', 'Parrot'], name=\"Max Speed\")\n>>> ser\nFalcon    390.0\nFalcon    350.0\nParrot     30.0\nParrot     20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby([\"a\", \"b\", \"a\", \"b\"]).mean()\na    210.0\nb    185.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(ser > 100).mean()\nMax Speed\nFalse     25.0\nTrue     370.0\nName: Max Speed, dtype: float64\n\n**Grouping by Indexes**\n\nWe can groupby different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> ser = pd.Series([390., 350., 30., 20.], index=index, name=\"Max Speed\")\n>>> ser\nAnimal  Type\nFalcon  Captive    390.0\n        Wild       350.0\nParrot  Captive     30.0\n        Wild        20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nAnimal\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=\"Type\").mean()\nType\nCaptive    210.0\nWild       185.0\nName: Max Speed, dtype: float64\n\nWe can also choose to include `NA` in group keys or not by defining\n`dropna` parameter, the default setting is `True`:\n\n>>> ser = pd.Series([1, 2, 3, 3], index=[\"a\", 'a', 'b', np.nan])\n>>> ser.groupby(level=0).sum()\na    3\nb    3\ndtype: int64\n\n>>> ser.groupby(level=0, dropna=False).sum()\na    3\nb    3\nNaN  3\ndtype: int64\n\n>>> arrays = ['Falcon', 'Falcon', 'Parrot', 'Parrot']\n>>> ser = pd.Series([390., 350., 30., 20.], index=arrays, name=\"Max Speed\")\n>>> ser.groupby([\"a\", \"b\", \"a\", np.nan]).mean()\na    210.0\nb    350.0\nName: Max Speed, dtype: float64\n\n>>> ser.groupby([\"a\", \"b\", \"a\", np.nan], dropna=False).mean()\na    210.0\nb    350.0\nNaN   20.0\nName: Max Speed, dtype: float64\n\"\"\"\n    )\n    @Appender(generic._shared_docs[\"groupby\"] % _shared_doc_kwargs)\n    def groupby(\n        self,\n        by=None,\n        axis=0,\n        level=None,\n        as_index: bool = True,\n        sort: bool = True,\n        group_keys: bool = True,\n        squeeze: bool = False,\n        observed: bool = False,\n        dropna: bool = True,\n    ) -> \"SeriesGroupBy\":\n        from pandas.core.groupby.generic import SeriesGroupBy\n\n        if level is None and by is None:\n            raise TypeError(\"You have to supply one of 'by' and 'level'\")\n        axis = self._get_axis_number(axis)\n\n        return SeriesGroupBy(\n            obj=self,\n            keys=by,\n            axis=axis,\n            level=level,\n            as_index=as_index,\n            sort=sort,\n            group_keys=group_keys,\n            squeeze=squeeze,\n            observed=observed,\n            dropna=dropna,\n        )\n\n    # ----------------------------------------------------------------------\n    # Statistics, overridden ndarray methods\n\n    # TODO: integrate bottleneck\n\n    def count(self, level=None):\n        \"\"\"\n        Return number of non-NA/null observations in the Series.\n\n        Parameters\n        ----------\n        level : int or level name, default None\n            If the axis is a MultiIndex (hierarchical), count along a\n            particular level, collapsing into a smaller Series.\n\n        Returns\n        -------\n        int or Series (if level specified)\n            Number of non-null values in the Series.\n\n        See Also\n        --------\n        DataFrame.count : Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.0, 1.0, np.nan])\n        >>> s.count()\n        2\n        \"\"\"\n        if level is None:\n            return notna(self.array).sum()\n\n        if isinstance(level, str):\n            level = self.index._get_level_number(level)\n\n        lev = self.index.levels[level]\n        level_codes = np.array(self.index.codes[level], subok=False, copy=True)\n\n        mask = level_codes == -1\n        if mask.any():\n            level_codes[mask] = cnt = len(lev)\n            lev = lev.insert(cnt, lev._na_value)\n\n        obs = level_codes[notna(self._values)]\n        out = np.bincount(obs, minlength=len(lev) or None)\n        return self._constructor(out, index=lev, dtype=\"int64\").__finalize__(\n            self, method=\"count\"\n        )\n\n    def mode(self, dropna=True) -> \"Series\":\n        \"\"\"\n        Return the mode(s) of the dataset.\n\n        Always returns Series even if only one value is returned.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        Series\n            Modes of the Series in sorted order.\n        \"\"\"\n        # TODO: Add option for bins like value_counts()\n        return algorithms.mode(self, dropna=dropna)\n\n    def unique(self):\n        \"\"\"\n        Return unique values of Series object.\n\n        Uniques are returned in order of appearance. Hash table-based unique,\n        therefore does NOT sort.\n\n        Returns\n        -------\n        ndarray or ExtensionArray\n            The unique values returned as a NumPy array. See Notes.\n\n        See Also\n        --------\n        unique : Top-level unique method for any 1-d array-like object.\n        Index.unique : Return Index with unique values from an Index object.\n\n        Notes\n        -----\n        Returns the unique values as a NumPy array. In case of an\n        extension-array backed Series, a new\n        :class:`~api.extensions.ExtensionArray` of that type with just\n        the unique values is returned. This includes\n\n            * Categorical\n            * Period\n            * Datetime with Timezone\n            * Interval\n            * Sparse\n            * IntegerNA\n\n        See Examples section.\n\n        Examples\n        --------\n        >>> pd.Series([2, 1, 3, 3], name='A').unique()\n        array([2, 1, 3])\n\n        >>> pd.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()\n        array(['2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\n\n        >>> pd.Series([pd.Timestamp('2016-01-01', tz='US/Eastern')\n        ...            for _ in range(3)]).unique()\n        <DatetimeArray>\n        ['2016-01-01 00:00:00-05:00']\n        Length: 1, dtype: datetime64[ns, US/Eastern]\n\n        An unordered Categorical will return categories in the order of\n        appearance.\n\n        >>> pd.Series(pd.Categorical(list('baabc'))).unique()\n        [b, a, c]\n        Categories (3, object): [b, a, c]\n\n        An ordered Categorical preserves the category ordering.\n\n        >>> pd.Series(pd.Categorical(list('baabc'), categories=list('abc'),\n        ...                          ordered=True)).unique()\n        [b, a, c]\n        Categories (3, object): [a < b < c]\n        \"\"\"\n        result = super().unique()\n        return result\n\n    def drop_duplicates(self, keep=\"first\", inplace=False) -> Optional[\"Series\"]:\n        \"\"\"\n        Return Series with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        inplace : bool, default ``False``\n            If ``True``, performs operation inplace and returns None.\n\n        Returns\n        -------\n        Series\n            Series with duplicates dropped.\n\n        See Also\n        --------\n        Index.drop_duplicates : Equivalent method on Index.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Series.duplicated : Related method on Series, indicating duplicate\n            Series values.\n\n        Examples\n        --------\n        Generate a Series with duplicated entries.\n\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'],\n        ...               name='animal')\n        >>> s\n        0      lama\n        1       cow\n        2      lama\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        With the 'keep' parameter, the selection behaviour of duplicated values\n        can be changed. The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> s.drop_duplicates()\n        0      lama\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n\n        The value 'last' for parameter 'keep' keeps the last occurrence for\n        each set of duplicated entries.\n\n        >>> s.drop_duplicates(keep='last')\n        1       cow\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        The value ``False`` for parameter 'keep' discards all sets of\n        duplicated entries. Setting the value of 'inplace' to ``True`` performs\n        the operation inplace and returns ``None``.\n\n        >>> s.drop_duplicates(keep=False, inplace=True)\n        >>> s\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        result = super().drop_duplicates(keep=keep)\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    def duplicated(self, keep=\"first\") -> \"Series\":\n        \"\"\"\n        Indicate duplicate Series values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        Series. Either all duplicates, all except the first or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series\n            Series indicating whether each value has occurred in the\n            preceding values.\n\n        See Also\n        --------\n        Index.duplicated : Equivalent method on pandas.Index.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Series.drop_duplicates : Remove duplicate values from Series.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set on False and all others on True:\n\n        >>> animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> animals.duplicated()\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        which is equivalent to\n\n        >>> animals.duplicated(keep='first')\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> animals.duplicated(keep='last')\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> animals.duplicated(keep=False)\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n        \"\"\"\n        return super().duplicated(keep=keep)\n\n    def idxmin(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the minimum value.\n\n        If multiple values equal the minimum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmin. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the minimum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmin : Return indices of the minimum values\n            along the given axis.\n        DataFrame.idxmin : Return index of first occurrence of minimum\n            over requested axis.\n        Series.idxmax : Return index *label* of the first occurrence\n            of maximum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmin``. This method\n        returns the label of the minimum, while ``ndarray.argmin`` returns\n        the position. To get the position, use ``series.values.argmin()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 1],\n        ...               index=['A', 'B', 'C', 'D'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    1.0\n        dtype: float64\n\n        >>> s.idxmin()\n        'A'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmin(skipna=False)\n        nan\n        \"\"\"\n        skipna = nv.validate_argmin_with_skipna(skipna, args, kwargs)\n        i = nanops.nanargmin(self._values, skipna=skipna)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def idxmax(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the maximum value.\n\n        If multiple values equal the maximum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmax. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the maximum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmax : Return indices of the maximum values\n            along the given axis.\n        DataFrame.idxmax : Return index of first occurrence of maximum\n            over requested axis.\n        Series.idxmin : Return index *label* of the first occurrence\n            of minimum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmax``. This method\n        returns the label of the maximum, while ``ndarray.argmax`` returns\n        the position. To get the position, use ``series.values.argmax()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 3, 4],\n        ...               index=['A', 'B', 'C', 'D', 'E'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    3.0\n        E    4.0\n        dtype: float64\n\n        >>> s.idxmax()\n        'C'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmax(skipna=False)\n        nan\n        \"\"\"\n        skipna = nv.validate_argmax_with_skipna(skipna, args, kwargs)\n        i = nanops.nanargmax(self._values, skipna=skipna)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def round(self, decimals=0, *args, **kwargs) -> \"Series\":\n        \"\"\"\n        Round each value in a Series to the given number of decimals.\n\n        Parameters\n        ----------\n        decimals : int, default 0\n            Number of decimal places to round to. If decimals is negative,\n            it specifies the number of positions to the left of the decimal point.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Series\n            Rounded values of the Series.\n\n        See Also\n        --------\n        numpy.around : Round values of an np.array.\n        DataFrame.round : Round values of a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.1, 1.3, 2.7])\n        >>> s.round()\n        0    0.0\n        1    1.0\n        2    3.0\n        dtype: float64\n        \"\"\"\n        nv.validate_round(args, kwargs)\n        result = self._values.round(decimals)\n        result = self._constructor(result, index=self.index).__finalize__(\n            self, method=\"round\"\n        )\n\n        return result\n\n    def quantile(self, q=0.5, interpolation=\"linear\"):\n        \"\"\"\n        Return value at the given quantile.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            The quantile(s) to compute, which can lie in range: 0 <= q <= 1.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            This optional parameter specifies the interpolation method to use,\n            when the desired quantile lies between two data points `i` and `j`:\n\n                * linear: `i + (j - i) * fraction`, where `fraction` is the\n                  fractional part of the index surrounded by `i` and `j`.\n                * lower: `i`.\n                * higher: `j`.\n                * nearest: `i` or `j` whichever is nearest.\n                * midpoint: (`i` + `j`) / 2.\n\n        Returns\n        -------\n        float or Series\n            If ``q`` is an array, a Series will be returned where the\n            index is ``q`` and the values are the quantiles, otherwise\n            a float will be returned.\n\n        See Also\n        --------\n        core.window.Rolling.quantile : Calculate the rolling quantile.\n        numpy.percentile : Returns the q-th percentile(s) of the array elements.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.quantile(.5)\n        2.5\n        >>> s.quantile([.25, .5, .75])\n        0.25    1.75\n        0.50    2.50\n        0.75    3.25\n        dtype: float64\n        \"\"\"\n        validate_percentile(q)\n\n        # We dispatch to DataFrame so that core.internals only has to worry\n        #  about 2D cases.\n        df = self.to_frame()\n\n        result = df.quantile(q=q, interpolation=interpolation, numeric_only=False)\n        if result.ndim == 2:\n            result = result.iloc[:, 0]\n\n        if is_list_like(q):\n            result.name = self.name\n            return self._constructor(result, index=Float64Index(q), name=self.name)\n        else:\n            # scalar\n            return result.iloc[0]\n\n    def corr(self, other, method=\"pearson\", min_periods=None) -> float:\n        \"\"\"\n        Compute correlation with `other` Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the correlation.\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method used to compute correlation:\n\n            - pearson : Standard correlation coefficient\n            - kendall : Kendall Tau correlation coefficient\n            - spearman : Spearman rank correlation\n            - callable: Callable with input two 1d ndarrays and returning a float.\n\n            .. versionadded:: 0.24.0\n                Note that the returned matrix from corr will have 1 along the\n                diagonals and will be symmetric regardless of the callable's\n                behavior.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n\n        Returns\n        -------\n        float\n            Correlation with other.\n\n        See Also\n        --------\n        DataFrame.corr : Compute pairwise correlation between columns.\n        DataFrame.corrwith : Compute pairwise correlation with another\n            DataFrame or Series.\n\n        Examples\n        --------\n        >>> def histogram_intersection(a, b):\n        ...     v = np.minimum(a, b).sum().round(decimals=1)\n        ...     return v\n        >>> s1 = pd.Series([.2, .0, .6, .2])\n        >>> s2 = pd.Series([.3, .6, .0, .1])\n        >>> s1.corr(s2, method=histogram_intersection)\n        0.3\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n\n        if method in [\"pearson\", \"spearman\", \"kendall\"] or callable(method):\n            return nanops.nancorr(\n                this.values, other.values, method=method, min_periods=min_periods\n            )\n\n        raise ValueError(\n            \"method must be either 'pearson', \"\n            \"'spearman', 'kendall', or a callable, \"\n            f\"'{method}' was supplied\"\n        )\n\n    def cov(self, other, min_periods=None) -> float:\n        \"\"\"\n        Compute covariance with Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the covariance.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n\n        Returns\n        -------\n        float\n            Covariance between Series and other normalized by N-1\n            (unbiased estimator).\n\n        See Also\n        --------\n        DataFrame.cov : Compute pairwise covariance of columns.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([0.90010907, 0.13484424, 0.62036035])\n        >>> s2 = pd.Series([0.12528585, 0.26962463, 0.51111198])\n        >>> s1.cov(s2)\n        -0.01685762652715874\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancov(this.values, other.values, min_periods=min_periods)\n\n    def diff(self, periods: int = 1) -> \"Series\":\n        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of a Series element compared with another\n        element in the Series (default is element in previous row).\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative\n            values.\n\n        Returns\n        -------\n        Series\n            First differences of the Series.\n\n        See Also\n        --------\n        Series.pct_change: Percent change over given number of periods.\n        Series.shift: Shift index by desired number of periods with an\n            optional time freq.\n        DataFrame.diff: First discrete difference of object.\n\n        Notes\n        -----\n        For boolean dtypes, this uses :meth:`operator.xor` rather than\n        :meth:`operator.sub`.\n\n        Examples\n        --------\n        Difference with previous row\n\n        >>> s = pd.Series([1, 1, 2, 3, 5, 8])\n        >>> s.diff()\n        0    NaN\n        1    0.0\n        2    1.0\n        3    1.0\n        4    2.0\n        5    3.0\n        dtype: float64\n\n        Difference with 3rd previous row\n\n        >>> s.diff(periods=3)\n        0    NaN\n        1    NaN\n        2    NaN\n        3    2.0\n        4    4.0\n        5    6.0\n        dtype: float64\n\n        Difference with following row\n\n        >>> s.diff(periods=-1)\n        0    0.0\n        1   -1.0\n        2   -1.0\n        3   -2.0\n        4   -3.0\n        5    NaN\n        dtype: float64\n        \"\"\"\n        result = algorithms.diff(self.array, periods)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"diff\"\n        )\n\n    def autocorr(self, lag=1) -> float:\n        \"\"\"\n        Compute the lag-N autocorrelation.\n\n        This method computes the Pearson correlation between\n        the Series and its shifted self.\n\n        Parameters\n        ----------\n        lag : int, default 1\n            Number of lags to apply before performing autocorrelation.\n\n        Returns\n        -------\n        float\n            The Pearson correlation between self and self.shift(lag).\n\n        See Also\n        --------\n        Series.corr : Compute the correlation between two Series.\n        Series.shift : Shift index by desired number of periods.\n        DataFrame.corr : Compute pairwise correlation of columns.\n        DataFrame.corrwith : Compute pairwise correlation between rows or\n            columns of two DataFrame objects.\n\n        Notes\n        -----\n        If the Pearson correlation is not well defined return 'NaN'.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.25, 0.5, 0.2, -0.05])\n        >>> s.autocorr()  # doctest: +ELLIPSIS\n        0.10355...\n        >>> s.autocorr(lag=2)  # doctest: +ELLIPSIS\n        -0.99999...\n\n        If the Pearson correlation is not well defined, then 'NaN' is returned.\n\n        >>> s = pd.Series([1, 0, 0, 0])\n        >>> s.autocorr()\n        nan\n        \"\"\"\n        return self.corr(self.shift(lag))\n\n    def dot(self, other):\n        \"\"\"\n        Compute the dot product between the Series and the columns of other.\n\n        This method computes the dot product between the Series and another\n        one, or the Series and each columns of a DataFrame, or the Series and\n        each columns of an array.\n\n        It can also be called using `self @ other` in Python >= 3.5.\n\n        Parameters\n        ----------\n        other : Series, DataFrame or array-like\n            The other object to compute the dot product with its columns.\n\n        Returns\n        -------\n        scalar, Series or numpy.ndarray\n            Return the dot product of the Series and other if other is a\n            Series, the Series of the dot product of Series and each rows of\n            other if other is a DataFrame or a numpy.ndarray between the Series\n            and each columns of the numpy array.\n\n        See Also\n        --------\n        DataFrame.dot: Compute the matrix product with the DataFrame.\n        Series.mul: Multiplication of series and other, element-wise.\n\n        Notes\n        -----\n        The Series and other has to share the same index if other is a Series\n        or a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0, 1, 2, 3])\n        >>> other = pd.Series([-1, 2, -3, 4])\n        >>> s.dot(other)\n        8\n        >>> s @ other\n        8\n        >>> df = pd.DataFrame([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(df)\n        0    24\n        1    14\n        dtype: int64\n        >>> arr = np.array([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(arr)\n        array([24, 14])\n        \"\"\"\n        if isinstance(other, (Series, ABCDataFrame)):\n            common = self.index.union(other.index)\n            if len(common) > len(self.index) or len(common) > len(other.index):\n                raise ValueError(\"matrices are not aligned\")\n\n            left = self.reindex(index=common, copy=False)\n            right = other.reindex(index=common, copy=False)\n            lvals = left.values\n            rvals = right.values\n        else:\n            lvals = self.values\n            rvals = np.asarray(other)\n            if lvals.shape[0] != rvals.shape[0]:\n                raise Exception(\n                    f\"Dot product shape mismatch, {lvals.shape} vs {rvals.shape}\"\n                )\n\n        if isinstance(other, ABCDataFrame):\n            return self._constructor(\n                np.dot(lvals, rvals), index=other.columns\n            ).__finalize__(self, method=\"dot\")\n        elif isinstance(other, Series):\n            return np.dot(lvals, rvals)\n        elif isinstance(rvals, np.ndarray):\n            return np.dot(lvals, rvals)\n        else:  # pragma: no cover\n            raise TypeError(f\"unsupported type: {type(other)}\")\n\n    def __matmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(other)\n\n    def __rmatmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(np.transpose(other))\n\n    @doc(base.IndexOpsMixin.searchsorted, klass=\"Series\")\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        return algorithms.searchsorted(self._values, value, side=side, sorter=sorter)\n\n    # -------------------------------------------------------------------\n    # Combination\n\n    def append(self, to_append, ignore_index=False, verify_integrity=False):\n        \"\"\"\n        Concatenate two or more Series.\n\n        Parameters\n        ----------\n        to_append : Series or list/tuple of Series\n            Series to append with self.\n        ignore_index : bool, default False\n            If True, do not use the index labels.\n        verify_integrity : bool, default False\n            If True, raise Exception on creating index with duplicates.\n\n        Returns\n        -------\n        Series\n            Concatenated Series.\n\n        See Also\n        --------\n        concat : General function to concatenate DataFrame or Series objects.\n\n        Notes\n        -----\n        Iteratively appending to a Series can be more computationally intensive\n        than a single concatenate. A better solution is to append values to a\n        list and then concatenate the list with the original Series all at\n        once.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, 2, 3])\n        >>> s2 = pd.Series([4, 5, 6])\n        >>> s3 = pd.Series([4, 5, 6], index=[3, 4, 5])\n        >>> s1.append(s2)\n        0    1\n        1    2\n        2    3\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s1.append(s3)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `ignore_index` set to True:\n\n        >>> s1.append(s2, ignore_index=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `verify_integrity` set to True:\n\n        >>> s1.append(s2, verify_integrity=True)\n        Traceback (most recent call last):\n        ...\n        ValueError: Indexes have overlapping values: [0, 1, 2]\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        if isinstance(to_append, (list, tuple)):\n            to_concat = [self]\n            to_concat.extend(to_append)\n        else:\n            to_concat = [self, to_append]\n        if any(isinstance(x, (ABCDataFrame,)) for x in to_concat[1:]):\n            msg = \"to_append should be a Series or list/tuple of Series, got DataFrame\"\n            raise TypeError(msg)\n        return concat(\n            to_concat, ignore_index=ignore_index, verify_integrity=verify_integrity\n        )\n\n    def _binop(self, other, func, level=None, fill_value=None):\n        \"\"\"\n        Perform generic binary operation with optional fill value.\n\n        Parameters\n        ----------\n        other : Series\n        func : binary operator\n        fill_value : float or object\n            Value to substitute for NA/null values. If both Series are NA in a\n            location, the result will be NA regardless of the passed fill value.\n        level : int or level name, default None\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level.\n\n        Returns\n        -------\n        Series\n        \"\"\"\n        if not isinstance(other, Series):\n            raise AssertionError(\"Other operand must be Series\")\n\n        this = self\n\n        if not self.index.equals(other.index):\n            this, other = self.align(other, level=level, join=\"outer\", copy=False)\n\n        this_vals, other_vals = ops.fill_binop(this.values, other.values, fill_value)\n\n        with np.errstate(all=\"ignore\"):\n            result = func(this_vals, other_vals)\n\n        name = ops.get_op_result_name(self, other)\n        ret = this._construct_result(result, name)\n        return ret\n\n    def _construct_result(\n        self, result: Union[ArrayLike, Tuple[ArrayLike, ArrayLike]], name: Label\n    ) -> Union[\"Series\", Tuple[\"Series\", \"Series\"]]:\n        \"\"\"\n        Construct an appropriately-labelled Series from the result of an op.\n\n        Parameters\n        ----------\n        result : ndarray or ExtensionArray\n        name : Label\n\n        Returns\n        -------\n        Series\n            In the case of __divmod__ or __rdivmod__, a 2-tuple of Series.\n        \"\"\"\n        if isinstance(result, tuple):\n            # produced by divmod or rdivmod\n\n            res1 = self._construct_result(result[0], name=name)\n            res2 = self._construct_result(result[1], name=name)\n\n            # GH#33427 assertions to keep mypy happy\n            assert isinstance(res1, Series)\n            assert isinstance(res2, Series)\n            return (res1, res2)\n\n        # We do not pass dtype to ensure that the Series constructor\n        #  does inference in the case where `result` has object-dtype.\n        out = self._constructor(result, index=self.index)\n        out = out.__finalize__(self)\n\n        # Set the result's name after __finalize__ is called because __finalize__\n        #  would set it back to self.name\n        out.name = name\n        return out\n\n    def combine(self, other, func, fill_value=None) -> \"Series\":\n        \"\"\"\n        Combine the Series with a Series or scalar according to `func`.\n\n        Combine the Series and `other` using `func` to perform elementwise\n        selection for combined Series.\n        `fill_value` is assumed when value is missing at some index\n        from one of the two objects being combined.\n\n        Parameters\n        ----------\n        other : Series or scalar\n            The value(s) to be combined with the `Series`.\n        func : function\n            Function that takes two scalars as inputs and returns an element.\n        fill_value : scalar, optional\n            The value to assume when an index is missing from\n            one Series or the other. The default specifies to use the\n            appropriate NaN value for the underlying dtype of the Series.\n\n        Returns\n        -------\n        Series\n            The result of combining the Series with the other object.\n\n        See Also\n        --------\n        Series.combine_first : Combine Series values, choosing the calling\n            Series' values first.\n\n        Examples\n        --------\n        Consider 2 Datasets ``s1`` and ``s2`` containing\n        highest clocked speeds of different birds.\n\n        >>> s1 = pd.Series({'falcon': 330.0, 'eagle': 160.0})\n        >>> s1\n        falcon    330.0\n        eagle     160.0\n        dtype: float64\n        >>> s2 = pd.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})\n        >>> s2\n        falcon    345.0\n        eagle     200.0\n        duck       30.0\n        dtype: float64\n\n        Now, to combine the two datasets and view the highest speeds\n        of the birds across the two datasets\n\n        >>> s1.combine(s2, max)\n        duck        NaN\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n\n        In the previous example, the resulting value for duck is missing,\n        because the maximum of a NaN and a float is a NaN.\n        So, in the example, we set ``fill_value=0``,\n        so the maximum value returned will be the value from some dataset.\n\n        >>> s1.combine(s2, max, fill_value=0)\n        duck       30.0\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n        \"\"\"\n        if fill_value is None:\n            fill_value = na_value_for_dtype(self.dtype, compat=False)\n\n        if isinstance(other, Series):\n            # If other is a Series, result is based on union of Series,\n            # so do this element by element\n            new_index = self.index.union(other.index)\n            new_name = ops.get_op_result_name(self, other)\n            new_values = []\n            for idx in new_index:\n                lv = self.get(idx, fill_value)\n                rv = other.get(idx, fill_value)\n                with np.errstate(all=\"ignore\"):\n                    new_values.append(func(lv, rv))\n        else:\n            # Assume that other is a scalar, so apply the function for\n            # each element in the Series\n            new_index = self.index\n            with np.errstate(all=\"ignore\"):\n                new_values = [func(lv, other) for lv in self._values]\n            new_name = self.name\n\n        if is_categorical_dtype(self.dtype):\n            pass\n        elif is_extension_array_dtype(self.dtype):\n            # TODO: can we do this for only SparseDtype?\n            # The function can return something of any type, so check\n            # if the type is compatible with the calling EA.\n            new_values = maybe_cast_to_extension_array(type(self._values), new_values)\n        return self._constructor(new_values, index=new_index, name=new_name)\n\n    def combine_first(self, other) -> \"Series\":\n        \"\"\"\n        Combine Series values, choosing the calling Series's values first.\n\n        Parameters\n        ----------\n        other : Series\n            The value(s) to be combined with the `Series`.\n\n        Returns\n        -------\n        Series\n            The result of combining the Series with the other object.\n\n        See Also\n        --------\n        Series.combine : Perform elementwise operation on two Series\n            using a given function.\n\n        Notes\n        -----\n        Result index will be the union of the two indexes.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, np.nan])\n        >>> s2 = pd.Series([3, 4])\n        >>> s1.combine_first(s2)\n        0    1.0\n        1    4.0\n        dtype: float64\n        \"\"\"\n        new_index = self.index.union(other.index)\n        this = self.reindex(new_index, copy=False)\n        other = other.reindex(new_index, copy=False)\n        if this.dtype.kind == \"M\" and other.dtype.kind != \"M\":\n            other = to_datetime(other)\n\n        return this.where(notna(this), other)\n\n    def update(self, other) -> None:\n        \"\"\"\n        Modify Series in place using non-NA values from passed\n        Series. Aligns on index.\n\n        Parameters\n        ----------\n        other : Series, or object coercible into Series\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s.update(pd.Series(['d', 'e'], index=[0, 2]))\n        >>> s\n        0    d\n        1    b\n        2    e\n        dtype: object\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6, 7, 8]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        If ``other`` contains NaNs the corresponding values are not updated\n        in the original Series.\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, np.nan, 6]))\n        >>> s\n        0    4\n        1    2\n        2    6\n        dtype: int64\n\n        ``other`` can also be a non-Series object type\n        that is coercible into a Series\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update([4, np.nan, 6])\n        >>> s\n        0    4\n        1    2\n        2    6\n        dtype: int64\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update({1: 9})\n        >>> s\n        0    1\n        1    9\n        2    3\n        dtype: int64\n        \"\"\"\n\n        if not isinstance(other, Series):\n            other = Series(other)\n\n        other = other.reindex_like(self)\n        mask = notna(other)\n\n        self._mgr = self._mgr.putmask(mask=mask, new=other)\n        self._maybe_update_cacher()\n\n    # ----------------------------------------------------------------------\n    # Reindexing, sorting\n\n    def sort_values(\n        self,\n        axis=0,\n        ascending=True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        ignore_index: bool = False,\n        key: ValueKeyFunc = None,\n    ):\n        \"\"\"\n        Sort by the values.\n\n        Sort a Series in ascending or descending order by some\n        criterion.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            Axis to direct sorting. The value 'index' is accepted for\n            compatibility with DataFrame.sort_values.\n        ascending : bool, default True\n            If True, sort values in ascending order, otherwise descending.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort' or 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. 'mergesort' is the only stable  algorithm.\n        na_position : {'first' or 'last'}, default 'last'\n            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n            the end.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the series values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect a\n            ``Series`` and return an array-like.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series\n            Series ordered by values.\n\n        See Also\n        --------\n        Series.sort_index : Sort by the Series indices.\n        DataFrame.sort_values : Sort DataFrame by the values along either axis.\n        DataFrame.sort_index : Sort DataFrame by indices.\n\n        Examples\n        --------\n        >>> s = pd.Series([np.nan, 1, 3, 10, 5])\n        >>> s\n        0     NaN\n        1     1.0\n        2     3.0\n        3     10.0\n        4     5.0\n        dtype: float64\n\n        Sort values ascending order (default behaviour)\n\n        >>> s.sort_values(ascending=True)\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        0     NaN\n        dtype: float64\n\n        Sort values descending order\n\n        >>> s.sort_values(ascending=False)\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values inplace\n\n        >>> s.sort_values(ascending=False, inplace=True)\n        >>> s\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values putting NAs first\n\n        >>> s.sort_values(na_position='first')\n        0     NaN\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        dtype: float64\n\n        Sort a series of strings\n\n        >>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n        >>> s\n        0    z\n        1    b\n        2    d\n        3    a\n        4    c\n        dtype: object\n\n        >>> s.sort_values()\n        3    a\n        1    b\n        4    c\n        2    d\n        0    z\n        dtype: object\n\n        Sort using a key function. Your `key` function will be\n        given the ``Series`` of values and should return an array-like.\n\n        >>> s = pd.Series(['a', 'B', 'c', 'D', 'e'])\n        >>> s.sort_values()\n        1    B\n        3    D\n        0    a\n        2    c\n        4    e\n        dtype: object\n        >>> s.sort_values(key=lambda x: x.str.lower())\n        0    a\n        1    B\n        2    c\n        3    D\n        4    e\n        dtype: object\n\n        NumPy ufuncs work well here. For example, we can\n        sort by the ``sin`` of the value\n\n        >>> s = pd.Series([-4, -2, 0, 2, 4])\n        >>> s.sort_values(key=np.sin)\n        1   -2\n        4    4\n        2    0\n        0   -4\n        3    2\n        dtype: int64\n\n        More complicated user-defined functions can be used,\n        as long as they expect a Series and return an array-like\n\n        >>> s.sort_values(key=lambda x: (np.tan(x.cumsum())))\n        0   -4\n        3    2\n        4    4\n        1   -2\n        2    0\n        dtype: int64\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n\n        # GH 5856/5853\n        if inplace and self._is_cached:\n            raise ValueError(\n                \"This Series is a view of some other array, to \"\n                \"sort in-place you must create a copy\"\n            )\n\n        def _try_kind_sort(arr):\n            arr = ensure_key_mapped(arr, key)\n            arr = getattr(arr, \"_values\", arr)\n\n            # easier to ask forgiveness than permission\n            try:\n                # if kind==mergesort, it can fail for object dtype\n                return arr.argsort(kind=kind)\n            except TypeError:\n                # stable sort not available for object dtype\n                # uses the argsort default quicksort\n                return arr.argsort(kind=\"quicksort\")\n\n        arr = self._values\n        sorted_index = np.empty(len(self), dtype=np.int32)\n\n        bad = isna(arr)\n\n        good = ~bad\n        idx = ibase.default_index(len(self))\n\n        argsorted = _try_kind_sort(self[good])\n\n        if is_list_like(ascending):\n            if len(ascending) != 1:\n                raise ValueError(\n                    f\"Length of ascending ({len(ascending)}) must be 1 for Series\"\n                )\n            ascending = ascending[0]\n\n        if not is_bool(ascending):\n            raise ValueError(\"ascending must be boolean\")\n\n        if not ascending:\n            argsorted = argsorted[::-1]\n\n        if na_position == \"last\":\n            n = good.sum()\n            sorted_index[:n] = idx[good][argsorted]\n            sorted_index[n:] = idx[bad]\n        elif na_position == \"first\":\n            n = bad.sum()\n            sorted_index[n:] = idx[good][argsorted]\n            sorted_index[:n] = idx[bad]\n        else:\n            raise ValueError(f\"invalid na_position: {na_position}\")\n\n        result = self._constructor(arr[sorted_index], index=self.index[sorted_index])\n\n        if ignore_index:\n            result.index = ibase.default_index(len(sorted_index))\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_values\")\n\n    def sort_index(\n        self,\n        axis=0,\n        level=None,\n        ascending: bool = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        sort_remaining: bool = True,\n        ignore_index: bool = False,\n        key: IndexKeyFunc = None,\n    ):\n        \"\"\"\n        Sort Series by index labels.\n\n        Returns a new Series sorted by label if `inplace` argument is\n        ``False``, otherwise updates the original series and returns None.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Axis to direct sorting. This can only be 0 for Series.\n        level : int, optional\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level individually.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information.  'mergesort' is the only stable algorithm. For\n            DataFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'last'}, default 'last'\n            If 'first' puts NaNs at the beginning, 'last' puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series\n            The original Series sorted by the labels.\n\n        See Also\n        --------\n        DataFrame.sort_index: Sort DataFrame by the index.\n        DataFrame.sort_values: Sort DataFrame by the value.\n        Series.sort_values : Sort Series by the value.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])\n        >>> s.sort_index()\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        Sort Descending\n\n        >>> s.sort_index(ascending=False)\n        4    d\n        3    a\n        2    b\n        1    c\n        dtype: object\n\n        Sort Inplace\n\n        >>> s.sort_index(inplace=True)\n        >>> s\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        By default NaNs are put at the end, but use `na_position` to place\n        them at the beginning\n\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])\n        >>> s.sort_index(na_position='first')\n        NaN     d\n         1.0    c\n         2.0    b\n         3.0    a\n        dtype: object\n\n        Specify index level to sort\n\n        >>> arrays = [np.array(['qux', 'qux', 'foo', 'foo',\n        ...                     'baz', 'baz', 'bar', 'bar']),\n        ...           np.array(['two', 'one', 'two', 'one',\n        ...                     'two', 'one', 'two', 'one'])]\n        >>> s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)\n        >>> s.sort_index(level=1)\n        bar  one    8\n        baz  one    6\n        foo  one    4\n        qux  one    2\n        bar  two    7\n        baz  two    5\n        foo  two    3\n        qux  two    1\n        dtype: int64\n\n        Does not sort by remaining levels when sorting by levels\n\n        >>> s.sort_index(level=1, sort_remaining=False)\n        qux  one    2\n        foo  one    4\n        baz  one    6\n        bar  one    8\n        qux  two    1\n        foo  two    3\n        baz  two    5\n        bar  two    7\n        dtype: int64\n\n        Apply a key function before sorting\n\n        >>> s = pd.Series([1, 2, 3, 4], index=['A', 'b', 'C', 'd'])\n        >>> s.sort_index(key=lambda x : x.str.lower())\n        A    1\n        b    2\n        C    3\n        d    4\n        dtype: int64\n        \"\"\"\n\n        # TODO: this can be combined with DataFrame.sort_index impl as\n        # almost identical\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n        index = ensure_key_mapped(self.index, key, levels=level)\n\n        if level is not None:\n            new_index, indexer = index.sortlevel(\n                level, ascending=ascending, sort_remaining=sort_remaining\n            )\n\n        elif isinstance(index, MultiIndex):\n            from pandas.core.sorting import lexsort_indexer\n\n            labels = index._sort_levels_monotonic()\n\n            indexer = lexsort_indexer(\n                labels._get_codes_for_sorting(),\n                orders=ascending,\n                na_position=na_position,\n            )\n        else:\n            from pandas.core.sorting import nargsort\n\n            # Check monotonic-ness before sort an index\n            # GH11080\n            if (ascending and index.is_monotonic_increasing) or (\n                not ascending and index.is_monotonic_decreasing\n            ):\n                if inplace:\n                    return\n                else:\n                    return self.copy()\n\n            indexer = nargsort(\n                index, kind=kind, ascending=ascending, na_position=na_position\n            )\n\n        indexer = ensure_platform_int(indexer)\n        new_index = self.index.take(indexer)\n        new_index = new_index._sort_levels_monotonic()\n\n        new_values = self._values.take(indexer)\n        result = self._constructor(new_values, index=new_index)\n\n        if ignore_index:\n            result.index = ibase.default_index(len(result))\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_index\")\n\n    def argsort(self, axis=0, kind=\"quicksort\", order=None) -> \"Series\":\n        \"\"\"\n        Override ndarray.argsort. Argsorts the value, omitting NA/null values,\n        and places the result in the same locations as the non-NA values.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Has no effect but is accepted for compatibility with numpy.\n        kind : {'mergesort', 'quicksort', 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See np.sort for more\n            information. 'mergesort' is the only stable algorithm.\n        order : None\n            Has no effect but is accepted for compatibility with numpy.\n\n        Returns\n        -------\n        Series\n            Positions of values within the sort order with -1 indicating\n            nan values.\n\n        See Also\n        --------\n        numpy.ndarray.argsort : Returns the indices that would sort this array.\n        \"\"\"\n        values = self._values\n        mask = isna(values)\n\n        if mask.any():\n            result = Series(-1, index=self.index, name=self.name, dtype=\"int64\")\n            notmask = ~mask\n            result[notmask] = np.argsort(values[notmask], kind=kind)\n            return self._constructor(result, index=self.index).__finalize__(\n                self, method=\"argsort\"\n            )\n        else:\n            return self._constructor(\n                np.argsort(values, kind=kind), index=self.index, dtype=\"int64\"\n            ).__finalize__(self, method=\"argsort\")\n\n    def nlargest(self, n=5, keep=\"first\") -> \"Series\":\n        \"\"\"\n        Return the largest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many descending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n                of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n                order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n                size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` largest values in the Series, sorted in decreasing order.\n\n        See Also\n        --------\n        Series.nsmallest: Get the `n` smallest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values(ascending=False).head(n)`` for small `n`\n        relative to the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Malta\": 434000, \"Maldives\": 434000,\n        ...                         \"Brunei\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Montserrat      5200\n        dtype: int64\n\n        The `n` largest elements where ``n=5`` by default.\n\n        >>> s.nlargest()\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3``. Default `keep` value is 'first'\n        so Malta will be kept.\n\n        >>> s.nlargest(3)\n        France    65000000\n        Italy     59000000\n        Malta       434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` and keeping the last duplicates.\n        Brunei will be kept since it is the last with value 434000 based on\n        the index order.\n\n        >>> s.nlargest(3, keep='last')\n        France      65000000\n        Italy       59000000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has five elements due to the three duplicates.\n\n        >>> s.nlargest(3, keep='all')\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nlargest()\n\n    def nsmallest(self, n=5, keep=\"first\") -> \"Series\":\n        \"\"\"\n        Return the smallest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many ascending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n                of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n                order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n                size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` smallest values in the Series, sorted in increasing order.\n\n        See Also\n        --------\n        Series.nlargest: Get the `n` largest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values().head(n)`` for small `n` relative to\n        the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Brunei\": 434000, \"Malta\": 434000,\n        ...                         \"Maldives\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Brunei        434000\n        Malta         434000\n        Maldives      434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Montserrat      5200\n        dtype: int64\n\n        The `n` smallest elements where ``n=5`` by default.\n\n        >>> s.nsmallest()\n        Montserrat    5200\n        Nauru        11300\n        Tuvalu       11300\n        Anguilla     11300\n        Iceland     337000\n        dtype: int64\n\n        The `n` smallest elements where ``n=3``. Default `keep` value is\n        'first' so Nauru and Tuvalu will be kept.\n\n        >>> s.nsmallest(3)\n        Montserrat   5200\n        Nauru       11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` and keeping the last\n        duplicates. Anguilla and Tuvalu will be kept since they are the last\n        with value 11300 based on the index order.\n\n        >>> s.nsmallest(3, keep='last')\n        Montserrat   5200\n        Anguilla    11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has four elements due to the three duplicates.\n\n        >>> s.nsmallest(3, keep='all')\n        Montserrat   5200\n        Nauru       11300\n        Tuvalu      11300\n        Anguilla    11300\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nsmallest()\n\n    def swaplevel(self, i=-2, j=-1, copy=True) -> \"Series\":\n        \"\"\"\n        Swap levels i and j in a :class:`MultiIndex`.\n\n        Default is to swap the two innermost levels of the index.\n\n        Parameters\n        ----------\n        i, j : int, str\n            Level of the indices to be swapped. Can pass level name as string.\n        copy : bool, default True\n            Whether to copy underlying data.\n\n        Returns\n        -------\n        Series\n            Series with levels swapped in MultiIndex.\n        \"\"\"\n        assert isinstance(self.index, MultiIndex)\n        new_index = self.index.swaplevel(i, j)\n        return self._constructor(self._values, index=new_index, copy=copy).__finalize__(\n            self, method=\"swaplevel\"\n        )\n\n    def reorder_levels(self, order) -> \"Series\":\n        \"\"\"\n        Rearrange index levels using input order.\n\n        May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int representing new level order\n            Reference level by number or key.\n\n        Returns\n        -------\n        type of caller (new object)\n        \"\"\"\n        if not isinstance(self.index, MultiIndex):  # pragma: no cover\n            raise Exception(\"Can only reorder levels on a hierarchical axis.\")\n\n        result = self.copy()\n        assert isinstance(result.index, MultiIndex)\n        result.index = result.index.reorder_levels(order)\n        return result\n\n    def explode(self) -> \"Series\":\n        \"\"\"\n        Transform each element of a list-like to a row, replicating the\n        index values.\n\n        .. versionadded:: 0.25.0\n\n        Returns\n        -------\n        Series\n            Exploded lists to rows; index will be duplicated for these rows.\n\n        See Also\n        --------\n        Series.str.split : Split string values on specified separator.\n        Series.unstack : Unstack, a.k.a. pivot, Series with MultiIndex\n            to produce DataFrame.\n        DataFrame.melt : Unpivot a DataFrame from wide format to long format.\n        DataFrame.explode : Explode a DataFrame from list-like\n            columns to long format.\n\n        Notes\n        -----\n        This routine will explode list-likes including lists, tuples,\n        Series, and np.ndarray. The result dtype of the subset rows will\n        be object. Scalars will be returned unchanged. Empty list-likes will\n        result in a np.nan for that row.\n\n        Examples\n        --------\n        >>> s = pd.Series([[1, 2, 3], 'foo', [], [3, 4]])\n        >>> s\n        0    [1, 2, 3]\n        1          foo\n        2           []\n        3       [3, 4]\n        dtype: object\n\n        >>> s.explode()\n        0      1\n        0      2\n        0      3\n        1    foo\n        2    NaN\n        3      3\n        3      4\n        dtype: object\n        \"\"\"\n        if not len(self) or not is_object_dtype(self):\n            return self.copy()\n\n        values, counts = reshape.explode(np.asarray(self.array))\n\n        result = self._constructor(\n            values, index=self.index.repeat(counts), name=self.name\n        )\n        return result\n\n    def unstack(self, level=-1, fill_value=None):\n        \"\"\"\n        Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.\n        The level involved will automatically get sorted.\n\n        Parameters\n        ----------\n        level : int, str, or list of these, default last level\n            Level(s) to unstack, can pass level name.\n        fill_value : scalar value, default None\n            Value to use when replacing NaN values.\n\n        Returns\n        -------\n        DataFrame\n            Unstacked Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4],\n        ...               index=pd.MultiIndex.from_product([['one', 'two'],\n        ...                                                 ['a', 'b']]))\n        >>> s\n        one  a    1\n             b    2\n        two  a    3\n             b    4\n        dtype: int64\n\n        >>> s.unstack(level=-1)\n             a  b\n        one  1  2\n        two  3  4\n\n        >>> s.unstack(level=0)\n           one  two\n        a    1    3\n        b    2    4\n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n\n        return unstack(self, level, fill_value)\n\n    # ----------------------------------------------------------------------\n    # function application\n\n    def map(self, arg, na_action=None) -> \"Series\":\n        \"\"\"\n        Map values of Series according to input correspondence.\n\n        Used for substituting each value in a Series with another value,\n        that may be derived from a function, a ``dict`` or\n        a :class:`Series`.\n\n        Parameters\n        ----------\n        arg : function, collections.abc.Mapping subclass or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NaN values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        Series\n            Same index as caller.\n\n        See Also\n        --------\n        Series.apply : For applying more complex functions on a Series.\n        DataFrame.apply : Apply a function row-/column-wise.\n        DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n\n        Notes\n        -----\n        When ``arg`` is a dictionary, values in Series that are not in the\n        dictionary (as keys) are converted to ``NaN``. However, if the\n        dictionary is a ``dict`` subclass that defines ``__missing__`` (i.e.\n        provides a method for default values), then this default is used\n        rather than ``NaN``.\n\n        Examples\n        --------\n        >>> s = pd.Series(['cat', 'dog', np.nan, 'rabbit'])\n        >>> s\n        0      cat\n        1      dog\n        2      NaN\n        3   rabbit\n        dtype: object\n\n        ``map`` accepts a ``dict`` or a ``Series``. Values that are not found\n        in the ``dict`` are converted to ``NaN``, unless the dict has a default\n        value (e.g. ``defaultdict``):\n\n        >>> s.map({'cat': 'kitten', 'dog': 'puppy'})\n        0   kitten\n        1    puppy\n        2      NaN\n        3      NaN\n        dtype: object\n\n        It also accepts a function:\n\n        >>> s.map('I am a {}'.format)\n        0       I am a cat\n        1       I am a dog\n        2       I am a nan\n        3    I am a rabbit\n        dtype: object\n\n        To avoid applying the function to missing values (and keep them as\n        ``NaN``) ``na_action='ignore'`` can be used:\n\n        >>> s.map('I am a {}'.format, na_action='ignore')\n        0     I am a cat\n        1     I am a dog\n        2            NaN\n        3  I am a rabbit\n        dtype: object\n        \"\"\"\n        new_values = super()._map_values(arg, na_action=na_action)\n        return self._constructor(new_values, index=self.index).__finalize__(\n            self, method=\"map\"\n        )\n\n    def _gotitem(self, key, ndim, subset=None) -> \"Series\":\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : 1,2\n            Requested ndim of result.\n        subset : object, default None\n            Subset to act on.\n        \"\"\"\n        return self\n\n    _agg_see_also_doc = dedent(\n        \"\"\"\n    See Also\n    --------\n    Series.apply : Invoke function on a Series.\n    Series.transform : Transform function producing a Series with like indexes.\n    \"\"\"\n    )\n\n    _agg_examples_doc = dedent(\n        \"\"\"\n    Examples\n    --------\n    >>> s = pd.Series([1, 2, 3, 4])\n    >>> s\n    0    1\n    1    2\n    2    3\n    3    4\n    dtype: int64\n\n    >>> s.agg('min')\n    1\n\n    >>> s.agg(['min', 'max'])\n    min   1\n    max   4\n    dtype: int64\n    \"\"\"\n    )\n\n    @Substitution(\n        see_also=_agg_see_also_doc,\n        examples=_agg_examples_doc,\n        versionadded=\"\\n.. versionadded:: 0.20.0\\n\",\n        **_shared_doc_kwargs,\n    )\n    @Appender(generic._shared_docs[\"aggregate\"])\n    def aggregate(self, func, axis=0, *args, **kwargs):\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n        result, how = self._aggregate(func, *args, **kwargs)\n        if result is None:\n\n            # we can be called from an inner function which\n            # passes this meta-data\n            kwargs.pop(\"_axis\", None)\n            kwargs.pop(\"_level\", None)\n\n            # try a regular apply, this evaluates lambdas\n            # row-by-row; however if the lambda is expected a Series\n            # expression, e.g.: lambda x: x-x.quantile(0.25)\n            # this will fail, so we can try a vectorized evaluation\n\n            # we cannot FIRST try the vectorized evaluation, because\n            # then .agg and .apply would have different semantics if the\n            # operation is actually defined on the Series, e.g. str\n            try:\n                result = self.apply(func, *args, **kwargs)\n            except (ValueError, AttributeError, TypeError):\n                result = func(self, *args, **kwargs)\n\n        return result\n\n    agg = aggregate\n\n    @Appender(generic._shared_docs[\"transform\"] % _shared_doc_kwargs)\n    def transform(self, func, axis=0, *args, **kwargs):\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n        return super().transform(func, *args, **kwargs)\n\n    def apply(self, func, convert_dtype=True, args=(), **kwds):\n        \"\"\"\n        Invoke function on values of Series.\n\n        Can be ufunc (a NumPy function that applies to the entire Series)\n        or a Python function that only works on single values.\n\n        Parameters\n        ----------\n        func : function\n            Python function or NumPy ufunc to apply.\n        convert_dtype : bool, default True\n            Try to find better dtype for elementwise function results. If\n            False, leave as dtype=object.\n        args : tuple\n            Positional arguments passed to func after the series value.\n        **kwds\n            Additional keyword arguments passed to func.\n\n        Returns\n        -------\n        Series or DataFrame\n            If func returns a Series object the result will be a DataFrame.\n\n        See Also\n        --------\n        Series.map: For element-wise operations.\n        Series.agg: Only perform aggregating type operations.\n        Series.transform: Only perform transforming type operations.\n\n        Examples\n        --------\n        Create a series with typical summer temperatures for each city.\n\n        >>> s = pd.Series([20, 21, 12],\n        ...               index=['London', 'New York', 'Helsinki'])\n        >>> s\n        London      20\n        New York    21\n        Helsinki    12\n        dtype: int64\n\n        Square the values by defining a function and passing it as an\n        argument to ``apply()``.\n\n        >>> def square(x):\n        ...     return x ** 2\n        >>> s.apply(square)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Square the values by passing an anonymous function as an\n        argument to ``apply()``.\n\n        >>> s.apply(lambda x: x ** 2)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Define a custom function that needs additional positional\n        arguments and pass these additional arguments using the\n        ``args`` keyword.\n\n        >>> def subtract_custom_value(x, custom_value):\n        ...     return x - custom_value\n\n        >>> s.apply(subtract_custom_value, args=(5,))\n        London      15\n        New York    16\n        Helsinki     7\n        dtype: int64\n\n        Define a custom function that takes keyword arguments\n        and pass these arguments to ``apply``.\n\n        >>> def add_custom_values(x, **kwargs):\n        ...     for month in kwargs:\n        ...         x += kwargs[month]\n        ...     return x\n\n        >>> s.apply(add_custom_values, june=30, july=20, august=25)\n        London      95\n        New York    96\n        Helsinki    87\n        dtype: int64\n\n        Use a function from the Numpy library.\n\n        >>> s.apply(np.log)\n        London      2.995732\n        New York    3.044522\n        Helsinki    2.484907\n        dtype: float64\n        \"\"\"\n        if len(self) == 0:\n            return self._constructor(dtype=self.dtype, index=self.index).__finalize__(\n                self, method=\"apply\"\n            )\n\n        # dispatch to agg\n        if isinstance(func, (list, dict)):\n            return self.aggregate(func, *args, **kwds)\n\n        # if we are a string, try to dispatch\n        if isinstance(func, str):\n            return self._try_aggregate_string_function(func, *args, **kwds)\n\n        # handle ufuncs and lambdas\n        if kwds or args and not isinstance(func, np.ufunc):\n\n            def f(x):\n                return func(x, *args, **kwds)\n\n        else:\n            f = func\n\n        with np.errstate(all=\"ignore\"):\n            if isinstance(f, np.ufunc):\n                return f(self)\n\n            # row-wise access\n            if is_extension_array_dtype(self.dtype) and hasattr(self._values, \"map\"):\n                # GH#23179 some EAs do not have `map`\n                mapped = self._values.map(f)\n            else:\n                values = self.astype(object)._values\n                mapped = lib.map_infer(values, f, convert=convert_dtype)\n\n        if len(mapped) and isinstance(mapped[0], Series):\n            # GH 25959 use pd.array instead of tolist\n            # so extension arrays can be used\n            return self._constructor_expanddim(pd.array(mapped), index=self.index)\n        else:\n            return self._constructor(mapped, index=self.index).__finalize__(\n                self, method=\"apply\"\n            )\n\n    def _reduce(\n        self, op, name, axis=0, skipna=True, numeric_only=None, filter_type=None, **kwds\n    ):\n        \"\"\"\n        Perform a reduction operation.\n\n        If we have an ndarray as a value, then simply perform the operation,\n        otherwise delegate to the object.\n        \"\"\"\n        delegate = self._values\n\n        if axis is not None:\n            self._get_axis_number(axis)\n\n        if isinstance(delegate, ExtensionArray):\n            # dispatch to ExtensionArray interface\n            return delegate._reduce(name, skipna=skipna, **kwds)\n\n        else:\n            # dispatch to numpy arrays\n            if numeric_only:\n                raise NotImplementedError(\n                    f\"Series.{name} does not implement numeric_only.\"\n                )\n            with np.errstate(all=\"ignore\"):\n                return op(delegate, skipna=skipna, **kwds)\n\n    def _reindex_indexer(self, new_index, indexer, copy):\n        if indexer is None:\n            if copy:\n                return self.copy()\n            return self\n\n        new_values = algorithms.take_1d(\n            self._values, indexer, allow_fill=True, fill_value=None\n        )\n        return self._constructor(new_values, index=new_index)\n\n    def _needs_reindex_multi(self, axes, method, level):\n        \"\"\"\n        Check if we do need a multi reindex; this is for compat with\n        higher dims.\n        \"\"\"\n        return False\n\n    @doc(NDFrame.align, **_shared_doc_kwargs)\n    def align(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy=True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n        broadcast_axis=None,\n    ):\n        return super().align(\n            other,\n            join=join,\n            axis=axis,\n            level=level,\n            copy=copy,\n            fill_value=fill_value,\n            method=method,\n            limit=limit,\n            fill_axis=fill_axis,\n            broadcast_axis=broadcast_axis,\n        )\n\n    def rename(\n        self,\n        index=None,\n        *,\n        axis=None,\n        copy=True,\n        inplace=False,\n        level=None,\n        errors=\"ignore\",\n    ):\n        \"\"\"\n        Alter Series index labels or name.\n\n        Function / dict values must be unique (1-to-1). Labels not contained in\n        a dict / Series will be left as-is. Extra labels listed don't throw an\n        error.\n\n        Alternatively, change ``Series.name`` with a scalar value.\n\n        See the :ref:`user guide <basics.rename>` for more.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Unused. Accepted for compatibility with DataFrame method only.\n        index : scalar, hashable sequence, dict-like or function, optional\n            Functions or dict-like are transformations to apply to\n            the index.\n            Scalar or hashable sequence-like will alter the ``Series.name``\n            attribute.\n\n        **kwargs\n            Additional keyword arguments passed to the function. Only the\n            \"inplace\" keyword is used.\n\n        Returns\n        -------\n        Series\n            Series with index labels or name altered.\n\n        See Also\n        --------\n        DataFrame.rename : Corresponding DataFrame method.\n        Series.rename_axis : Set the name of the axis.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n        >>> s.rename(\"my_name\")  # scalar, changes Series.name\n        0    1\n        1    2\n        2    3\n        Name: my_name, dtype: int64\n        >>> s.rename(lambda x: x ** 2)  # function, changes labels\n        0    1\n        1    2\n        4    3\n        dtype: int64\n        >>> s.rename({1: 3, 2: 5})  # mapping, changes labels\n        0    1\n        3    2\n        5    3\n        dtype: int64\n        \"\"\"\n        if callable(index) or is_dict_like(index):\n            return super().rename(\n                index, copy=copy, inplace=inplace, level=level, errors=errors\n            )\n        else:\n            return self._set_name(index, inplace=inplace)\n\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n\n        >>> s.set_axis(['a', 'b', 'c'], axis=0)\n        a    1\n        b    2\n        c    3\n        dtype: int64\n    \"\"\"\n    )\n    @Substitution(\n        **_shared_doc_kwargs,\n        extended_summary_sub=\"\",\n        axis_description_sub=\"\",\n        see_also_sub=\"\",\n    )\n    @Appender(generic.NDFrame.set_axis.__doc__)\n    def set_axis(self, labels, axis: Axis = 0, inplace: bool = False):\n        return super().set_axis(labels, axis=axis, inplace=inplace)\n\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(generic.NDFrame.reindex.__doc__)\n    def reindex(self, index=None, **kwargs):\n        return super().reindex(index=index, **kwargs)\n\n    def drop(\n        self,\n        labels=None,\n        axis=0,\n        index=None,\n        columns=None,\n        level=None,\n        inplace=False,\n        errors=\"raise\",\n    ) -> \"Series\":\n        \"\"\"\n        Return Series with specified index labels removed.\n\n        Remove elements of a Series based on specifying the index labels.\n        When using a multi-index, labels on different levels can be removed\n        by specifying the level.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index labels to drop.\n        axis : 0, default 0\n            Redundant for application on Series.\n        index : single label or list-like\n            Redundant for application on Series, but 'index' can be used instead\n            of 'labels'.\n        columns : single label or list-like\n            No change is made to the Series; use 'index' or 'labels' instead.\n        level : int or level name, optional\n            For MultiIndex, level for which the labels will be removed.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and only existing labels are dropped.\n\n        Returns\n        -------\n        Series\n            Series with specified index labels removed.\n\n        Raises\n        ------\n        KeyError\n            If none of the labels are found in the index.\n\n        See Also\n        --------\n        Series.reindex : Return only specified index labels of Series.\n        Series.dropna : Return series without null values.\n        Series.drop_duplicates : Return Series with duplicate values removed.\n        DataFrame.drop : Drop specified labels from rows or columns.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=np.arange(3), index=['A', 'B', 'C'])\n        >>> s\n        A  0\n        B  1\n        C  2\n        dtype: int64\n\n        Drop labels B en C\n\n        >>> s.drop(labels=['B', 'C'])\n        A  0\n        dtype: int64\n\n        Drop 2nd level label in MultiIndex Series\n\n        >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n        ...                              ['speed', 'weight', 'length']],\n        ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> s = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...               index=midx)\n        >>> s\n        lama    speed      45.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        dtype: float64\n\n        >>> s.drop(labels='weight', level=1)\n        lama    speed      45.0\n                length      1.2\n        cow     speed      30.0\n                length      1.5\n        falcon  speed     320.0\n                length      0.3\n        dtype: float64\n        \"\"\"\n        return super().drop(\n            labels=labels,\n            axis=axis,\n            index=index,\n            columns=columns,\n            level=level,\n            inplace=inplace,\n            errors=errors,\n        )\n\n    @doc(NDFrame.fillna, **_shared_doc_kwargs)\n    def fillna(\n        self,\n        value=None,\n        method=None,\n        axis=None,\n        inplace=False,\n        limit=None,\n        downcast=None,\n    ) -> Optional[\"Series\"]:\n        return super().fillna(\n            value=value,\n            method=method,\n            axis=axis,\n            inplace=inplace,\n            limit=limit,\n            downcast=downcast,\n        )\n\n    @doc(NDFrame.replace, **_shared_doc_kwargs)\n    def replace(\n        self,\n        to_replace=None,\n        value=None,\n        inplace=False,\n        limit=None,\n        regex=False,\n        method=\"pad\",\n    ):\n        return super().replace(\n            to_replace=to_replace,\n            value=value,\n            inplace=inplace,\n            limit=limit,\n            regex=regex,\n            method=method,\n        )\n\n    @doc(NDFrame.shift, **_shared_doc_kwargs)\n    def shift(self, periods=1, freq=None, axis=0, fill_value=None) -> \"Series\":\n        return super().shift(\n            periods=periods, freq=freq, axis=axis, fill_value=fill_value\n        )\n\n    def memory_usage(self, index=True, deep=False):\n        \"\"\"\n        Return the memory usage of the Series.\n\n        The memory usage can optionally include the contribution of\n        the index and of elements of `object` dtype.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the Series index.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned value.\n\n        Returns\n        -------\n        int\n            Bytes of memory consumed.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\n            array.\n        DataFrame.memory_usage : Bytes consumed by a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(range(3))\n        >>> s.memory_usage()\n        152\n\n        Not including the index gives the size of the rest of the data, which\n        is necessarily smaller:\n\n        >>> s.memory_usage(index=False)\n        24\n\n        The memory footprint of `object` values is ignored by default:\n\n        >>> s = pd.Series([\"a\", \"b\"])\n        >>> s.values\n        array(['a', 'b'], dtype=object)\n        >>> s.memory_usage()\n        144\n        >>> s.memory_usage(deep=True)\n        260\n        \"\"\"\n        v = super().memory_usage(deep=deep)\n        if index:\n            v += self.index.memory_usage(deep=deep)\n        return v\n\n    def isin(self, values) -> \"Series\":\n        \"\"\"\n        Check whether `values` are contained in Series.\n\n        Return a boolean Series showing whether each element in the Series\n        matches an element in the passed sequence of `values` exactly.\n\n        Parameters\n        ----------\n        values : set or list-like\n            The sequence of values to test. Passing in a single string will\n            raise a ``TypeError``. Instead, turn a single string into a\n            list of one element.\n\n        Returns\n        -------\n        Series\n            Series of booleans indicating if each element is in values.\n\n        Raises\n        ------\n        TypeError\n          * If `values` is a string\n\n        See Also\n        --------\n        DataFrame.isin : Equivalent method on DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama',\n        ...                'hippo'], name='animal')\n        >>> s.isin(['cow', 'lama'])\n        0     True\n        1     True\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n\n        Passing a single string as ``s.isin('lama')`` will raise an error. Use\n        a list of one element instead:\n\n        >>> s.isin(['lama'])\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n        \"\"\"\n        result = algorithms.isin(self, values)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"isin\"\n        )\n\n    def between(self, left, right, inclusive=True) -> \"Series\":\n        \"\"\"\n        Return boolean Series equivalent to left <= series <= right.\n\n        This function returns a boolean vector containing `True` wherever the\n        corresponding Series element is between the boundary values `left` and\n        `right`. NA values are treated as `False`.\n\n        Parameters\n        ----------\n        left : scalar or list-like\n            Left boundary.\n        right : scalar or list-like\n            Right boundary.\n        inclusive : bool, default True\n            Include boundaries.\n\n        Returns\n        -------\n        Series\n            Series representing whether each element is between left and\n            right (inclusive).\n\n        See Also\n        --------\n        Series.gt : Greater than of series and other.\n        Series.lt : Less than of series and other.\n\n        Notes\n        -----\n        This function is equivalent to ``(left <= ser) & (ser <= right)``\n\n        Examples\n        --------\n        >>> s = pd.Series([2, 0, 4, 8, np.nan])\n\n        Boundary values are included by default:\n\n        >>> s.between(1, 4)\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        With `inclusive` set to ``False`` boundary values are excluded:\n\n        >>> s.between(1, 4, inclusive=False)\n        0     True\n        1    False\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        `left` and `right` can be any scalar value:\n\n        >>> s = pd.Series(['Alice', 'Bob', 'Carol', 'Eve'])\n        >>> s.between('Anna', 'Daniel')\n        0    False\n        1     True\n        2     True\n        3    False\n        dtype: bool\n        \"\"\"\n        if inclusive:\n            lmask = self >= left\n            rmask = self <= right\n        else:\n            lmask = self > left\n            rmask = self < right\n\n        return lmask & rmask\n\n    # ----------------------------------------------------------------------\n    # Convert to types that support pd.NA\n\n    def _convert_dtypes(\n        self,\n        infer_objects: bool = True,\n        convert_string: bool = True,\n        convert_integer: bool = True,\n        convert_boolean: bool = True,\n    ) -> \"Series\":\n        input_series = self\n        if infer_objects:\n            input_series = input_series.infer_objects()\n            if is_object_dtype(input_series):\n                input_series = input_series.copy()\n\n        if convert_string or convert_integer or convert_boolean:\n            inferred_dtype = convert_dtypes(\n                input_series._values, convert_string, convert_integer, convert_boolean\n            )\n            try:\n                result = input_series.astype(inferred_dtype)\n            except TypeError:\n                result = input_series.copy()\n        else:\n            result = input_series.copy()\n        return result\n\n    @Appender(generic._shared_docs[\"isna\"] % _shared_doc_kwargs)\n    def isna(self) -> \"Series\":\n        return super().isna()\n\n    @Appender(generic._shared_docs[\"isna\"] % _shared_doc_kwargs)\n    def isnull(self) -> \"Series\":\n        return super().isnull()\n\n    @Appender(generic._shared_docs[\"notna\"] % _shared_doc_kwargs)\n    def notna(self) -> \"Series\":\n        return super().notna()\n\n    @Appender(generic._shared_docs[\"notna\"] % _shared_doc_kwargs)\n    def notnull(self) -> \"Series\":\n        return super().notnull()\n\n    def dropna(self, axis=0, inplace=False, how=None):\n        \"\"\"\n        Return a new Series with missing values removed.\n\n        See the :ref:`User Guide <missing_data>` for more on which values are\n        considered missing, and how to work with missing data.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            There is only one axis to drop values from.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        how : str, optional\n            Not in use. Kept for compatibility.\n\n        Returns\n        -------\n        Series\n            Series with NA entries dropped from it.\n\n        See Also\n        --------\n        Series.isna: Indicate missing values.\n        Series.notna : Indicate existing (non-missing) values.\n        Series.fillna : Replace missing values.\n        DataFrame.dropna : Drop rows or columns which contain NA values.\n        Index.dropna : Drop missing indices.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1., 2., np.nan])\n        >>> ser\n        0    1.0\n        1    2.0\n        2    NaN\n        dtype: float64\n\n        Drop NA values from a Series.\n\n        >>> ser.dropna()\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Keep the Series with valid entries in the same variable.\n\n        >>> ser.dropna(inplace=True)\n        >>> ser\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Empty strings are not considered NA values. ``None`` is considered an\n        NA value.\n\n        >>> ser = pd.Series([np.NaN, 2, pd.NaT, '', None, 'I stay'])\n        >>> ser\n        0       NaN\n        1         2\n        2       NaT\n        3\n        4      None\n        5    I stay\n        dtype: object\n        >>> ser.dropna()\n        1         2\n        3\n        5    I stay\n        dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis or 0)\n\n        if self._can_hold_na:\n            result = remove_na_arraylike(self)\n            if inplace:\n                self._update_inplace(result)\n            else:\n                return result\n        else:\n            if inplace:\n                # do nothing\n                pass\n            else:\n                return self.copy()\n\n    # ----------------------------------------------------------------------\n    # Time series-oriented methods\n\n    def to_timestamp(self, freq=None, how=\"start\", copy=True) -> \"Series\":\n        \"\"\"\n        Cast to DatetimeIndex of Timestamps, at *beginning* of period.\n\n        Parameters\n        ----------\n        freq : str, default frequency of PeriodIndex\n            Desired frequency.\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series with DatetimeIndex\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        if not isinstance(self.index, PeriodIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\n        new_index = self.index.to_timestamp(freq=freq, how=how)  # type: ignore\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_timestamp\"\n        )\n\n    def to_period(self, freq=None, copy=True) -> \"Series\":\n        \"\"\"\n        Convert Series from DatetimeIndex to PeriodIndex with desired\n        frequency (inferred from index if not passed).\n\n        Parameters\n        ----------\n        freq : str, default None\n            Frequency associated with the PeriodIndex.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series\n            Series with index converted to PeriodIndex.\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\n        new_index = self.index.to_period(freq=freq)  # type: ignore\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_period\"\n        )\n\n    # ----------------------------------------------------------------------\n    # Add index\n    _AXIS_ORDERS = [\"index\"]\n    _AXIS_REVERSED = False\n    _AXIS_LEN = len(_AXIS_ORDERS)\n    _info_axis_number = 0\n    _info_axis_name = \"index\"\n\n    index: \"Index\" = properties.AxisProperty(\n        axis=0, doc=\"The index (axis labels) of the Series.\"\n    )\n\n    # ----------------------------------------------------------------------\n    # Accessor Methods\n    # ----------------------------------------------------------------------\n    str = CachedAccessor(\"str\", StringMethods)\n    dt = CachedAccessor(\"dt\", CombinedDatetimelikeProperties)\n    cat = CachedAccessor(\"cat\", CategoricalAccessor)\n    plot = CachedAccessor(\"plot\", pandas.plotting.PlotAccessor)\n    sparse = CachedAccessor(\"sparse\", SparseAccessor)\n\n    # ----------------------------------------------------------------------\n    # Add plotting methods to Series\n    hist = pandas.plotting.hist_series\n\n\nSeries._add_numeric_operations()\nSeries._add_series_or_dataframe_operations()\n\n# Add arithmetic!\nops.add_flex_arithmetic_methods(Series)\nops.add_special_arithmetic_methods(Series)\n",
                "max_stars_repo_path": "BugsInPy/BugsInPy/temp/projects/pandas/bug-3-fixed/pandas/pandas/core/series.py,BugsInPy/BugsInPy/temp/projects/pandas/bug-3-buggy/pandas/pandas/core/series.py",
                "max_stars_repo_name": "NA",
                "max_stars_count": 0,
                "__cluster__": 79
            },
            {
                "id": "pretrain_python_data_64323",
                "content": "<reponame>sisiyan/pandas<gh_stars>1-10\n\"\"\"\nData structure for 1-dimensional cross-sectional and time series data\n\"\"\"\nfrom __future__ import annotations\n\nfrom io import StringIO\nfrom shutil import get_terminal_size\nfrom textwrap import dedent\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Hashable,\n    Iterable,\n    Sequence,\n    Union,\n    cast,\n    overload,\n)\nimport warnings\nimport weakref\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import (\n    lib,\n    properties,\n    reshape,\n    tslibs,\n)\nfrom pandas._libs.lib import no_default\nfrom pandas._typing import (\n    AggFuncType,\n    ArrayLike,\n    Axis,\n    Dtype,\n    DtypeObj,\n    FillnaOptions,\n    FrameOrSeriesUnion,\n    IndexKeyFunc,\n    NpDtype,\n    SingleManager,\n    StorageOptions,\n    ValueKeyFunc,\n)\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import InvalidIndexError\nfrom pandas.util._decorators import (\n    Appender,\n    Substitution,\n    doc,\n)\nfrom pandas.util._validators import (\n    validate_bool_kwarg,\n    validate_percentile,\n)\n\nfrom pandas.core.dtypes.cast import (\n    convert_dtypes,\n    maybe_box_native,\n    maybe_cast_pointwise_result,\n    validate_numeric_casting,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_platform_int,\n    is_bool,\n    is_dict_like,\n    is_integer,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_scalar,\n    pandas_dtype,\n    validate_all_hashable,\n)\nfrom pandas.core.dtypes.generic import ABCDataFrame\nfrom pandas.core.dtypes.inference import is_hashable\nfrom pandas.core.dtypes.missing import (\n    isna,\n    na_value_for_dtype,\n    notna,\n    remove_na_arraylike,\n)\n\nfrom pandas.core import (\n    algorithms,\n    base,\n    generic,\n    missing,\n    nanops,\n    ops,\n)\nfrom pandas.core.accessor import CachedAccessor\nfrom pandas.core.apply import SeriesApply\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.arrays.categorical import CategoricalAccessor\nfrom pandas.core.arrays.sparse import SparseAccessor\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    create_series_with_explicit_dtype,\n    ensure_wrapped_if_datetimelike,\n    extract_array,\n    is_empty_data,\n    sanitize_array,\n)\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.indexers import (\n    deprecate_ndim_indexing,\n    unpack_1tuple,\n)\nfrom pandas.core.indexes.accessors import CombinedDatetimelikeProperties\nfrom pandas.core.indexes.api import (\n    CategoricalIndex,\n    DatetimeIndex,\n    Float64Index,\n    Index,\n    MultiIndex,\n    PeriodIndex,\n    TimedeltaIndex,\n    ensure_index,\n)\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexing import check_bool_indexer\nfrom pandas.core.internals import (\n    SingleArrayManager,\n    SingleBlockManager,\n)\nfrom pandas.core.shared_docs import _shared_docs\nfrom pandas.core.sorting import (\n    ensure_key_mapped,\n    nargsort,\n)\nfrom pandas.core.strings import StringMethods\nfrom pandas.core.tools.datetimes import to_datetime\n\nimport pandas.io.formats.format as fmt\nimport pandas.plotting\n\nif TYPE_CHECKING:\n    from typing import Literal\n\n    from pandas._typing import (\n        TimedeltaConvertibleTypes,\n        TimestampConvertibleTypes,\n    )\n\n    from pandas.core.frame import DataFrame\n    from pandas.core.groupby.generic import SeriesGroupBy\n    from pandas.core.resample import Resampler\n\n__all__ = [\"Series\"]\n\n_shared_doc_kwargs = {\n    \"axes\": \"index\",\n    \"klass\": \"Series\",\n    \"axes_single_arg\": \"{0 or 'index'}\",\n    \"axis\": \"\"\"axis : {0 or 'index'}\n        Parameter needed for compatibility with DataFrame.\"\"\",\n    \"inplace\": \"\"\"inplace : bool, default False\n        If True, performs operation inplace and returns None.\"\"\",\n    \"unique\": \"np.ndarray\",\n    \"duplicated\": \"Series\",\n    \"optional_by\": \"\",\n    \"optional_mapper\": \"\",\n    \"optional_labels\": \"\",\n    \"optional_axis\": \"\",\n    \"replace_iloc\": \"\"\"\n    This differs from updating with ``.loc`` or ``.iloc``, which require\n    you to specify a location to update with some value.\"\"\",\n}\n\n\ndef _coerce_method(converter):\n    \"\"\"\n    Install the scalar coercion methods.\n    \"\"\"\n\n    def wrapper(self):\n        if len(self) == 1:\n            return converter(self.iloc[0])\n        raise TypeError(f\"cannot convert the series to {converter}\")\n\n    wrapper.__name__ = f\"__{converter.__name__}__\"\n    return wrapper\n\n\n# ----------------------------------------------------------------------\n# Series class\n\n\nclass Series(base.IndexOpsMixin, generic.NDFrame):\n    \"\"\"\n    One-dimensional ndarray with axis labels (including time series).\n\n    Labels need not be unique but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for performing operations involving the index. Statistical\n    methods from ndarray have been overridden to automatically exclude\n    missing data (currently represented as NaN).\n\n    Operations between Series (+, -, /, *, **) align values based on their\n    associated index values-- they need not be the same length. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Series. If data is a dict, argument order is\n        maintained.\n    index : array-like or Index (1d)\n        Values must be hashable and have the same length as `data`.\n        Non-unique index values are allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n        and index is None, then the keys in the data are used as the index. If the\n        index is not None, the resulting Series is reindexed with the index values.\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\n        Data type for the output Series. If not specified, this will be\n        inferred from `data`.\n        See the :ref:`user guide <basics.dtypes>` for more usages.\n    name : str, optional\n        The name to give to the Series.\n    copy : bool, default False\n        Copy input data.\n        If False, the memory location for the values is shared and thus\n        a `view` on the input data will be returned.\n        If True, the memory location for the values is not shared, and a `copy`\n        of the input data will be returned.\n\n    Examples\n    --------\n    Constructing Series from a dictionary with an Index specified\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = pd.Series(data=d, index=['a', 'b', 'c'])\n    >>> ser\n    a   1\n    b   2\n    c   3\n    dtype: int64\n\n    The keys of the dictionary match with the Index values, hence the Index\n    values have no effect.\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = pd.Series(data=d, index=['x', 'y', 'z'])\n    >>> ser\n    x   NaN\n    y   NaN\n    z   NaN\n    dtype: float64\n\n    Note that the Index is first build with the keys from the dictionary.\n    After this the Series is reindexed with the given Index values, hence we\n    get all NaN as a result.\n    \"\"\"\n\n    _typ = \"series\"\n    _HANDLED_TYPES = (Index, ExtensionArray, np.ndarray)\n\n    _name: Hashable\n    _metadata: list[str] = [\"name\"]\n    _internal_names_set = {\"index\"} | generic.NDFrame._internal_names_set\n    _accessors = {\"dt\", \"cat\", \"str\", \"sparse\"}\n    _hidden_attrs = (\n        base.IndexOpsMixin._hidden_attrs\n        | generic.NDFrame._hidden_attrs\n        | frozenset([\"compress\", \"ptp\"])\n    )\n\n    # Override cache_readonly bc Series is mutable\n    # error: Incompatible types in assignment (expression has type \"property\",\n    # base class \"IndexOpsMixin\" defined the type as \"Callable[[IndexOpsMixin], bool]\")\n    hasnans = property(  # type: ignore[assignment]\n        base.IndexOpsMixin.hasnans.func, doc=base.IndexOpsMixin.hasnans.__doc__\n    )\n    __hash__ = generic.NDFrame.__hash__\n    _mgr: SingleManager\n    div: Callable[[Series, Any], Series]\n    rdiv: Callable[[Series, Any], Series]\n\n    # ----------------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self,\n        data=None,\n        index=None,\n        dtype: Dtype | None = None,\n        name=None,\n        copy: bool = False,\n        fastpath: bool = False,\n    ):\n\n        if (\n            isinstance(data, (SingleBlockManager, SingleArrayManager))\n            and index is None\n            and dtype is None\n            and copy is False\n        ):\n            # GH#33357 called with just the SingleBlockManager\n            NDFrame.__init__(self, data)\n            self.name = name\n            return\n\n        # we are called internally, so short-circuit\n        if fastpath:\n\n            # data is an ndarray, index is defined\n            if not isinstance(data, (SingleBlockManager, SingleArrayManager)):\n                manager = get_option(\"mode.data_manager\")\n                if manager == \"block\":\n                    data = SingleBlockManager.from_array(data, index)\n                elif manager == \"array\":\n                    data = SingleArrayManager.from_array(data, index)\n            if copy:\n                data = data.copy()\n            if index is None:\n                index = data.index\n\n        else:\n\n            name = ibase.maybe_extract_name(name, data, type(self))\n\n            if is_empty_data(data) and dtype is None:\n                # gh-17261\n                warnings.warn(\n                    \"The default dtype for empty Series will be 'object' instead \"\n                    \"of 'float64' in a future version. Specify a dtype explicitly \"\n                    \"to silence this warning.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n                # uncomment the line below when removing the DeprecationWarning\n                # dtype = np.dtype(object)\n\n            if index is not None:\n                index = ensure_index(index)\n\n            if data is None:\n                data = {}\n            if dtype is not None:\n                dtype = self._validate_dtype(dtype)\n\n            if isinstance(data, MultiIndex):\n                raise NotImplementedError(\n                    \"initializing a Series from a MultiIndex is not supported\"\n                )\n            elif isinstance(data, Index):\n\n                if dtype is not None:\n                    # astype copies\n                    data = data.astype(dtype)\n                else:\n                    # GH#24096 we need to ensure the index remains immutable\n                    data = data._values.copy()\n                copy = False\n\n            elif isinstance(data, np.ndarray):\n                # error: Argument 1 to \"len\" has incompatible type \"dtype\"; expected\n                # \"Sized\"\n                if len(data.dtype):  # type: ignore[arg-type]\n                    # GH#13296 we are dealing with a compound dtype, which\n                    #  should be treated as 2D\n                    raise ValueError(\n                        \"Cannot construct a Series from an ndarray with \"\n                        \"compound dtype.  Use DataFrame instead.\"\n                    )\n            elif isinstance(data, Series):\n                if index is None:\n                    index = data.index\n                else:\n                    data = data.reindex(index, copy=copy)\n                    copy = False\n                data = data._mgr\n            elif is_dict_like(data):\n                data, index = self._init_dict(data, index, dtype)\n                dtype = None\n                copy = False\n            elif isinstance(data, (SingleBlockManager, SingleArrayManager)):\n                if index is None:\n                    index = data.index\n                elif not data.index.equals(index) or copy:\n                    # GH#19275 SingleBlockManager input should only be called\n                    # internally\n                    raise AssertionError(\n                        \"Cannot pass both SingleBlockManager \"\n                        \"`data` argument and a different \"\n                        \"`index` argument. `copy` must be False.\"\n                    )\n\n            elif isinstance(data, ExtensionArray):\n                pass\n            else:\n                data = com.maybe_iterable_to_list(data)\n\n            if index is None:\n                if not is_list_like(data):\n                    data = [data]\n                index = ibase.default_index(len(data))\n            elif is_list_like(data):\n                com.require_length_match(data, index)\n\n            # create/copy the manager\n            if isinstance(data, (SingleBlockManager, SingleArrayManager)):\n                if dtype is not None:\n                    data = data.astype(dtype=dtype, errors=\"ignore\", copy=copy)\n                elif copy:\n                    data = data.copy()\n            else:\n                data = sanitize_array(data, index, dtype, copy)\n\n                manager = get_option(\"mode.data_manager\")\n                if manager == \"block\":\n                    data = SingleBlockManager.from_array(data, index)\n                elif manager == \"array\":\n                    data = SingleArrayManager.from_array(data, index)\n\n        generic.NDFrame.__init__(self, data)\n        self.name = name\n        self._set_axis(0, index, fastpath=True)\n\n    def _init_dict(self, data, index=None, dtype: Dtype | None = None):\n        \"\"\"\n        Derive the \"_mgr\" and \"index\" attributes of a new Series from a\n        dictionary input.\n\n        Parameters\n        ----------\n        data : dict or dict-like\n            Data used to populate the new Series.\n        index : Index or index-like, default None\n            Index for the new Series: if None, use dict keys.\n        dtype : dtype, default None\n            The dtype for the new Series: if None, infer from data.\n\n        Returns\n        -------\n        _data : BlockManager for the new Series\n        index : index for the new Series\n        \"\"\"\n        # Looking for NaN in dict doesn't work ({np.nan : 1}[float('nan')]\n        # raises KeyError), so we iterate the entire dict, and align\n        if data:\n            # GH:34717, issue was using zip to extract key and values from data.\n            # using generators in effects the performance.\n            # Below is the new way of extracting the keys and values\n\n            keys = tuple(data.keys())\n            values = list(data.values())  # Generating list of values- faster way\n        elif index is not None:\n            # fastpath for Series(data=None). Just use broadcasting a scalar\n            # instead of reindexing.\n            values = na_value_for_dtype(pandas_dtype(dtype))\n            keys = index\n        else:\n            keys, values = (), []\n\n        # Input is now list-like, so rely on \"standard\" construction:\n\n        # TODO: passing np.float64 to not break anything yet. See GH-17261\n        s = create_series_with_explicit_dtype(\n            # error: Argument \"index\" to \"create_series_with_explicit_dtype\" has\n            # incompatible type \"Tuple[Any, ...]\"; expected \"Union[ExtensionArray,\n            # ndarray, Index, None]\"\n            values,\n            index=keys,  # type: ignore[arg-type]\n            dtype=dtype,\n            dtype_if_empty=np.float64,\n        )\n\n        # Now we just make sure the order is respected, if any\n        if data and index is not None:\n            s = s.reindex(index, copy=False)\n        return s._mgr, s.index\n\n    # ----------------------------------------------------------------------\n\n    @property\n    def _constructor(self) -> type[Series]:\n        return Series\n\n    @property\n    def _constructor_expanddim(self) -> type[DataFrame]:\n        \"\"\"\n        Used when a manipulation result has one higher dimension as the\n        original, such as Series.to_frame()\n        \"\"\"\n        from pandas.core.frame import DataFrame\n\n        return DataFrame\n\n    # types\n    @property\n    def _can_hold_na(self) -> bool:\n        return self._mgr._can_hold_na\n\n    _index: Index | None = None\n\n    def _set_axis(self, axis: int, labels, fastpath: bool = False) -> None:\n        \"\"\"\n        Override generic, we want to set the _typ here.\n\n        This is called from the cython code when we set the `index` attribute\n        directly, e.g. `series.index = [1, 2, 3]`.\n        \"\"\"\n        if not fastpath:\n            labels = ensure_index(labels)\n\n        if labels._is_all_dates:\n            deep_labels = labels\n            if isinstance(labels, CategoricalIndex):\n                deep_labels = labels.categories\n\n            if not isinstance(\n                deep_labels, (DatetimeIndex, PeriodIndex, TimedeltaIndex)\n            ):\n                try:\n                    labels = DatetimeIndex(labels)\n                    # need to set here because we changed the index\n                    if fastpath:\n                        self._mgr.set_axis(axis, labels)\n                except (tslibs.OutOfBoundsDatetime, ValueError):\n                    # labels may exceeds datetime bounds,\n                    # or not be a DatetimeIndex\n                    pass\n\n        object.__setattr__(self, \"_index\", labels)\n        if not fastpath:\n            # The ensure_index call above ensures we have an Index object\n            self._mgr.set_axis(axis, labels)\n\n    # ndarray compatibility\n    @property\n    def dtype(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._mgr.dtype\n\n    @property\n    def dtypes(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        # DataFrame compatibility\n        return self.dtype\n\n    @property\n    def name(self) -> Hashable:\n        \"\"\"\n        Return the name of the Series.\n\n        The name of a Series becomes its index or column name if it is used\n        to form a DataFrame. It is also used whenever displaying the Series\n        using the interpreter.\n\n        Returns\n        -------\n        label (hashable object)\n            The name of the Series, also the column name if part of a DataFrame.\n\n        See Also\n        --------\n        Series.rename : Sets the Series name when given a scalar input.\n        Index.name : Corresponding Index property.\n\n        Examples\n        --------\n        The Series name can be set initially when calling the constructor.\n\n        >>> s = pd.Series([1, 2, 3], dtype=np.int64, name='Numbers')\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Numbers, dtype: int64\n        >>> s.name = \"Integers\"\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Integers, dtype: int64\n\n        The name of a Series within a DataFrame is its column name.\n\n        >>> df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],\n        ...                   columns=[\"Odd Numbers\", \"Even Numbers\"])\n        >>> df\n           Odd Numbers  Even Numbers\n        0            1             2\n        1            3             4\n        2            5             6\n        >>> df[\"Even Numbers\"].name\n        'Even Numbers'\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value: Hashable) -> None:\n        validate_all_hashable(value, error_name=f\"{type(self).__name__}.name\")\n        object.__setattr__(self, \"_name\", value)\n\n    @property\n    def values(self):\n        \"\"\"\n        Return Series as ndarray or ndarray-like depending on the dtype.\n\n        .. warning::\n\n           We recommend using :attr:`Series.array` or\n           :meth:`Series.to_numpy`, depending on whether you need\n           a reference to the underlying data or a NumPy array.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n\n        See Also\n        --------\n        Series.array : Reference to the underlying data.\n        Series.to_numpy : A NumPy array representing the underlying data.\n\n        Examples\n        --------\n        >>> pd.Series([1, 2, 3]).values\n        array([1, 2, 3])\n\n        >>> pd.Series(list('aabc')).values\n        array(['a', 'a', 'b', 'c'], dtype=object)\n\n        >>> pd.Series(list('aabc')).astype('category').values\n        ['a', 'a', 'b', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n\n        Timezone aware datetime data is converted to UTC:\n\n        >>> pd.Series(pd.date_range('20130101', periods=3,\n        ...                         tz='US/Eastern')).values\n        array(['2013-01-01T05:00:00.000000000',\n               '2013-01-02T05:00:00.000000000',\n               '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')\n        \"\"\"\n        return self._mgr.external_values()\n\n    @property\n    def _values(self):\n        \"\"\"\n        Return the internal repr of this data (defined by Block.interval_values).\n        This are the values as stored in the Block (ndarray or ExtensionArray\n        depending on the Block class), with datetime64[ns] and timedelta64[ns]\n        wrapped in ExtensionArrays to match Index._values behavior.\n\n        Differs from the public ``.values`` for certain data types, because of\n        historical backwards compatibility of the public attribute (e.g. period\n        returns object ndarray and datetimetz a datetime64[ns] ndarray for\n        ``.values`` while it returns an ExtensionArray for ``._values`` in those\n        cases).\n\n        Differs from ``.array`` in that this still returns the numpy array if\n        the Block is backed by a numpy array (except for datetime64 and\n        timedelta64 dtypes), while ``.array`` ensures to always return an\n        ExtensionArray.\n\n        Overview:\n\n        dtype       | values        | _values       | array         |\n        ----------- | ------------- | ------------- | ------------- |\n        Numeric     | ndarray       | ndarray       | PandasArray   |\n        Category    | Categorical   | Categorical   | Categorical   |\n        dt64[ns]    | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        dt64[ns tz] | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        td64[ns]    | ndarray[m8ns] | TimedeltaArray| ndarray[m8ns] |\n        Period      | ndarray[obj]  | PeriodArray   | PeriodArray   |\n        Nullable    | EA            | EA            | EA            |\n\n        \"\"\"\n        return self._mgr.internal_values()\n\n    # error: Decorated property not supported\n    @Appender(base.IndexOpsMixin.array.__doc__)  # type: ignore[misc]\n    @property\n    def array(self) -> ExtensionArray:\n        return self._mgr.array_values()\n\n    # ops\n    def ravel(self, order=\"C\"):\n        \"\"\"\n        Return the flattened underlying data as an ndarray.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n            Flattened data of the Series.\n\n        See Also\n        --------\n        numpy.ndarray.ravel : Return a flattened array.\n        \"\"\"\n        return self._values.ravel(order=order)\n\n    def __len__(self) -> int:\n        \"\"\"\n        Return the length of the Series.\n        \"\"\"\n        return len(self._mgr)\n\n    def view(self, dtype: Dtype | None = None) -> Series:\n        \"\"\"\n        Create a new view of the Series.\n\n        This function will return a new Series with a view of the same\n        underlying values in memory, optionally reinterpreted with a new data\n        type. The new data type must preserve the same size in bytes as to not\n        cause index misalignment.\n\n        Parameters\n        ----------\n        dtype : data type\n            Data type object or one of their string representations.\n\n        Returns\n        -------\n        Series\n            A new Series object as a view of the same data in memory.\n\n        See Also\n        --------\n        numpy.ndarray.view : Equivalent numpy function to create a new view of\n            the same data in memory.\n\n        Notes\n        -----\n        Series are instantiated with ``dtype=float64`` by default. While\n        ``numpy.ndarray.view()`` will return a view with the same data type as\n        the original array, ``Series.view()`` (without specified dtype)\n        will try using ``float64`` and may fail if the original data type size\n        in bytes is not the same.\n\n        Examples\n        --------\n        >>> s = pd.Series([-2, -1, 0, 1, 2], dtype='int8')\n        >>> s\n        0   -2\n        1   -1\n        2    0\n        3    1\n        4    2\n        dtype: int8\n\n        The 8 bit signed integer representation of `-1` is `0b11111111`, but\n        the same bytes represent 255 if read as an 8 bit unsigned integer:\n\n        >>> us = s.view('uint8')\n        >>> us\n        0    254\n        1    255\n        2      0\n        3      1\n        4      2\n        dtype: uint8\n\n        The views share the same underlying values:\n\n        >>> us[0] = 128\n        >>> s\n        0   -128\n        1     -1\n        2      0\n        3      1\n        4      2\n        dtype: int8\n        \"\"\"\n        return self._constructor(\n            self._values.view(dtype), index=self.index\n        ).__finalize__(self, method=\"view\")\n\n    # ----------------------------------------------------------------------\n    # NDArray Compat\n    _HANDLED_TYPES = (Index, ExtensionArray, np.ndarray)\n\n    def __array__(self, dtype: NpDtype | None = None) -> np.ndarray:\n        \"\"\"\n        Return the values as a NumPy array.\n\n        Users should not call this directly. Rather, it is invoked by\n        :func:`numpy.array` and :func:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to use for the resulting NumPy array. By default,\n            the dtype is inferred from the data.\n\n        Returns\n        -------\n        numpy.ndarray\n            The values in the series converted to a :class:`numpy.ndarray`\n            with the specified `dtype`.\n\n        See Also\n        --------\n        array : Create a new array from data.\n        Series.array : Zero-copy view to the array backing the Series.\n        Series.to_numpy : Series method for similar behavior.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1, 2, 3])\n        >>> np.asarray(ser)\n        array([1, 2, 3])\n\n        For timezone-aware data, the timezones may be retained with\n        ``dtype='object'``\n\n        >>> tzser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\n        >>> np.asarray(tzser, dtype=\"object\")\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET', freq='D'),\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET', freq='D')],\n              dtype=object)\n\n        Or the values may be localized to UTC and the tzinfo discarded with\n        ``dtype='datetime64[ns]'``\n\n        >>> np.asarray(tzser, dtype=\"datetime64[ns]\")  # doctest: +ELLIPSIS\n        array(['1999-12-31T23:00:00.000000000', ...],\n              dtype='datetime64[ns]')\n        \"\"\"\n        return np.asarray(self._values, dtype)\n\n    # ----------------------------------------------------------------------\n    # Unary Methods\n\n    # coercion\n    __float__ = _coerce_method(float)\n    __long__ = _coerce_method(int)\n    __int__ = _coerce_method(int)\n\n    # ----------------------------------------------------------------------\n\n    # indexers\n    @property\n    def axes(self) -> list[Index]:\n        \"\"\"\n        Return a list of the row axis labels.\n        \"\"\"\n        return [self.index]\n\n    # ----------------------------------------------------------------------\n    # Indexing Methods\n\n    @Appender(generic.NDFrame.take.__doc__)\n    def take(self, indices, axis=0, is_copy=None, **kwargs) -> Series:\n        if is_copy is not None:\n            warnings.warn(\n                \"is_copy is deprecated and will be removed in a future version. \"\n                \"'take' always returns a copy, so there is no need to specify this.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n        nv.validate_take((), kwargs)\n\n        indices = ensure_platform_int(indices)\n        new_index = self.index.take(indices)\n        new_values = self._values.take(indices)\n\n        result = self._constructor(new_values, index=new_index, fastpath=True)\n        return result.__finalize__(self, method=\"take\")\n\n    def _take_with_is_copy(self, indices, axis=0) -> Series:\n        \"\"\"\n        Internal version of the `take` method that sets the `_is_copy`\n        attribute to keep track of the parent dataframe (using in indexing\n        for the SettingWithCopyWarning). For Series this does the same\n        as the public take (it never sets `_is_copy`).\n\n        See the docstring of `take` for full explanation of the parameters.\n        \"\"\"\n        return self.take(indices=indices, axis=axis)\n\n    def _ixs(self, i: int, axis: int = 0):\n        \"\"\"\n        Return the i-th value or values in the Series by location.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        scalar (int) or Series (slice, sequence)\n        \"\"\"\n        return self._values[i]\n\n    def _slice(self, slobj: slice, axis: int = 0) -> Series:\n        # axis kwarg is retained for compat with NDFrame method\n        #  _slice is *always* positional\n        return self._get_values(slobj)\n\n    def __getitem__(self, key):\n        key = com.apply_if_callable(key, self)\n\n        if key is Ellipsis:\n            return self\n\n        key_is_scalar = is_scalar(key)\n        if isinstance(key, (list, tuple)):\n            key = unpack_1tuple(key)\n\n        if is_integer(key) and self.index._should_fallback_to_positional():\n            return self._values[key]\n\n        elif key_is_scalar:\n            return self._get_value(key)\n\n        if is_hashable(key):\n            # Otherwise index.get_value will raise InvalidIndexError\n            try:\n                # For labels that don't resolve as scalars like tuples and frozensets\n                result = self._get_value(key)\n\n                return result\n\n            except (KeyError, TypeError):\n                if isinstance(key, tuple) and isinstance(self.index, MultiIndex):\n                    # We still have the corner case where a tuple is a key\n                    # in the first level of our MultiIndex\n                    return self._get_values_tuple(key)\n\n        if is_iterator(key):\n            key = list(key)\n\n        if com.is_bool_indexer(key):\n            key = check_bool_indexer(self.index, key)\n            key = np.asarray(key, dtype=bool)\n            return self._get_values(key)\n\n        return self._get_with(key)\n\n    def _get_with(self, key):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            # _convert_slice_indexer to determine if this slice is positional\n            #  or label based, and if the latter, convert to positional\n            slobj = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            return self._slice(slobj)\n        elif isinstance(key, ABCDataFrame):\n            raise TypeError(\n                \"Indexing a Series with DataFrame is not \"\n                \"supported, use the appropriate DataFrame column\"\n            )\n        elif isinstance(key, tuple):\n            return self._get_values_tuple(key)\n\n        elif not is_list_like(key):\n            # e.g. scalars that aren't recognized by lib.is_scalar, GH#32684\n            return self.loc[key]\n\n        if not isinstance(key, (list, np.ndarray, ExtensionArray, Series, Index)):\n            key = list(key)\n\n        if isinstance(key, Index):\n            key_type = key.inferred_type\n        else:\n            key_type = lib.infer_dtype(key, skipna=False)\n\n        # Note: The key_type == \"boolean\" case should be caught by the\n        #  com.is_bool_indexer check in __getitem__\n        if key_type == \"integer\":\n            # We need to decide whether to treat this as a positional indexer\n            #  (i.e. self.iloc) or label-based (i.e. self.loc)\n            if not self.index._should_fallback_to_positional():\n                return self.loc[key]\n            else:\n                return self.iloc[key]\n\n        # handle the dup indexing case GH#4246\n        return self.loc[key]\n\n    def _get_values_tuple(self, key):\n        # mpl hackaround\n        if com.any_none(*key):\n            result = self._get_values(key)\n            deprecate_ndim_indexing(result, stacklevel=5)\n            return result\n\n        if not isinstance(self.index, MultiIndex):\n            raise KeyError(\"key of type tuple not found and not a MultiIndex\")\n\n        # If key is contained, would have returned by now\n        indexer, new_index = self.index.get_loc_level(key)\n        return self._constructor(self._values[indexer], index=new_index).__finalize__(\n            self\n        )\n\n    def _get_values(self, indexer):\n        try:\n            new_mgr = self._mgr.getitem_mgr(indexer)\n            return self._constructor(new_mgr).__finalize__(self)\n        except ValueError:\n            # mpl compat if we look up e.g. ser[:, np.newaxis];\n            #  see tests.series.timeseries.test_mpl_compat_hack\n            # the asarray is needed to avoid returning a 2D DatetimeArray\n            return np.asarray(self._values[indexer])\n\n    def _get_value(self, label, takeable: bool = False):\n        \"\"\"\n        Quickly retrieve single value at passed index label.\n\n        Parameters\n        ----------\n        label : object\n        takeable : interpret the index as indexers, default False\n\n        Returns\n        -------\n        scalar value\n        \"\"\"\n        if takeable:\n            return self._values[label]\n\n        # Similar to Index.get_value, but we do not fall back to positional\n        loc = self.index.get_loc(label)\n        return self.index._get_values_for_loc(self, loc, label)\n\n    def __setitem__(self, key, value) -> None:\n        key = com.apply_if_callable(key, self)\n        cacher_needs_updating = self._check_is_chained_assignment_possible()\n\n        if key is Ellipsis:\n            key = slice(None)\n\n        try:\n            self._set_with_engine(key, value)\n        except (KeyError, ValueError):\n            values = self._values\n            if is_integer(key) and self.index.inferred_type != \"integer\":\n                # positional setter\n                values[key] = value\n            else:\n                # GH#12862 adding a new key to the Series\n                self.loc[key] = value\n\n        except TypeError as err:\n            if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):\n                raise KeyError(\n                    \"key of type tuple not found and not a MultiIndex\"\n                ) from err\n\n            if com.is_bool_indexer(key):\n                key = check_bool_indexer(self.index, key)\n                key = np.asarray(key, dtype=bool)\n                try:\n                    self._where(~key, value, inplace=True)\n                except InvalidIndexError:\n                    self.iloc[key] = value\n                return\n\n            else:\n                self._set_with(key, value)\n\n        if cacher_needs_updating:\n            self._maybe_update_cacher()\n\n    def _set_with_engine(self, key, value) -> None:\n        # fails with AttributeError for IntervalIndex\n        loc = self.index._engine.get_loc(key)\n        # error: Argument 1 to \"validate_numeric_casting\" has incompatible type\n        # \"Union[dtype, ExtensionDtype]\"; expected \"dtype\"\n        validate_numeric_casting(self.dtype, value)  # type: ignore[arg-type]\n        self._values[loc] = value\n\n    def _set_with(self, key, value):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            indexer = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            return self._set_values(indexer, value)\n\n        else:\n            assert not isinstance(key, tuple)\n\n            if is_scalar(key):\n                key = [key]\n\n            if isinstance(key, Index):\n                key_type = key.inferred_type\n                key = key._values\n            else:\n                key_type = lib.infer_dtype(key, skipna=False)\n\n            # Note: key_type == \"boolean\" should not occur because that\n            #  should be caught by the is_bool_indexer check in __setitem__\n            if key_type == \"integer\":\n                if not self.index._should_fallback_to_positional():\n                    self._set_labels(key, value)\n                else:\n                    self._set_values(key, value)\n            else:\n                self.loc[key] = value\n\n    def _set_labels(self, key, value) -> None:\n        key = com.asarray_tuplesafe(key)\n        indexer: np.ndarray = self.index.get_indexer(key)\n        mask = indexer == -1\n        if mask.any():\n            raise KeyError(f\"{key[mask]} not in index\")\n        self._set_values(indexer, value)\n\n    def _set_values(self, key, value) -> None:\n        if isinstance(key, Series):\n            key = key._values\n\n        self._mgr = self._mgr.setitem(indexer=key, value=value)\n        self._maybe_update_cacher()\n\n    def _set_value(self, label, value, takeable: bool = False):\n        \"\"\"\n        Quickly set single value at passed label.\n\n        If label is not contained, a new object is created with the label\n        placed at the end of the result index.\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed.\n        value : object\n            Scalar value.\n        takeable : interpret the index as indexers, default False\n        \"\"\"\n        if not takeable:\n            try:\n                loc = self.index.get_loc(label)\n            except KeyError:\n                # set using a non-recursive method\n                self.loc[label] = value\n                return\n        else:\n            loc = label\n\n        self._set_values(loc, value)\n\n    # ----------------------------------------------------------------------\n    # Lookup Caching\n\n    @property\n    def _is_cached(self) -> bool:\n        \"\"\"Return boolean indicating if self is cached or not.\"\"\"\n        return getattr(self, \"_cacher\", None) is not None\n\n    def _get_cacher(self):\n        \"\"\"return my cacher or None\"\"\"\n        cacher = getattr(self, \"_cacher\", None)\n        if cacher is not None:\n            cacher = cacher[1]()\n        return cacher\n\n    def _reset_cacher(self) -> None:\n        \"\"\"\n        Reset the cacher.\n        \"\"\"\n        if hasattr(self, \"_cacher\"):\n            # should only get here with self.ndim == 1\n            del self._cacher\n\n    def _set_as_cached(self, item, cacher) -> None:\n        \"\"\"\n        Set the _cacher attribute on the calling object with a weakref to\n        cacher.\n        \"\"\"\n        self._cacher = (item, weakref.ref(cacher))\n\n    def _clear_item_cache(self) -> None:\n        # no-op for Series\n        pass\n\n    def _check_is_chained_assignment_possible(self) -> bool:\n        \"\"\"\n        See NDFrame._check_is_chained_assignment_possible.__doc__\n        \"\"\"\n        if self._is_view and self._is_cached:\n            ref = self._get_cacher()\n            if ref is not None and ref._is_mixed_type:\n                self._check_setitem_copy(stacklevel=4, t=\"referent\", force=True)\n            return True\n        return super()._check_is_chained_assignment_possible()\n\n    def _maybe_update_cacher(\n        self, clear: bool = False, verify_is_copy: bool = True\n    ) -> None:\n        \"\"\"\n        See NDFrame._maybe_update_cacher.__doc__\n        \"\"\"\n        cacher = getattr(self, \"_cacher\", None)\n        if cacher is not None:\n            assert self.ndim == 1\n            ref: DataFrame = cacher[1]()\n\n            # we are trying to reference a dead referent, hence\n            # a copy\n            if ref is None:\n                del self._cacher\n            else:\n                if len(self) == len(ref):\n                    # otherwise, either self or ref has swapped in new arrays\n                    ref._maybe_cache_changed(cacher[0], self)\n                else:\n                    # GH#33675 we have swapped in a new array, so parent\n                    #  reference to self is now invalid\n                    ref._item_cache.pop(cacher[0], None)\n\n        super()._maybe_update_cacher(clear=clear, verify_is_copy=verify_is_copy)\n\n    # ----------------------------------------------------------------------\n    # Unsorted\n\n    @property\n    def _is_mixed_type(self):\n        return False\n\n    def repeat(self, repeats, axis=None) -> Series:\n        \"\"\"\n        Repeat elements of a Series.\n\n        Returns a new Series where each element of the current Series\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            Series.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        Series\n            Newly created Series with repeated elements.\n\n        See Also\n        --------\n        Index.repeat : Equivalent function for Index.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s\n        0    a\n        1    b\n        2    c\n        dtype: object\n        >>> s.repeat(2)\n        0    a\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        dtype: object\n        >>> s.repeat([1, 2, 3])\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        2    c\n        dtype: object\n        \"\"\"\n        nv.validate_repeat((), {\"axis\": axis})\n        new_index = self.index.repeat(repeats)\n        new_values = self._values.repeat(repeats)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"repeat\"\n        )\n\n    def reset_index(self, level=None, drop=False, name=None, inplace=False):\n        \"\"\"\n        Generate a new DataFrame or Series with the index reset.\n\n        This is useful when the index needs to be treated as a column, or\n        when the index is meaningless and needs to be reset to the default\n        before another operation.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default optional\n            For a Series with a MultiIndex, only remove the specified levels\n            from the index. Removes all levels by default.\n        drop : bool, default False\n            Just reset the index, without inserting it as a column in\n            the new DataFrame.\n        name : object, optional\n            The name to use for the column containing the original Series\n            values. Uses ``self.name`` by default. This argument is ignored\n            when `drop` is True.\n        inplace : bool, default False\n            Modify the Series in place (do not create a new object).\n\n        Returns\n        -------\n        Series or DataFrame or None\n            When `drop` is False (the default), a DataFrame is returned.\n            The newly created columns will come first in the DataFrame,\n            followed by the original Series values.\n            When `drop` is True, a `Series` is returned.\n            In either case, if ``inplace=True``, no value is returned.\n\n        See Also\n        --------\n        DataFrame.reset_index: Analogous function for DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4], name='foo',\n        ...               index=pd.Index(['a', 'b', 'c', 'd'], name='idx'))\n\n        Generate a DataFrame with default index.\n\n        >>> s.reset_index()\n          idx  foo\n        0   a    1\n        1   b    2\n        2   c    3\n        3   d    4\n\n        To specify the name of the new column use `name`.\n\n        >>> s.reset_index(name='values')\n          idx  values\n        0   a       1\n        1   b       2\n        2   c       3\n        3   d       4\n\n        To generate a new Series with the default set `drop` to True.\n\n        >>> s.reset_index(drop=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        To update the Series in place, without generating a new one\n        set `inplace` to True. Note that it also requires ``drop=True``.\n\n        >>> s.reset_index(inplace=True, drop=True)\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        The `level` parameter is interesting for Series with a multi-level\n        index.\n\n        >>> arrays = [np.array(['bar', 'bar', 'baz', 'baz']),\n        ...           np.array(['one', 'two', 'one', 'two'])]\n        >>> s2 = pd.Series(\n        ...     range(4), name='foo',\n        ...     index=pd.MultiIndex.from_arrays(arrays,\n        ...                                     names=['a', 'b']))\n\n        To remove a specific level from the Index, use `level`.\n\n        >>> s2.reset_index(level='a')\n               a  foo\n        b\n        one  bar    0\n        two  bar    1\n        one  baz    2\n        two  baz    3\n\n        If `level` is not set, all levels are removed from the Index.\n\n        >>> s2.reset_index()\n             a    b  foo\n        0  bar  one    0\n        1  bar  two    1\n        2  baz  one    2\n        3  baz  two    3\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if drop:\n            new_index = ibase.default_index(len(self))\n            if level is not None:\n                if not isinstance(level, (tuple, list)):\n                    level = [level]\n                level = [self.index._get_level_number(lev) for lev in level]\n                if len(level) < self.index.nlevels:\n                    new_index = self.index.droplevel(level)\n\n            if inplace:\n                self.index = new_index\n                # set name if it was passed, otherwise, keep the previous name\n                self.name = name or self.name\n            else:\n                return self._constructor(\n                    self._values.copy(), index=new_index\n                ).__finalize__(self, method=\"reset_index\")\n        elif inplace:\n            raise TypeError(\n                \"Cannot reset_index inplace on a Series to create a DataFrame\"\n            )\n        else:\n            df = self.to_frame(name)\n            return df.reset_index(level=level, drop=drop)\n\n    # ----------------------------------------------------------------------\n    # Rendering Methods\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular Series.\n        \"\"\"\n        buf = StringIO(\"\")\n        width, height = get_terminal_size()\n        max_rows = (\n            height\n            if get_option(\"display.max_rows\") == 0\n            else get_option(\"display.max_rows\")\n        )\n        min_rows = (\n            height\n            if get_option(\"display.max_rows\") == 0\n            else get_option(\"display.min_rows\")\n        )\n        show_dimensions = get_option(\"display.show_dimensions\")\n\n        self.to_string(\n            buf=buf,\n            name=self.name,\n            dtype=self.dtype,\n            min_rows=min_rows,\n            max_rows=max_rows,\n            length=show_dimensions,\n        )\n        return buf.getvalue()\n\n    def to_string(\n        self,\n        buf=None,\n        na_rep=\"NaN\",\n        float_format=None,\n        header=True,\n        index=True,\n        length=False,\n        dtype=False,\n        name=False,\n        max_rows=None,\n        min_rows=None,\n    ):\n        \"\"\"\n        Render a string representation of the Series.\n\n        Parameters\n        ----------\n        buf : StringIO-like, optional\n            Buffer to write to.\n        na_rep : str, optional\n            String representation of NaN to use, default 'NaN'.\n        float_format : one-parameter function, optional\n            Formatter function to apply to columns' elements if they are\n            floats, default None.\n        header : bool, default True\n            Add the Series header (index name).\n        index : bool, optional\n            Add index (row) labels, default True.\n        length : bool, default False\n            Add the Series length.\n        dtype : bool, default False\n            Add the Series dtype.\n        name : bool, default False\n            Add the Series name if not None.\n        max_rows : int, optional\n            Maximum number of rows to show before truncating. If None, show\n            all.\n        min_rows : int, optional\n            The number of rows to display in a truncated repr (when number\n            of rows is above `max_rows`).\n\n        Returns\n        -------\n        str or None\n            String representation of Series if ``buf=None``, otherwise None.\n        \"\"\"\n        formatter = fmt.SeriesFormatter(\n            self,\n            name=name,\n            length=length,\n            header=header,\n            index=index,\n            dtype=dtype,\n            na_rep=na_rep,\n            float_format=float_format,\n            min_rows=min_rows,\n            max_rows=max_rows,\n        )\n        result = formatter.to_string()\n\n        # catch contract violations\n        if not isinstance(result, str):\n            raise AssertionError(\n                \"result must be of type str, type \"\n                f\"of result is {repr(type(result).__name__)}\"\n            )\n\n        if buf is None:\n            return result\n        else:\n            try:\n                buf.write(result)\n            except AttributeError:\n                with open(buf, \"w\") as f:\n                    f.write(result)\n\n    @doc(\n        klass=_shared_doc_kwargs[\"klass\"],\n        storage_options=generic._shared_docs[\"storage_options\"],\n        examples=dedent(\n            \"\"\"\n            Examples\n            --------\n            >>> s = pd.Series([\"elk\", \"pig\", \"dog\", \"quetzal\"], name=\"animal\")\n            >>> print(s.to_markdown())\n            |    | animal   |\n            |---:|:---------|\n            |  0 | elk      |\n            |  1 | pig      |\n            |  2 | dog      |\n            |  3 | quetzal  |\n            \"\"\"\n        ),\n    )\n    def to_markdown(\n        self,\n        buf: IO[str] | None = None,\n        mode: str = \"wt\",\n        index: bool = True,\n        storage_options: StorageOptions = None,\n        **kwargs,\n    ) -> str | None:\n        \"\"\"\n        Print {klass} in Markdown-friendly format.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        buf : str, Path or StringIO-like, optional, default None\n            Buffer to write to. If None, the output is returned as a string.\n        mode : str, optional\n            Mode in which file is opened, \"wt\" by default.\n        index : bool, optional, default True\n            Add index (row) labels.\n\n            .. versionadded:: 1.1.0\n        {storage_options}\n\n            .. versionadded:: 1.2.0\n\n        **kwargs\n            These parameters will be passed to `tabulate \\\n                <https://pypi.org/project/tabulate>`_.\n\n        Returns\n        -------\n        str\n            {klass} in Markdown-friendly format.\n\n        Notes\n        -----\n        Requires the `tabulate <https://pypi.org/project/tabulate>`_ package.\n\n        Examples\n        --------\n        >>> s = pd.Series([\"elk\", \"pig\", \"dog\", \"quetzal\"], name=\"animal\")\n        >>> print(s.to_markdown())\n        |    | animal   |\n        |---:|:---------|\n        |  0 | elk      |\n        |  1 | pig      |\n        |  2 | dog      |\n        |  3 | quetzal  |\n\n        Output markdown with a tabulate option.\n\n        >>> print(s.to_markdown(tablefmt=\"grid\"))\n        +----+----------+\n        |    | animal   |\n        +====+==========+\n        |  0 | elk      |\n        +----+----------+\n        |  1 | pig      |\n        +----+----------+\n        |  2 | dog      |\n        +----+----------+\n        |  3 | quetzal  |\n        +----+----------+\n        \"\"\"\n        return self.to_frame().to_markdown(\n            buf, mode, index, storage_options=storage_options, **kwargs\n        )\n\n    # ----------------------------------------------------------------------\n\n    def items(self) -> Iterable[tuple[Hashable, Any]]:\n        \"\"\"\n        Lazily iterate over (index, value) tuples.\n\n        This method returns an iterable tuple (index, value). This is\n        convenient if you want to create a lazy iterator.\n\n        Returns\n        -------\n        iterable\n            Iterable of tuples containing the (index, value) pairs from a\n            Series.\n\n        See Also\n        --------\n        DataFrame.items : Iterate over (column name, Series) pairs.\n        DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs.\n\n        Examples\n        --------\n        >>> s = pd.Series(['A', 'B', 'C'])\n        >>> for index, value in s.items():\n        ...     print(f\"Index : {index}, Value : {value}\")\n        Index : 0, Value : A\n        Index : 1, Value : B\n        Index : 2, Value : C\n        \"\"\"\n        return zip(iter(self.index), iter(self))\n\n    @Appender(items.__doc__)\n    def iteritems(self) -> Iterable[tuple[Hashable, Any]]:\n        return self.items()\n\n    # ----------------------------------------------------------------------\n    # Misc public methods\n\n    def keys(self) -> Index:\n        \"\"\"\n        Return alias for index.\n\n        Returns\n        -------\n        Index\n            Index of the Series.\n        \"\"\"\n        return self.index\n\n    def to_dict(self, into=dict):\n        \"\"\"\n        Convert Series to {label -> value} dict or dict-like object.\n\n        Parameters\n        ----------\n        into : class, default dict\n            The collections.abc.Mapping subclass to use as the return\n            object. Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Key-value representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.to_dict()\n        {0: 1, 1: 2, 2: 3, 3: 4}\n        >>> from collections import OrderedDict, defaultdict\n        >>> s.to_dict(OrderedDict)\n        OrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])\n        >>> dd = defaultdict(list)\n        >>> s.to_dict(dd)\n        defaultdict(<class 'list'>, {0: 1, 1: 2, 2: 3, 3: 4})\n        \"\"\"\n        # GH16122\n        into_c = com.standardize_mapping(into)\n        return into_c((k, maybe_box_native(v)) for k, v in self.items())\n\n    def to_frame(self, name=None) -> DataFrame:\n        \"\"\"\n        Convert Series to DataFrame.\n\n        Parameters\n        ----------\n        name : object, default None\n            The passed name should substitute for the series name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([\"a\", \"b\", \"c\"],\n        ...               name=\"vals\")\n        >>> s.to_frame()\n          vals\n        0    a\n        1    b\n        2    c\n        \"\"\"\n        if name is None:\n            df = self._constructor_expanddim(self)\n        else:\n            df = self._constructor_expanddim({name: self})\n\n        return df\n\n    def _set_name(self, name, inplace=False) -> Series:\n        \"\"\"\n        Set the Series name.\n\n        Parameters\n        ----------\n        name : str\n        inplace : bool\n            Whether to modify `self` directly or return a copy.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        ser = self if inplace else self.copy()\n        ser.name = name\n        return ser\n\n    @Appender(\n        \"\"\"\nExamples\n--------\n>>> ser = pd.Series([390., 350., 30., 20.],\n...                 index=['Falcon', 'Falcon', 'Parrot', 'Parrot'], name=\"Max Speed\")\n>>> ser\nFalcon    390.0\nFalcon    350.0\nParrot     30.0\nParrot     20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby([\"a\", \"b\", \"a\", \"b\"]).mean()\na    210.0\nb    185.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(ser > 100).mean()\nMax Speed\nFalse     25.0\nTrue     370.0\nName: Max Speed, dtype: float64\n\n**Grouping by Indexes**\n\nWe can groupby different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> ser = pd.Series([390., 350., 30., 20.], index=index, name=\"Max Speed\")\n>>> ser\nAnimal  Type\nFalcon  Captive    390.0\n        Wild       350.0\nParrot  Captive     30.0\n        Wild        20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nAnimal\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=\"Type\").mean()\nType\nCaptive    210.0\nWild       185.0\nName: Max Speed, dtype: float64\n\nWe can also choose to include `NA` in group keys or not by defining\n`dropna` parameter, the default setting is `True`:\n\n>>> ser = pd.Series([1, 2, 3, 3], index=[\"a\", 'a', 'b', np.nan])\n>>> ser.groupby(level=0).sum()\na    3\nb    3\ndtype: int64\n\n>>> ser.groupby(level=0, dropna=False).sum()\na    3\nb    3\nNaN  3\ndtype: int64\n\n>>> arrays = ['Falcon', 'Falcon', 'Parrot', 'Parrot']\n>>> ser = pd.Series([390., 350., 30., 20.], index=arrays, name=\"Max Speed\")\n>>> ser.groupby([\"a\", \"b\", \"a\", np.nan]).mean()\na    210.0\nb    350.0\nName: Max Speed, dtype: float64\n\n>>> ser.groupby([\"a\", \"b\", \"a\", np.nan], dropna=False).mean()\na    210.0\nb    350.0\nNaN   20.0\nName: Max Speed, dtype: float64\n\"\"\"\n    )\n    @Appender(generic._shared_docs[\"groupby\"] % _shared_doc_kwargs)\n    def groupby(\n        self,\n        by=None,\n        axis=0,\n        level=None,\n        as_index: bool = True,\n        sort: bool = True,\n        group_keys: bool = True,\n        squeeze: bool | lib.NoDefault = no_default,\n        observed: bool = False,\n        dropna: bool = True,\n    ) -> SeriesGroupBy:\n        from pandas.core.groupby.generic import SeriesGroupBy\n\n        if squeeze is not no_default:\n            warnings.warn(\n                (\n                    \"The `squeeze` parameter is deprecated and \"\n                    \"will be removed in a future version.\"\n                ),\n                FutureWarning,\n                stacklevel=2,\n            )\n        else:\n            squeeze = False\n\n        if level is None and by is None:\n            raise TypeError(\"You have to supply one of 'by' and 'level'\")\n        axis = self._get_axis_number(axis)\n\n        # error: Argument \"squeeze\" to \"SeriesGroupBy\" has incompatible type\n        # \"Union[bool, NoDefault]\"; expected \"bool\"\n        return SeriesGroupBy(\n            obj=self,\n            keys=by,\n            axis=axis,\n            level=level,\n            as_index=as_index,\n            sort=sort,\n            group_keys=group_keys,\n            squeeze=squeeze,  # type: ignore[arg-type]\n            observed=observed,\n            dropna=dropna,\n        )\n\n    # ----------------------------------------------------------------------\n    # Statistics, overridden ndarray methods\n\n    # TODO: integrate bottleneck\n\n    def count(self, level=None):\n        \"\"\"\n        Return number of non-NA/null observations in the Series.\n\n        Parameters\n        ----------\n        level : int or level name, default None\n            If the axis is a MultiIndex (hierarchical), count along a\n            particular level, collapsing into a smaller Series.\n\n        Returns\n        -------\n        int or Series (if level specified)\n            Number of non-null values in the Series.\n\n        See Also\n        --------\n        DataFrame.count : Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.0, 1.0, np.nan])\n        >>> s.count()\n        2\n        \"\"\"\n        if level is None:\n            return notna(self._values).sum().astype(\"int64\")\n        else:\n            warnings.warn(\n                \"Using the level keyword in DataFrame and Series aggregations is \"\n                \"deprecated and will be removed in a future version. Use groupby \"\n                \"instead. ser.count(level=1) should use ser.groupby(level=1).count().\",\n                FutureWarning,\n                stacklevel=2,\n            )\n            if not isinstance(self.index, MultiIndex):\n                raise ValueError(\"Series.count level is only valid with a MultiIndex\")\n\n        index = self.index\n        assert isinstance(index, MultiIndex)  # for mypy\n\n        if isinstance(level, str):\n            level = index._get_level_number(level)\n\n        lev = index.levels[level]\n        level_codes = np.array(index.codes[level], subok=False, copy=True)\n\n        mask = level_codes == -1\n        if mask.any():\n            level_codes[mask] = cnt = len(lev)\n            lev = lev.insert(cnt, lev._na_value)\n\n        obs = level_codes[notna(self._values)]\n        out = np.bincount(obs, minlength=len(lev) or None)\n        return self._constructor(out, index=lev, dtype=\"int64\").__finalize__(\n            self, method=\"count\"\n        )\n\n    def mode(self, dropna=True) -> Series:\n        \"\"\"\n        Return the mode(s) of the Series.\n\n        The mode is the value that appears most often. There can be multiple modes.\n\n        Always returns Series even if only one value is returned.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        Series\n            Modes of the Series in sorted order.\n        \"\"\"\n        # TODO: Add option for bins like value_counts()\n        return algorithms.mode(self, dropna=dropna)\n\n    def unique(self) -> ArrayLike:\n        \"\"\"\n        Return unique values of Series object.\n\n        Uniques are returned in order of appearance. Hash table-based unique,\n        therefore does NOT sort.\n\n        Returns\n        -------\n        ndarray or ExtensionArray\n            The unique values returned as a NumPy array. See Notes.\n\n        See Also\n        --------\n        unique : Top-level unique method for any 1-d array-like object.\n        Index.unique : Return Index with unique values from an Index object.\n\n        Notes\n        -----\n        Returns the unique values as a NumPy array. In case of an\n        extension-array backed Series, a new\n        :class:`~api.extensions.ExtensionArray` of that type with just\n        the unique values is returned. This includes\n\n            * Categorical\n            * Period\n            * Datetime with Timezone\n            * Interval\n            * Sparse\n            * IntegerNA\n\n        See Examples section.\n\n        Examples\n        --------\n        >>> pd.Series([2, 1, 3, 3], name='A').unique()\n        array([2, 1, 3])\n\n        >>> pd.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()\n        array(['2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\n\n        >>> pd.Series([pd.Timestamp('2016-01-01', tz='US/Eastern')\n        ...            for _ in range(3)]).unique()\n        <DatetimeArray>\n        ['2016-01-01 00:00:00-05:00']\n        Length: 1, dtype: datetime64[ns, US/Eastern]\n\n        An Categorical will return categories in the order of\n        appearance and with the same dtype.\n\n        >>> pd.Series(pd.Categorical(list('baabc'))).unique()\n        ['b', 'a', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> pd.Series(pd.Categorical(list('baabc'), categories=list('abc'),\n        ...                          ordered=True)).unique()\n        ['b', 'a', 'c']\n        Categories (3, object): ['a' < 'b' < 'c']\n        \"\"\"\n        return super().unique()\n\n    @overload\n    def drop_duplicates(self, keep=..., inplace: Literal[False] = ...) -> Series:\n        ...\n\n    @overload\n    def drop_duplicates(self, keep, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def drop_duplicates(self, *, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def drop_duplicates(self, keep=..., inplace: bool = ...) -> Series | None:\n        ...\n\n    def drop_duplicates(self, keep=\"first\", inplace=False) -> Series | None:\n        \"\"\"\n        Return Series with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        inplace : bool, default ``False``\n            If ``True``, performs operation inplace and returns None.\n\n        Returns\n        -------\n        Series or None\n            Series with duplicates dropped or None if ``inplace=True``.\n\n        See Also\n        --------\n        Index.drop_duplicates : Equivalent method on Index.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Series.duplicated : Related method on Series, indicating duplicate\n            Series values.\n\n        Examples\n        --------\n        Generate a Series with duplicated entries.\n\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'],\n        ...               name='animal')\n        >>> s\n        0      lama\n        1       cow\n        2      lama\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        With the 'keep' parameter, the selection behaviour of duplicated values\n        can be changed. The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> s.drop_duplicates()\n        0      lama\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n\n        The value 'last' for parameter 'keep' keeps the last occurrence for\n        each set of duplicated entries.\n\n        >>> s.drop_duplicates(keep='last')\n        1       cow\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        The value ``False`` for parameter 'keep' discards all sets of\n        duplicated entries. Setting the value of 'inplace' to ``True`` performs\n        the operation inplace and returns ``None``.\n\n        >>> s.drop_duplicates(keep=False, inplace=True)\n        >>> s\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        result = super().drop_duplicates(keep=keep)\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    def duplicated(self, keep=\"first\") -> Series:\n        \"\"\"\n        Indicate duplicate Series values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        Series. Either all duplicates, all except the first or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series[bool]\n            Series indicating whether each value has occurred in the\n            preceding values.\n\n        See Also\n        --------\n        Index.duplicated : Equivalent method on pandas.Index.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Series.drop_duplicates : Remove duplicate values from Series.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set on False and all others on True:\n\n        >>> animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> animals.duplicated()\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        which is equivalent to\n\n        >>> animals.duplicated(keep='first')\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> animals.duplicated(keep='last')\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> animals.duplicated(keep=False)\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n        \"\"\"\n        res = self._duplicated(keep=keep)\n        result = self._constructor(res, index=self.index)\n        return result.__finalize__(self, method=\"duplicated\")\n\n    def idxmin(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the minimum value.\n\n        If multiple values equal the minimum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmin. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the minimum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmin : Return indices of the minimum values\n            along the given axis.\n        DataFrame.idxmin : Return index of first occurrence of minimum\n            over requested axis.\n        Series.idxmax : Return index *label* of the first occurrence\n            of maximum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmin``. This method\n        returns the label of the minimum, while ``ndarray.argmin`` returns\n        the position. To get the position, use ``series.values.argmin()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 1],\n        ...               index=['A', 'B', 'C', 'D'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    1.0\n        dtype: float64\n\n        >>> s.idxmin()\n        'A'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmin(skipna=False)\n        nan\n        \"\"\"\n        i = self.argmin(axis, skipna, *args, **kwargs)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def idxmax(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the maximum value.\n\n        If multiple values equal the maximum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmax. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the maximum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmax : Return indices of the maximum values\n            along the given axis.\n        DataFrame.idxmax : Return index of first occurrence of maximum\n            over requested axis.\n        Series.idxmin : Return index *label* of the first occurrence\n            of minimum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmax``. This method\n        returns the label of the maximum, while ``ndarray.argmax`` returns\n        the position. To get the position, use ``series.values.argmax()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 3, 4],\n        ...               index=['A', 'B', 'C', 'D', 'E'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    3.0\n        E    4.0\n        dtype: float64\n\n        >>> s.idxmax()\n        'C'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmax(skipna=False)\n        nan\n        \"\"\"\n        i = self.argmax(axis, skipna, *args, **kwargs)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def round(self, decimals=0, *args, **kwargs) -> Series:\n        \"\"\"\n        Round each value in a Series to the given number of decimals.\n\n        Parameters\n        ----------\n        decimals : int, default 0\n            Number of decimal places to round to. If decimals is negative,\n            it specifies the number of positions to the left of the decimal point.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Series\n            Rounded values of the Series.\n\n        See Also\n        --------\n        numpy.around : Round values of an np.array.\n        DataFrame.round : Round values of a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.1, 1.3, 2.7])\n        >>> s.round()\n        0    0.0\n        1    1.0\n        2    3.0\n        dtype: float64\n        \"\"\"\n        nv.validate_round(args, kwargs)\n        result = self._values.round(decimals)\n        result = self._constructor(result, index=self.index).__finalize__(\n            self, method=\"round\"\n        )\n\n        return result\n\n    def quantile(self, q=0.5, interpolation=\"linear\"):\n        \"\"\"\n        Return value at the given quantile.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            The quantile(s) to compute, which can lie in range: 0 <= q <= 1.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            This optional parameter specifies the interpolation method to use,\n            when the desired quantile lies between two data points `i` and `j`:\n\n                * linear: `i + (j - i) * fraction`, where `fraction` is the\n                  fractional part of the index surrounded by `i` and `j`.\n                * lower: `i`.\n                * higher: `j`.\n                * nearest: `i` or `j` whichever is nearest.\n                * midpoint: (`i` + `j`) / 2.\n\n        Returns\n        -------\n        float or Series\n            If ``q`` is an array, a Series will be returned where the\n            index is ``q`` and the values are the quantiles, otherwise\n            a float will be returned.\n\n        See Also\n        --------\n        core.window.Rolling.quantile : Calculate the rolling quantile.\n        numpy.percentile : Returns the q-th percentile(s) of the array elements.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.quantile(.5)\n        2.5\n        >>> s.quantile([.25, .5, .75])\n        0.25    1.75\n        0.50    2.50\n        0.75    3.25\n        dtype: float64\n        \"\"\"\n        validate_percentile(q)\n\n        # We dispatch to DataFrame so that core.internals only has to worry\n        #  about 2D cases.\n        df = self.to_frame()\n\n        result = df.quantile(q=q, interpolation=interpolation, numeric_only=False)\n        if result.ndim == 2:\n            result = result.iloc[:, 0]\n\n        if is_list_like(q):\n            result.name = self.name\n            return self._constructor(result, index=Float64Index(q), name=self.name)\n        else:\n            # scalar\n            return result.iloc[0]\n\n    def corr(self, other, method=\"pearson\", min_periods=None) -> float:\n        \"\"\"\n        Compute correlation with `other` Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the correlation.\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method used to compute correlation:\n\n            - pearson : Standard correlation coefficient\n            - kendall : Kendall Tau correlation coefficient\n            - spearman : Spearman rank correlation\n            - callable: Callable with input two 1d ndarrays and returning a float.\n\n            .. versionadded:: 0.24.0\n                Note that the returned matrix from corr will have 1 along the\n                diagonals and will be symmetric regardless of the callable's\n                behavior.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n\n        Returns\n        -------\n        float\n            Correlation with other.\n\n        See Also\n        --------\n        DataFrame.corr : Compute pairwise correlation between columns.\n        DataFrame.corrwith : Compute pairwise correlation with another\n            DataFrame or Series.\n\n        Examples\n        --------\n        >>> def histogram_intersection(a, b):\n        ...     v = np.minimum(a, b).sum().round(decimals=1)\n        ...     return v\n        >>> s1 = pd.Series([.2, .0, .6, .2])\n        >>> s2 = pd.Series([.3, .6, .0, .1])\n        >>> s1.corr(s2, method=histogram_intersection)\n        0.3\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n\n        if method in [\"pearson\", \"spearman\", \"kendall\"] or callable(method):\n            return nanops.nancorr(\n                this.values, other.values, method=method, min_periods=min_periods\n            )\n\n        raise ValueError(\n            \"method must be either 'pearson', \"\n            \"'spearman', 'kendall', or a callable, \"\n            f\"'{method}' was supplied\"\n        )\n\n    def cov(\n        self,\n        other: Series,\n        min_periods: int | None = None,\n        ddof: int | None = 1,\n    ) -> float:\n        \"\"\"\n        Compute covariance with Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the covariance.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n        ddof : int, default 1\n            Delta degrees of freedom.  The divisor used in calculations\n            is ``N - ddof``, where ``N`` represents the number of elements.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        float\n            Covariance between Series and other normalized by N-1\n            (unbiased estimator).\n\n        See Also\n        --------\n        DataFrame.cov : Compute pairwise covariance of columns.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([0.90010907, 0.13484424, 0.62036035])\n        >>> s2 = pd.Series([0.12528585, 0.26962463, 0.51111198])\n        >>> s1.cov(s2)\n        -0.01685762652715874\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancov(\n            this.values, other.values, min_periods=min_periods, ddof=ddof\n        )\n\n    @doc(\n        klass=\"Series\",\n        extra_params=\"\",\n        other_klass=\"DataFrame\",\n        examples=dedent(\n            \"\"\"\n        Difference with previous row\n\n        >>> s = pd.Series([1, 1, 2, 3, 5, 8])\n        >>> s.diff()\n        0    NaN\n        1    0.0\n        2    1.0\n        3    1.0\n        4    2.0\n        5    3.0\n        dtype: float64\n\n        Difference with 3rd previous row\n\n        >>> s.diff(periods=3)\n        0    NaN\n        1    NaN\n        2    NaN\n        3    2.0\n        4    4.0\n        5    6.0\n        dtype: float64\n\n        Difference with following row\n\n        >>> s.diff(periods=-1)\n        0    0.0\n        1   -1.0\n        2   -1.0\n        3   -2.0\n        4   -3.0\n        5    NaN\n        dtype: float64\n\n        Overflow in input dtype\n\n        >>> s = pd.Series([1, 0], dtype=np.uint8)\n        >>> s.diff()\n        0      NaN\n        1    255.0\n        dtype: float64\"\"\"\n        ),\n    )\n    def diff(self, periods: int = 1) -> Series:\n        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of a {klass} element compared with another\n        element in the {klass} (default is element in previous row).\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative\n            values.\n        {extra_params}\n        Returns\n        -------\n        {klass}\n            First differences of the Series.\n\n        See Also\n        --------\n        {klass}.pct_change: Percent change over given number of periods.\n        {klass}.shift: Shift index by desired number of periods with an\n            optional time freq.\n        {other_klass}.diff: First discrete difference of object.\n\n        Notes\n        -----\n        For boolean dtypes, this uses :meth:`operator.xor` rather than\n        :meth:`operator.sub`.\n        The result is calculated according to current dtype in {klass},\n        however dtype of the result is always float64.\n\n        Examples\n        --------\n        {examples}\n        \"\"\"\n        result = algorithms.diff(self._values, periods)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"diff\"\n        )\n\n    def autocorr(self, lag=1) -> float:\n        \"\"\"\n        Compute the lag-N autocorrelation.\n\n        This method computes the Pearson correlation between\n        the Series and its shifted self.\n\n        Parameters\n        ----------\n        lag : int, default 1\n            Number of lags to apply before performing autocorrelation.\n\n        Returns\n        -------\n        float\n            The Pearson correlation between self and self.shift(lag).\n\n        See Also\n        --------\n        Series.corr : Compute the correlation between two Series.\n        Series.shift : Shift index by desired number of periods.\n        DataFrame.corr : Compute pairwise correlation of columns.\n        DataFrame.corrwith : Compute pairwise correlation between rows or\n            columns of two DataFrame objects.\n\n        Notes\n        -----\n        If the Pearson correlation is not well defined return 'NaN'.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.25, 0.5, 0.2, -0.05])\n        >>> s.autocorr()  # doctest: +ELLIPSIS\n        0.10355...\n        >>> s.autocorr(lag=2)  # doctest: +ELLIPSIS\n        -0.99999...\n\n        If the Pearson correlation is not well defined, then 'NaN' is returned.\n\n        >>> s = pd.Series([1, 0, 0, 0])\n        >>> s.autocorr()\n        nan\n        \"\"\"\n        return self.corr(self.shift(lag))\n\n    def dot(self, other):\n        \"\"\"\n        Compute the dot product between the Series and the columns of other.\n\n        This method computes the dot product between the Series and another\n        one, or the Series and each columns of a DataFrame, or the Series and\n        each columns of an array.\n\n        It can also be called using `self @ other` in Python >= 3.5.\n\n        Parameters\n        ----------\n        other : Series, DataFrame or array-like\n            The other object to compute the dot product with its columns.\n\n        Returns\n        -------\n        scalar, Series or numpy.ndarray\n            Return the dot product of the Series and other if other is a\n            Series, the Series of the dot product of Series and each rows of\n            other if other is a DataFrame or a numpy.ndarray between the Series\n            and each columns of the numpy array.\n\n        See Also\n        --------\n        DataFrame.dot: Compute the matrix product with the DataFrame.\n        Series.mul: Multiplication of series and other, element-wise.\n\n        Notes\n        -----\n        The Series and other has to share the same index if other is a Series\n        or a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0, 1, 2, 3])\n        >>> other = pd.Series([-1, 2, -3, 4])\n        >>> s.dot(other)\n        8\n        >>> s @ other\n        8\n        >>> df = pd.DataFrame([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(df)\n        0    24\n        1    14\n        dtype: int64\n        >>> arr = np.array([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(arr)\n        array([24, 14])\n        \"\"\"\n        if isinstance(other, (Series, ABCDataFrame)):\n            common = self.index.union(other.index)\n            if len(common) > len(self.index) or len(common) > len(other.index):\n                raise ValueError(\"matrices are not aligned\")\n\n            left = self.reindex(index=common, copy=False)\n            right = other.reindex(index=common, copy=False)\n            lvals = left.values\n            rvals = right.values\n        else:\n            lvals = self.values\n            rvals = np.asarray(other)\n            if lvals.shape[0] != rvals.shape[0]:\n                raise Exception(\n                    f\"Dot product shape mismatch, {lvals.shape} vs {rvals.shape}\"\n                )\n\n        if isinstance(other, ABCDataFrame):\n            return self._constructor(\n                np.dot(lvals, rvals), index=other.columns\n            ).__finalize__(self, method=\"dot\")\n        elif isinstance(other, Series):\n            return np.dot(lvals, rvals)\n        elif isinstance(rvals, np.ndarray):\n            return np.dot(lvals, rvals)\n        else:  # pragma: no cover\n            raise TypeError(f\"unsupported type: {type(other)}\")\n\n    def __matmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(other)\n\n    def __rmatmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(np.transpose(other))\n\n    @doc(base.IndexOpsMixin.searchsorted, klass=\"Series\")\n    def searchsorted(self, value, side=\"left\", sorter=None) -> np.ndarray:\n        return algorithms.searchsorted(self._values, value, side=side, sorter=sorter)\n\n    # -------------------------------------------------------------------\n    # Combination\n\n    def append(\n        self, to_append, ignore_index: bool = False, verify_integrity: bool = False\n    ):\n        \"\"\"\n        Concatenate two or more Series.\n\n        Parameters\n        ----------\n        to_append : Series or list/tuple of Series\n            Series to append with self.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n        verify_integrity : bool, default False\n            If True, raise Exception on creating index with duplicates.\n\n        Returns\n        -------\n        Series\n            Concatenated Series.\n\n        See Also\n        --------\n        concat : General function to concatenate DataFrame or Series objects.\n\n        Notes\n        -----\n        Iteratively appending to a Series can be more computationally intensive\n        than a single concatenate. A better solution is to append values to a\n        list and then concatenate the list with the original Series all at\n        once.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, 2, 3])\n        >>> s2 = pd.Series([4, 5, 6])\n        >>> s3 = pd.Series([4, 5, 6], index=[3, 4, 5])\n        >>> s1.append(s2)\n        0    1\n        1    2\n        2    3\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s1.append(s3)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `ignore_index` set to True:\n\n        >>> s1.append(s2, ignore_index=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `verify_integrity` set to True:\n\n        >>> s1.append(s2, verify_integrity=True)\n        Traceback (most recent call last):\n        ...\n        ValueError: Indexes have overlapping values: [0, 1, 2]\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        if isinstance(to_append, (list, tuple)):\n            to_concat = [self]\n            to_concat.extend(to_append)\n        else:\n            to_concat = [self, to_append]\n        if any(isinstance(x, (ABCDataFrame,)) for x in to_concat[1:]):\n            msg = \"to_append should be a Series or list/tuple of Series, got DataFrame\"\n            raise TypeError(msg)\n        return concat(\n            to_concat, ignore_index=ignore_index, verify_integrity=verify_integrity\n        )\n\n    def _binop(self, other: Series, func, level=None, fill_value=None):\n        \"\"\"\n        Perform generic binary operation with optional fill value.\n\n        Parameters\n        ----------\n        other : Series\n        func : binary operator\n        fill_value : float or object\n            Value to substitute for NA/null values. If both Series are NA in a\n            location, the result will be NA regardless of the passed fill value.\n        level : int or level name, default None\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level.\n\n        Returns\n        -------\n        Series\n        \"\"\"\n        if not isinstance(other, Series):\n            raise AssertionError(\"Other operand must be Series\")\n\n        this = self\n\n        if not self.index.equals(other.index):\n            this, other = self.align(other, level=level, join=\"outer\", copy=False)\n\n        this_vals, other_vals = ops.fill_binop(this._values, other._values, fill_value)\n\n        with np.errstate(all=\"ignore\"):\n            result = func(this_vals, other_vals)\n\n        name = ops.get_op_result_name(self, other)\n        return this._construct_result(result, name)\n\n    def _construct_result(\n        self, result: ArrayLike | tuple[ArrayLike, ArrayLike], name: Hashable\n    ) -> Series | tuple[Series, Series]:\n        \"\"\"\n        Construct an appropriately-labelled Series from the result of an op.\n\n        Parameters\n        ----------\n        result : ndarray or ExtensionArray\n        name : Label\n\n        Returns\n        -------\n        Series\n            In the case of __divmod__ or __rdivmod__, a 2-tuple of Series.\n        \"\"\"\n        if isinstance(result, tuple):\n            # produced by divmod or rdivmod\n\n            res1 = self._construct_result(result[0], name=name)\n            res2 = self._construct_result(result[1], name=name)\n\n            # GH#33427 assertions to keep mypy happy\n            assert isinstance(res1, Series)\n            assert isinstance(res2, Series)\n            return (res1, res2)\n\n        # We do not pass dtype to ensure that the Series constructor\n        #  does inference in the case where `result` has object-dtype.\n        out = self._constructor(result, index=self.index)\n        out = out.__finalize__(self)\n\n        # Set the result's name after __finalize__ is called because __finalize__\n        #  would set it back to self.name\n        out.name = name\n        return out\n\n    @doc(\n        generic._shared_docs[\"compare\"],\n        \"\"\"\nReturns\n-------\nSeries or DataFrame\n    If axis is 0 or 'index' the result will be a Series.\n    The resulting index will be a MultiIndex with 'self' and 'other'\n    stacked alternately at the inner level.\n\n    If axis is 1 or 'columns' the result will be a DataFrame.\n    It will have two columns namely 'self' and 'other'.\n\nSee Also\n--------\nDataFrame.compare : Compare with another DataFrame and show differences.\n\nNotes\n-----\nMatching NaNs will not appear as a difference.\n\nExamples\n--------\n>>> s1 = pd.Series([\"a\", \"b\", \"c\", \"d\", \"e\"])\n>>> s2 = pd.Series([\"a\", \"a\", \"c\", \"b\", \"e\"])\n\nAlign the differences on columns\n\n>>> s1.compare(s2)\n  self other\n1    b     a\n3    d     b\n\nStack the differences on indices\n\n>>> s1.compare(s2, align_axis=0)\n1  self     b\n   other    a\n3  self     d\n   other    b\ndtype: object\n\nKeep all original rows\n\n>>> s1.compare(s2, keep_shape=True)\n  self other\n0  NaN   NaN\n1    b     a\n2  NaN   NaN\n3    d     b\n4  NaN   NaN\n\nKeep all original rows and also all original values\n\n>>> s1.compare(s2, keep_shape=True, keep_equal=True)\n  self other\n0    a     a\n1    b     a\n2    c     c\n3    d     b\n4    e     e\n\"\"\",\n        klass=_shared_doc_kwargs[\"klass\"],\n    )\n    def compare(\n        self,\n        other: Series,\n        align_axis: Axis = 1,\n        keep_shape: bool = False,\n        keep_equal: bool = False,\n    ) -> FrameOrSeriesUnion:\n        return super().compare(\n            other=other,\n            align_axis=align_axis,\n            keep_shape=keep_shape,\n            keep_equal=keep_equal,\n        )\n\n    def combine(self, other, func, fill_value=None) -> Series:\n        \"\"\"\n        Combine the Series with a Series or scalar according to `func`.\n\n        Combine the Series and `other` using `func` to perform elementwise\n        selection for combined Series.\n        `fill_value` is assumed when value is missing at some index\n        from one of the two objects being combined.\n\n        Parameters\n        ----------\n        other : Series or scalar\n            The value(s) to be combined with the `Series`.\n        func : function\n            Function that takes two scalars as inputs and returns an element.\n        fill_value : scalar, optional\n            The value to assume when an index is missing from\n            one Series or the other. The default specifies to use the\n            appropriate NaN value for the underlying dtype of the Series.\n\n        Returns\n        -------\n        Series\n            The result of combining the Series with the other object.\n\n        See Also\n        --------\n        Series.combine_first : Combine Series values, choosing the calling\n            Series' values first.\n\n        Examples\n        --------\n        Consider 2 Datasets ``s1`` and ``s2`` containing\n        highest clocked speeds of different birds.\n\n        >>> s1 = pd.Series({'falcon': 330.0, 'eagle': 160.0})\n        >>> s1\n        falcon    330.0\n        eagle     160.0\n        dtype: float64\n        >>> s2 = pd.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})\n        >>> s2\n        falcon    345.0\n        eagle     200.0\n        duck       30.0\n        dtype: float64\n\n        Now, to combine the two datasets and view the highest speeds\n        of the birds across the two datasets\n\n        >>> s1.combine(s2, max)\n        duck        NaN\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n\n        In the previous example, the resulting value for duck is missing,\n        because the maximum of a NaN and a float is a NaN.\n        So, in the example, we set ``fill_value=0``,\n        so the maximum value returned will be the value from some dataset.\n\n        >>> s1.combine(s2, max, fill_value=0)\n        duck       30.0\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n        \"\"\"\n        if fill_value is None:\n            fill_value = na_value_for_dtype(self.dtype, compat=False)\n\n        if isinstance(other, Series):\n            # If other is a Series, result is based on union of Series,\n            # so do this element by element\n            new_index = self.index.union(other.index)\n            new_name = ops.get_op_result_name(self, other)\n            new_values = np.empty(len(new_index), dtype=object)\n            for i, idx in enumerate(new_index):\n                lv = self.get(idx, fill_value)\n                rv = other.get(idx, fill_value)\n                with np.errstate(all=\"ignore\"):\n                    new_values[i] = func(lv, rv)\n        else:\n            # Assume that other is a scalar, so apply the function for\n            # each element in the Series\n            new_index = self.index\n            new_values = np.empty(len(new_index), dtype=object)\n            with np.errstate(all=\"ignore\"):\n                new_values[:] = [func(lv, other) for lv in self._values]\n            new_name = self.name\n\n        # try_float=False is to match agg_series\n        npvalues = lib.maybe_convert_objects(new_values, try_float=False)\n        res_values = maybe_cast_pointwise_result(npvalues, self.dtype, same_dtype=False)\n        return self._constructor(res_values, index=new_index, name=new_name)\n\n    def combine_first(self, other) -> Series:\n        \"\"\"\n        Update null elements with value in the same location in 'other'.\n\n        Combine two Series objects by filling null values in one Series with\n        non-null values from the other Series. Result index will be the union\n        of the two indexes.\n\n        Parameters\n        ----------\n        other : Series\n            The value(s) to be used for filling null values.\n\n        Returns\n        -------\n        Series\n            The result of combining the provided Series with the other object.\n\n        See Also\n        --------\n        Series.combine : Perform element-wise operation on two Series\n            using a given function.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, np.nan])\n        >>> s2 = pd.Series([3, 4, 5])\n        >>> s1.combine_first(s2)\n        0    1.0\n        1    4.0\n        2    5.0\n        dtype: float64\n\n        Null values still persist if the location of that null value\n        does not exist in `other`\n\n        >>> s1 = pd.Series({'falcon': np.nan, 'eagle': 160.0})\n        >>> s2 = pd.Series({'eagle': 200.0, 'duck': 30.0})\n        >>> s1.combine_first(s2)\n        duck       30.0\n        eagle     160.0\n        falcon      NaN\n        dtype: float64\n        \"\"\"\n        new_index = self.index.union(other.index)\n        this = self.reindex(new_index, copy=False)\n        other = other.reindex(new_index, copy=False)\n        if this.dtype.kind == \"M\" and other.dtype.kind != \"M\":\n            other = to_datetime(other)\n\n        return this.where(notna(this), other)\n\n    def update(self, other) -> None:\n        \"\"\"\n        Modify Series in place using values from passed Series.\n\n        Uses non-NA values from passed Series to make updates. Aligns\n        on index.\n\n        Parameters\n        ----------\n        other : Series, or object coercible into Series\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s.update(pd.Series(['d', 'e'], index=[0, 2]))\n        >>> s\n        0    d\n        1    b\n        2    e\n        dtype: object\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6, 7, 8]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        If ``other`` contains NaNs the corresponding values are not updated\n        in the original Series.\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, np.nan, 6]))\n        >>> s\n        0    4\n        1    2\n        2    6\n        dtype: int64\n\n        ``other`` can also be a non-Series object type\n        that is coercible into a Series\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update([4, np.nan, 6])\n        >>> s\n        0    4\n        1    2\n        2    6\n        dtype: int64\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update({1: 9})\n        >>> s\n        0    1\n        1    9\n        2    3\n        dtype: int64\n        \"\"\"\n\n        if not isinstance(other, Series):\n            other = Series(other)\n\n        other = other.reindex_like(self)\n        mask = notna(other)\n\n        self._mgr = self._mgr.putmask(mask=mask, new=other)\n        self._maybe_update_cacher()\n\n    # ----------------------------------------------------------------------\n    # Reindexing, sorting\n\n    def sort_values(\n        self,\n        axis=0,\n        ascending: bool | int | Sequence[bool | int] = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        ignore_index: bool = False,\n        key: ValueKeyFunc = None,\n    ):\n        \"\"\"\n        Sort by the values.\n\n        Sort a Series in ascending or descending order by some\n        criterion.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            Axis to direct sorting. The value 'index' is accepted for\n            compatibility with DataFrame.sort_values.\n        ascending : bool or list of bools, default True\n            If True, sort values in ascending order, otherwise descending.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. 'mergesort' and 'stable' are the only stable  algorithms.\n        na_position : {'first' or 'last'}, default 'last'\n            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n            the end.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the series values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect a\n            ``Series`` and return an array-like.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series or None\n            Series ordered by values or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.sort_index : Sort by the Series indices.\n        DataFrame.sort_values : Sort DataFrame by the values along either axis.\n        DataFrame.sort_index : Sort DataFrame by indices.\n\n        Examples\n        --------\n        >>> s = pd.Series([np.nan, 1, 3, 10, 5])\n        >>> s\n        0     NaN\n        1     1.0\n        2     3.0\n        3     10.0\n        4     5.0\n        dtype: float64\n\n        Sort values ascending order (default behaviour)\n\n        >>> s.sort_values(ascending=True)\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        0     NaN\n        dtype: float64\n\n        Sort values descending order\n\n        >>> s.sort_values(ascending=False)\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values inplace\n\n        >>> s.sort_values(ascending=False, inplace=True)\n        >>> s\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values putting NAs first\n\n        >>> s.sort_values(na_position='first')\n        0     NaN\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        dtype: float64\n\n        Sort a series of strings\n\n        >>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n        >>> s\n        0    z\n        1    b\n        2    d\n        3    a\n        4    c\n        dtype: object\n\n        >>> s.sort_values()\n        3    a\n        1    b\n        4    c\n        2    d\n        0    z\n        dtype: object\n\n        Sort using a key function. Your `key` function will be\n        given the ``Series`` of values and should return an array-like.\n\n        >>> s = pd.Series(['a', 'B', 'c', 'D', 'e'])\n        >>> s.sort_values()\n        1    B\n        3    D\n        0    a\n        2    c\n        4    e\n        dtype: object\n        >>> s.sort_values(key=lambda x: x.str.lower())\n        0    a\n        1    B\n        2    c\n        3    D\n        4    e\n        dtype: object\n\n        NumPy ufuncs work well here. For example, we can\n        sort by the ``sin`` of the value\n\n        >>> s = pd.Series([-4, -2, 0, 2, 4])\n        >>> s.sort_values(key=np.sin)\n        1   -2\n        4    4\n        2    0\n        0   -4\n        3    2\n        dtype: int64\n\n        More complicated user-defined functions can be used,\n        as long as they expect a Series and return an array-like\n\n        >>> s.sort_values(key=lambda x: (np.tan(x.cumsum())))\n        0   -4\n        3    2\n        4    4\n        1   -2\n        2    0\n        dtype: int64\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n\n        # GH 5856/5853\n        if inplace and self._is_cached:\n            raise ValueError(\n                \"This Series is a view of some other array, to \"\n                \"sort in-place you must create a copy\"\n            )\n\n        if is_list_like(ascending):\n            ascending = cast(Sequence[Union[bool, int]], ascending)\n            if len(ascending) != 1:\n                raise ValueError(\n                    f\"Length of ascending ({len(ascending)}) must be 1 for Series\"\n                )\n            ascending = ascending[0]\n\n        if not is_bool(ascending):\n            raise ValueError(\"ascending must be boolean\")\n\n        if na_position not in [\"first\", \"last\"]:\n            raise ValueError(f\"invalid na_position: {na_position}\")\n\n        # GH 35922. Make sorting stable by leveraging nargsort\n        values_to_sort = ensure_key_mapped(self, key)._values if key else self._values\n        sorted_index = nargsort(values_to_sort, kind, bool(ascending), na_position)\n\n        result = self._constructor(\n            self._values[sorted_index], index=self.index[sorted_index]\n        )\n\n        if ignore_index:\n            result.index = ibase.default_index(len(sorted_index))\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_values\")\n\n    def sort_index(\n        self,\n        axis=0,\n        level=None,\n        ascending: bool | int | Sequence[bool | int] = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        sort_remaining: bool = True,\n        ignore_index: bool = False,\n        key: IndexKeyFunc = None,\n    ):\n        \"\"\"\n        Sort Series by index labels.\n\n        Returns a new Series sorted by label if `inplace` argument is\n        ``False``, otherwise updates the original series and returns None.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Axis to direct sorting. This can only be 0 for Series.\n        level : int, optional\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list-like of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level individually.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. 'mergesort' and 'stable' are the only stable algorithms. For\n            DataFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'last'}, default 'last'\n            If 'first' puts NaNs at the beginning, 'last' puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series or None\n            The original Series sorted by the labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.sort_index: Sort DataFrame by the index.\n        DataFrame.sort_values: Sort DataFrame by the value.\n        Series.sort_values : Sort Series by the value.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])\n        >>> s.sort_index()\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        Sort Descending\n\n        >>> s.sort_index(ascending=False)\n        4    d\n        3    a\n        2    b\n        1    c\n        dtype: object\n\n        Sort Inplace\n\n        >>> s.sort_index(inplace=True)\n        >>> s\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        By default NaNs are put at the end, but use `na_position` to place\n        them at the beginning\n\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])\n        >>> s.sort_index(na_position='first')\n        NaN     d\n         1.0    c\n         2.0    b\n         3.0    a\n        dtype: object\n\n        Specify index level to sort\n\n        >>> arrays = [np.array(['qux', 'qux', 'foo', 'foo',\n        ...                     'baz', 'baz', 'bar', 'bar']),\n        ...           np.array(['two', 'one', 'two', 'one',\n        ...                     'two', 'one', 'two', 'one'])]\n        >>> s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)\n        >>> s.sort_index(level=1)\n        bar  one    8\n        baz  one    6\n        foo  one    4\n        qux  one    2\n        bar  two    7\n        baz  two    5\n        foo  two    3\n        qux  two    1\n        dtype: int64\n\n        Does not sort by remaining levels when sorting by levels\n\n        >>> s.sort_index(level=1, sort_remaining=False)\n        qux  one    2\n        foo  one    4\n        baz  one    6\n        bar  one    8\n        qux  two    1\n        foo  two    3\n        baz  two    5\n        bar  two    7\n        dtype: int64\n\n        Apply a key function before sorting\n\n        >>> s = pd.Series([1, 2, 3, 4], index=['A', 'b', 'C', 'd'])\n        >>> s.sort_index(key=lambda x : x.str.lower())\n        A    1\n        b    2\n        C    3\n        d    4\n        dtype: int64\n        \"\"\"\n\n        return super().sort_index(\n            axis,\n            level,\n            ascending,\n            inplace,\n            kind,\n            na_position,\n            sort_remaining,\n            ignore_index,\n            key,\n        )\n\n    def argsort(self, axis=0, kind=\"quicksort\", order=None) -> Series:\n        \"\"\"\n        Return the integer indices that would sort the Series values.\n\n        Override ndarray.argsort. Argsorts the value, omitting NA/null values,\n        and places the result in the same locations as the non-NA values.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Has no effect but is accepted for compatibility with numpy.\n        kind : {'mergesort', 'quicksort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See :func:`numpy.sort` for more\n            information. 'mergesort' and 'stable' are the only stable algorithms.\n        order : None\n            Has no effect but is accepted for compatibility with numpy.\n\n        Returns\n        -------\n        Series[np.intp]\n            Positions of values within the sort order with -1 indicating\n            nan values.\n\n        See Also\n        --------\n        numpy.ndarray.argsort : Returns the indices that would sort this array.\n        \"\"\"\n        values = self._values\n        mask = isna(values)\n\n        if mask.any():\n            result = Series(-1, index=self.index, name=self.name, dtype=\"int64\")\n            notmask = ~mask\n            result[notmask] = np.argsort(values[notmask], kind=kind)\n            return self._constructor(result, index=self.index).__finalize__(\n                self, method=\"argsort\"\n            )\n        else:\n            return self._constructor(\n                np.argsort(values, kind=kind), index=self.index, dtype=\"int64\"\n            ).__finalize__(self, method=\"argsort\")\n\n    def nlargest(self, n=5, keep=\"first\") -> Series:\n        \"\"\"\n        Return the largest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many descending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n                of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n                order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n                size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` largest values in the Series, sorted in decreasing order.\n\n        See Also\n        --------\n        Series.nsmallest: Get the `n` smallest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values(ascending=False).head(n)`` for small `n`\n        relative to the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Malta\": 434000, \"Maldives\": 434000,\n        ...                         \"Brunei\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Montserrat      5200\n        dtype: int64\n\n        The `n` largest elements where ``n=5`` by default.\n\n        >>> s.nlargest()\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3``. Default `keep` value is 'first'\n        so Malta will be kept.\n\n        >>> s.nlargest(3)\n        France    65000000\n        Italy     59000000\n        Malta       434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` and keeping the last duplicates.\n        Brunei will be kept since it is the last with value 434000 based on\n        the index order.\n\n        >>> s.nlargest(3, keep='last')\n        France      65000000\n        Italy       59000000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has five elements due to the three duplicates.\n\n        >>> s.nlargest(3, keep='all')\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nlargest()\n\n    def nsmallest(self, n: int = 5, keep: str = \"first\") -> Series:\n        \"\"\"\n        Return the smallest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many ascending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n                of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n                order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n                size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` smallest values in the Series, sorted in increasing order.\n\n        See Also\n        --------\n        Series.nlargest: Get the `n` largest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values().head(n)`` for small `n` relative to\n        the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Brunei\": 434000, \"Malta\": 434000,\n        ...                         \"Maldives\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Brunei        434000\n        Malta         434000\n        Maldives      434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Montserrat      5200\n        dtype: int64\n\n        The `n` smallest elements where ``n=5`` by default.\n\n        >>> s.nsmallest()\n        Montserrat    5200\n        Nauru        11300\n        Tuvalu       11300\n        Anguilla     11300\n        Iceland     337000\n        dtype: int64\n\n        The `n` smallest elements where ``n=3``. Default `keep` value is\n        'first' so Nauru and Tuvalu will be kept.\n\n        >>> s.nsmallest(3)\n        Montserrat   5200\n        Nauru       11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` and keeping the last\n        duplicates. Anguilla and Tuvalu will be kept since they are the last\n        with value 11300 based on the index order.\n\n        >>> s.nsmallest(3, keep='last')\n        Montserrat   5200\n        Anguilla    11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has four elements due to the three duplicates.\n\n        >>> s.nsmallest(3, keep='all')\n        Montserrat   5200\n        Nauru       11300\n        Tuvalu      11300\n        Anguilla    11300\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nsmallest()\n\n    def swaplevel(self, i=-2, j=-1, copy=True) -> Series:\n        \"\"\"\n        Swap levels i and j in a :class:`MultiIndex`.\n\n        Default is to swap the two innermost levels of the index.\n\n        Parameters\n        ----------\n        i, j : int, str\n            Level of the indices to be swapped. Can pass level name as string.\n        copy : bool, default True\n            Whether to copy underlying data.\n\n        Returns\n        -------\n        Series\n            Series with levels swapped in MultiIndex.\n        \"\"\"\n        assert isinstance(self.index, MultiIndex)\n        new_index = self.index.swaplevel(i, j)\n        return self._constructor(self._values, index=new_index, copy=copy).__finalize__(\n            self, method=\"swaplevel\"\n        )\n\n    def reorder_levels(self, order) -> Series:\n        \"\"\"\n        Rearrange index levels using input order.\n\n        May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int representing new level order\n            Reference level by number or key.\n\n        Returns\n        -------\n        type of caller (new object)\n        \"\"\"\n        if not isinstance(self.index, MultiIndex):  # pragma: no cover\n            raise Exception(\"Can only reorder levels on a hierarchical axis.\")\n\n        result = self.copy()\n        assert isinstance(result.index, MultiIndex)\n        result.index = result.index.reorder_levels(order)\n        return result\n\n    def explode(self, ignore_index: bool = False) -> Series:\n        \"\"\"\n        Transform each element of a list-like to a row.\n\n        .. versionadded:: 0.25.0\n\n        Parameters\n        ----------\n        ignore_index : bool, default False\n            If True, the resulting index will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series\n            Exploded lists to rows; index will be duplicated for these rows.\n\n        See Also\n        --------\n        Series.str.split : Split string values on specified separator.\n        Series.unstack : Unstack, a.k.a. pivot, Series with MultiIndex\n            to produce DataFrame.\n        DataFrame.melt : Unpivot a DataFrame from wide format to long format.\n        DataFrame.explode : Explode a DataFrame from list-like\n            columns to long format.\n\n        Notes\n        -----\n        This routine will explode list-likes including lists, tuples, sets,\n        Series, and np.ndarray. The result dtype of the subset rows will\n        be object. Scalars will be returned unchanged, and empty list-likes will\n        result in a np.nan for that row. In addition, the ordering of elements in\n        the output will be non-deterministic when exploding sets.\n\n        Examples\n        --------\n        >>> s = pd.Series([[1, 2, 3], 'foo', [], [3, 4]])\n        >>> s\n        0    [1, 2, 3]\n        1          foo\n        2           []\n        3       [3, 4]\n        dtype: object\n\n        >>> s.explode()\n        0      1\n        0      2\n        0      3\n        1    foo\n        2    NaN\n        3      3\n        3      4\n        dtype: object\n        \"\"\"\n        if not len(self) or not is_object_dtype(self):\n            result = self.copy()\n            return result.reset_index(drop=True) if ignore_index else result\n\n        values, counts = reshape.explode(np.asarray(self._values))\n\n        if ignore_index:\n            index = ibase.default_index(len(values))\n        else:\n            index = self.index.repeat(counts)\n\n        return self._constructor(values, index=index, name=self.name)\n\n    def unstack(self, level=-1, fill_value=None) -> DataFrame:\n        \"\"\"\n        Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.\n\n        Parameters\n        ----------\n        level : int, str, or list of these, default last level\n            Level(s) to unstack, can pass level name.\n        fill_value : scalar value, default None\n            Value to use when replacing NaN values.\n\n        Returns\n        -------\n        DataFrame\n            Unstacked Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4],\n        ...               index=pd.MultiIndex.from_product([['one', 'two'],\n        ...                                                 ['a', 'b']]))\n        >>> s\n        one  a    1\n             b    2\n        two  a    3\n             b    4\n        dtype: int64\n\n        >>> s.unstack(level=-1)\n             a  b\n        one  1  2\n        two  3  4\n\n        >>> s.unstack(level=0)\n           one  two\n        a    1    3\n        b    2    4\n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n\n        return unstack(self, level, fill_value)\n\n    # ----------------------------------------------------------------------\n    # function application\n\n    def map(self, arg, na_action=None) -> Series:\n        \"\"\"\n        Map values of Series according to input correspondence.\n\n        Used for substituting each value in a Series with another value,\n        that may be derived from a function, a ``dict`` or\n        a :class:`Series`.\n\n        Parameters\n        ----------\n        arg : function, collections.abc.Mapping subclass or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NaN values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        Series\n            Same index as caller.\n\n        See Also\n        --------\n        Series.apply : For applying more complex functions on a Series.\n        DataFrame.apply : Apply a function row-/column-wise.\n        DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n\n        Notes\n        -----\n        When ``arg`` is a dictionary, values in Series that are not in the\n        dictionary (as keys) are converted to ``NaN``. However, if the\n        dictionary is a ``dict`` subclass that defines ``__missing__`` (i.e.\n        provides a method for default values), then this default is used\n        rather than ``NaN``.\n\n        Examples\n        --------\n        >>> s = pd.Series(['cat', 'dog', np.nan, 'rabbit'])\n        >>> s\n        0      cat\n        1      dog\n        2      NaN\n        3   rabbit\n        dtype: object\n\n        ``map`` accepts a ``dict`` or a ``Series``. Values that are not found\n        in the ``dict`` are converted to ``NaN``, unless the dict has a default\n        value (e.g. ``defaultdict``):\n\n        >>> s.map({'cat': 'kitten', 'dog': 'puppy'})\n        0   kitten\n        1    puppy\n        2      NaN\n        3      NaN\n        dtype: object\n\n        It also accepts a function:\n\n        >>> s.map('I am a {}'.format)\n        0       I am a cat\n        1       I am a dog\n        2       I am a nan\n        3    I am a rabbit\n        dtype: object\n\n        To avoid applying the function to missing values (and keep them as\n        ``NaN``) ``na_action='ignore'`` can be used:\n\n        >>> s.map('I am a {}'.format, na_action='ignore')\n        0     I am a cat\n        1     I am a dog\n        2            NaN\n        3  I am a rabbit\n        dtype: object\n        \"\"\"\n        new_values = super()._map_values(arg, na_action=na_action)\n        return self._constructor(new_values, index=self.index).__finalize__(\n            self, method=\"map\"\n        )\n\n    def _gotitem(self, key, ndim, subset=None) -> Series:\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : {1, 2}\n            Requested ndim of result.\n        subset : object, default None\n            Subset to act on.\n        \"\"\"\n        return self\n\n    _agg_see_also_doc = dedent(\n        \"\"\"\n    See Also\n    --------\n    Series.apply : Invoke function on a Series.\n    Series.transform : Transform function producing a Series with like indexes.\n    \"\"\"\n    )\n\n    _agg_examples_doc = dedent(\n        \"\"\"\n    Examples\n    --------\n    >>> s = pd.Series([1, 2, 3, 4])\n    >>> s\n    0    1\n    1    2\n    2    3\n    3    4\n    dtype: int64\n\n    >>> s.agg('min')\n    1\n\n    >>> s.agg(['min', 'max'])\n    min   1\n    max   4\n    dtype: int64\n    \"\"\"\n    )\n\n    @doc(\n        generic._shared_docs[\"aggregate\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        axis=_shared_doc_kwargs[\"axis\"],\n        see_also=_agg_see_also_doc,\n        examples=_agg_examples_doc,\n    )\n    def aggregate(self, func=None, axis=0, *args, **kwargs):\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n\n        # if func is None, will switch to user-provided \"named aggregation\" kwargs\n        if func is None:\n            func = dict(kwargs.items())\n\n        op = SeriesApply(self, func, convert_dtype=False, args=args, kwargs=kwargs)\n        result = op.agg()\n        return result\n\n    agg = aggregate\n\n    @doc(\n        _shared_docs[\"transform\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        axis=_shared_doc_kwargs[\"axis\"],\n    )\n    def transform(\n        self, func: AggFuncType, axis: Axis = 0, *args, **kwargs\n    ) -> FrameOrSeriesUnion:\n        # Validate axis argument\n        self._get_axis_number(axis)\n        result = SeriesApply(\n            self, func=func, convert_dtype=True, args=args, kwargs=kwargs\n        ).transform()\n        return result\n\n    def apply(\n        self,\n        func: AggFuncType,\n        convert_dtype: bool = True,\n        args: tuple[Any, ...] = (),\n        **kwargs,\n    ) -> FrameOrSeriesUnion:\n        \"\"\"\n        Invoke function on values of Series.\n\n        Can be ufunc (a NumPy function that applies to the entire Series)\n        or a Python function that only works on single values.\n\n        Parameters\n        ----------\n        func : function\n            Python function or NumPy ufunc to apply.\n        convert_dtype : bool, default True\n            Try to find better dtype for elementwise function results. If\n            False, leave as dtype=object. Note that the dtype is always\n            preserved for extension array dtypes, such as Categorical.\n        args : tuple\n            Positional arguments passed to func after the series value.\n        **kwargs\n            Additional keyword arguments passed to func.\n\n        Returns\n        -------\n        Series or DataFrame\n            If func returns a Series object the result will be a DataFrame.\n\n        See Also\n        --------\n        Series.map: For element-wise operations.\n        Series.agg: Only perform aggregating type operations.\n        Series.transform: Only perform transforming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n        for more details.\n\n        Examples\n        --------\n        Create a series with typical summer temperatures for each city.\n\n        >>> s = pd.Series([20, 21, 12],\n        ...               index=['London', 'New York', 'Helsinki'])\n        >>> s\n        London      20\n        New York    21\n        Helsinki    12\n        dtype: int64\n\n        Square the values by defining a function and passing it as an\n        argument to ``apply()``.\n\n        >>> def square(x):\n        ...     return x ** 2\n        >>> s.apply(square)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Square the values by passing an anonymous function as an\n        argument to ``apply()``.\n\n        >>> s.apply(lambda x: x ** 2)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Define a custom function that needs additional positional\n        arguments and pass these additional arguments using the\n        ``args`` keyword.\n\n        >>> def subtract_custom_value(x, custom_value):\n        ...     return x - custom_value\n\n        >>> s.apply(subtract_custom_value, args=(5,))\n        London      15\n        New York    16\n        Helsinki     7\n        dtype: int64\n\n        Define a custom function that takes keyword arguments\n        and pass these arguments to ``apply``.\n\n        >>> def add_custom_values(x, **kwargs):\n        ...     for month in kwargs:\n        ...         x += kwargs[month]\n        ...     return x\n\n        >>> s.apply(add_custom_values, june=30, july=20, august=25)\n        London      95\n        New York    96\n        Helsinki    87\n        dtype: int64\n\n        Use a function from the Numpy library.\n\n        >>> s.apply(np.log)\n        London      2.995732\n        New York    3.044522\n        Helsinki    2.484907\n        dtype: float64\n        \"\"\"\n        return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n\n    def _reduce(\n        self,\n        op,\n        name: str,\n        *,\n        axis=0,\n        skipna=True,\n        numeric_only=None,\n        filter_type=None,\n        **kwds,\n    ):\n        \"\"\"\n        Perform a reduction operation.\n\n        If we have an ndarray as a value, then simply perform the operation,\n        otherwise delegate to the object.\n        \"\"\"\n        delegate = self._values\n\n        if axis is not None:\n            self._get_axis_number(axis)\n\n        if isinstance(delegate, ExtensionArray):\n            # dispatch to ExtensionArray interface\n            return delegate._reduce(name, skipna=skipna, **kwds)\n\n        else:\n            # dispatch to numpy arrays\n            if numeric_only:\n                raise NotImplementedError(\n                    f\"Series.{name} does not implement numeric_only.\"\n                )\n            with np.errstate(all=\"ignore\"):\n                return op(delegate, skipna=skipna, **kwds)\n\n    def _reindex_indexer(\n        self, new_index: Index | None, indexer: np.ndarray | None, copy: bool\n    ) -> Series:\n        # Note: new_index is None iff indexer is None\n        # if not None, indexer is np.intp\n        if indexer is None:\n            if copy:\n                return self.copy()\n            return self\n\n        new_values = algorithms.take_nd(\n            self._values, indexer, allow_fill=True, fill_value=None\n        )\n        return self._constructor(new_values, index=new_index)\n\n    def _needs_reindex_multi(self, axes, method, level) -> bool:\n        \"\"\"\n        Check if we do need a multi reindex; this is for compat with\n        higher dims.\n        \"\"\"\n        return False\n\n    # error: Cannot determine type of 'align'\n    @doc(\n        NDFrame.align,  # type: ignore[has-type]\n        klass=_shared_doc_kwargs[\"klass\"],\n        axes_single_arg=_shared_doc_kwargs[\"axes_single_arg\"],\n    )\n    def align(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy=True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n        broadcast_axis=None,\n    ):\n        return super().align(\n            other,\n            join=join,\n            axis=axis,\n            level=level,\n            copy=copy,\n            fill_value=fill_value,\n            method=method,\n            limit=limit,\n            fill_axis=fill_axis,\n            broadcast_axis=broadcast_axis,\n        )\n\n    def rename(\n        self,\n        index=None,\n        *,\n        axis=None,\n        copy=True,\n        inplace=False,\n        level=None,\n        errors=\"ignore\",\n    ):\n        \"\"\"\n        Alter Series index labels or name.\n\n        Function / dict values must be unique (1-to-1). Labels not contained in\n        a dict / Series will be left as-is. Extra labels listed don't throw an\n        error.\n\n        Alternatively, change ``Series.name`` with a scalar value.\n\n        See the :ref:`user guide <basics.rename>` for more.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Unused. Accepted for compatibility with DataFrame method only.\n        index : scalar, hashable sequence, dict-like or function, optional\n            Functions or dict-like are transformations to apply to\n            the index.\n            Scalar or hashable sequence-like will alter the ``Series.name``\n            attribute.\n\n        **kwargs\n            Additional keyword arguments passed to the function. Only the\n            \"inplace\" keyword is used.\n\n        Returns\n        -------\n        Series or None\n            Series with index labels or name altered or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.rename : Corresponding DataFrame method.\n        Series.rename_axis : Set the name of the axis.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n        >>> s.rename(\"my_name\")  # scalar, changes Series.name\n        0    1\n        1    2\n        2    3\n        Name: my_name, dtype: int64\n        >>> s.rename(lambda x: x ** 2)  # function, changes labels\n        0    1\n        1    2\n        4    3\n        dtype: int64\n        >>> s.rename({1: 3, 2: 5})  # mapping, changes labels\n        0    1\n        3    2\n        5    3\n        dtype: int64\n        \"\"\"\n        if callable(index) or is_dict_like(index):\n            return super().rename(\n                index, copy=copy, inplace=inplace, level=level, errors=errors\n            )\n        else:\n            return self._set_name(index, inplace=inplace)\n\n    @overload\n    def set_axis(\n        self, labels, axis: Axis = ..., inplace: Literal[False] = ...\n    ) -> Series:\n        ...\n\n    @overload\n    def set_axis(self, labels, axis: Axis, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def set_axis(self, labels, *, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def set_axis(self, labels, axis: Axis = ..., inplace: bool = ...) -> Series | None:\n        ...\n\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n\n        >>> s.set_axis(['a', 'b', 'c'], axis=0)\n        a    1\n        b    2\n        c    3\n        dtype: int64\n    \"\"\"\n    )\n    @Substitution(\n        **_shared_doc_kwargs,\n        extended_summary_sub=\"\",\n        axis_description_sub=\"\",\n        see_also_sub=\"\",\n    )\n    @Appender(generic.NDFrame.set_axis.__doc__)\n    def set_axis(self, labels, axis: Axis = 0, inplace: bool = False):\n        return super().set_axis(labels, axis=axis, inplace=inplace)\n\n    # error: Cannot determine type of 'reindex'\n    @doc(\n        NDFrame.reindex,  # type: ignore[has-type]\n        klass=_shared_doc_kwargs[\"klass\"],\n        axes=_shared_doc_kwargs[\"axes\"],\n        optional_labels=_shared_doc_kwargs[\"optional_labels\"],\n        optional_axis=_shared_doc_kwargs[\"optional_axis\"],\n    )\n    def reindex(self, index=None, **kwargs):\n        return super().reindex(index=index, **kwargs)\n\n    def drop(\n        self,\n        labels=None,\n        axis=0,\n        index=None,\n        columns=None,\n        level=None,\n        inplace=False,\n        errors=\"raise\",\n    ) -> Series:\n        \"\"\"\n        Return Series with specified index labels removed.\n\n        Remove elements of a Series based on specifying the index labels.\n        When using a multi-index, labels on different levels can be removed\n        by specifying the level.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index labels to drop.\n        axis : 0, default 0\n            Redundant for application on Series.\n        index : single label or list-like\n            Redundant for application on Series, but 'index' can be used instead\n            of 'labels'.\n        columns : single label or list-like\n            No change is made to the Series; use 'index' or 'labels' instead.\n        level : int or level name, optional\n            For MultiIndex, level for which the labels will be removed.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and only existing labels are dropped.\n\n        Returns\n        -------\n        Series or None\n            Series with specified index labels removed or None if ``inplace=True``.\n\n        Raises\n        ------\n        KeyError\n            If none of the labels are found in the index.\n\n        See Also\n        --------\n        Series.reindex : Return only specified index labels of Series.\n        Series.dropna : Return series without null values.\n        Series.drop_duplicates : Return Series with duplicate values removed.\n        DataFrame.drop : Drop specified labels from rows or columns.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=np.arange(3), index=['A', 'B', 'C'])\n        >>> s\n        A  0\n        B  1\n        C  2\n        dtype: int64\n\n        Drop labels B en C\n\n        >>> s.drop(labels=['B', 'C'])\n        A  0\n        dtype: int64\n\n        Drop 2nd level label in MultiIndex Series\n\n        >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n        ...                              ['speed', 'weight', 'length']],\n        ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> s = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...               index=midx)\n        >>> s\n        lama    speed      45.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        dtype: float64\n\n        >>> s.drop(labels='weight', level=1)\n        lama    speed      45.0\n                length      1.2\n        cow     speed      30.0\n                length      1.5\n        falcon  speed     320.0\n                length      0.3\n        dtype: float64\n        \"\"\"\n        return super().drop(\n            labels=labels,\n            axis=axis,\n            index=index,\n            columns=columns,\n            level=level,\n            inplace=inplace,\n            errors=errors,\n        )\n\n    @overload\n    def fillna(\n        self,\n        value=...,\n        method: FillnaOptions | None = ...,\n        axis: Axis | None = ...,\n        inplace: Literal[False] = ...,\n        limit=...,\n        downcast=...,\n    ) -> Series:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        method: FillnaOptions | None,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        method: FillnaOptions | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        method: FillnaOptions | None,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        *,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        method: FillnaOptions | None,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value=...,\n        method: FillnaOptions | None = ...,\n        axis: Axis | None = ...,\n        inplace: bool = ...,\n        limit=...,\n        downcast=...,\n    ) -> Series | None:\n        ...\n\n    # error: Cannot determine type of 'fillna'\n    @doc(NDFrame.fillna, **_shared_doc_kwargs)  # type: ignore[has-type]\n    def fillna(\n        self,\n        value: object | ArrayLike | None = None,\n        method: FillnaOptions | None = None,\n        axis=None,\n        inplace=False,\n        limit=None,\n        downcast=None,\n    ) -> Series | None:\n        return super().fillna(\n            value=value,\n            method=method,\n            axis=axis,\n            inplace=inplace,\n            limit=limit,\n            downcast=downcast,\n        )\n\n    def pop(self, item: Hashable) -> Any:\n        \"\"\"\n        Return item and drops from series. Raise KeyError if not found.\n\n        Parameters\n        ----------\n        item : label\n            Index of the element that needs to be removed.\n\n        Returns\n        -------\n        Value that is popped from series.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1,2,3])\n\n        >>> ser.pop(0)\n        1\n\n        >>> ser\n        1    2\n        2    3\n        dtype: int64\n        \"\"\"\n        return super().pop(item=item)\n\n    # error: Cannot determine type of 'replace'\n    @doc(\n        NDFrame.replace,  # type: ignore[has-type]\n        klass=_shared_doc_kwargs[\"klass\"],\n        inplace=_shared_doc_kwargs[\"inplace\"],\n        replace_iloc=_shared_doc_kwargs[\"replace_iloc\"],\n    )\n    def replace(\n        self,\n        to_replace=None,\n        value=None,\n        inplace=False,\n        limit=None,\n        regex=False,\n        method=\"pad\",\n    ):\n        return super().replace(\n            to_replace=to_replace,\n            value=value,\n            inplace=inplace,\n            limit=limit,\n            regex=regex,\n            method=method,\n        )\n\n    def _replace_single(self, to_replace, method: str, inplace: bool, limit):\n        \"\"\"\n        Replaces values in a Series using the fill method specified when no\n        replacement value is given in the replace method\n        \"\"\"\n\n        orig_dtype = self.dtype\n        result = self if inplace else self.copy()\n        fill_f = missing.get_fill_func(method)\n\n        mask = missing.mask_missing(result.values, to_replace)\n        values, _ = fill_f(result.values, limit=limit, mask=mask)\n\n        if values.dtype == orig_dtype and inplace:\n            return\n\n        result = self._constructor(values, index=self.index, dtype=self.dtype)\n        result = result.__finalize__(self)\n\n        if inplace:\n            self._update_inplace(result)\n            return\n\n        return result\n\n    # error: Cannot determine type of 'shift'\n    @doc(NDFrame.shift, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def shift(self, periods=1, freq=None, axis=0, fill_value=None) -> Series:\n        return super().shift(\n            periods=periods, freq=freq, axis=axis, fill_value=fill_value\n        )\n\n    def memory_usage(self, index: bool = True, deep: bool = False) -> int:\n        \"\"\"\n        Return the memory usage of the Series.\n\n        The memory usage can optionally include the contribution of\n        the index and of elements of `object` dtype.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the Series index.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned value.\n\n        Returns\n        -------\n        int\n            Bytes of memory consumed.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\n            array.\n        DataFrame.memory_usage : Bytes consumed by a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(range(3))\n        >>> s.memory_usage()\n        152\n\n        Not including the index gives the size of the rest of the data, which\n        is necessarily smaller:\n\n        >>> s.memory_usage(index=False)\n        24\n\n        The memory footprint of `object` values is ignored by default:\n\n        >>> s = pd.Series([\"a\", \"b\"])\n        >>> s.values\n        array(['a', 'b'], dtype=object)\n        >>> s.memory_usage()\n        144\n        >>> s.memory_usage(deep=True)\n        244\n        \"\"\"\n        v = self._memory_usage(deep=deep)\n        if index:\n            v += self.index.memory_usage(deep=deep)\n        return v\n\n    def isin(self, values) -> Series:\n        \"\"\"\n        Whether elements in Series are contained in `values`.\n\n        Return a boolean Series showing whether each element in the Series\n        matches an element in the passed sequence of `values` exactly.\n\n        Parameters\n        ----------\n        values : set or list-like\n            The sequence of values to test. Passing in a single string will\n            raise a ``TypeError``. Instead, turn a single string into a\n            list of one element.\n\n        Returns\n        -------\n        Series\n            Series of booleans indicating if each element is in values.\n\n        Raises\n        ------\n        TypeError\n          * If `values` is a string\n\n        See Also\n        --------\n        DataFrame.isin : Equivalent method on DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama',\n        ...                'hippo'], name='animal')\n        >>> s.isin(['cow', 'lama'])\n        0     True\n        1     True\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n\n        Passing a single string as ``s.isin('lama')`` will raise an error. Use\n        a list of one element instead:\n\n        >>> s.isin(['lama'])\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n\n        Strings and integers are distinct and are therefore not comparable:\n\n        >>> pd.Series([1]).isin(['1'])\n        0    False\n        dtype: bool\n        >>> pd.Series([1.1]).isin(['1.1'])\n        0    False\n        dtype: bool\n        \"\"\"\n        result = algorithms.isin(self._values, values)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"isin\"\n        )\n\n    def between(self, left, right, inclusive=True) -> Series:\n        \"\"\"\n        Return boolean Series equivalent to left <= series <= right.\n\n        This function returns a boolean vector containing `True` wherever the\n        corresponding Series element is between the boundary values `left` and\n        `right`. NA values are treated as `False`.\n\n        Parameters\n        ----------\n        left : scalar or list-like\n            Left boundary.\n        right : scalar or list-like\n            Right boundary.\n        inclusive : bool, default True\n            Include boundaries.\n\n        Returns\n        -------\n        Series\n            Series representing whether each element is between left and\n            right (inclusive).\n\n        See Also\n        --------\n        Series.gt : Greater than of series and other.\n        Series.lt : Less than of series and other.\n\n        Notes\n        -----\n        This function is equivalent to ``(left <= ser) & (ser <= right)``\n\n        Examples\n        --------\n        >>> s = pd.Series([2, 0, 4, 8, np.nan])\n\n        Boundary values are included by default:\n\n        >>> s.between(1, 4)\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        With `inclusive` set to ``False`` boundary values are excluded:\n\n        >>> s.between(1, 4, inclusive=False)\n        0     True\n        1    False\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        `left` and `right` can be any scalar value:\n\n        >>> s = pd.Series(['Alice', 'Bob', 'Carol', 'Eve'])\n        >>> s.between('Anna', 'Daniel')\n        0    False\n        1     True\n        2     True\n        3    False\n        dtype: bool\n        \"\"\"\n        if inclusive:\n            lmask = self >= left\n            rmask = self <= right\n        else:\n            lmask = self > left\n            rmask = self < right\n\n        return lmask & rmask\n\n    # ----------------------------------------------------------------------\n    # Convert to types that support pd.NA\n\n    def _convert_dtypes(\n        self,\n        infer_objects: bool = True,\n        convert_string: bool = True,\n        convert_integer: bool = True,\n        convert_boolean: bool = True,\n        convert_floating: bool = True,\n    ) -> Series:\n        input_series = self\n        if infer_objects:\n            input_series = input_series.infer_objects()\n            if is_object_dtype(input_series):\n                input_series = input_series.copy()\n\n        if convert_string or convert_integer or convert_boolean or convert_floating:\n            inferred_dtype = convert_dtypes(\n                input_series._values,\n                convert_string,\n                convert_integer,\n                convert_boolean,\n                convert_floating,\n            )\n            try:\n                result = input_series.astype(inferred_dtype)\n            except TypeError:\n                result = input_series.copy()\n        else:\n            result = input_series.copy()\n        return result\n\n    # error: Cannot determine type of 'isna'\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def isna(self) -> Series:\n        return generic.NDFrame.isna(self)\n\n    # error: Cannot determine type of 'isna'\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def isnull(self) -> Series:\n        return super().isnull()\n\n    # error: Cannot determine type of 'notna'\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def notna(self) -> Series:\n        return super().notna()\n\n    # error: Cannot determine type of 'notna'\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def notnull(self) -> Series:\n        return super().notnull()\n\n    def dropna(self, axis=0, inplace=False, how=None):\n        \"\"\"\n        Return a new Series with missing values removed.\n\n        See the :ref:`User Guide <missing_data>` for more on which values are\n        considered missing, and how to work with missing data.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            There is only one axis to drop values from.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        how : str, optional\n            Not in use. Kept for compatibility.\n\n        Returns\n        -------\n        Series or None\n            Series with NA entries dropped from it or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.isna: Indicate missing values.\n        Series.notna : Indicate existing (non-missing) values.\n        Series.fillna : Replace missing values.\n        DataFrame.dropna : Drop rows or columns which contain NA values.\n        Index.dropna : Drop missing indices.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1., 2., np.nan])\n        >>> ser\n        0    1.0\n        1    2.0\n        2    NaN\n        dtype: float64\n\n        Drop NA values from a Series.\n\n        >>> ser.dropna()\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Keep the Series with valid entries in the same variable.\n\n        >>> ser.dropna(inplace=True)\n        >>> ser\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Empty strings are not considered NA values. ``None`` is considered an\n        NA value.\n\n        >>> ser = pd.Series([np.NaN, 2, pd.NaT, '', None, 'I stay'])\n        >>> ser\n        0       NaN\n        1         2\n        2       NaT\n        3\n        4      None\n        5    I stay\n        dtype: object\n        >>> ser.dropna()\n        1         2\n        3\n        5    I stay\n        dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis or 0)\n\n        if self._can_hold_na:\n            result = remove_na_arraylike(self)\n            if inplace:\n                self._update_inplace(result)\n            else:\n                return result\n        else:\n            if inplace:\n                # do nothing\n                pass\n            else:\n                return self.copy()\n\n    # ----------------------------------------------------------------------\n    # Time series-oriented methods\n\n    # error: Cannot determine type of 'asfreq'\n    @doc(NDFrame.asfreq, **_shared_doc_kwargs)  # type: ignore[has-type]\n    def asfreq(\n        self,\n        freq,\n        method=None,\n        how: str | None = None,\n        normalize: bool = False,\n        fill_value=None,\n    ) -> Series:\n        return super().asfreq(\n            freq=freq,\n            method=method,\n            how=how,\n            normalize=normalize,\n            fill_value=fill_value,\n        )\n\n    # error: Cannot determine type of 'resample'\n    @doc(NDFrame.resample, **_shared_doc_kwargs)  # type: ignore[has-type]\n    def resample(\n        self,\n        rule,\n        axis=0,\n        closed: str | None = None,\n        label: str | None = None,\n        convention: str = \"start\",\n        kind: str | None = None,\n        loffset=None,\n        base: int | None = None,\n        on=None,\n        level=None,\n        origin: str | TimestampConvertibleTypes = \"start_day\",\n        offset: TimedeltaConvertibleTypes | None = None,\n    ) -> Resampler:\n        return super().resample(\n            rule=rule,\n            axis=axis,\n            closed=closed,\n            label=label,\n            convention=convention,\n            kind=kind,\n            loffset=loffset,\n            base=base,\n            on=on,\n            level=level,\n            origin=origin,\n            offset=offset,\n        )\n\n    def to_timestamp(self, freq=None, how=\"start\", copy=True) -> Series:\n        \"\"\"\n        Cast to DatetimeIndex of Timestamps, at *beginning* of period.\n\n        Parameters\n        ----------\n        freq : str, default frequency of PeriodIndex\n            Desired frequency.\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series with DatetimeIndex\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        if not isinstance(self.index, PeriodIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\n        new_index = self.index.to_timestamp(freq=freq, how=how)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_timestamp\"\n        )\n\n    def to_period(self, freq=None, copy=True) -> Series:\n        \"\"\"\n        Convert Series from DatetimeIndex to PeriodIndex.\n\n        Parameters\n        ----------\n        freq : str, default None\n            Frequency associated with the PeriodIndex.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series\n            Series with index converted to PeriodIndex.\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\n        new_index = self.index.to_period(freq=freq)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_period\"\n        )\n\n    # ----------------------------------------------------------------------\n    # Add index\n    _AXIS_ORDERS = [\"index\"]\n    _AXIS_REVERSED = False\n    _AXIS_LEN = len(_AXIS_ORDERS)\n    _info_axis_number = 0\n    _info_axis_name = \"index\"\n\n    index: Index = properties.AxisProperty(\n        axis=0, doc=\"The index (axis labels) of the Series.\"\n    )\n\n    # ----------------------------------------------------------------------\n    # Accessor Methods\n    # ----------------------------------------------------------------------\n    str = CachedAccessor(\"str\", StringMethods)\n    dt = CachedAccessor(\"dt\", CombinedDatetimelikeProperties)\n    cat = CachedAccessor(\"cat\", CategoricalAccessor)\n    plot = CachedAccessor(\"plot\", pandas.plotting.PlotAccessor)\n    sparse = CachedAccessor(\"sparse\", SparseAccessor)\n\n    # ----------------------------------------------------------------------\n    # Add plotting methods to Series\n    hist = pandas.plotting.hist_series\n\n    # ----------------------------------------------------------------------\n    # Template-Based Arithmetic/Comparison Methods\n\n    def _cmp_method(self, other, op):\n        res_name = ops.get_op_result_name(self, other)\n\n        if isinstance(other, Series) and not self._indexed_same(other):\n            raise ValueError(\"Can only compare identically-labeled Series objects\")\n\n        lvalues = self._values\n        rvalues = extract_array(other, extract_numpy=True, extract_range=True)\n\n        with np.errstate(all=\"ignore\"):\n            res_values = ops.comparison_op(lvalues, rvalues, op)\n\n        return self._construct_result(res_values, name=res_name)\n\n    def _logical_method(self, other, op):\n        res_name = ops.get_op_result_name(self, other)\n        self, other = ops.align_method_SERIES(self, other, align_asobject=True)\n\n        lvalues = self._values\n        rvalues = extract_array(other, extract_numpy=True, extract_range=True)\n\n        res_values = ops.logical_op(lvalues, rvalues, op)\n        return self._construct_result(res_values, name=res_name)\n\n    def _arith_method(self, other, op):\n        res_name = ops.get_op_result_name(self, other)\n        self, other = ops.align_method_SERIES(self, other)\n\n        lvalues = self._values\n        rvalues = extract_array(other, extract_numpy=True, extract_range=True)\n        rvalues = ops.maybe_prepare_scalar_for_op(rvalues, lvalues.shape)\n        rvalues = ensure_wrapped_if_datetimelike(rvalues)\n\n        with np.errstate(all=\"ignore\"):\n            result = ops.arithmetic_op(lvalues, rvalues, op)\n\n        return self._construct_result(result, name=res_name)\n\n\nSeries._add_numeric_operations()\n\n# Add arithmetic!\nops.add_flex_arithmetic_methods(Series)\n",
                "max_stars_repo_path": "pandas/core/series.py",
                "max_stars_repo_name": "sisiyan/pandas",
                "max_stars_count": 1,
                "__cluster__": 79
            }
        ]
    ],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [
        [
            {
                "id": "pretrain_python_data_700",
                "content": "\"\"\"\nThis module implements the Request class which is used to represent HTTP\nrequests in Scrapy.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\nfrom w3lib.url import safe_url_string\n\nfrom scrapy.http.headers import Headers\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.trackref import object_ref\nfrom scrapy.utils.url import escape_ajax\nfrom scrapy.http.common import obsolete_setter\nfrom scrapy.utils.curl import curl_to_request_kwargs\n\n\nclass Request(object_ref):\n\n    def __init__(self, url, callback=None, method='GET', headers=None, body=None,\n                 cookies=None, meta=None, encoding='utf-8', priority=0,\n                 dont_filter=False, errback=None, flags=None, cb_kwargs=None):\n\n        self._encoding = encoding  # this one has to be set first\n        self.method = str(method).upper()\n        self._set_url(url)\n        self._set_body(body)\n        assert isinstance(priority, int), \"Request priority not an integer: %r\" % priority\n        self.priority = priority\n\n        if callback is not None and not callable(callback):\n            raise TypeError('callback must be a callable, got %s' % type(callback).__name__)\n        if errback is not None and not callable(errback):\n            raise TypeError('errback must be a callable, got %s' % type(errback).__name__)\n        self.callback = callback\n        self.errback = errback\n\n        self.cookies = cookies or {}\n        self.headers = Headers(headers or {}, encoding=encoding)\n        self.dont_filter = dont_filter\n\n        self._meta = dict(meta) if meta else None\n        self._cb_kwargs = dict(cb_kwargs) if cb_kwargs else None\n        self.flags = [] if flags is None else list(flags)\n\n    @property\n    def cb_kwargs(self):\n        if self._cb_kwargs is None:\n            self._cb_kwargs = {}\n        return self._cb_kwargs\n\n    @property\n    def meta(self):\n        if self._meta is None:\n            self._meta = {}\n        return self._meta\n\n    def _get_url(self):\n        return self._url\n\n    def _set_url(self, url):\n        if not isinstance(url, str):\n            raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n\n        s = safe_url_string(url, self.encoding)\n        self._url = escape_ajax(s)\n\n        if ('://' not in self._url) and (not self._url.startswith('data:')):\n            raise ValueError('Missing scheme in request url: %s' % self._url)\n\n    url = property(_get_url, obsolete_setter(_set_url, 'url'))\n\n    def _get_body(self):\n        return self._body\n\n    def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        else:\n            self._body = to_bytes(body, self.encoding)\n\n    body = property(_get_body, obsolete_setter(_set_body, 'body'))\n\n    @property\n    def encoding(self):\n        return self._encoding\n\n    def __str__(self):\n        return \"<%s %s>\" % (self.method, self.url)\n\n    __repr__ = __str__\n\n    def copy(self):\n        \"\"\"Return a copy of this Request\"\"\"\n        return self.replace()\n\n    def replace(self, *args, **kwargs):\n        \"\"\"Create a new Request with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', 'flags',\n                  'encoding', 'priority', 'dont_filter', 'callback', 'errback', 'cb_kwargs']:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)\n\n    @classmethod\n    def from_curl(cls, curl_command, ignore_unknown_options=True, **kwargs):\n        \"\"\"Create a Request object from a string containing a `cURL\n        <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n        URL, the headers, the cookies and the body. It accepts the same\n        arguments as the :class:`Request` class, taking preference and\n        overriding the values of the same arguments contained in the cURL\n        command.\n\n        Unrecognized options are ignored by default. To raise an error when\n        finding unknown options call this method by passing\n        ``ignore_unknown_options=False``.\n\n        .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`\n                     subclasses, such as :class:`~scrapy.http.JSONRequest`, or\n                     :class:`~scrapy.http.XmlRpcRequest`, as well as having\n                     :ref:`downloader middlewares <topics-downloader-middleware>`\n                     and\n                     :ref:`spider middlewares <topics-spider-middleware>`\n                     enabled, such as\n                     :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,\n                     :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,\n                     or\n                     :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,\n                     may modify the :class:`~scrapy.http.Request` object.\n\n        To translate a cURL command into a Scrapy request,\n        you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.\n\n       \"\"\"\n        request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n        request_kwargs.update(kwargs)\n        return cls(**request_kwargs)\n",
                "max_stars_repo_path": "scrapy/http/request/__init__.py",
                "max_stars_repo_name": "joybhallaa/scrapy",
                "max_stars_count": 1,
                "__cluster__": 260
            },
            {
                "id": "test_scrapy-bug-37",
                "content": "\"\"\"\nThis module implements the Request class which is used to represent HTTP\nrequests in Scrapy.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\nimport six\nfrom w3lib.url import safe_url_string\n\nfrom scrapy.http.headers import Headers\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.trackref import object_ref\nfrom scrapy.utils.url import escape_ajax\nfrom scrapy.http.common import obsolete_setter\nfrom scrapy.utils.curl import curl_to_request_kwargs\n\n\nclass Request(object_ref):\n\n    def __init__(self, url, callback=None, method='GET', headers=None, body=None,\n                 cookies=None, meta=None, encoding='utf-8', priority=0,\n                 dont_filter=False, errback=None, flags=None, cb_kwargs=None):\n\n        self._encoding = encoding  # this one has to be set first\n        self.method = str(method).upper()\n        self._set_url(url)\n        self._set_body(body)\n        assert isinstance(priority, int), \"Request priority not an integer: %r\" % priority\n        self.priority = priority\n\n        if callback is not None and not callable(callback):\n            raise TypeError('callback must be a callable, got %s' % type(callback).__name__)\n        if errback is not None and not callable(errback):\n            raise TypeError('errback must be a callable, got %s' % type(errback).__name__)\n        assert callback or not errback, \"Cannot use errback without a callback\"\n        self.callback = callback\n        self.errback = errback\n\n        self.cookies = cookies or {}\n        self.headers = Headers(headers or {}, encoding=encoding)\n        self.dont_filter = dont_filter\n\n        self._meta = dict(meta) if meta else None\n        self._cb_kwargs = dict(cb_kwargs) if cb_kwargs else None\n        self.flags = [] if flags is None else list(flags)\n\n    @property\n    def cb_kwargs(self):\n        if self._cb_kwargs is None:\n            self._cb_kwargs = {}\n        return self._cb_kwargs\n\n    @property\n    def meta(self):\n        if self._meta is None:\n            self._meta = {}\n        return self._meta\n\n    def _get_url(self):\n        return self._url\n\n    def _set_url(self, url):\n        if not isinstance(url, six.string_types):\n            raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n\n        s = safe_url_string(url, self.encoding)\n        self._url = escape_ajax(s)\n\n        if ':' not in self._url:\n            raise ValueError('Missing scheme in request url: %s' % self._url)\n\n    url = property(_get_url, obsolete_setter(_set_url, 'url'))\n\n    def _get_body(self):\n        return self._body\n\n    def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        else:\n            self._body = to_bytes(body, self.encoding)\n\n    body = property(_get_body, obsolete_setter(_set_body, 'body'))\n\n    @property\n    def encoding(self):\n        return self._encoding\n\n    def __str__(self):\n        return \"<%s %s>\" % (self.method, self.url)\n\n    __repr__ = __str__\n\n    def copy(self):\n        \"\"\"Return a copy of this Request\"\"\"\n        return self.replace()\n\n    def replace(self, *args, **kwargs):\n        \"\"\"Create a new Request with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', 'flags',\n                  'encoding', 'priority', 'dont_filter', 'callback', 'errback', 'cb_kwargs']:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)\n\n    @classmethod\n    def from_curl(cls, curl_command, ignore_unknown_options=True, **kwargs):\n        \"\"\"Create a Request object from a string containing a `cURL\n        <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n        URL, the headers, the cookies and the body. It accepts the same\n        arguments as the :class:`Request` class, taking preference and\n        overriding the values of the same arguments contained in the cURL\n        command.\n\n        Unrecognized options are ignored by default. To raise an error when\n        finding unknown options call this method by passing\n        ``ignore_unknown_options=False``.\n\n        .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`\n                     subclasses, such as :class:`~scrapy.http.JSONRequest`, or\n                     :class:`~scrapy.http.XmlRpcRequest`, as well as having\n                     :ref:`downloader middlewares <topics-downloader-middleware>`\n                     and\n                     :ref:`spider middlewares <topics-spider-middleware>`\n                     enabled, such as\n                     :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,\n                     :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,\n                     or\n                     :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,\n                     may modify the :class:`~scrapy.http.Request` object.\n\n       \"\"\"\n        request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n        request_kwargs.update(kwargs)\n        return cls(**request_kwargs)\n\n\n\"\"\"\nThis module implements the Request class which is used to represent HTTP\nrequests in Scrapy.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\nimport six\nfrom w3lib.url import safe_url_string\n\nfrom scrapy.http.headers import Headers\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.trackref import object_ref\nfrom scrapy.utils.url import escape_ajax\nfrom scrapy.http.common import obsolete_setter\nfrom scrapy.utils.curl import curl_to_request_kwargs\n\n\nclass Request(object_ref):\n\n    def __init__(self, url, callback=None, method='GET', headers=None, body=None,\n                 cookies=None, meta=None, encoding='utf-8', priority=0,\n                 dont_filter=False, errback=None, flags=None, cb_kwargs=None):\n\n        self._encoding = encoding  # this one has to be set first\n        self.method = str(method).upper()\n        self._set_url(url)\n        self._set_body(body)\n        assert isinstance(priority, int), \"Request priority not an integer: %r\" % priority\n        self.priority = priority\n\n        if callback is not None and not callable(callback):\n            raise TypeError('callback must be a callable, got %s' % type(callback).__name__)\n        if errback is not None and not callable(errback):\n            raise TypeError('errback must be a callable, got %s' % type(errback).__name__)\n        assert callback or not errback, \"Cannot use errback without a callback\"\n        self.callback = callback\n        self.errback = errback\n\n        self.cookies = cookies or {}\n        self.headers = Headers(headers or {}, encoding=encoding)\n        self.dont_filter = dont_filter\n\n        self._meta = dict(meta) if meta else None\n        self._cb_kwargs = dict(cb_kwargs) if cb_kwargs else None\n        self.flags = [] if flags is None else list(flags)\n\n    @property\n    def cb_kwargs(self):\n        if self._cb_kwargs is None:\n            self._cb_kwargs = {}\n        return self._cb_kwargs\n\n    @property\n    def meta(self):\n        if self._meta is None:\n            self._meta = {}\n        return self._meta\n\n    def _get_url(self):\n        return self._url\n\n    def _set_url(self, url):\n        if not isinstance(url, six.string_types):\n            raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n\n        s = safe_url_string(url, self.encoding)\n        self._url = escape_ajax(s)\n\n        if ('://' not in self._url) and (not self._url.startswith('data:')):\n            raise ValueError('Missing scheme in request url: %s' % self._url)\n\n    url = property(_get_url, obsolete_setter(_set_url, 'url'))\n\n    def _get_body(self):\n        return self._body\n\n    def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        else:\n            self._body = to_bytes(body, self.encoding)\n\n    body = property(_get_body, obsolete_setter(_set_body, 'body'))\n\n    @property\n    def encoding(self):\n        return self._encoding\n\n    def __str__(self):\n        return \"<%s %s>\" % (self.method, self.url)\n\n    __repr__ = __str__\n\n    def copy(self):\n        \"\"\"Return a copy of this Request\"\"\"\n        return self.replace()\n\n    def replace(self, *args, **kwargs):\n        \"\"\"Create a new Request with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', 'flags',\n                  'encoding', 'priority', 'dont_filter', 'callback', 'errback', 'cb_kwargs']:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)\n\n    @classmethod\n    def from_curl(cls, curl_command, ignore_unknown_options=True, **kwargs):\n        \"\"\"Create a Request object from a string containing a `cURL\n        <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n        URL, the headers, the cookies and the body. It accepts the same\n        arguments as the :class:`Request` class, taking preference and\n        overriding the values of the same arguments contained in the cURL\n        command.\n\n        Unrecognized options are ignored by default. To raise an error when\n        finding unknown options call this method by passing\n        ``ignore_unknown_options=False``.\n\n        .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`\n                     subclasses, such as :class:`~scrapy.http.JSONRequest`, or\n                     :class:`~scrapy.http.XmlRpcRequest`, as well as having\n                     :ref:`downloader middlewares <topics-downloader-middleware>`\n                     and\n                     :ref:`spider middlewares <topics-spider-middleware>`\n                     enabled, such as\n                     :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,\n                     :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,\n                     or\n                     :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,\n                     may modify the :class:`~scrapy.http.Request` object.\n\n       \"\"\"\n        request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n        request_kwargs.update(kwargs)\n        return cls(**request_kwargs)\n",
                "max_stars_repo_path": "BugsInPy/BugsInPy/temp/projects/scrapy/bug-37-fixed/scrapy/scrapy/http/request/__init__.py,BugsInPy/BugsInPy/temp/projects/scrapy/bug-37-buggy/scrapy/scrapy/http/request/__init__.py",
                "max_stars_repo_name": "NA",
                "max_stars_count": 0,
                "__cluster__": 260
            }
        ]
    ],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [
        [
            {
                "id": "pretrain_python_data_132295",
                "content": "\"\"\"\nOffsite Spider Middleware\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nimport re\n\nfrom scrapy import signals\nfrom scrapy.http import Request\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy import log\n\nclass OffsiteMiddleware(object):\n\n    def __init__(self, stats):\n        self.stats = stats\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o\n\n    def process_spider_output(self, response, result, spider):\n        for x in result:\n            if isinstance(x, Request):\n                if x.dont_filter or self.should_follow(x, spider):\n                    yield x\n                else:\n                    domain = urlparse_cached(x).hostname\n                    if domain and domain not in self.domains_seen:\n                        self.domains_seen.add(domain)\n                        log.msg(format=\"Filtered offsite request to %(domain)r: %(request)s\",\n                                level=log.DEBUG, spider=spider, domain=domain, request=x)\n                        self.stats.inc_value('offsite/domains', spider=spider)\n                    self.stats.inc_value('offsite/filtered', spider=spider)\n            else:\n                yield x\n\n    def should_follow(self, request, spider):\n        regex = self.host_regex\n        # hostname can be None for wrong urls (like javascript links)\n        host = urlparse_cached(request).hostname or ''\n        return bool(regex.search(host))\n\n    def get_host_regex(self, spider):\n        \"\"\"Override this method to implement a different offsite policy\"\"\"\n        allowed_domains = getattr(spider, 'allowed_domains', None)\n        if not allowed_domains:\n            return re.compile('') # allow all by default\n        regex = r'^(.*\\.)?(%s)$' % '|'.join(re.escape(d) for d in allowed_domains if d is not None)\n        return re.compile(regex)\n\n    def spider_opened(self, spider):\n        self.host_regex = self.get_host_regex(spider)\n        self.domains_seen = set()\n",
                "max_stars_repo_path": "scrapy/contrib/spidermiddleware/offsite.py",
                "max_stars_repo_name": "h4ck3rm1k3/scrapy",
                "max_stars_count": 26,
                "__cluster__": 263
            },
            {
                "id": "test_scrapy-bug-1",
                "content": "\"\"\"\nOffsite Spider Middleware\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\nimport re\nimport logging\nimport warnings\n\nfrom scrapy import signals\nfrom scrapy.http import Request\nfrom scrapy.utils.httpobj import urlparse_cached\n\nlogger = logging.getLogger(__name__)\n\n\nclass OffsiteMiddleware(object):\n\n    def __init__(self, stats):\n        self.stats = stats\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o\n\n    def process_spider_output(self, response, result, spider):\n        for x in result:\n            if isinstance(x, Request):\n                if x.dont_filter or self.should_follow(x, spider):\n                    yield x\n                else:\n                    domain = urlparse_cached(x).hostname\n                    if domain and domain not in self.domains_seen:\n                        self.domains_seen.add(domain)\n                        logger.debug(\n                            \"Filtered offsite request to %(domain)r: %(request)s\",\n                            {'domain': domain, 'request': x}, extra={'spider': spider})\n                        self.stats.inc_value('offsite/domains', spider=spider)\n                    self.stats.inc_value('offsite/filtered', spider=spider)\n            else:\n                yield x\n\n    def should_follow(self, request, spider):\n        regex = self.host_regex\n        # hostname can be None for wrong urls (like javascript links)\n        host = urlparse_cached(request).hostname or ''\n        return bool(regex.search(host))\n\n    def get_host_regex(self, spider):\n        \"\"\"Override this method to implement a different offsite policy\"\"\"\n        allowed_domains = getattr(spider, 'allowed_domains', None)\n        if not allowed_domains:\n            return re.compile('')  # allow all by default\n        url_pattern = re.compile(\"^https?://.*$\")\n        for domain in allowed_domains:\n            if url_pattern.match(domain):\n                message = (\"allowed_domains accepts only domains, not URLs. \"\n                           \"Ignoring URL entry %s in allowed_domains.\" % domain)\n                warnings.warn(message, URLWarning)\n        domains = [re.escape(d) for d in allowed_domains if d is not None]\n        regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n        return re.compile(regex)\n\n    def spider_opened(self, spider):\n        self.host_regex = self.get_host_regex(spider)\n        self.domains_seen = set()\n\n\nclass URLWarning(Warning):\n    pass\n\n\n\"\"\"\nOffsite Spider Middleware\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\nimport re\nimport logging\nimport warnings\n\nfrom scrapy import signals\nfrom scrapy.http import Request\nfrom scrapy.utils.httpobj import urlparse_cached\n\nlogger = logging.getLogger(__name__)\n\n\nclass OffsiteMiddleware(object):\n\n    def __init__(self, stats):\n        self.stats = stats\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o\n\n    def process_spider_output(self, response, result, spider):\n        for x in result:\n            if isinstance(x, Request):\n                if x.dont_filter or self.should_follow(x, spider):\n                    yield x\n                else:\n                    domain = urlparse_cached(x).hostname\n                    if domain and domain not in self.domains_seen:\n                        self.domains_seen.add(domain)\n                        logger.debug(\n                            \"Filtered offsite request to %(domain)r: %(request)s\",\n                            {'domain': domain, 'request': x}, extra={'spider': spider})\n                        self.stats.inc_value('offsite/domains', spider=spider)\n                    self.stats.inc_value('offsite/filtered', spider=spider)\n            else:\n                yield x\n\n    def should_follow(self, request, spider):\n        regex = self.host_regex\n        # hostname can be None for wrong urls (like javascript links)\n        host = urlparse_cached(request).hostname or ''\n        return bool(regex.search(host))\n\n    def get_host_regex(self, spider):\n        \"\"\"Override this method to implement a different offsite policy\"\"\"\n        allowed_domains = getattr(spider, 'allowed_domains', None)\n        if not allowed_domains:\n            return re.compile('')  # allow all by default\n        url_pattern = re.compile(\"^https?://.*$\")\n        domains = []\n        for domain in allowed_domains:\n            if domain is None:\n                continue\n            elif url_pattern.match(domain):\n                message = (\"allowed_domains accepts only domains, not URLs. \"\n                           \"Ignoring URL entry %s in allowed_domains.\" % domain)\n                warnings.warn(message, URLWarning)\n            else:\n                domains.append(re.escape(domain))\n        regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n        return re.compile(regex)\n\n    def spider_opened(self, spider):\n        self.host_regex = self.get_host_regex(spider)\n        self.domains_seen = set()\n\n\nclass URLWarning(Warning):\n    pass\n",
                "max_stars_repo_path": "BugsInPy/BugsInPy/temp/projects/scrapy/bug-1-fixed/scrapy/scrapy/spidermiddlewares/offsite.py,BugsInPy/BugsInPy/temp/projects/scrapy/bug-1-buggy/scrapy/scrapy/spidermiddlewares/offsite.py",
                "max_stars_repo_name": "NA",
                "max_stars_count": 0,
                "__cluster__": 263
            }
        ]
    ],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    []
]